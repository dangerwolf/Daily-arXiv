{"id": "2409.08897", "title": "Ensuring Adherence to Standards in Experiment-Related Metadata Entered Via Spreadsheets", "url": "https://arxiv.org/abs/2409.08897", "pdf": "https://arxiv.org/pdf/2409.08897", "abs": "https://arxiv.org/abs/2409.08897", "authors": ["Martin J. O'Connor", "Josef Hardi", "Marcos Mart\u00ednez-Romero", "Sowmya Somasundaram", "Brendan Honick", "Stephen A. Fisher", "Ajay Pillai", "Mark A. Musen"], "categories": ["cs.DL"], "comment": null, "summary": "Scientists increasingly recognize the importance of providing rich, standards-adherent metadata to describe their experimental results. Despite the availability of sophisticated tools to assist in the process of data annotation, investigators generally seem to prefer to use spreadsheets when supplying metadata, despite the limitations of spreadsheets in ensuring metadata consistency and compliance with formal specifications. In this paper, we describe an end-to-end approach that supports spreadsheet-based entry of metadata, while ensuring rigorous adherence to community-based metadata standards and providing quality control. Our methods employ several key components, including customizable templates that represent metadata standards and that can inform the spreadsheets that investigators use to author metadata, controlled terminologies and ontologies for defining metadata values that can be accessed directly from a spreadsheet, and an interactive Web-based tool that allows users to rapidly identify and fix errors in their spreadsheet-based metadata. We demonstrate how this approach is being deployed in a biomedical consortium known as HuBMAP to define and collect metadata about a wide range of biological assays.", "AI": {"tldr": "This paper introduces a system for spreadsheet-based metadata entry that ensures adherence to community standards and provides quality control.", "motivation": "Researchers prefer spreadsheets for metadata entry despite their limitations in consistency and standard compliance.", "method": "The system uses customizable templates, controlled terminologies/ontologies, and a web-based tool for error detection and correction.", "result": "The approach is being used in the HuBMAP consortium for metadata collection across various biological assays.", "conclusion": "The system facilitates spreadsheet-based metadata entry while maintaining rigor and quality control through templates, terminologies, and a web-based tool."}}
{"id": "2507.10456", "title": "Radif Corpus: A Symbolic Dataset for Non-Metric Iranian Classical Music", "url": "https://arxiv.org/abs/2507.10456", "pdf": "https://arxiv.org/pdf/2507.10456", "abs": "https://arxiv.org/abs/2507.10456", "authors": ["Maziar Kanani", "Sean O Leary", "James McDermott"], "categories": ["cs.SD", "eess.AS"], "comment": null, "summary": "Non-metric music forms the core of the repertoire in Iranian classical music. Dastgahi music serves as the underlying theoretical system for both Iranian art music and certain folk traditions. At the heart of Iranian classical music lies the radif, a foundational repertoire that organizes melodic material central to performance and pedagogy.\n  In this study, we introduce a digital corpus representing the complete non-metrical radif repertoire, covering all 13 existing components of this repertoire. We provide MIDI files (about 281 minutes in total) and data spreadsheets describing notes, note durations, intervals, and hierarchical structures for 228 pieces of music. We faithfully represent the tonality including quarter-tones, and the non-metric aspect. Furthermore, we provide supporting basic statistics, and measures of complexity and similarity over the corpus.\n  Our corpus provides a platform for computational studies of Iranian classical music. Researchers might employ it in studying melodic patterns, investigating improvisational styles, or for other tasks in music information retrieval, music theory, and computational (ethno)musicology.", "AI": {"tldr": "A digital corpus of non-metric Iranian classical music (radif) is introduced, containing MIDI files, data spreadsheets, and supporting statistics.", "motivation": "Lack of digital resources for studying Iranian classical music.", "method": "Creating a digital corpus of the complete non-metrical radif repertoire, including MIDI files, data spreadsheets, and basic statistics.", "result": "A corpus of 228 pieces (281 minutes) with notes, durations, intervals, and hierarchical structures is created. Tonality including quarter-tones and the non-metric aspect are faithfully represented.", "conclusion": "The corpus enables computational studies of Iranian classical music, such as melodic pattern analysis, improvisation style investigation, and music information retrieval tasks."}}
{"id": "2507.16073", "title": "Buckaroo: A Direct Manipulation Visual Data Wrangler", "url": "https://arxiv.org/abs/2507.16073", "pdf": "https://arxiv.org/pdf/2507.16073", "abs": "https://arxiv.org/abs/2507.16073", "authors": ["Annabelle Warner", "Andrew McNutt", "Paul Rosen", "El Kindi Rezig"], "categories": ["cs.HC", "cs.DB"], "comment": "Accepted to VLDB25 Demo track", "summary": "Preparing datasets -- a critical phase known as data wrangling -- constitutes the dominant phase of data science development, consuming upwards of 80% of the total project time. This phase encompasses a myriad of tasks: parsing data, restructuring it for analysis, repairing inaccuracies, merging sources, eliminating duplicates, and ensuring overall data integrity. Traditional approaches, typically through manual coding in languages such as Python or using spreadsheets, are not only laborious but also error-prone. These issues range from missing entries and formatting inconsistencies to data type inaccuracies, all of which can affect the quality of downstream tasks if not properly corrected. To address these challenges, we present Buckaroo, a visualization system to highlight discrepancies in data and enable on-the-spot corrections through direct manipulations of visual objects. Buckaroo (1) automatically finds \"interesting\" data groups that exhibit anomalies compared to the rest of the groups and recommends them for inspection; (2) suggests wrangling actions that the user can choose to repair the anomalies; and (3) allows users to visually manipulate their data by displaying the effects of their wrangling actions and offering the ability to undo or redo these actions, which supports the iterative nature of data wrangling. A video companion is available at https://youtu.be/iXdCYbvpQVE", "AI": {"tldr": "This paper introduces Buckaroo, a visualization system designed to streamline data wrangling by highlighting discrepancies, suggesting wrangling actions, and allowing visual data manipulation.", "motivation": "Traditional data wrangling methods are time-consuming, error-prone, and can negatively impact data quality.", "method": "Buckaroo automatically identifies anomalous data groups, suggests wrangling actions, and enables users to visually manipulate data with undo/redo functionality.", "result": "The paper presents Buckaroo, a system that helps users to visually manipulate their data by displaying the effects of their wrangling actions and offering the ability to undo or redo these actions, which supports the iterative nature of data wrangling.", "conclusion": "Buckaroo addresses the challenges of data wrangling by providing a visualization system that simplifies discrepancy detection and correction through direct visual manipulation."}}
{"id": "2407.10657", "title": "An Empirical Study of Validating Synthetic Data for Formula Generation", "url": "https://arxiv.org/abs/2407.10657", "pdf": "https://arxiv.org/pdf/2407.10657", "abs": "https://arxiv.org/abs/2407.10657", "authors": ["Usneek Singh", "Jos\u00e9 Cambronero", "Sumit Gulwani", "Aditya Kanade", "Anirudh Khatry", "Vu Le", "Mukul Singh", "Gust Verbruggen"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Findings of NAACL", "summary": "Large language models (LLMs) can be leveraged to help with writing formulas in spreadsheets, but resources on these formulas are scarce, impacting both the base performance of pre-trained models and limiting the ability to fine-tune them. Given a corpus of formulas, we can use a(nother) model to generate synthetic natural language utterances for fine-tuning. However, it is important to validate whether the NL generated by the LLM is indeed accurate to be beneficial for fine-tuning. In this paper, we provide empirical results on the impact of validating these synthetic training examples with surrogate objectives that evaluate the accuracy of the synthetic annotations. We demonstrate that validation improves performance over raw data across four models (2 open and 2 closed weight). Interestingly, we show that although validation tends to prune more challenging examples, it increases the complexity of problems that models can solve after being fine-tuned on validated data.", "AI": {"tldr": "This paper explores using LLMs to generate spreadsheet formulas and improves performance by validating synthetic training examples with surrogate objectives.", "motivation": "Resources for spreadsheet formulas are scarce, limiting the performance of LLMs in this area. The paper aims to address this by generating synthetic natural language utterances for fine-tuning.", "method": "The paper uses a model to generate synthetic natural language utterances for spreadsheet formulas and validates these examples with surrogate objectives to ensure accuracy.", "result": "Validation improves performance across four models and increases the complexity of problems that models can solve.", "conclusion": "Validating synthetic training examples improves the performance of LLMs in generating spreadsheet formulas, even though it prunes challenging examples."}}
{"id": "2507.06171", "title": "Data-Semantics-Aware Recommendation of Diverse Pivot Tables", "url": "https://arxiv.org/abs/2507.06171", "pdf": "https://arxiv.org/pdf/2507.06171", "abs": "https://arxiv.org/abs/2507.06171", "authors": ["Whanhee Cho", "Anna Fariha"], "categories": ["cs.DB"], "comment": null, "summary": "Data summarization is essential to discover insights from large datasets. In a spreadsheets, pivot tables offer a convenient way to summarize tabular data by computing aggregates over some attributes, grouped by others. However, identifying attribute combinations that will result in useful pivot tables remains a challenge, especially for high-dimensional datasets. We formalize the problem of automatically recommending insightful and interpretable pivot tables, eliminating the tedious manual process. A crucial aspect of recommending a set of pivot tables is to diversify them. Traditional works inadequately address the table-diversification problem, which leads us to consider the problem of pivot table diversification.\n  We present SAGE, a data-semantics-aware system for recommending k-budgeted diverse pivot tables, overcoming the shortcomings of prior work for top-k recommendations that cause redundancy. SAGE ensures that each pivot table is insightful, interpretable, and adaptive to the user's actions and preferences, while also guaranteeing that the set of pivot tables are different from each other, offering a diverse recommendation. We make two key technical contributions: (1) a data-semantics-aware model to measure the utility of a single pivot table and the diversity of a set of pivot tables, and (2) a scalable greedy algorithm that can efficiently select a set of diverse pivot tables of high utility, by leveraging data semantics to significantly reduce the combinatorial search space. Our extensive experiments on three real-world datasets show that SAGE outperforms alternative approaches, and efficiently scales to accommodate high-dimensional datasets. Additionally, we present several case studies to highlight SAGE's qualitative effectiveness over commercial software and Large Language Models (LLMs).", "AI": {"tldr": "This paper introduces SAGE, a system for recommending diverse and insightful pivot tables from large datasets, overcoming limitations of existing methods in table diversification and scalability.", "motivation": "The motivation is to automate the discovery of insightful pivot tables from high-dimensional datasets, a task that is challenging and tedious to perform manually.", "method": "SAGE uses a data-semantics-aware model to measure the utility and diversity of pivot tables and employs a scalable greedy algorithm to efficiently select a set of diverse tables.", "result": "Experiments on real-world datasets demonstrate that SAGE outperforms alternative approaches in terms of effectiveness and scalability. Case studies highlight its qualitative advantages over commercial software and LLMs.", "conclusion": "SAGE provides an effective and efficient solution for recommending diverse and insightful pivot tables, addressing the limitations of previous methods and offering improvements over existing tools."}}
{"id": "2506.17330", "title": "Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE", "url": "https://arxiv.org/abs/2506.17330", "pdf": "https://arxiv.org/pdf/2506.17330", "abs": "https://arxiv.org/abs/2506.17330", "authors": ["Simon Thorne"], "categories": ["cs.SE"], "comment": "18 Pages, 10 Tables, 1 Colour Figure", "summary": "Large Language Models (LLMs) have demonstrated some significant capabilities across various domains; however, their effectiveness in spreadsheet related tasks remains underexplored. This study introduces a foundation for a comprehensive benchmark framework to evaluate the performance of leading LLMs in executing spreadsheet functions, formula generation and data manipulation tasks. The benchmark encompasses tasks ranging from basic formula creation to complex, real world spreadsheet scenarios. Our findings reveal that while LLMs exhibit proficiency in straightforward tasks, they often falter in complex, multi step operations, frequently producing plausible yet incorrect outputs. These results underscore the limitations of current LLMs in handling spreadsheet tasks that require precise logical reasoning and highlight the need for integrating symbolic reasoning capabilities into LLM architectures. To support this, we introduce FLARE (Formula Logic, Auditing, Reasoning and Evaluation) a new benchmark for evaluating LLM performance on real-world spreadsheet logic, auditing, and reasoning tasks.", "AI": {"tldr": "This paper evaluates the performance of Large Language Models (LLMs) on spreadsheet tasks, finding they struggle with complex operations.", "motivation": "To assess LLMs' capabilities in spreadsheet-related tasks, which are currently underexplored.", "method": "The authors created a benchmark framework with tasks ranging from basic formula creation to real-world scenarios and tested leading LLMs.", "result": "LLMs perform well on simple tasks but struggle with complex, multi-step operations, often producing incorrect results.", "conclusion": "Current LLMs lack the precise logical reasoning needed for complex spreadsheet tasks, highlighting the need for integrating symbolic reasoning capabilities. The authors introduce FLARE, a new benchmark for evaluating LLM performance on real-world spreadsheet logic, auditing, and reasoning tasks."}}
{"id": "2506.12339", "title": "SheetMind: An End-to-End LLM-Powered Multi-Agent Framework for Spreadsheet Automation", "url": "https://arxiv.org/abs/2506.12339", "pdf": "https://arxiv.org/pdf/2506.12339", "abs": "https://arxiv.org/abs/2506.12339", "authors": ["Ruiyan Zhu", "Xi Cheng", "Ke Liu", "Brian Zhu", "Daniel Jin", "Neeraj Parihar", "Zhoutian Xu", "Oliver Gao"], "categories": ["cs.HC", "cs.AI"], "comment": "Ruiyan Zhu and Xi Cheng contributed equally to this work", "summary": "We present SheetMind, a modular multi-agent framework powered by large language models (LLMs) for spreadsheet automation via natural language instructions. The system comprises three specialized agents: a Manager Agent that decomposes complex user instructions into subtasks; an Action Agent that translates these into structured commands using a Backus Naur Form (BNF) grammar; and a Reflection Agent that validates alignment between generated actions and the user's original intent. Integrated into Google Sheets via a Workspace extension, SheetMind supports real-time interaction without requiring scripting or formula knowledge. Experiments on benchmark datasets demonstrate an 80 percent success rate on single step tasks and approximately 70 percent on multi step instructions, outperforming ablated and baseline variants. Our results highlight the effectiveness of multi agent decomposition and grammar based execution for bridging natural language and spreadsheet functionalities.", "AI": {"tldr": "SheetMind is a multi-agent system using LLMs to automate spreadsheet tasks from natural language, achieving 80% success on single-step and 70% on multi-step tasks.", "motivation": "To enable spreadsheet automation using natural language instructions without scripting or formula knowledge.", "method": "A multi-agent framework with Manager, Action, and Reflection Agents; Action Agent uses BNF grammar for structured commands; integrated into Google Sheets.", "result": "80% success rate on single-step tasks and 70% on multi-step instructions on benchmark datasets, outperforming baselines.", "conclusion": "Multi-agent decomposition and grammar-based execution are effective for bridging natural language and spreadsheet functionalities."}}
{"id": "2506.09216", "title": "\"How do you even know that stuff?\": Barriers to expertise sharing among spreadsheet users", "url": "https://arxiv.org/abs/2506.09216", "pdf": "https://arxiv.org/pdf/2506.09216", "abs": "https://arxiv.org/abs/2506.09216", "authors": ["Qing", "Xia", "Advait Sarkar", "Duncan Brumby", "Anna Cox"], "categories": ["cs.HC", "cs.CY"], "comment": "Accepted at CSCW 2025", "summary": "Spreadsheet collaboration provides valuable opportunities for learning and expertise sharing between colleagues. Sharing expertise is essential for the retention of important technical skillsets within organisations, but previous studies suggest that spreadsheet experts often fail to disseminate their knowledge to others. We suggest that social norms and beliefs surrounding the value of spreadsheet use significantly influence user engagement in sharing behaviours. To explore this, we conducted 31 semi-structured interviews with professional spreadsheet users from two separate samples. We found that spreadsheet providers face challenges in adapting highly personalised strategies to often subjective standards and evaluating the appropriate social timing of sharing. In addition, conflicted self-evaluations of one's spreadsheet expertise, dismissive normative beliefs about the value of this knowledge, and concerns about the potential disruptions associated with collaboration can further deter sharing. We suggest these observations reflect the challenges of long-term learning in feature-rich software designed primarily with initial learnability in mind. We therefore provide implications for design to navigate this tension. Overall, our findings demonstrate how the complex interaction between technology design and social dynamics can shape collaborative learning behaviours in the context of feature-rich software.", "AI": {"tldr": "This paper investigates the factors influencing knowledge sharing among spreadsheet users in organizations.", "motivation": "The motivation is to understand why spreadsheet experts often fail to share their knowledge, which is crucial for retaining technical skills within organizations.", "method": "The study uses 31 semi-structured interviews with professional spreadsheet users from two separate samples.", "result": "The results indicate that challenges include adapting personalized strategies, evaluating social timing of sharing, conflicted self-evaluations of expertise, dismissive beliefs about the value of knowledge, and concerns about collaboration disruptions.", "conclusion": "The conclusion is that the interaction between technology design and social dynamics shapes collaborative learning behaviors, highlighting the challenges of long-term learning in feature-rich software. Implications for design to navigate this tension are provided."}}
{"id": "2506.03232", "title": "Pivoting the paradigm: the role of spreadsheets in K-12 data science", "url": "https://arxiv.org/abs/2506.03232", "pdf": "https://arxiv.org/pdf/2506.03232", "abs": "https://arxiv.org/abs/2506.03232", "authors": ["Oren Tirschwell", "Nicholas Jon Horton"], "categories": ["stat.OT", "cs.CY"], "comment": null, "summary": "Spreadsheet tools are widely accessible to and commonly used by K-12 students and teachers. They have an important role in data collection and organization. Beyond data organization, spreadsheets also make data visible and easy to interact with, facilitating student engagement in data exploration and analysis. Though not suitable for all circumstances, spreadsheets can and do help foster data and computing skills for K-12 students. This paper 1) reviews prior frameworks on K-12 data tools; 2) proposes data-driven learning outcomes that can be accomplished by incorporating spreadsheets into the curriculum; and 3) discusses how spreadsheets can help develop data acumen and computational fluency. We provide example class activities, identify challenges and barriers to adoption, suggest pedagogical approaches to ease the learning curve for instructors and students, and discuss the need for professional development to facilitate deeper use of spreadsheets for data science and STEM disciplines.", "AI": {"tldr": "This paper discusses the role of spreadsheets in K-12 education for data collection, organization, exploration, and analysis.", "motivation": "To highlight the importance of spreadsheets in fostering data and computing skills for K-12 students.", "method": "Reviewing prior frameworks, proposing data-driven learning outcomes, and discussing how spreadsheets can develop data acumen and computational fluency.", "result": "Provides example class activities, identifies challenges and barriers to adoption, and suggests pedagogical approaches.", "conclusion": "Emphasizes the need for professional development to facilitate deeper use of spreadsheets for data science and STEM disciplines."}}
{"id": "2505.23667", "title": "Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models", "url": "https://arxiv.org/abs/2505.23667", "pdf": "https://arxiv.org/pdf/2505.23667", "abs": "https://arxiv.org/abs/2505.23667", "authors": ["Lang Cao", "Jingxian Xu", "Hanbing Liu", "Jinyu Wang", "Mengyu Zhou", "Haoyu Dong", "Shi Han", "Dongmei Zhang"], "categories": ["cs.AI"], "comment": null, "summary": "Tables are a fundamental structure for organizing and analyzing data, making effective table understanding a critical capability for intelligent systems. While large language models (LMs) demonstrate strong general reasoning abilities, they continue to struggle with accurate numerical or symbolic reasoning over tabular data, especially in complex scenarios. Spreadsheet formulas provide a powerful and expressive medium for representing executable symbolic operations, encoding rich reasoning patterns that remain largely underutilized. In this paper, we propose Formula Tuning (Fortune), a reinforcement learning (RL) framework that trains LMs to generate executable spreadsheet formulas for question answering over general tabular data. Formula Tuning reduces the reliance on supervised formula annotations by using binary answer correctness as a reward signal, guiding the model to learn formula derivation through reasoning. We provide a theoretical analysis of its advantages and demonstrate its effectiveness through extensive experiments on seven table reasoning benchmarks. Formula Tuning substantially enhances LM performance, particularly on multi-step numerical and symbolic reasoning tasks, enabling a 7B model to outperform OpenAI o1 on table understanding. This highlights the potential of formula-driven RL to advance symbolic table reasoning in LMs.", "AI": {"tldr": "This paper introduces Formula Tuning (Fortune), a reinforcement learning framework that trains LMs to generate spreadsheet formulas for question answering over tabular data.", "motivation": "Large language models struggle with numerical and symbolic reasoning over tables, and spreadsheet formulas, which encode rich reasoning patterns, are underutilized.", "method": "The paper proposes a reinforcement learning framework called Formula Tuning (Fortune) that trains LMs to generate executable spreadsheet formulas, using answer correctness as a reward signal.", "result": "Formula Tuning substantially enhances LM performance, particularly on multi-step numerical and symbolic reasoning tasks, enabling a 7B model to outperform OpenAI o1 on table understanding.", "conclusion": "Formula-driven RL has the potential to advance symbolic table reasoning in LMs."}}
{"id": "2505.23296", "title": "Is spreadsheet syntax better than numeric indexing for cell selection?", "url": "https://arxiv.org/abs/2505.23296", "pdf": "https://arxiv.org/pdf/2505.23296", "abs": "https://arxiv.org/abs/2505.23296", "authors": ["Philip Heltweg", "Dirk Riehle", "Georg-Daniel Schwarz"], "categories": ["cs.PL"], "comment": null, "summary": "Selecting a subset of cells is a common task in data engineering, for example, to remove errors or select only specific parts of a table. Multiple approaches to express this selection exist. One option is numeric indexing, commonly found in general programming languages, where a tuple of numbers identifies the cell. Alternatively, the separate dimensions can be referred to using different enumeration schemes like \"A1\" for the first cell, commonly found in software such as spreadsheet systems.\n  In a large-scale controlled experiment with student participants as proxy for data practitioners, we compare the two options with respect to speed and correctness of reading and writing code.\n  The results show that, when reading code, participants make less mistakes using spreadsheet-style syntax. Additionally, when writing code, they make fewer mistakes and are faster when using spreadsheet syntax compared to numeric syntax.\n  From this, a domain-specific syntax, such as spreadsheet syntax for data engineering, appears to be a promising alternative to explore in future tools to support practitioners without a software engineering background.", "AI": {"tldr": "This paper compares spreadsheet-style syntax and numeric indexing for cell selection in data engineering.", "motivation": "To determine the impact of syntax on the speed and correctness of reading and writing code for cell selection.", "method": "A controlled experiment was conducted with students as proxy for data practitioners, comparing spreadsheet-style syntax and numeric indexing.", "result": "Participants made fewer mistakes when reading code with spreadsheet-style syntax. When writing code, they made fewer mistakes and were faster with spreadsheet syntax.", "conclusion": "Spreadsheet syntax is a promising alternative for data engineering tools to support practitioners without a software engineering background."}}
{"id": "2504.20681", "title": "Data Encryption Battlefield: A Deep Dive into the Dynamic Confrontations in Ransomware Attacks", "url": "https://arxiv.org/abs/2504.20681", "pdf": "https://arxiv.org/pdf/2504.20681", "abs": "https://arxiv.org/abs/2504.20681", "authors": ["Arash Mahboubi", "Hamed Aboutorab", "Seyit Camtepe", "Hang Thanh Bui", "Khanh Luong", "Keyvan Ansari", "Shenlu Wang", "Bazara Barry"], "categories": ["cs.CR"], "comment": null, "summary": "In the rapidly evolving landscape of cybersecurity threats, ransomware represents a significant challenge. Attackers increasingly employ sophisticated encryption methods, such as entropy reduction through Base64 encoding, and partial or intermittent encryption to evade traditional detection methods. This study explores the dynamic battle between adversaries who continuously refine encryption strategies and defenders developing advanced countermeasures to protect vulnerable data. We investigate the application of online incremental machine learning algorithms designed to predict file encryption activities despite adversaries evolving obfuscation techniques. Our analysis utilizes an extensive dataset of 32.6 GB, comprising 11,928 files across multiple formats, including Microsoft Word documents (doc), PowerPoint presentations (ppt), Excel spreadsheets (xlsx), image formats (jpg, jpeg, png, tif, gif), PDFs (pdf), audio (mp3), and video (mp4) files. These files were encrypted by 75 distinct ransomware families, facilitating a robust empirical evaluation of machine learning classifiers effectiveness against diverse encryption tactics. Results highlight the Hoeffding Tree algorithms superior incremental learning capability, particularly effective in detecting traditional and AES-Base64 encryption methods employed to lower entropy. Conversely, the Random Forest classifier with warm-start functionality excels at identifying intermittent encryption methods, demonstrating the necessity of tailored machine learning solutions to counter sophisticated ransomware strategies.", "AI": {"tldr": "This research investigates using online incremental machine learning to detect ransomware encryption, including obfuscation techniques like Base64 encoding and intermittent encryption.", "motivation": "The increasing sophistication of ransomware encryption methods necessitates advanced detection techniques.", "method": "The study uses online incremental machine learning algorithms, specifically Hoeffding Trees and Random Forests, to predict file encryption activities.  A dataset of 32.6 GB with 11,928 files encrypted by 75 ransomware families was used.", "result": "Hoeffding Trees excel at detecting traditional and AES-Base64 encryption, while Random Forests are better at identifying intermittent encryption.", "conclusion": "Tailored machine learning solutions are necessary to counter sophisticated ransomware strategies."}}
{"id": "2504.20657", "title": "Image deidentification in the XNAT ecosystem: use cases and solutions", "url": "https://arxiv.org/abs/2504.20657", "pdf": "https://arxiv.org/pdf/2504.20657", "abs": "https://arxiv.org/abs/2504.20657", "authors": ["Alex Michie", "Simon J Doran"], "categories": ["cs.CV"], "comment": "For submission to MELBA (Machine Learning for Biomedical Imaging) special issue on the MIDI-B deidentification challenge (https://www.synapse.org/Synapse:syn53065760). 11 pages, 1 fig, 2 tables; 1 supplementary data file (supplementary_tables_S1_S2_S3.xlsx) containing three spreadsheet tabs", "summary": "XNAT is a server-based data management platform widely used in academia for curating large databases of DICOM images for research projects. We describe in detail a deidentification workflow for DICOM data using facilities in XNAT, together with independent tools in the XNAT \"ecosystem\". We list different contexts in which deidentification might be needed, based on our prior experience. The starting point for participation in the Medical Image De-Identification Benchmark (MIDI-B) challenge was a set of pre-existing local methodologies, which were adapted during the validation phase of the challenge. Our result in the test phase was 97.91\\%, considerably lower than our peers, due largely to an arcane technical incompatibility of our methodology with the challenge's Synapse platform, which prevented us receiving feedback during the validation phase. Post-submission, additional discrepancy reports from the organisers and via the MIDI-B Continuous Benchmarking facility, enabled us to improve this score significantly to 99.61\\%. An entirely rule-based approach was shown to be capable of removing all name-related information in the test corpus, but exhibited failures in dealing fully with address data. Initial experiments using published machine-learning models to remove addresses were partially successful but showed the models to be \"over-aggressive\" on other types of free-text data, leading to a slight overall degradation in performance to 99.54\\%. Future development will therefore focus on improving address-recognition capabilities, but also on better removal of identifiable data burned into the image pixels. Several technical aspects relating to the \"answer key\" are still under discussion with the challenge organisers, but we estimate that our percentage of genuine deidentification failures on the MIDI-B test corpus currently stands at 0.19\\%. (Abridged from original for arXiv submission)", "AI": {"tldr": "This paper describes a deidentification workflow for DICOM data using XNAT and other tools, and its application to the Medical Image De-Identification Benchmark (MIDI-B) challenge.", "motivation": "The motivation is to develop and validate a robust deidentification workflow for DICOM data, which is crucial for protecting patient privacy in research projects.", "method": "The method involves using XNAT facilities and independent tools in the XNAT ecosystem, along with rule-based approaches and machine-learning models for removing identifiable information from DICOM images.", "result": "The initial result in the MIDI-B test phase was 97.91%, which improved to 99.61% after addressing technical incompatibilities and discrepancies. Experiments with machine-learning models for address removal led to a slight degradation in performance to 99.54%. The estimated percentage of genuine deidentification failures is 0.19%.", "conclusion": "An entirely rule-based approach can remove name-related information, but struggles with address data. Machine-learning models show promise but need refinement to avoid over-aggressiveness. Future work will focus on improving address recognition and removing identifiable data burned into image pixels."}}
{"id": "2408.12622", "title": "The AI Risk Repository: A Comprehensive Meta-Review, Database, and Taxonomy of Risks From Artificial Intelligence", "url": "https://arxiv.org/abs/2408.12622", "pdf": "https://arxiv.org/pdf/2408.12622", "abs": "https://arxiv.org/abs/2408.12622", "authors": ["Peter Slattery", "Alexander K. Saeri", "Emily A. C. Grundy", "Jess Graham", "Michael Noetel", "Risto Uuk", "James Dao", "Soroush Pour", "Stephen Casper", "Neil Thompson"], "categories": ["cs.AI", "cs.CR", "cs.ET", "cs.LG", "eess.SY"], "comment": null, "summary": "The risks posed by Artificial Intelligence (AI) are of considerable concern to academics, auditors, policymakers, AI companies, and the public. However, a lack of shared understanding of AI risks can impede our ability to comprehensively discuss, research, and react to them. This paper addresses this gap by creating an AI Risk Repository to serve as a common frame of reference. This comprises a living database of 777 risks extracted from 43 taxonomies, which can be filtered based on two overarching taxonomies and easily accessed, modified, and updated via our website and online spreadsheets. We construct our Repository with a systematic review of taxonomies and other structured classifications of AI risk followed by an expert consultation. We develop our taxonomies of AI risk using a best-fit framework synthesis. Our high-level Causal Taxonomy of AI Risks classifies each risk by its causal factors (1) Entity: Human, AI; (2) Intentionality: Intentional, Unintentional; and (3) Timing: Pre-deployment; Post-deployment. Our mid-level Domain Taxonomy of AI Risks classifies risks into seven AI risk domains: (1) Discrimination & toxicity, (2) Privacy & security, (3) Misinformation, (4) Malicious actors & misuse, (5) Human-computer interaction, (6) Socioeconomic & environmental, and (7) AI system safety, failures, & limitations. These are further divided into 23 subdomains. The AI Risk Repository is, to our knowledge, the first attempt to rigorously curate, analyze, and extract AI risk frameworks into a publicly accessible, comprehensive, extensible, and categorized risk database. This creates a foundation for a more coordinated, coherent, and complete approach to defining, auditing, and managing the risks posed by AI systems.", "AI": {"tldr": "This paper introduces an AI Risk Repository, a comprehensive database of AI risks extracted from various taxonomies, to facilitate a shared understanding and management of these risks.", "motivation": "The lack of a shared understanding of AI risks hinders effective discussion, research, and response to them.", "method": "The authors conducted a systematic review of AI risk taxonomies and expert consultation to build the AI Risk Repository. They developed two taxonomies: a Causal Taxonomy (Entity, Intentionality, Timing) and a Domain Taxonomy (seven domains and 23 subdomains).", "result": "The AI Risk Repository comprises 777 risks extracted from 43 taxonomies, categorized by the Causal and Domain Taxonomies, and is accessible online.", "conclusion": "The AI Risk Repository is the first attempt to create a publicly accessible, comprehensive, extensible, and categorized AI risk database, providing a foundation for a more coordinated and complete approach to managing AI risks."}}
{"id": "2407.09025", "title": "SpreadsheetLLM: Encoding Spreadsheets for Large Language Models", "url": "https://arxiv.org/abs/2407.09025", "pdf": "https://arxiv.org/pdf/2407.09025", "abs": "https://arxiv.org/abs/2407.09025", "authors": ["Haoyu Dong", "Jianbo Zhao", "Yuzhang Tian", "Junyu Xiong", "Shiyu Xia", "Mengyu Zhou", "Yun Lin", "Jos\u00e9 Cambronero", "Yeye He", "Shi Han", "Dongmei Zhang"], "categories": ["cs.AI"], "comment": null, "summary": "Spreadsheets are characterized by their extensive two-dimensional grids, flexible layouts, and varied formatting options, which pose significant challenges for large language models (LLMs). In response, we introduce SpreadsheetLLM, pioneering an efficient encoding method designed to unleash and optimize LLMs' powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs' token constraints, making it impractical for most applications. To tackle this challenge, we develop SheetCompressor, an innovative encoding framework that compresses spreadsheets effectively for LLMs. It comprises three modules: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in the spreadsheet table detection task, outperforming the vanilla approach by 25.6% in GPT4's in-context learning setting. Moreover, fine-tuned LLM with SheetCompressor has an average compression ratio of 25 times, and achieves a state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%. Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet understanding and validate it in a new and demanding spreadsheet QA task. We methodically leverage the inherent layout and structure of spreadsheets, demonstrating that SpreadsheetLLM is highly effective across a variety of spreadsheet tasks.", "AI": {"tldr": "SpreadsheetLLM introduces an efficient encoding method (SheetCompressor) to improve LLMs' performance on spreadsheet tasks, achieving state-of-the-art results in table detection and QA.", "motivation": "LLMs struggle with spreadsheets due to their complex layouts and formatting.", "method": "Developed SheetCompressor, an encoding framework with structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. Also, proposed Chain of Spreadsheet for downstream tasks.", "result": "SheetCompressor achieves a 25x compression ratio and outperforms existing models by 12.3% on spreadsheet table detection, reaching a 78.9% F1 score. SpreadsheetLLM is effective in spreadsheet QA.", "conclusion": "SpreadsheetLLM effectively leverages spreadsheet layout and structure, demonstrating its effectiveness across various spreadsheet tasks."}}
{"id": "2403.03636", "title": "SheetAgent: Towards A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models", "url": "https://arxiv.org/abs/2403.03636", "pdf": "https://arxiv.org/pdf/2403.03636", "abs": "https://arxiv.org/abs/2403.03636", "authors": ["Yibin Chen", "Yifu Yuan", "Zeyu Zhang", "Yan Zheng", "Jinyi Liu", "Fei Ni", "Jianye Hao", "Hangyu Mao", "Fuzheng Zhang"], "categories": ["cs.AI", "cs.LG"], "comment": "Accepted by International World Wide Web Conference (WWW) 2025 (oral)", "summary": "Spreadsheets are ubiquitous across the World Wide Web, playing a critical role in enhancing work efficiency across various domains. Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce SheetRM, a benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges. To mitigate the above challenges, we further propose SheetAgent, a novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of three collaborative modules: Planner, Informer, and Retriever, achieving both advanced reasoning and accurate manipulation over spreadsheets without human interaction through iterative task reasoning and reflection. Extensive experiments demonstrate that SheetAgent delivers 20--40\\% pass rate improvements on multiple benchmarks over baselines, achieving enhanced precision in spreadsheet manipulation and demonstrating superior table reasoning abilities. More details and visualizations are available at the project website: https://sheetagent.github.io/. The datasets and source code are available at https://anonymous.4open.science/r/SheetAgent.", "AI": {"tldr": "This paper introduces SheetRM, a new benchmark for evaluating LLMs in complex spreadsheet manipulation tasks, and proposes SheetAgent, a novel autonomous agent that outperforms existing baselines.", "motivation": "Existing LLMs struggle with complex and realistic spreadsheet manipulation tasks requiring reasoning. This paper aims to bridge the gap with real-world requirements.", "method": "The paper introduces SheetRM, a benchmark with long-horizon and multi-category tasks. It also proposes SheetAgent, an autonomous agent with Planner, Informer, and Retriever modules for iterative task reasoning and reflection.", "result": "SheetAgent achieves 20-40% improvement in pass rate on multiple benchmarks compared to baselines.", "conclusion": "SheetAgent enhances precision in spreadsheet manipulation and demonstrates superior table reasoning abilities."}}
{"id": "2403.19318", "title": "TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios", "url": "https://arxiv.org/abs/2403.19318", "pdf": "https://arxiv.org/pdf/2403.19318", "abs": "https://arxiv.org/abs/2403.19318", "authors": ["Xiaokang Zhang", "Sijia Luo", "Bohan Zhang", "Zeyao Ma", "Jing Zhang", "Yang Li", "Guanlin Li", "Zijun Yao", "Kangli Xu", "Jinchang Zhou", "Daniel Zhang-Li", "Jifan Yu", "Shu Zhao", "Juanzi Li", "Jie Tang"], "categories": ["cs.CL"], "comment": "https://tablellm.github.io/", "summary": "We introduce TableLLM, a robust large language model (LLM) with 8 billion parameters, purpose-built for proficiently handling tabular data manipulation tasks, whether they are embedded within documents or spreadsheets, catering to real-world office scenarios. We propose a distant supervision method for training, which comprises a reasoning process extension strategy, aiding in training LLMs to understand reasoning patterns more effectively as well as a cross-way validation strategy, ensuring the quality of the automatically generated data. To evaluate the performance of TableLLM, we have crafted benchmarks tailored to address both document and spreadsheet formats as well as constructed a well-organized evaluation pipeline capable of handling both scenarios. Thorough evaluations underscore the advantages of TableLLM when compared to various existing general-purpose and tabular data-focused LLMs. We have publicly released the model checkpoint, source code, benchmarks, and a web application for user interaction. Our codes and data are publicly available at https://github.com/TableLLM/TableLLM.", "AI": {"tldr": "TableLLM: A robust 8B LLM for tabular data manipulation in documents/spreadsheets.", "motivation": "Handling tabular data in real-world office scenarios.", "method": "Distant supervision with reasoning process extension and cross-way validation.", "result": "TableLLM outperforms existing LLMs on document and spreadsheet benchmarks.", "conclusion": "TableLLM is effective for tabular data manipulation and is publicly available."}}
{"id": "2502.11267", "title": "Prompting in the Dark: Assessing Human Performance in Prompt Engineering for Data Labeling When Gold Labels Are Absent", "url": "https://arxiv.org/abs/2502.11267", "pdf": "https://arxiv.org/pdf/2502.11267", "abs": "https://arxiv.org/abs/2502.11267", "authors": ["Zeyu He", "Saniya Naphade", "Ting-Hao 'Kenneth' Huang"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted By CHI 2025", "summary": "Millions of users prompt large language models (LLMs) for various tasks, but how good are people at prompt engineering? Do users actually get closer to their desired outcome over multiple iterations of their prompts? These questions are crucial when no gold-standard labels are available to measure progress. This paper investigates a scenario in LLM-powered data labeling, \"prompting in the dark,\" where users iteratively prompt LLMs to label data without using manually-labeled benchmarks. We developed PromptingSheet, a Google Sheets add-on that enables users to compose, revise, and iteratively label data through spreadsheets. Through a study with 20 participants, we found that prompting in the dark was highly unreliable -- only 9 participants improved labeling accuracy after four or more iterations. Automated prompt optimization tools like DSPy also struggled when few gold labels were available. Our findings highlight the importance of gold labels and the needs, as well as the risks, of automated support in human prompt engineering, providing insights for future tool design.", "AI": {"tldr": "This paper studies how well people perform prompt engineering in a data labeling scenario without gold-standard labels, finding it unreliable and highlighting the need for gold labels and careful automated support.", "motivation": "Investigating the effectiveness of iterative prompt engineering by users when gold-standard labels are unavailable, particularly in LLM-powered data labeling.", "method": "Developing PromptingSheet, a Google Sheets add-on, and conducting a user study with 20 participants to iteratively label data.", "result": "Finding that prompting in the dark is unreliable, with only 9 out of 20 participants improving labeling accuracy after multiple iterations. Automated prompt optimization tools also struggled with limited gold labels.", "conclusion": "The study emphasizes the importance of gold labels and the necessity for careful automated support in human prompt engineering, offering insights for future tool design."}}
{"id": "2502.05113", "title": "GiesKaNe: Bridging Past and Present in Grammatical Theory and Practical Application", "url": "https://arxiv.org/abs/2502.05113", "pdf": "https://arxiv.org/pdf/2502.05113", "abs": "https://arxiv.org/abs/2502.05113", "authors": ["Volker Emmrich"], "categories": ["cs.CL"], "comment": null, "summary": "This article explores the requirements for corpus compilation within the GiesKaNe project (University of Giessen and Kassel, Syntactic Basic Structures of New High German). The project is defined by three central characteristics: it is a reference corpus, a historical corpus, and a syntactically deeply annotated treebank. As a historical corpus, GiesKaNe aims to establish connections with both historical and contemporary corpora, ensuring its relevance across temporal and linguistic contexts. The compilation process strikes the balance between innovation and adherence to standards, addressing both internal project goals and the broader interests of the research community. The methodological complexity of such a project is managed through a complementary interplay of human expertise and machine-assisted processes. The article discusses foundational topics such as tokenization, normalization, sentence definition, tagging, parsing, and inter-annotator agreement, alongside advanced considerations. These include comparisons between grammatical models, annotation schemas, and established de facto annotation standards as well as the integration of human and machine collaboration. Notably, a novel method for machine-assisted classification of texts along the continuum of conceptual orality and literacy is proposed, offering new perspectives on text selection. Furthermore, the article introduces an approach to deriving de facto standard annotations from existing ones, mediating between standardization and innovation. In the course of describing the workflow the article demonstrates that even ambitious projects like GiesKaNe can be effectively implemented using existing research infrastructure, requiring no specialized annotation tools. Instead, it is shown that the workflow can be based on the strategic use of a simple spreadsheet and integrates the capabilities of the existing infrastructure.", "AI": {"tldr": "GiesKaNe project explores corpus compilation for a historical, syntactically annotated treebank, balancing innovation and standards using human expertise and machine assistance.", "motivation": "To create a reference and historical corpus that connects historical and contemporary linguistic contexts.", "method": "Combining human expertise and machine-assisted processes for tokenization, normalization, tagging, parsing, and inter-annotator agreement. A novel method for machine-assisted classification of texts along the continuum of conceptual orality and literacy is proposed.", "result": "Demonstrates that ambitious projects like GiesKaNe can be effectively implemented using existing research infrastructure and a simple spreadsheet.", "conclusion": "The GiesKaNe project effectively balances innovation and adherence to standards in corpus compilation by integrating human and machine collaboration and leveraging existing research infrastructure."}}
{"id": "2502.04389", "title": "Overcoming Vision Language Model Challenges in Diagram Understanding: A Proof-of-Concept with XML-Driven Large Language Models Solutions", "url": "https://arxiv.org/abs/2502.04389", "pdf": "https://arxiv.org/pdf/2502.04389", "abs": "https://arxiv.org/abs/2502.04389", "authors": ["Shue Shiinoki", "Ryo Koshihara", "Hayato Motegi", "Masumi Morishige"], "categories": ["cs.SE", "cs.AI"], "comment": "The related code is available at \\url{https://github.com/galirage/spreadsheet-intelligence}, which provides the core library developed for this research. The experimental code using this library can be found at \\url{https://github.com/galirage/XMLDriven-Diagram-Understanding}", "summary": "Diagrams play a crucial role in visually conveying complex relationships and processes within business documentation. Despite recent advances in Vision-Language Models (VLMs) for various image understanding tasks, accurately identifying and extracting the structures and relationships depicted in diagrams continues to pose significant challenges. This study addresses these challenges by proposing a text-driven approach that bypasses reliance on VLMs' visual recognition capabilities. Instead, it utilizes the editable source files--such as xlsx, pptx or docx--where diagram elements (e.g., shapes, lines, annotations) are preserved as textual metadata. In our proof-of-concept, we extracted diagram information from xlsx-based system design documents and transformed the extracted shape data into textual input for Large Language Models (LLMs). This approach allowed the LLM to analyze relationships and generate responses to business-oriented questions without the bottleneck of image-based processing. Experimental comparisons with a VLM-based method demonstrated that the proposed text-driven framework yielded more accurate answers for questions requiring detailed comprehension of diagram structures.The results obtained in this study are not limited to the tested .xlsx files but can also be extended to diagrams in other documents with source files, such as Office pptx and docx formats. These findings highlight the feasibility of circumventing VLM constraints through direct textual extraction from original source files. By enabling robust diagram understanding through LLMs, our method offers a promising path toward enhanced workflow efficiency and information analysis in real-world business scenarios.", "AI": {"tldr": "This paper introduces a text-driven approach to understand diagrams in business documents by extracting textual metadata from source files (e.g., xlsx, pptx, docx) and feeding it to LLMs, bypassing the limitations of VLMs.", "motivation": "Accurately identifying and extracting structures and relationships in diagrams within business documentation is challenging for current Vision-Language Models (VLMs).", "method": "The study proposes extracting diagram information from editable source files (xlsx, pptx, docx) as textual metadata and using this data as input for Large Language Models (LLMs) to analyze relationships.", "result": "The proposed text-driven framework outperformed a VLM-based method in answering questions requiring detailed comprehension of diagram structures.", "conclusion": "The text-driven method offers a promising way to improve workflow efficiency and information analysis in real-world business scenarios by enabling robust diagram understanding through LLMs, circumventing VLM constraints."}}
{"id": "2501.18268", "title": "Reducing Aleatoric and Epistemic Uncertainty through Multi-modal Data Acquisition", "url": "https://arxiv.org/abs/2501.18268", "pdf": "https://arxiv.org/pdf/2501.18268", "abs": "https://arxiv.org/abs/2501.18268", "authors": ["Arthur Hoarau", "Benjamin Quost", "S\u00e9bastien Destercke", "Willem Waegeman"], "categories": ["cs.LG"], "comment": null, "summary": "To generate accurate and reliable predictions, modern AI systems need to combine data from multiple modalities, such as text, images, audio, spreadsheets, and time series. Multi-modal data introduces new opportunities and challenges for disentangling uncertainty: it is commonly assumed in the machine learning community that epistemic uncertainty can be reduced by collecting more data, while aleatoric uncertainty is irreducible. However, this assumption is challenged in modern AI systems when information is obtained from different modalities. This paper introduces an innovative data acquisition framework where uncertainty disentanglement leads to actionable decisions, allowing sampling in two directions: sample size and data modality. The main hypothesis is that aleatoric uncertainty decreases as the number of modalities increases, while epistemic uncertainty decreases by collecting more observations. We provide proof-of-concept implementations on two multi-modal datasets to showcase our data acquisition framework, which combines ideas from active learning, active feature acquisition and uncertainty quantification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u91c7\u96c6\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u4e0d\u786e\u5b9a\u6027\u6765\u8fdb\u884c\u51b3\u7b56\uff0c\u5141\u8bb8\u5728\u6837\u672c\u5927\u5c0f\u548c\u6570\u636e\u6a21\u6001\u4e24\u4e2a\u65b9\u5411\u4e0a\u8fdb\u884c\u91c7\u6837\u3002", "motivation": "\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u6570\u636e\u4e2d\u4e0d\u786e\u5b9a\u6027\u89e3\u8026\u7684\u95ee\u9898\uff0c\u6311\u6218\u4e86\u673a\u5668\u5b66\u4e60\u793e\u533a\u4e2d\u5173\u4e8e\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u7684\u4f20\u7edf\u5047\u8bbe\u3002", "method": "\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u3001\u4e3b\u52a8\u7279\u5f81\u83b7\u53d6\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7b49\u601d\u60f3\u3002", "result": "\u5728\u4e24\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6982\u5ff5\u9a8c\u8bc1\u3002", "conclusion": "\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u968f\u7740\u6a21\u6001\u6570\u91cf\u7684\u589e\u52a0\u800c\u51cf\u5c11\uff0c\u800c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u901a\u8fc7\u6536\u96c6\u66f4\u591a\u89c2\u5bdf\u7ed3\u679c\u800c\u51cf\u5c11\u3002"}}
{"id": "2407.04065", "title": "On the Workflows and Smells of Leaderboard Operations (LBOps): An Exploratory Study of Foundation Model Leaderboards", "url": "https://arxiv.org/abs/2407.04065", "pdf": "https://arxiv.org/pdf/2407.04065", "abs": "https://arxiv.org/abs/2407.04065", "authors": ["Zhimin Zhao", "Abdul Ali Bangash", "Filipe Roseiro C\u00f4go", "Bram Adams", "Ahmed E. Hassan"], "categories": ["cs.SE", "cs.LG"], "comment": "Awesome Foundation Model Leaderboard List: https://github.com/SAILResearch/awesome-foundation-model-leaderboards; Foundation Model Leaderboard Search Toolkit: https://huggingface.co/spaces/zhiminy/awesome-foundation-model-leaderboard-search", "summary": "Foundation models (FM), such as large language models (LLMs), which are large-scale machine learning (ML) models, have demonstrated remarkable adaptability in various downstream software engineering (SE) tasks, such as code completion, code understanding, and software development. As a result, FM leaderboards have become essential tools for SE teams to compare and select the best third-party FMs for their specific products and purposes. However, the lack of standardized guidelines for FM evaluation and comparison threatens the transparency of FM leaderboards and limits stakeholders' ability to perform effective FM selection. As a first step towards addressing this challenge, our research focuses on understanding how these FM leaderboards operate in real-world scenarios (\"leaderboard operations\") and identifying potential pitfalls and areas for improvement (\"leaderboard smells\"). In this regard, we collect up to 1,045 FM leaderboards from five different sources: GitHub, Hugging Face Spaces, Papers With Code, spreadsheet and independent platform, to examine their documentation and engage in direct communication with leaderboard operators to understand their workflows. Through card sorting and negotiated agreement, we identify five distinct workflow patterns and develop a domain model that captures the key components and their interactions within these workflows. We then identify eight unique types of leaderboard smells in LBOps. By mitigating these smells, SE teams can improve transparency, accountability, and collaboration in current LBOps practices, fostering a more robust and responsible ecosystem for FM comparison and selection.", "AI": {"tldr": "This paper analyzes FM leaderboards for SE tasks, identifies workflow patterns and smells, and suggests improvements for transparency and accountability.", "motivation": "Lack of standardized guidelines for FM evaluation threatens the transparency of FM leaderboards and limits effective FM selection.", "method": "Collect 1,045 FM leaderboards, examine documentation, communicate with operators, use card sorting and negotiated agreement to identify workflow patterns, and develop a domain model.", "result": "Identified five distinct workflow patterns and eight unique types of leaderboard smells in LBOps.", "conclusion": "Mitigating these smells improves transparency, accountability, and collaboration in FM comparison and selection."}}
{"id": "2412.11711", "title": "MiMoTable: A Multi-scale Spreadsheet Benchmark with Meta Operations for Table Reasoning", "url": "https://arxiv.org/abs/2412.11711", "pdf": "https://arxiv.org/pdf/2412.11711", "abs": "https://arxiv.org/abs/2412.11711", "authors": ["Zheng Li", "Yang Du", "Mao Zheng", "Mingyang Song"], "categories": ["cs.CL"], "comment": "Accepted by COLING 2025", "summary": "Extensive research has been conducted to explore the capability of Large Language Models (LLMs) for table reasoning and has significantly improved the performance on existing benchmarks. However, tables and user questions in real-world applications are more complex and diverse, presenting an unignorable gap compared to the existing benchmarks. To fill the gap, we propose a \\textbf{M}ult\\textbf{i}-scale spreadsheet benchmark with \\textbf{M}eta \\textbf{o}perations for \\textbf{Table} reasoning, named as MiMoTable. Specifically, MiMoTable incorporates two key features. First, the tables in MiMoTable are all spreadsheets used in real-world scenarios, which cover seven domains and contain different types. Second, we define a new criterion with six categories of meta operations for measuring the difficulty of each question in MiMoTable, simultaneously as a new perspective for measuring the difficulty of the existing benchmarks. Experimental results show that Claude-3.5-Sonnet achieves the best performance with 77.4\\% accuracy, indicating that there is still significant room to improve for LLMs on MiMoTable. Furthermore, we grade the difficulty of existing benchmarks according to our new criteria. Experiments have shown that the performance of LLMs decreases as the difficulty of benchmarks increases, thereby proving the effectiveness of our proposed new criterion.", "AI": {"tldr": "This paper introduces MiMoTable, a new multi-scale spreadsheet benchmark for table reasoning that uses real-world spreadsheets and meta-operations to evaluate question difficulty.", "motivation": "Existing table reasoning benchmarks don't reflect the complexity of real-world tables and questions.", "method": "The authors created MiMoTable, a benchmark with real-world spreadsheets across seven domains and a new criterion with six categories of meta-operations to measure question difficulty.", "result": "Claude-3.5-Sonnet achieved the best performance on MiMoTable with 77.4% accuracy, but there's still room for improvement. The paper also graded existing benchmarks by difficulty, showing that LLM performance decreases as difficulty increases.", "conclusion": "MiMoTable is a more realistic and challenging benchmark for table reasoning, and the proposed difficulty criteria are effective in evaluating LLM performance."}}
{"id": "2412.15030", "title": "When Copilot Becomes Autopilot: Generative AI's Critical Risk to Knowledge Work and a Critical Solution", "url": "https://arxiv.org/abs/2412.15030", "pdf": "https://arxiv.org/pdf/2412.15030", "abs": "https://arxiv.org/abs/2412.15030", "authors": ["Advait Sarkar", "Xiaotong", "Xu", "Neil Toronto", "Ian Drosos", "Christian Poelitz"], "categories": ["cs.HC"], "comment": null, "summary": "Generative AI, with its tendency to \"hallucinate\" incorrect results, may pose a risk to knowledge work by introducing errors. On the other hand, it may also provide unprecedented opportunities for users, particularly non-experts, to learn and apply advanced software features and greatly increase the scope and complexity of tasks they can successfully achieve.\n  As an example of a complex knowledge workflow that is subject to risks and opportunities from generative AI, we consider the spreadsheet. AI hallucinations are an important challenge, but they are not the greatest risk posed by generative AI to spreadsheet workflows. Rather, as more work can be safely delegated to AI, the risk is that human critical thinking -- the ability to holistically and rigorously evaluate a problem and its solutions -- is degraded in the process. The solution is to design the interfaces of generative AI systems deliberately to foster and encourage critical thinking in knowledge work, building primarily on a long history of research on critical thinking tools for education.\n  We discuss a prototype system for the activity of critical shortlisting in spreadsheets. The system uses generative AI to suggest shortlisting criteria and applies these criteria to sort rows in a spreadsheet. It also generates \"provocations\": short text snippets that critique the AI-generated criteria, highlighting risks, shortcomings, and alternatives. Our prototype opens up a rich and completely unexplored design space of critical thinking tools for modern AI-assisted knowledge work. We outline a research agenda for AI as a critic or provocateur, including questions about where and when provocations should appear, their form and content, and potential design trade-offs.", "AI": {"tldr": "This paper discusses the risks and opportunities of using generative AI in knowledge work, specifically spreadsheets. It argues that the greatest risk is the degradation of human critical thinking and proposes designing AI systems to foster critical thinking. A prototype system for critical shortlisting in spreadsheets is presented, using AI to suggest criteria and generate provocations.", "motivation": "The motivation is to address the risks and opportunities of generative AI in knowledge work, particularly the potential degradation of human critical thinking.", "method": "The method involves designing a prototype system for critical shortlisting in spreadsheets that uses generative AI to suggest criteria and generate provocations. The system also generates short text snippets that critique the AI-generated criteria, highlighting risks, shortcomings, and alternatives.", "result": "The prototype system opens up a rich and completely unexplored design space of critical thinking tools for modern AI-assisted knowledge work.", "conclusion": "The conclusion is that AI can be designed as a critic or provocateur to foster and encourage critical thinking in knowledge work. The paper outlines a research agenda for AI as a critic or provocateur, including questions about where and when provocations should appear, their form and content, and potential design trade-offs."}}
{"id": "2412.14062", "title": "Understanding and Evaluating Trust in Generative AI and Large Language Models for Spreadsheets", "url": "https://arxiv.org/abs/2412.14062", "pdf": "https://arxiv.org/pdf/2412.14062", "abs": "https://arxiv.org/abs/2412.14062", "authors": ["Simon Thorne"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Generative AI and Large Language Models (LLMs) hold promise for automating spreadsheet formula creation. However, due to hallucinations, bias and variable user skill, outputs obtained from generative AI cannot be assumed to be accurate or trustworthy. To address these challenges, a trustworthiness framework is proposed based on evaluating the transparency and dependability of the formula. The transparency of the formula is explored through explainability (understanding the formula's reasoning) and visibility (inspecting the underlying algorithms). The dependability of the generated formula is evaluated in terms of reliability (consistency and accuracy) and ethical considerations (bias and fairness). The paper also examines the drivers to these metrics in the form of hallucinations, training data bias and poorly constructed prompts. Finally, examples of mistrust in technology are considered and the consequences explored.", "AI": {"tldr": "This paper proposes a framework for evaluating the trustworthiness of spreadsheet formulas generated by AI, focusing on transparency and dependability.", "motivation": "The motivation is that AI-generated spreadsheet formulas can be inaccurate or untrustworthy due to hallucinations and biases.", "method": "The method involves evaluating transparency (explainability and visibility) and dependability (reliability and ethical considerations) of the formulas.", "result": "The paper examines the drivers of these metrics, such as hallucinations and training data bias.", "conclusion": "The paper considers examples of mistrust in technology and their consequences."}}
{"id": "2412.02357", "title": "Dynamic Prompt Middleware: Contextual Prompt Refinement Controls for Comprehension Tasks", "url": "https://arxiv.org/abs/2412.02357", "pdf": "https://arxiv.org/pdf/2412.02357", "abs": "https://arxiv.org/abs/2412.02357", "authors": ["Ian Drosos", "Jack Williams", "Advait Sarkar", "Nicholas Wilson"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Effective prompting of generative AI is challenging for many users, particularly in expressing context for comprehension tasks such as explaining spreadsheet formulas, Python code, and text passages. Prompt middleware aims to address this barrier by assisting in prompt construction, but barriers remain for users in expressing adequate control so that they can receive AI-responses that match their preferences.\n  We conduct a formative survey (n=38) investigating user needs for control over AI-generated explanations in comprehension tasks, which uncovers a trade-off between standardized but predictable support for prompting, and adaptive but unpredictable support tailored to the user and task. To explore this trade-off, we implement two prompt middleware approaches: Dynamic Prompt Refinement Control (Dynamic PRC) and Static Prompt Refinement Control (Static PRC). The Dynamic PRC approach generates context-specific UI elements that provide prompt refinements based on the user's prompt and user needs from the AI, while the Static PRC approach offers a preset list of generally applicable refinements.\n  We evaluate these two approaches with a controlled user study (n=16) to assess the impact of these approaches on user control of AI responses for crafting better explanations. Results show a preference for the Dynamic PRC approach as it afforded more control, lowered barriers to providing context, and encouraged exploration and reflection of the tasks, but that reasoning about the effects of different generated controls on the final output remains challenging. Drawing on participant feedback, we discuss design implications for future Dynamic PRC systems that enhance user control of AI responses. Our findings suggest that dynamic prompt middleware can improve the user experience of generative AI workflows by affording greater control and guide users to a better AI response.", "AI": {"tldr": "This paper explores the trade-off between standardized and adaptive prompt middleware approaches for improving user control over AI-generated explanations in comprehension tasks.", "motivation": "Many users find it challenging to effectively prompt generative AI, especially in expressing context for comprehension tasks. Prompt middleware aims to help, but users still need more control to get AI responses that match their preferences.", "method": "The authors conducted a formative survey (n=38) to identify user needs, then implemented and evaluated two prompt middleware approaches: Dynamic Prompt Refinement Control (Dynamic PRC) and Static Prompt Refinement Control (Static PRC) through a controlled user study (n=16).", "result": "The Dynamic PRC approach was preferred because it offered more control, lowered barriers to providing context, and encouraged exploration. However, reasoning about the effects of generated controls remained challenging.", "conclusion": "Dynamic prompt middleware can improve the user experience of generative AI workflows by affording greater control and guiding users to better AI responses. Future systems should focus on enhancing user control of AI responses."}}
{"id": "2406.12031", "title": "Large Scale Transfer Learning for Tabular Data via Language Modeling", "url": "https://arxiv.org/abs/2406.12031", "pdf": "https://arxiv.org/pdf/2406.12031", "abs": "https://arxiv.org/abs/2406.12031", "authors": ["Josh Gardner", "Juan C. Perdomo", "Ludwig Schmidt"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "NeurIPS 2024 camera-ready updates", "summary": "Tabular data -- structured, heterogeneous, spreadsheet-style data with rows and columns -- is widely used in practice across many domains. However, while recent foundation models have reduced the need for developing task-specific datasets and predictors in domains such as language modeling and computer vision, this transfer learning paradigm has not had similar impact in the tabular domain. In this work, we seek to narrow this gap and present TabuLa-8B, a language model for tabular prediction. We define a process for extracting a large, high-quality training dataset from the TabLib corpus, proposing methods for tabular data filtering and quality control. Using the resulting dataset, which comprises over 2.1B rows from over 4M unique tables, we fine-tune a Llama 3-8B large language model (LLM) for tabular data prediction (classification and binned regression) using a novel packing and attention scheme for tabular prediction. Through evaluation across a test suite of 329 datasets, we find that TabuLa-8B has zero-shot accuracy on unseen tables that is over 15 percentage points (pp) higher than random guessing, a feat that is not possible with existing state-of-the-art tabular prediction models (e.g. XGBoost, TabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the target datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN models that are explicitly trained on equal, or even up to 16x more data. We release our model, code, and data along with the publication of this paper.", "AI": {"tldr": "TabuLa-8B, a Llama 3-8B LLM fine-tuned for tabular data prediction, achieves state-of-the-art zero-shot and few-shot accuracy on unseen tables.", "motivation": "Existing transfer learning paradigms have not had similar impact in the tabular domain compared to language modeling and computer vision.", "method": "Fine-tuned a Llama 3-8B LLM on a large, high-quality dataset extracted from the TabLib corpus, using a novel packing and attention scheme.", "result": "TabuLa-8B achieves over 15 pp higher zero-shot accuracy than random guessing and 5-15 pp higher few-shot accuracy than XGBoost and TabPFN.", "conclusion": "TabuLa-8B demonstrates the potential of LLMs for tabular prediction, significantly outperforming existing state-of-the-art models in zero-shot and few-shot settings."}}
{"id": "2402.05121", "title": "Large Language Model for Table Processing: A Survey", "url": "https://arxiv.org/abs/2402.05121", "pdf": "https://arxiv.org/pdf/2402.05121", "abs": "https://arxiv.org/abs/2402.05121", "authors": ["Weizheng Lu", "Jing Zhang", "Ju Fan", "Zihao Fu", "Yueguo Chen", "Xiaoyong Du"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, web table question answering, and image table information extraction. Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides a comprehensive overview of table-related tasks, examining both user scenarios and technical aspects. It covers traditional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processing. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought.", "AI": {"tldr": "\u5bf9\u8868\u683c\u76f8\u5173\u4efb\u52a1\u7684LLM\u548cVLM\u5e94\u7528\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6982\u8ff0\uff0c\u6db5\u76d6\u4e86\u7528\u6237\u573a\u666f\u548c\u6280\u672f\u65b9\u9762\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7LLM\u6216VLM\u81ea\u52a8\u5316\u8868\u683c\u4efb\u52a1\uff0c\u5177\u6709\u663e\u8457\u7684\u516c\u5171\u5229\u76ca\uff0c\u5e76\u5f15\u8d77\u4e86\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u7684\u5174\u8da3\u3002", "method": "\u8003\u5bdf\u7528\u6237\u573a\u666f\u548c\u6280\u672f\u65b9\u9762\uff0c\u603b\u7ed3LLM\u548cVLM\u7684\u8bad\u7ec3\u6280\u672f\uff0c\u8ba8\u8bbaprompt\u5de5\u7a0b\uff0c\u7279\u522b\u662f\u7531LLM\u9a71\u52a8\u7684agent\u3002", "result": "\u7a81\u51fa\u4e86\u51e0\u4e2a\u6311\u6218\uff0c\u5305\u62ec\u670d\u52a1\u65f6\u591a\u6837\u5316\u7684\u7528\u6237\u8f93\u5165\u548c\u4f7f\u7528\u601d\u7ef4\u94fe\u7684\u7f13\u6162\u601d\u8003\u3002", "conclusion": "\u4e3a\u8868\u683c\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6982\u8ff0\uff0c\u5e76\u5f3a\u8c03\u4e86\u672a\u6765\u7814\u7a76\u7684\u6311\u6218\u3002"}}
{"id": "2406.14991", "title": "SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation", "url": "https://arxiv.org/abs/2406.14991", "pdf": "https://arxiv.org/pdf/2406.14991", "abs": "https://arxiv.org/abs/2406.14991", "authors": ["Zeyao Ma", "Bohan Zhang", "Jing Zhang", "Jifan Yu", "Xiaokang Zhang", "Xiaohan Zhang", "Sijia Luo", "Xi Wang", "Jie Tang"], "categories": ["cs.CL", "cs.SE"], "comment": "Neurips 2024 (Spotlight); Homepage: https://spreadsheetbench.github.io/", "summary": "We introduce SpreadsheetBench, a challenging spreadsheet manipulation benchmark exclusively derived from real-world scenarios, designed to immerse current large language models (LLMs) in the actual workflow of spreadsheet users. Unlike existing benchmarks that rely on synthesized queries and simplified spreadsheet files, SpreadsheetBench is built from 912 real questions gathered from online Excel forums, which reflect the intricate needs of users. The associated spreadsheets from the forums contain a variety of tabular data such as multiple tables, non-standard relational tables, and abundant non-textual elements. Furthermore, we propose a more reliable evaluation metric akin to online judge platforms, where multiple spreadsheet files are created as test cases for each instruction, ensuring the evaluation of robust solutions capable of handling spreadsheets with varying values. Our comprehensive evaluation of various LLMs under both single-round and multi-round inference settings reveals a substantial gap between the state-of-the-art (SOTA) models and human performance, highlighting the benchmark's difficulty.", "AI": {"tldr": "SpreadsheetBench is a new benchmark for LLMs using real-world spreadsheet questions from online forums.", "motivation": "Existing benchmarks don't reflect real-world spreadsheet usage.", "method": "Collected 912 real questions and spreadsheets from Excel forums; uses multiple test cases per instruction.", "result": "LLMs perform significantly worse than humans on SpreadsheetBench.", "conclusion": "Highlights the difficulty of real-world spreadsheet manipulation for LLMs."}}
{"id": "2409.20224", "title": "Trapped in Transformative Agreements? A Multifaceted Analysis of >1,000 Contracts", "url": "https://arxiv.org/abs/2409.20224", "pdf": "https://arxiv.org/pdf/2409.20224", "abs": "https://arxiv.org/abs/2409.20224", "authors": ["Laura Rothfritz", "W. Benedikt Schmal", "Ulrich Herb"], "categories": ["cs.DL"], "comment": "37 pages, appendix", "summary": "Transformative agreements between academic publishers and research institutions are ubiquitous. The 'Efficiency and Standards for Article Charges' (ESAC) Initiative lists more than 1,000 contracts in its database. We make use of this unique dataset by web-scraping the details of every contract to substantially expand the overview spreadsheet provided by the ESAC Initiative. Based on that hitherto unused data source, we combine qualitative and quantitative methods to conduct an in-depth analysis of the contract characteristics and the TA landscape. Our analysis demonstrates that research institutions seem to be 'trapped' in transformative agreements. Instead of being a bridge towards a fully Open Access world, academia is stuck in the hybrid system. This endows the legacy (non-Open Access) publishing houses with substantial market power. It raises entry barriers, lowers competition, and increases costs for libraries and universities.", "AI": {"tldr": "This paper analyzes transformative agreements (TAs) between academic publishers and research institutions using the ESAC Initiative database.", "motivation": "Research institutions are 'trapped' in transformative agreements, hindering the transition to full Open Access.", "method": "Web-scraping data from the ESAC Initiative database and combining qualitative and quantitative methods.", "result": "Academia is stuck in the hybrid system, endowing legacy publishers with market power, raising entry barriers, lowering competition, and increasing costs.", "conclusion": "TAs may not be an effective bridge to a fully Open Access world and may reinforce the power of legacy publishers."}}
{"id": "2409.12974", "title": "Exploring Higher Education Competencies through Spreadsheet Self-Assessment and Time", "url": "https://arxiv.org/abs/2409.12974", "pdf": "https://arxiv.org/pdf/2409.12974", "abs": "https://arxiv.org/abs/2409.12974", "authors": ["Maria Csernoch", "Judit T. Kiss", "Viktor Tak\u00e1cs", "Domici\u00e1n M\u00e1t\u00e9"], "categories": ["cs.HC"], "comment": "16 pages, 10 colour figures, 9 tables", "summary": "The present paper aims to explore higher education students' spreadsheet competencies and reliability through self-assessment and real-world problem-solving practices. Digital natives alleged skills and competences allowed us to hypothesize that students perform better in Excel than on paper, but the findings cannot confirm this hypothesis. However, our results indicate that students tend to inaccurately assess their spreadsheet competencies compared to their actual performance in both paper-based and Excel tasks. It has also be found that students need at least twice as much time to achieve the same high scores in the digital environment as they do on paper. The results violated the widely accepted assumption that digital native students do not need computer science education, since they are born with it. This study highlights the importance of accurate self-assessment in digital skill development and time management within higher education contexts, particularly in technology-driven disciplines.", "AI": {"tldr": "This paper investigates spreadsheet skills of higher education students, finding self-assessment inaccuracies and longer task completion times in Excel compared to paper.", "motivation": "To examine the spreadsheet competencies and reliability of higher education students in self-assessment versus real-world problem-solving.", "method": "Comparing student performance in Excel and paper-based tasks, analyzing self-assessment accuracy, and measuring task completion times.", "result": "Students inaccurately assess their spreadsheet skills and take twice as long to complete tasks in Excel compared to paper, contradicting the assumption that digital natives are inherently proficient.", "conclusion": "Accurate self-assessment and time management are crucial for digital skill development in higher education, especially in technology-driven fields. The assumption that digital natives don't need computer science education is violated."}}
{"id": "2403.07762", "title": "Supporting Annotators with Affordances for Efficiently Labeling Conversational Data", "url": "https://arxiv.org/abs/2403.07762", "pdf": "https://arxiv.org/pdf/2403.07762", "abs": "https://arxiv.org/abs/2403.07762", "authors": ["Austin Z. Henley", "David Piorkowski"], "categories": ["cs.HC"], "comment": null, "summary": "Without well-labeled ground truth data, machine learning-based systems would not be as ubiquitous as they are today, but these systems rely on substantial amounts of correctly labeled data. Unfortunately, crowdsourced labeling is time consuming and expensive. To address the concerns of effort and tedium, we designed CAL, a novel interface to aid in data labeling. We made several key design decisions for CAL, which include preventing inapt labels from being selected, guiding users in selecting an appropriate label when they need assistance, incorporating labeling documentation into the interface, and providing an efficient means to view previous labels. We implemented a production-quality implementation of CAL and report a user-study evaluation that compares CAL to a standard spreadsheet. Key findings of our study include users using CAL reported lower cognitive load, did not increase task time, users rated CAL to be easier to use, and users preferred CAL over the spreadsheet.", "AI": {"tldr": "This paper introduces CAL, a novel interface to improve data labeling efficiency and user experience.", "motivation": "Crowdsourced data labeling is time-consuming and expensive, motivating the need for a more efficient labeling tool.", "method": "The authors designed CAL with features like preventing inapt labels, guiding label selection, incorporating documentation, and providing efficient label viewing.", "result": "A user study showed that CAL reduced cognitive load, maintained task time, was easier to use, and was preferred over spreadsheets.", "conclusion": "CAL is a valuable tool for data labeling, improving efficiency and user experience compared to standard spreadsheets."}}
{"id": "2310.09985", "title": "Prompting for Discovery: Flexible Sense-Making for AI Art-Making with Dreamsheets", "url": "https://arxiv.org/abs/2310.09985", "pdf": "https://arxiv.org/pdf/2310.09985", "abs": "https://arxiv.org/abs/2310.09985", "authors": ["Shm Garanganao Almeda", "J. D. Zamfirescu-Pereira", "Kyu Won Kim", "Pradeep Mani Rathnam", "Bjoern Hartmann"], "categories": ["cs.HC"], "comment": "13 pages, 14 figures, currently under review", "summary": "Design space exploration (DSE) for Text-to-Image (TTI) models entails navigating a vast, opaque space of possible image outputs, through a commensurately vast input space of hyperparameters and prompt text. Minor adjustments to prompt input can surface unexpectedly disparate images. How can interfaces support end-users in reliably steering prompt-space explorations towards interesting results? Our design probe, DreamSheets, supports exploration strategies with LLM-based functions for assisted prompt construction and simultaneous display of generated results, hosted in a spreadsheet interface. The flexible layout and novel generative functions enable experimentation with user-defined workflows. Two studies, a preliminary lab study and a longitudinal study with five expert artists, revealed a set of strategies participants use to tackle the challenges of TTI design space exploration, and the interface features required to support them - like using text-generation to define local \"axes\" of exploration. We distill these insights into a UI mockup to guide future interfaces.", "AI": {"tldr": "This paper introduces DreamSheets, a spreadsheet interface that helps users explore the design space of Text-to-Image models by assisting with prompt construction and displaying generated results. It identifies exploration strategies and interface features through user studies.", "motivation": "Exploring the vast design space of Text-to-Image models is challenging due to the sensitivity of image outputs to minor changes in prompts. The paper aims to support users in reliably steering prompt-space explorations towards interesting results.", "method": "The authors developed DreamSheets, a spreadsheet interface with LLM-based functions for assisted prompt construction and simultaneous display of generated results. They conducted a preliminary lab study and a longitudinal study with expert artists to evaluate the interface and identify exploration strategies.", "result": "The studies revealed a set of strategies participants use to tackle the challenges of TTI design space exploration and the interface features required to support them, such as using text-generation to define local \"axes\" of exploration.", "conclusion": "The paper distills the insights from the user studies into a UI mockup to guide future interfaces for Text-to-Image model design space exploration."}}
{"id": "2402.14853", "title": "NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries", "url": "https://arxiv.org/abs/2402.14853", "pdf": "https://arxiv.org/pdf/2402.14853", "abs": "https://arxiv.org/abs/2402.14853", "authors": ["Wei Zhao", "Zhitao Hou", "Siyuan Wu", "Yan Gao", "Haoyu Dong", "Yao Wan", "Hongyu Zhang", "Yulei Sui", "Haidong Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear at EACL 2024", "summary": "Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets, is a widespread practice among users performing data analysis. However, crafting formulas on spreadsheets remains a tedious and error-prone task for many end-users, particularly when dealing with complex operations. To alleviate the burden associated with writing spreadsheet formulas, this paper introduces a novel benchmark task called NL2Formula, with the aim to generate executable formulas that are grounded on a spreadsheet table, given a Natural Language (NL) query as input. To accomplish this, we construct a comprehensive dataset consisting of 70,799 paired NL queries and corresponding spreadsheet formulas, covering 21,670 tables and 37 types of formula functions. We realize the NL2Formula task by providing a sequence-to-sequence baseline implementation called fCoder. Experimental results validate the effectiveness of fCoder, demonstrating its superior performance compared to the baseline models. Furthermore, we also compare fCoder with an initial GPT-3.5 model (i.e., text-davinci-003). Lastly, through in-depth error analysis, we identify potential challenges in the NL2Formula task and advocate for further investigation.", "AI": {"tldr": "This paper introduces NL2Formula, a new benchmark task for generating spreadsheet formulas from natural language queries, and provides a baseline implementation called fCoder.", "motivation": "Writing formulas on spreadsheets is tedious and error-prone, especially for complex operations.", "method": "A dataset of 70,799 paired NL queries and spreadsheet formulas is constructed. A sequence-to-sequence baseline implementation called fCoder is provided.", "result": "fCoder demonstrates superior performance compared to baseline models and performs comparably to GPT-3.5.", "conclusion": "The paper identifies potential challenges in the NL2Formula task and advocates for further investigation."}}
{"id": "2006.14706", "title": "Will Dynamic Arrays finally change the way Models are built?", "url": "https://arxiv.org/abs/2006.14706", "pdf": "https://arxiv.org/pdf/2006.14706", "abs": "https://arxiv.org/abs/2006.14706", "authors": ["Peter Bartholomew"], "categories": ["cs.SE"], "comment": "11 Pages, 5 Figures, Numerous Spreadsheet Formulae. This version email address update", "summary": "Spreadsheets offer a supremely successful and intuitive means of processing and exchanging numerical content. Its intuitive ad-hoc nature makes it hugely popular for use in diverse areas including business and engineering, yet these very same characteristics make it extraordinarily error-prone; many would question whether it is suitable for serious analysis or modelling tasks. A previous EuSpRIG paper examined the role of Names in increasing solution transparency and providing a readable notation to forge links with the problem domain. Extensive use was made of CSE array formulas, but it is acknowledged that their use makes spreadsheet development a distinctly cumbersome task. Since that time, the new dynamic arrays have been introduced and array calculation is now the default mode of operation for Excel. This paper examines the thesis that their adoption within a more professional development environment could replace traditional techniques where solution integrity is important. A major advantage of fully dynamic models is that they require less manual intervention to keep them updated and so have the potential to reduce the attendant errors and risk.", "AI": {"tldr": "This paper explores the use of new dynamic arrays in Excel to improve spreadsheet solution integrity, replacing traditional techniques.", "motivation": "Spreadsheets are popular but error-prone, raising concerns about their suitability for serious analysis. The paper aims to address these concerns by leveraging dynamic arrays.", "method": "The paper examines the adoption of dynamic arrays within a professional development environment.", "result": "The paper anticipates that fully dynamic models will require less manual intervention, reducing errors and risks.", "conclusion": "The paper concludes by suggesting that dynamic arrays can enhance solution integrity in spreadsheets by reducing manual intervention and associated errors."}}
{"id": "1704.01142", "title": "A Structured Approach to the development of Solutions in Excel", "url": "https://arxiv.org/abs/1704.01142", "pdf": "https://arxiv.org/pdf/1704.01142", "abs": "https://arxiv.org/abs/1704.01142", "authors": ["Peter Bartholomew"], "categories": ["cs.SE"], "comment": "12 pages, 6 figures. This version updated email address", "summary": "Spreadsheets offer a supremely successful democratisation platform, placing the manipulation and presentation of numbers within the grasp of users that have little or no mathematical expertise or IT experience. What appears to be almost completely lacking within a \"normal\" solution built using Excel default settings is the deployment of any structure that extends beyond a single-cell formula. The structural elements that allow conventional code to scale without escalating errors appear to be absent. This paper considers the use of controversial or lesser-used techniques to create a coherent solution strategy in which the problem is solved by a sequence of formulas resembling the steps of a programmed language.", "AI": {"tldr": "The paper discusses how to improve the structure of Excel solutions by using unconventional techniques to create a more coherent and scalable approach, resembling programmed language.", "motivation": "Lack of structure in typical Excel solutions limits scalability and increases errors.", "method": "Using controversial or lesser-used techniques to create a coherent solution strategy.", "result": "The paper aims to create a solution strategy where the problem is solved by a sequence of formulas resembling the steps of a programmed language.", "conclusion": "The paper explores methods to bring structure to Excel formulas, enabling more scalable and maintainable solutions."}}
{"id": "2402.00069", "title": "Using the Abstract Computer Architecture Description Language to Model AI Hardware Accelerators", "url": "https://arxiv.org/abs/2402.00069", "pdf": "https://arxiv.org/pdf/2402.00069", "abs": "https://arxiv.org/abs/2402.00069", "authors": ["Mika Markus M\u00fcller", "Alexander Richard Manfred Borst", "Konstantin L\u00fcbeck", "Alexander Louis-Ferdinand Jung", "Oliver Bringmann"], "categories": ["cs.AR", "cs.AI"], "comment": "Accepted Version for: MBMV'24", "summary": "Artificial Intelligence (AI) has witnessed remarkable growth, particularly through the proliferation of Deep Neural Networks (DNNs). These powerful models drive technological advancements across various domains. However, to harness their potential in real-world applications, specialized hardware accelerators are essential. This demand has sparked a market for parameterizable AI hardware accelerators offered by different vendors.\n  Manufacturers of AI-integrated products face a critical challenge: selecting an accelerator that aligns with their product's performance requirements. The decision involves choosing the right hardware and configuring a suitable set of parameters. However, comparing different accelerator design alternatives remains a complex task. Often, engineers rely on data sheets, spreadsheet calculations, or slow black-box simulators, which only offer a coarse understanding of the performance characteristics.\n  The Abstract Computer Architecture Description Language (ACADL) is a concise formalization of computer architecture block diagrams, which helps to communicate computer architecture on different abstraction levels and allows for inferring performance characteristics. In this paper, we demonstrate how to use the ACADL to model AI hardware accelerators, use their ACADL description to map DNNs onto them, and explain the timing simulation semantics to gather performance results.", "AI": {"tldr": "This paper introduces using Abstract Computer Architecture Description Language (ACADL) to model AI hardware accelerators for performance analysis.", "motivation": "Choosing the right AI accelerator and its parameters is challenging due to complex comparisons and reliance on inadequate tools.", "method": "Using ACADL to model AI hardware accelerators and map DNNs onto them, with timing simulation for performance evaluation.", "result": "Demonstrates the use of ACADL for modeling and performance analysis of AI hardware accelerators.", "conclusion": "ACADL can be used to model AI hardware accelerators, map DNNs and gather performance results."}}
{"id": "2401.11042", "title": "Does Using ChatGPT Result in Human Cognitive Augmentation?", "url": "https://arxiv.org/abs/2401.11042", "pdf": "https://arxiv.org/pdf/2401.11042", "abs": "https://arxiv.org/abs/2401.11042", "authors": ["Ron Fulbright", "Miranda Morrison"], "categories": ["cs.HC"], "comment": "12 pages, 5 figures", "summary": "Human cognitive performance is enhanced by the use of tools. For example, a human can produce a much greater, and more accurate, volume of mathematical calculation in a unit of time using a calculator or a spreadsheet application on a computer. Such tools have taken over the burden of lower level cognitive grunt work but the human still serves the role of the expert performing higher level thinking and reasoning. Recently, however, unsupervised, deep, machine learning has produced cognitive systems able to outperform humans in several domains. When humans use these tools in a human cog ensemble, the cognitive ability of the human is augmented. In some cases, even non experts can achieve, and even exceed, the performance of experts in a particular domain, synthetic expertise. A new cognitive system, ChatGPT, has burst onto the scene during the past year. This paper investigates human cognitive augmentation due to using ChatGPT by presenting the results of two experiments comparing responses created using ChatGPT with results created not using ChatGPT. We find using ChatGPT does not always result in cognitive augmentation and does not yet replace human judgement, discernment, and evaluation in certain types of tasks. In fact, ChatGPT was observed to result in misleading users resulting in negative cognitive augmentation.", "AI": {"tldr": "This paper investigates the cognitive augmentation effects of using ChatGPT, finding it doesn't always enhance performance and can sometimes mislead users.", "motivation": "To examine how ChatGPT affects human cognitive abilities, particularly in comparison to unaided human performance.", "method": "The study uses two experiments to compare responses generated with and without ChatGPT.", "result": "The experiments showed that ChatGPT use doesn't consistently improve cognitive performance and can occasionally lead to negative outcomes due to misleading information.", "conclusion": "ChatGPT does not replace human judgment and evaluation in certain tasks and can sometimes be detrimental to cognitive augmentation."}}
{"id": "2301.13779", "title": "FLAME: A small language model for spreadsheet formulas", "url": "https://arxiv.org/abs/2301.13779", "pdf": "https://arxiv.org/pdf/2301.13779", "abs": "https://arxiv.org/abs/2301.13779", "authors": ["Harshit Joshi", "Abishai Ebenezer", "Jos\u00e9 Cambronero", "Sumit Gulwani", "Aditya Kanade", "Vu Le", "Ivan Radi\u010dek", "Gust Verbruggen"], "categories": ["cs.PL", "cs.AI", "cs.SE"], "comment": "Accepted to AAAI 2024", "summary": "Spreadsheets are a vital tool for end-user data management. Using large language models for formula authoring assistance in these environments can be difficult, as these models are expensive to train and challenging to deploy due to their size (up to billions of parameters). We present FLAME, a transformer-based model trained exclusively on Excel formulas that leverages domain insights to achieve competitive performance while being substantially smaller (60M parameters) and training on two orders of magnitude less data. We curate a training dataset using sketch deduplication, introduce an Excel-specific formula tokenizer, and use domain-specific versions of masked span prediction and noisy auto-encoding as pre-training objectives. We evaluate FLAME on formula repair, formula completion, and similarity-based formula retrieval. FLAME can outperform much larger models, such as the Davinci (175B) and Cushman (12B) variants of Codex and CodeT5 (220M), in 10 of 14 evaluation settings for the repair and completion tasks. For formula retrieval, FLAME outperforms CodeT5, CodeBERT, and GraphCodeBERT.", "AI": {"tldr": "FLAME is a small transformer model for Excel formulas that outperforms larger models on formula repair, completion, and retrieval.", "motivation": "Using large language models for formula authoring assistance is difficult due to their size and training cost.", "method": "A transformer-based model trained exclusively on Excel formulas with sketch deduplication, an Excel-specific formula tokenizer, and domain-specific pre-training objectives.", "result": "FLAME outperforms much larger models in 10 of 14 evaluation settings for repair and completion and outperforms other models for formula retrieval.", "conclusion": "FLAME demonstrates competitive performance with a substantially smaller size and less training data by leveraging domain insights."}}
{"id": "2312.09107", "title": "A Comprehensive Approach to Ensuring Quality in Spreadsheet-Based Metadata", "url": "https://arxiv.org/abs/2312.09107", "pdf": "https://arxiv.org/pdf/2312.09107", "abs": "https://arxiv.org/abs/2312.09107", "authors": ["Martin J. O'Connor", "Marcos Mart\u00ednez-Romero", "Mete Ugur Akdogan", "Josef Hardi", "Mark A. Musen"], "categories": ["cs.DL"], "comment": null, "summary": "While scientists increasingly recognize the importance of metadata in describing their data, spreadsheets remain the preferred tool for supplying this information despite their limitations in ensuring compliance and quality. Various tools have been developed to address these limitations, but they suffer from their own shortcomings, such as steep learning curves and limited customization. In this paper, we describe an end-to-end approach that supports spreadsheet-based entry of metadata while providing rigorous compliance and quality control. Our approach employs several key strategies, including customizable templates for defining metadata, integral support for the use of controlled terminologies when defining these templates, and an interactive Web-based tool that allows users to rapidly identify and fix errors in the spreadsheet-based metadata they supply. We demonstrate how this approach is being deployed in a biomedical consortium to define and collect metadata about scientific experiments.", "AI": {"tldr": "This paper introduces a new approach for spreadsheet-based metadata entry with compliance and quality control.", "motivation": "Spreadsheets are widely used for metadata entry but have limitations in compliance and quality control. Existing tools have shortcomings like steep learning curves and limited customization.", "method": "The approach uses customizable templates, controlled terminologies, and a web-based tool for error identification and correction.", "result": "The approach is deployed in a biomedical consortium for metadata collection about scientific experiments.", "conclusion": "The paper demonstrates a practical approach to improve metadata quality and compliance using spreadsheets."}}
{"id": "2312.06517", "title": "Facilitating Digital Agriculture with Simple Databases", "url": "https://arxiv.org/abs/2312.06517", "pdf": "https://arxiv.org/pdf/2312.06517", "abs": "https://arxiv.org/abs/2312.06517", "authors": ["Dennis Buckmaster", "Sami Basir", "Hanae Sakata"], "categories": ["cs.DB"], "comment": "6 pages, 1 table, 1 figure. Journal of Extension Tools of the Trade, in press", "summary": "As an on-ramp to databases, we offer several well-structured private database templates as open source resources for agriculturalists, particularly those with modest spreadsheet skills. These farmer-oriented Air table databases use simple data-validated forms, with the look and feel of a customized app, to yield operational data that is tidy, machine- and human-readable, editable, and exportable for analysis in other software. Such data can facilitate logistics, provide contextual metadata, and improve enterprise analysis. A recorded workshop explaining how to build a database for activity records is presented. These resources may facilitate infusion of digital agriculture principles through Extension and structured educational programming.", "AI": {"tldr": "This paper introduces open-source Air table database templates for agriculturalists with spreadsheet skills.", "motivation": "To provide well-structured private database templates for agriculturalists to improve data management and analysis.", "method": "Using simple data-validated forms within Air table databases to collect operational data.", "result": "Yielding tidy, machine- and human-readable, editable, and exportable data for logistics, metadata, and enterprise analysis.", "conclusion": "These resources can help infuse digital agriculture principles through Extension and educational programs."}}
{"id": "2310.17306", "title": "FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language", "url": "https://arxiv.org/abs/2310.17306", "pdf": "https://arxiv.org/pdf/2310.17306", "abs": "https://arxiv.org/abs/2310.17306", "authors": ["Mukul Singh", "Jos\u00e9 Cambronero", "Sumit Gulwani", "Vu Le", "Carina Negreanu", "Elnaz Nouri", "Mohammad Raza", "Gust Verbruggen"], "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.PL"], "comment": "Contains inappropriately sourced conjecture of OpenAI's ChatGPT parameter count from www.forbes.com/sites/forbestechcouncil/2023/02/17/is-bigger-better-why-the-chatgpt-vs-gpt-3-vs-gpt-4-battle-is-just-a-family-chat, a citation which was omitted. The authors do not have direct knowledge or verification of this information, and relied solely on this article, which may lead to public confusion", "summary": "Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires them to understand and implement the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system. To evaluate FormaT5 on diverse and real scenarios, we create an extensive benchmark of 1053 CF tasks, containing real-world descriptions collected from four different sources. We release our benchmarks to encourage research in this area. Abstention and filling allow FormaT5 to outperform 8 different neural approaches on our benchmarks, both with and without examples. Our results illustrate the value of building domain-specific learning systems.", "AI": {"tldr": "FormaT5\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u6839\u636e\u76ee\u6807\u8868\u683c\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u6761\u4ef6\u683c\u5f0f(CF)\u89c4\u5219\u3002", "motivation": "\u7528\u6237\u5728\u7535\u5b50\u8868\u683c\u8f6f\u4ef6\u4e2d\u7f16\u5199\u6570\u636e\u76f8\u5173\u7684\u6761\u4ef6\u683c\u5f0f(CF)\u89c4\u5219\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u9700\u8981\u7406\u89e3\u548c\u5b9e\u73b0\u5e95\u5c42\u903b\u8f91\u3002", "method": "FormaT5\u901a\u8fc7\u9884\u6d4b\u5360\u4f4d\u7b26\u6765\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u5f03\u6743\u76ee\u6807\u6765\u89e3\u51b3\u6b20\u89c4\u8303\u95ee\u9898\u3002\u8fd9\u4e9b\u5360\u4f4d\u7b26\u53ef\u4ee5\u7531\u7b2c\u4e8c\u4e2a\u6a21\u578b\u6216\u7f16\u7a0b\u793a\u4f8b\u7cfb\u7edf\u586b\u5145\u3002", "result": "FormaT5\u5728\u5305\u542b\u6765\u81ea\u56db\u4e2a\u4e0d\u540c\u6765\u6e90\u7684\u771f\u5b9e\u4e16\u754c\u63cf\u8ff0\u76841053\u4e2aCF\u4efb\u52a1\u7684\u5927\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f18\u4e8e8\u79cd\u4e0d\u540c\u7684\u795e\u7ecf\u65b9\u6cd5\u3002", "conclusion": "\u5f03\u6743\u548c\u586b\u5145\u5141\u8bb8FormaT5\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5176\u4ed6\u795e\u7ecf\u65b9\u6cd5\uff0c\u65e0\u8bba\u6709\u6ca1\u6709\u793a\u4f8b\uff0c\u90fd\u8bc1\u660e\u4e86\u6784\u5efa\u7279\u5b9a\u9886\u57df\u5b66\u4e60\u7cfb\u7edf\u7684\u4ef7\u503c\u3002"}}
{"id": "2310.20395", "title": "Spreadsheet-based Configuration of Families of Real-Time Specifications", "url": "https://arxiv.org/abs/2310.20395", "pdf": "https://arxiv.org/pdf/2310.20395", "abs": "https://arxiv.org/abs/2310.20395", "authors": ["Jos\u00e9 Proen\u00e7a", "David Pereira", "Giann Spilere Nandi", "Sina Borrami", "Jonas Melchert"], "categories": ["cs.SE"], "comment": "In Proceedings TiCSA 2023, arXiv:2310.18720", "summary": "Model checking real-time systems is complex, and requires a careful trade-off between including enough detail to be useful and not too much detail to avoid state explosion. This work exploits variability of the formal model being analysed and the requirements being checked, to facilitate the model-checking of variations of real-time specifications.  This work results from the collaboration between academics and Alstom, a railway company with a concrete use-case, in the context of the VALU3S European project. The configuration of the variability of the formal specifications is described in MS Excel spreadsheets with a particular structure, making it easy to use also by developers. These spreadsheets are processed automatically by our prototype tool that generates instances and runs the model checker.  We propose the extension of our previous work by exploiting analysis over valid combination of features, while preserving the simplicity of a spreadsheet-based interface with the model checker.", "AI": {"tldr": "This paper introduces a method for model checking real-time systems by exploiting variability in formal models and requirements, using MS Excel spreadsheets for configuration.", "motivation": "To address the complexity of model checking real-time systems and the trade-off between detail and state explosion.", "method": "Using MS Excel spreadsheets to describe variability in formal specifications, automatically generating instances, and running a model checker.", "result": "A prototype tool that processes spreadsheets and runs the model checker.", "conclusion": "The paper proposes extending previous work by analyzing valid feature combinations while maintaining a simple spreadsheet interface."}}
{"id": "2305.19308", "title": "SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models", "url": "https://arxiv.org/abs/2305.19308", "pdf": "https://arxiv.org/pdf/2305.19308", "abs": "https://arxiv.org/abs/2305.19308", "authors": ["Hongxin Li", "Jingran Su", "Yuntao Chen", "Qing Li", "Zhaoxiang Zhang"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "Accepted to NeurIPS 2023", "summary": "Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill to automate these burdensome works. With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal. In this work, we propose a SheetCopilot agent that takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for LLMs to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of LLMs in software control tasks. Our SheetCopilot correctly completes 44.3\\% of tasks for a single generation, outperforming the strong code generation baseline by a wide margin. Our project page:https://sheetcopilot.github.io/.", "AI": {"tldr": "This paper introduces SheetCopilot, an LLM-powered agent for automating spreadsheet tasks using natural language. It defines atomic actions and uses a state machine for task planning.", "motivation": "End users spend excessive time on repetitive, error-prone spreadsheet tasks but lack automation skills. LLMs offer a solution by enabling natural language control of software.", "method": "The paper proposes SheetCopilot, an agent that translates natural language tasks into spreadsheet actions. It uses a set of atomic actions and a state machine-based task planning framework.", "result": "SheetCopilot achieves a 44.3% task completion rate, significantly outperforming code generation baselines.", "conclusion": "The paper demonstrates the potential of LLMs for controlling spreadsheet software through natural language, offering a promising approach to automate common end-user tasks."}}
{"id": "2310.17414", "title": "LEI2JSON: Schema-based Validation and Conversion of Livestock Event Information", "url": "https://arxiv.org/abs/2310.17414", "pdf": "https://arxiv.org/pdf/2310.17414", "abs": "https://arxiv.org/abs/2310.17414", "authors": ["Mahir Habib", "Muhammad Ashad Kabir", "Lihong Zheng"], "categories": ["eess.SY", "cs.SE"], "comment": "20 pages, 6 figures", "summary": "Livestock producers often need help in standardising (i.e., converting and validating) their livestock event data. This article introduces a novel solution, LEI2JSON (Livestock Event Information To JSON). The tool is an add-on for Google Sheets, adhering to the livestock event information (LEI) schema. The core objective of LEI2JSON is to provide livestock producers with an efficient mechanism to standardise their data, leading to substantial savings in time and resources. This is achieved by building the spreadsheet template with the appropriate column headers, notes, and validation rules, converting the spreadsheet data into JSON format, and validating the output against the schema. LEI2JSON facilitates the seamless storage of livestock event information locally or on Google Drive in JSON. Additionally, we have conducted an extensive experimental evaluation to assess the effectiveness of the tool.", "AI": {"tldr": "LEI2JSON is a Google Sheets add-on that helps livestock producers standardize their event data into JSON format, saving time and resources.", "motivation": "Livestock producers need help standardizing livestock event data.", "method": "A Google Sheets add-on, LEI2JSON, is used to build spreadsheet templates with column headers, notes, and validation rules, convert data to JSON, and validate against a schema.", "result": "LEI2JSON facilitates seamless storage of livestock event information and has been evaluated for effectiveness.", "conclusion": "LEI2JSON provides an efficient mechanism to standardize livestock data, leading to savings in time and resources."}}
{"id": "2310.16700", "title": "Streamlining Knowledge Graph Construction with a fa\u00e7ade: The SPARQL Anything project", "url": "https://arxiv.org/abs/2310.16700", "pdf": "https://arxiv.org/pdf/2310.16700", "abs": "https://arxiv.org/abs/2310.16700", "authors": ["Luigi Asprino", "Enrico Daga", "Justin Dowdy", "Paul Mulholland", "Aldo Gangemi", "Marco Ratta"], "categories": ["cs.DB", "cs.DS"], "comment": "15 pages", "summary": "What should a data integration framework for knowledge engineers look like? Recent research on Knowledge Graph construction proposes the design of a fa\u00e7ade, a notion borrowed from object-oriented software engineering. This idea is applied to SPARQL Anything, a system that allows querying heterogeneous resources as-if they were in RDF, in plain SPARQL 1.1, by overloading the SERVICE clause. SPARQL Anything supports a wide variety of file formats, from popular ones (CSV, JSON, XML, Spreadsheets) to others that are not supported by alternative solutions (Markdown, YAML, DOCx, Bibtex). Features include querying Web APIs with high flexibility, parametrised queries, and chaining multiple transformations into complex pipelines. In this paper, we describe the design rationale and software architecture of the SPARQL Anything system. We provide references to an extensive set of reusable, real-world scenarios from various application domains. We report on the value-to-users of the founding assumptions of its design, compared to alternative solutions through a community survey and a field report from the industry.", "AI": {"tldr": "This paper introduces SPARQL Anything, a data integration framework that allows querying heterogeneous resources as if they were in RDF using SPARQL 1.1.", "motivation": "To provide a flexible and unified way for knowledge engineers to query diverse data sources.", "method": "The system overloads the SERVICE clause in SPARQL to support various file formats and Web APIs, enabling complex data transformation pipelines.", "result": "The paper presents the design rationale, software architecture, reusable scenarios, and a community survey and field report demonstrating the value of SPARQL Anything compared to other solutions.", "conclusion": "SPARQL Anything offers a valuable approach to data integration by providing a flexible and unified query interface for heterogeneous data sources."}}
{"id": "2310.14495", "title": "InstructExcel: A Benchmark for Natural Language Instruction in Excel", "url": "https://arxiv.org/abs/2310.14495", "pdf": "https://arxiv.org/pdf/2310.14495", "abs": "https://arxiv.org/abs/2310.14495", "authors": ["Justin Payan", "Swaroop Mishra", "Mukul Singh", "Carina Negreanu", "Christian Poelitz", "Chitta Baral", "Subhro Roy", "Rasika Chakravarthy", "Benjamin Van Durme", "Elnaz Nouri"], "categories": ["cs.CL", "cs.AI"], "comment": "Findings of EMNLP 2023, 18 pages", "summary": "With the evolution of Large Language Models (LLMs) we can solve increasingly more complex NLP tasks across various domains, including spreadsheets. This work investigates whether LLMs can generate code (Excel OfficeScripts, a TypeScript API for executing many tasks in Excel) that solves Excel specific tasks provided via natural language user instructions. To do so we introduce a new large-scale benchmark, InstructExcel, created by leveraging the 'Automate' feature in Excel to automatically generate OfficeScripts from users' actions. Our benchmark includes over 10k samples covering 170+ Excel operations across 2,000 publicly available Excel spreadsheets. Experiments across various zero-shot and few-shot settings show that InstructExcel is a hard benchmark for state of the art models like GPT-4. We observe that (1) using GPT-4 over GPT-3.5, (2) providing more in-context examples, and (3) dynamic prompting can help improve performance on this benchmark.", "AI": {"tldr": "This paper introduces InstructExcel, a new benchmark for evaluating LLMs' ability to generate Excel OfficeScripts from natural language instructions. It finds that InstructExcel is challenging for current LLMs like GPT-4, but performance can be improved by using better models, more examples, and dynamic prompting.", "motivation": "To investigate whether LLMs can generate code (Excel OfficeScripts) that solves Excel specific tasks provided via natural language user instructions.", "method": "A new large-scale benchmark, InstructExcel, was created by leveraging the 'Automate' feature in Excel to automatically generate OfficeScripts from users' actions. Experiments were conducted in various zero-shot and few-shot settings using models like GPT-4.", "result": "InstructExcel is a hard benchmark for state-of-the-art models like GPT-4. Performance can be improved by using GPT-4 over GPT-3.5, providing more in-context examples, and dynamic prompting.", "conclusion": "The paper concludes that InstructExcel is a challenging benchmark for LLMs in generating Excel OfficeScripts, but techniques like better models, more examples, and dynamic prompting can improve performance."}}
{"id": "2311.10728", "title": "Improving Feedback from Automated Reviews of Student Spreadsheets", "url": "https://arxiv.org/abs/2311.10728", "pdf": "https://arxiv.org/pdf/2311.10728", "abs": "https://arxiv.org/abs/2311.10728", "authors": ["S\u00f6ren Aguirre Reid", "Frank Kammer", "Jonas-Ian Kuche", "Pia-Doreen Ritzke", "Markus Siepermann", "Max Stephan", "Armin Wagenknecht"], "categories": ["cs.CY"], "comment": null, "summary": "Spreadsheets are one of the most widely used tools for end users. As a result, spreadsheets such as Excel are now included in many curricula. However, digital solutions for assessing spreadsheet assignments are still scarce in the teaching context. Therefore, we have developed an Intelligent Tutoring System (ITS) to review students' Excel submissions and provide individualized feedback automatically. Although the lecturer only needs to provide one reference solution, the students' submissions are analyzed automatically in several ways: value matching, detailed analysis of the formulas, and quality assessment of the solution. To take the students' learning level into account, we have developed feedback levels for an ITS that provide gradually more information about the error by using one of the different analyses. Feedback at a higher level has been shown to lead to a higher percentage of correct submissions and was also perceived as well understandable and helpful by the students.", "AI": {"tldr": "This paper introduces an Intelligent Tutoring System (ITS) for automatically assessing and providing individualized feedback on students' Excel submissions.", "motivation": "Digital solutions for assessing spreadsheet assignments are scarce in teaching, hindering effective learning.", "method": "The ITS analyzes submissions through value matching, formula analysis, and solution quality assessment, offering tiered feedback based on student learning levels.", "result": "Higher-level feedback leads to a higher percentage of correct submissions and is perceived as understandable and helpful by students.", "conclusion": "The developed ITS effectively reviews Excel submissions and provides valuable, tailored feedback to students, enhancing their learning experience."}}
{"id": "2309.02110", "title": "Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!", "url": "https://arxiv.org/abs/2309.02110", "pdf": "https://arxiv.org/pdf/2309.02110", "abs": "https://arxiv.org/abs/2309.02110", "authors": ["James P. Dilger"], "categories": ["math.HO", "cs.CL"], "comment": null, "summary": "Wordle is a popular, online word game offered by the New York Times (nytimes.com). Currently there are some 2 million players of the English version worldwide. Players have 6 attempts to guess the daily word (target word) and after each attempt, the player receives color-coded information about the correctness and position of each letter in the guess. After either a successful completion of the puzzle or the final unsuccessful attempt, software can assess the player's luck and skill using Information Theory and can display data for the first, second, ..., sixth guesses of a random sample of all players. Recently, I discovered that the latter data is presented in a format that can easily be copied and pasted into a spreadsheet. I compiled data on Wordle players' first guesses from May 2023 - August 2023 and inferred some interesting information about Wordle players. A) Every day, about 0.2-0.5% of players solve the puzzle in one attempt. Because the odds of guessing the one of 2,315 possible target words at random is 0.043%, this implies that 4,000 - 10,000 players cheat by obtaining the target word outside of playing the game! B) At least 1/3 of the players have a favorite starting word, or cycle through several. And even though players should be aware that target words are never repeated, most players appear to remain loyal to their starting word even after its appearance as a target word. C) On August 15, 2023, about 30,000 players abruptly changed their starting word, presumably based on a crossword puzzle clue! Wordle players can be influenced! This study goes beyond social media postings, surveys, and Google Trends to provide solid, quantitative evidence about cheating in Wordle.", "AI": {"tldr": "This paper analyzes Wordle player data from May-August 2023 to infer player behavior.", "motivation": "The study aims to provide quantitative evidence about cheating and player behavior in Wordle, going beyond social media postings and surveys.", "method": "The author compiled data on Wordle players' first guesses and used Information Theory to assess luck and skill.", "result": "The study found evidence of cheating (0.2-0.5% solve in one attempt), loyalty to starting words, and susceptibility to external influences (crossword clue).", "conclusion": "The study provides solid, quantitative evidence about cheating in Wordle and how player behavior can be influenced."}}
{"id": "2310.01297", "title": "Co-audit: tools to help humans double-check AI-generated content", "url": "https://arxiv.org/abs/2310.01297", "pdf": "https://arxiv.org/pdf/2310.01297", "abs": "https://arxiv.org/abs/2310.01297", "authors": ["Andrew D. Gordon", "Carina Negreanu", "Jos\u00e9 Cambronero", "Rasika Chakravarthy", "Ian Drosos", "Hao Fang", "Bhaskar Mitra", "Hannah Richardson", "Advait Sarkar", "Stephanie Simmons", "Jack Williams", "Ben Zorn"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.PL"], "comment": null, "summary": "Users are increasingly being warned to check AI-generated content for correctness. Still, as LLMs (and other generative models) generate more complex output, such as summaries, tables, or code, it becomes harder for the user to audit or evaluate the output for quality or correctness. Hence, we are seeing the emergence of tool-assisted experiences to help the user double-check a piece of AI-generated content. We refer to these as co-audit tools. Co-audit tools complement prompt engineering techniques: one helps the user construct the input prompt, while the other helps them check the output response. As a specific example, this paper describes recent research on co-audit tools for spreadsheet computations powered by generative models. We explain why co-audit experiences are essential for any application of generative AI where quality is important and errors are consequential (as is common in spreadsheet computations). We propose a preliminary list of principles for co-audit, and outline research challenges.", "AI": {"tldr": "This paper introduces co-audit tools to help users check the correctness and quality of AI-generated content, especially in complex outputs like summaries, tables, or code. It focuses on spreadsheet computations as an example.", "motivation": "The increasing complexity of AI-generated content makes it harder for users to evaluate its quality and correctness, necessitating tool-assisted experiences for double-checking.", "method": "The paper proposes co-audit tools as a complement to prompt engineering, outlines principles for co-audit, and identifies research challenges.", "result": "The paper describes recent research on co-audit tools for spreadsheet computations.", "conclusion": "Co-audit experiences are essential for applications of generative AI where quality is important and errors are consequential, as is common in spreadsheet computations."}}
{"id": "2309.00115", "title": "Excel as a Turing-complete Functional Programming Environment", "url": "https://arxiv.org/abs/2309.00115", "pdf": "https://arxiv.org/pdf/2309.00115", "abs": "https://arxiv.org/abs/2309.00115", "authors": ["Peter Bartholomew"], "categories": ["cs.SE"], "comment": "14 page, 6 figures", "summary": "Since the calculation engine of Excel was the subject of a major upgrade to accommodate Dynamic Arrays in 2018 there has been a series of seismic changes to the art of building spreadsheet solutions. This paper will show the ad-hoc end user practices of traditional spreadsheets can be replaced by radically different approaches that have far more in common with formal programming. It is too early to guess the extent to which the new functionality will be adopted by the business and engineering communities and the impact that may have upon risk. Nevertheless, some trends are emerging from pioneering work within the Excel community which we will discuss here.", "AI": {"tldr": "Excel's calculation engine upgrade in 2018 introduced Dynamic Arrays, leading to significant changes in spreadsheet solution development.", "motivation": "To demonstrate how traditional spreadsheet practices can be replaced by approaches more akin to formal programming due to the new Excel functionality.", "method": "Discussing emerging trends from pioneering work within the Excel community.", "result": "Identifying trends in how the new Excel functionality is being used.", "conclusion": "The adoption and impact of the new functionality on risk are still uncertain, but some trends are emerging."}}
{"id": "2309.12353", "title": "How Beaufort, Neumann and Gates met? Subject integration with spreadsheeting", "url": "https://arxiv.org/abs/2309.12353", "pdf": "https://arxiv.org/pdf/2309.12353", "abs": "https://arxiv.org/abs/2309.12353", "authors": ["Maria Csernoch", "Julia Csernoch"], "categories": ["cs.CY"], "comment": "17 pages, 14 figures", "summary": "Computational thinking should be the fourth fundamental skill, along with reading, writing, and arithmetic (3R). To reach the level where computational thinking skills, especially digital problem solving have their own schemata, there is a long way to go. In the present paper, a novel approach is detailed to support subject integration and building digital schemata, on the well-known Beaufort scale. The conversion of a traditional, paper-based problem and a data retrieval process are presented within the frame of a Grade 8 action research study. It is found that both students content knowledge and their digital skills developed more efficiently than in traditional course book and decontextualized digital environments. Furthermore, the method presented here can be adapted to any paper-based problems whose solutions would be more effective in a digital environment and which offer various forms for building schemata both in the subject matter and informatics.", "AI": {"tldr": "This paper introduces a novel method for subject integration and building digital schemata using the Beaufort scale in a Grade 8 action research study.", "motivation": "To promote computational thinking as a fundamental skill and address the need for developing digital problem-solving schemata.", "method": "Converting a traditional, paper-based problem and a data retrieval process within the frame of a Grade 8 action research study.", "result": "Students' content knowledge and digital skills developed more efficiently compared to traditional methods.", "conclusion": "The presented method can be adapted to other paper-based problems to enhance subject matter understanding and informatics skills."}}
{"id": "2309.00104", "title": "A Use Case-Engineering Resources Taxonomy for Analytical Spreadsheet Models", "url": "https://arxiv.org/abs/2309.00104", "pdf": "https://arxiv.org/pdf/2309.00104", "abs": "https://arxiv.org/abs/2309.00104", "authors": ["Thomas A. Grossman", "Vijay Mehrotra"], "categories": ["cs.SE"], "comment": "13 Pages, 7 Figures, 2 Tables", "summary": "This paper presents a taxonomy for analytical spreadsheet models. It considers both the use case that a spreadsheet is meant to serve, and the engineering resources devoted to its development. We extend a previous three-type taxonomy, to identify nine types of spreadsheet models, that encompass the many analytical spreadsheet models seen in the literature. We connect disparate research literature to distinguish between an \"analytical solution\" and an \"industrial-quality analytical spreadsheet model\". We explore the nature of each of the nine types, propose definitions for some, relate them to the literature, and hypothesize on how they might arise. The taxonomy aids in identifying where various spreadsheet development guidelines are most useful, provides a lens for viewing spreadsheet errors and risk, and offers a structure for understanding how spreadsheets change over time. This taxonomy opens the door to many interesting research questions, including refinements to itself.", "AI": {"tldr": "This paper introduces a new taxonomy for analytical spreadsheet models, expanding on a previous one to include nine types based on use case and development resources.", "motivation": "To categorize and understand the different types of analytical spreadsheet models used in practice and research.", "method": "The authors extend a previous three-type taxonomy and connect it to existing research literature to identify nine types of spreadsheet models.", "result": "The paper proposes definitions for the nine types, relates them to the literature, and hypothesizes on their development. It also discusses the taxonomy's utility in identifying relevant development guidelines, understanding spreadsheet errors and risk, and tracking changes over time.", "conclusion": "The taxonomy provides a framework for future research on spreadsheet development and error analysis."}}
{"id": "2309.00095", "title": "Experimenting with ChatGPT for Spreadsheet Formula Generation: Evidence of Risk in AI Generated Spreadsheets", "url": "https://arxiv.org/abs/2309.00095", "pdf": "https://arxiv.org/pdf/2309.00095", "abs": "https://arxiv.org/abs/2309.00095", "authors": ["Simon Thorne"], "categories": ["cs.SE"], "comment": "15 Pages", "summary": "Large Language Models (LLM) have become sophisticated enough that complex computer programs can be created through interpretation of plain English sentences and implemented in a variety of modern languages such as Python, Java Script, C++ and Spreadsheets. These tools are powerful and relatively accurate and therefore provide broad access to computer programming regardless of the background or knowledge of the individual using them. This paper presents a series of experiments with ChatGPT to explore the tool's ability to produce valid spreadsheet formulae and related computational outputs in situations where ChatGPT has to deduce, infer and problem solve the answer. The results show that in certain circumstances, ChatGPT can produce correct spreadsheet formulae with correct reasoning, deduction and inference. However, when information is limited, uncertain or the problem is too complex, the accuracy of ChatGPT breaks down as does its ability to reason, infer and deduce. This can also result in false statements and \"hallucinations\" that all subvert the process of creating spreadsheet formulae.", "AI": {"tldr": "This paper evaluates ChatGPT's ability to generate spreadsheet formulas.", "motivation": "To explore the accessibility of computer programming through LLMs for individuals with varying backgrounds.", "method": "Experiments were conducted using ChatGPT to produce spreadsheet formulas and computational outputs, testing its deduction, inference, and problem-solving skills.", "result": "ChatGPT can generate correct spreadsheet formulas with correct reasoning in certain circumstances. However, accuracy decreases with limited or uncertain information, or complex problems, leading to errors and hallucinations.", "conclusion": "ChatGPT's ability to create spreadsheet formulas is limited by information availability and problem complexity, which can result in inaccuracies."}}
{"id": "2308.14784", "title": "Generating tabular datasets under differential privacy", "url": "https://arxiv.org/abs/2308.14784", "pdf": "https://arxiv.org/pdf/2308.14784", "abs": "https://arxiv.org/abs/2308.14784", "authors": ["Gianluca Truda"], "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DB"], "comment": null, "summary": "Machine Learning (ML) is accelerating progress across fields and industries, but relies on accessible and high-quality training data. Some of the most important datasets are found in biomedical and financial domains in the form of spreadsheets and relational databases. But this tabular data is often sensitive in nature. Synthetic data generation offers the potential to unlock sensitive data, but generative models tend to memorise and regurgitate training data, which undermines the privacy goal. To remedy this, researchers have incorporated the mathematical framework of Differential Privacy (DP) into the training process of deep neural networks. But this creates a trade-off between the quality and privacy of the resulting data. Generative Adversarial Networks (GANs) are the dominant paradigm for synthesising tabular data under DP, but suffer from unstable adversarial training and mode collapse, which are exacerbated by the privacy constraints and challenging tabular data modality. This work optimises the quality-privacy trade-off of generative models, producing higher quality tabular datasets with the same privacy guarantees. We implement novel end-to-end models that leverage attention mechanisms to learn reversible tabular representations. We also introduce TableDiffusion, the first differentially-private diffusion model for tabular data synthesis. Our experiments show that TableDiffusion produces higher-fidelity synthetic datasets, avoids the mode collapse problem, and achieves state-of-the-art performance on privatised tabular data synthesis. By implementing TableDiffusion to predict the added noise, we enabled it to bypass the challenges of reconstructing mixed-type tabular data. Overall, the diffusion paradigm proves vastly more data and privacy efficient than the adversarial paradigm, due to augmented re-use of each data batch and a smoother iterative training process.", "AI": {"tldr": "This paper introduces TableDiffusion, a differentially-private diffusion model for synthesizing tabular data, achieving state-of-the-art performance with improved quality and privacy trade-offs compared to GANs.", "motivation": "The need to unlock sensitive tabular data in biomedical and financial domains while preserving privacy, addressing the limitations of GANs under differential privacy.", "method": "Development of novel end-to-end models with attention mechanisms and TableDiffusion, a differentially-private diffusion model for tabular data synthesis.", "result": "TableDiffusion produces higher-fidelity synthetic datasets, avoids mode collapse, and achieves state-of-the-art performance on privatized tabular data synthesis.", "conclusion": "The diffusion paradigm is more data and privacy efficient than the adversarial paradigm for synthesizing tabular data."}}
{"id": "2308.10922", "title": "DataVinci: Learning Syntactic and Semantic String Repairs", "url": "https://arxiv.org/abs/2308.10922", "pdf": "https://arxiv.org/pdf/2308.10922", "abs": "https://arxiv.org/abs/2308.10922", "authors": ["Mukul Singh", "Jos\u00e9 Cambronero", "Sumit Gulwani", "Vu Le", "Carina Negreanu", "Gust Verbruggen"], "categories": ["cs.DB", "cs.AI"], "comment": "13 pages", "summary": "String data is common in real-world datasets: 67.6% of values in a sample of 1.8 million real Excel spreadsheets from the web were represented as text. Systems that successfully clean such string data can have a significant impact on real users. While prior work has explored errors in string data, proposed approaches have often been limited to error detection or require that the user provide annotations, examples, or constraints to fix the errors. Furthermore, these systems have focused independently on syntactic errors or semantic errors in strings, but ignore that strings often contain both syntactic and semantic substrings. We introduce DataVinci, a fully unsupervised string data error detection and repair system. DataVinci learns regular-expression-based patterns that cover a majority of values in a column and reports values that do not satisfy such patterns as data errors. DataVinci can automatically derive edits to the data error based on the majority patterns and constraints learned over other columns without the need for further user interaction. To handle strings with both syntactic and semantic substrings, DataVinci uses an LLM to abstract (and re-concretize) portions of strings that are semantic prior to learning majority patterns and deriving edits. Because not all data can result in majority patterns, DataVinci leverages execution information from an existing program (which reads the target data) to identify and correct data repairs that would not otherwise be identified. DataVinci outperforms 7 baselines on both error detection and repair when evaluated on 4 existing and new benchmarks.", "AI": {"tldr": "DataVinci is a fully unsupervised string data error detection and repair system that learns regular-expression-based patterns and uses an LLM to handle strings with both syntactic and semantic substrings.", "motivation": "Existing systems are limited to error detection or require user input, and they often focus independently on syntactic or semantic errors.", "method": "DataVinci learns regular-expression-based patterns, uses an LLM to abstract semantic substrings, and leverages execution information from an existing program.", "result": "DataVinci outperforms 7 baselines on both error detection and repair on 4 benchmarks.", "conclusion": "DataVinci is an effective unsupervised system for detecting and repairing string data errors, even those with both syntactic and semantic components."}}
{"id": "2308.07357", "title": "Demonstration of CORNET: A System For Learning Spreadsheet Formatting Rules By Example", "url": "https://arxiv.org/abs/2308.07357", "pdf": "https://arxiv.org/pdf/2308.07357", "abs": "https://arxiv.org/abs/2308.07357", "authors": ["Mukul Singh", "Jose Cambronero", "Sumit Gulwani", "Vu Le", "Carina Negreanu", "Gust Verbruggen"], "categories": ["cs.SE", "cs.AI", "cs.DB"], "comment": "4 Pages, VLDB 2023 Demonstration Track", "summary": "Data management and analysis tasks are often carried out using spreadsheet software. A popular feature in most spreadsheet platforms is the ability to define data-dependent formatting rules. These rules can express actions such as \"color red all entries in a column that are negative\" or \"bold all rows not containing error or failure.\" Unfortunately, users who want to exercise this functionality need to manually write these conditional formatting (CF) rules. We introduce CORNET, a system that automatically learns such conditional formatting rules from user examples. CORNET takes inspiration from inductive program synthesis and combines symbolic rule enumeration, based on semi-supervised clustering and iterative decision tree learning, with a neural ranker to produce accurate conditional formatting rules. In this demonstration, we show CORNET in action as a simple add-in to Microsoft Excel. After the user provides one or two formatted cells as examples, CORNET generates formatting rule suggestions for the user to apply to the spreadsheet.", "AI": {"tldr": "CORNET\u662f\u4e00\u4e2a\u81ea\u52a8\u5b66\u4e60\u7535\u5b50\u8868\u683c\u6761\u4ef6\u683c\u5f0f\u89c4\u5219\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u7528\u6237\u63d0\u4f9b\u7684\u4f8b\u5b50\u751f\u6210\u683c\u5f0f\u89c4\u5219\u5efa\u8bae\u3002", "motivation": "\u7528\u6237\u9700\u8981\u624b\u52a8\u7f16\u5199\u6761\u4ef6\u683c\u5f0f\u89c4\u5219\uff0c\u6bd4\u8f83\u9ebb\u70e6\u3002", "method": "\u7ed3\u5408\u4e86\u5f52\u7eb3\u7a0b\u5e8f\u5408\u6210\uff0c\u57fa\u4e8e\u534a\u76d1\u7763\u805a\u7c7b\u548c\u8fed\u4ee3\u51b3\u7b56\u6811\u5b66\u4e60\u7684\u7b26\u53f7\u89c4\u5219\u679a\u4e3e\uff0c\u4ee5\u53ca\u795e\u7ecf\u6392\u5e8f\u5668\u3002", "result": "CORNET\u53ef\u4ee5\u6839\u636e\u7528\u6237\u63d0\u4f9b\u7684\u5c11\u91cf\u4f8b\u5b50\uff0c\u751f\u6210\u51c6\u786e\u7684\u6761\u4ef6\u683c\u5f0f\u89c4\u5219\u3002", "conclusion": "CORNET\u4f5c\u4e3a\u4e00\u4e2a\u7b80\u5355\u7684Microsoft Excel\u63d2\u4ef6\uff0c\u53ef\u4ee5\u5e2e\u52a9\u7528\u6237\u81ea\u52a8\u751f\u6210\u6761\u4ef6\u683c\u5f0f\u89c4\u5219\u3002"}}
{"id": "2307.14565", "title": "Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples", "url": "https://arxiv.org/abs/2307.14565", "pdf": "https://arxiv.org/pdf/2307.14565", "abs": "https://arxiv.org/abs/2307.14565", "authors": ["Peng Li", "Yeye He", "Cong Yan", "Yue Wang", "Surajit Chaudhuri"], "categories": ["cs.DB", "cs.LG"], "comment": "full version of a paper accepted to VLDB 2023", "summary": "Relational tables, where each row corresponds to an entity and each column corresponds to an attribute, have been the standard for tables in relational databases. However, such a standard cannot be taken for granted when dealing with tables \"in the wild\". Our survey of real spreadsheet-tables and web-tables shows that over 30% of such tables do not conform to the relational standard, for which complex table-restructuring transformations are needed before these tables can be queried easily using SQL-based analytics tools. Unfortunately, the required transformations are non-trivial to program, which has become a substantial pain point for technical and non-technical users alike, as evidenced by large numbers of forum questions in places like StackOverflow and Excel/Power-BI/Tableau forums.\n  We develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations (in Python or other languages), to transform non-relational tables into standard relational forms for downstream analytics, obviating the need for users to manually program transformations. We compile an extensive benchmark for this new task, by collecting 244 real test cases from user spreadsheets and online forums. Our evaluation suggests that Auto-Tables can successfully synthesize transformations for over 70% of test cases at interactive speeds, without requiring any input from users, making this an effective tool for both technical and non-technical users to prepare data for analytics.", "AI": {"tldr": "This paper introduces Auto-Tables, a system that automatically transforms non-relational tables into standard relational forms for easier analysis.", "motivation": "Many real-world tables don't conform to the relational standard, requiring complex transformations that are difficult for users to program.", "method": "The system synthesizes multi-step transformation pipelines in Python to restructure tables.", "result": "Auto-Tables successfully synthesizes transformations for over 70% of test cases at interactive speeds.", "conclusion": "Auto-Tables is an effective tool for preparing data for analytics by automatically transforming non-relational tables."}}
{"id": "2309.12317", "title": "Using a Catenary Trajectory to Reduce Wellbore Friction in Horizontal Extended Reach Drilling", "url": "https://arxiv.org/abs/2309.12317", "pdf": "https://arxiv.org/pdf/2309.12317", "abs": "https://arxiv.org/abs/2309.12317", "authors": ["Vu Nguyen"], "categories": ["cs.RO"], "comment": null, "summary": "Wellbore friction is one of the biggest concerns when drilling due to its relation to the total cost. The catenary concept was introduced to reduce wellbore friction, but it requires detailed analyses. This project would fill this gap. A catenary shape is simply the natural shape of a rope, chain, or drill string. The drill string will then hang freely inside the wellbore. Perfectly, there should be no contact between the hole and the string, and thus no friction. Torque and drag should be minimized this way. A case study is introduced to examine the outcome between Catenary Trajectory Design and traditional 2D Arc design. The calculation procedure of Catenary Trajectory and 2D Arc Design can be found in an MS Excel spreadsheet which is easy to use and reliable for designing catenary well trajectories for extended-reach wells.", "AI": {"tldr": "This paper introduces the catenary concept to reduce wellbore friction in drilling, aiming to minimize torque and drag.", "motivation": "Wellbore friction is a significant concern in drilling due to its impact on total cost. The catenary concept offers a potential solution, but requires further analysis.", "method": "The paper uses a case study to compare Catenary Trajectory Design with traditional 2D Arc design. Calculation procedures are provided in an MS Excel spreadsheet.", "result": "The case study examines the outcome of using Catenary Trajectory Design compared to traditional 2D Arc design.", "conclusion": "The MS Excel spreadsheet is easy to use and reliable for designing catenary well trajectories for extended-reach wells."}}
{"id": "2306.12850", "title": "Don't Treat the Symptom, Find the Cause! Efficient Artificial-Intelligence Methods for (Interactive) Debugging", "url": "https://arxiv.org/abs/2306.12850", "pdf": "https://arxiv.org/pdf/2306.12850", "abs": "https://arxiv.org/abs/2306.12850", "authors": ["Patrick Rodler"], "categories": ["cs.AI", "cs.DM", "cs.LO"], "comment": "Habilitation Thesis", "summary": "In the modern world, we are permanently using, leveraging, interacting with, and relying upon systems of ever higher sophistication, ranging from our cars, recommender systems in e-commerce, and networks when we go online, to integrated circuits when using our PCs and smartphones, the power grid to ensure our energy supply, security-critical software when accessing our bank accounts, and spreadsheets for financial planning and decision making. The complexity of these systems coupled with our high dependency on them implies both a non-negligible likelihood of system failures, and a high potential that such failures have significant negative effects on our everyday life. For that reason, it is a vital requirement to keep the harm of emerging failures to a minimum, which means minimizing the system downtime as well as the cost of system repair. This is where model-based diagnosis comes into play.\n  Model-based diagnosis is a principled, domain-independent approach that can be generally applied to troubleshoot systems of a wide variety of types, including all the ones mentioned above, and many more. It exploits and orchestrates i.a. techniques for knowledge representation, automated reasoning, heuristic problem solving, intelligent search, optimization, stochastics, statistics, decision making under uncertainty, machine learning, as well as calculus, combinatorics and set theory to detect, localize, and fix faults in abnormally behaving systems.\n  In this thesis, we will give an introduction to the topic of model-based diagnosis, point out the major challenges in the field, and discuss a selection of approaches from our research addressing these issues.", "AI": {"tldr": "This thesis introduces model-based diagnosis for complex systems, highlighting its importance in minimizing downtime and repair costs.", "motivation": "The increasing complexity and our reliance on sophisticated systems lead to potential failures with significant negative consequences, making it crucial to minimize harm from these failures.", "method": "The thesis introduces model-based diagnosis, a domain-independent approach that uses knowledge representation, automated reasoning, and various other techniques to detect, localize, and fix faults.", "result": "The thesis discusses a selection of research approaches addressing major challenges in model-based diagnosis.", "conclusion": "Model-based diagnosis is vital for troubleshooting systems and minimizing the impact of failures."}}
{"id": "2304.07303", "title": "Smart Metro: Deep Learning Approaches to Forecasting the MRT Line 3 Ridership", "url": "https://arxiv.org/abs/2304.07303", "pdf": "https://arxiv.org/pdf/2304.07303", "abs": "https://arxiv.org/abs/2304.07303", "authors": ["Jayrald Empino", "Jean Allyson Junsay", "Mary Grace Verzon", "Mideth Abisado", "Shekinah Lor Huyo-a", "Gabriel Avelino Sampedro"], "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": null, "summary": "Since its establishment in 1999, the Metro Rail Transit Line 3 (MRT3) has served as a transportation option for numerous passengers in Metro Manila, Philippines. The Philippine government's transportation department records more than a thousand people using the MRT3 daily and forecasting the daily passenger count may be rather challenging. The MRT3's daily ridership fluctuates owing to variables such as holidays, working days, and other unexpected issues. Commuters do not know how many other commuters are on their route on a given day, which may hinder their ability to plan an efficient itinerary. Currently, the DOTr depends on spreadsheets containing historical data, which might be challenging to examine. This study presents a time series prediction of daily traffic to anticipate future attendance at a particular station on specific days.", "AI": {"tldr": "This study focuses on predicting daily traffic in Metro Rail Transit Line 3 (MRT3) in Metro Manila, Philippines, to help commuters plan efficient itineraries.", "motivation": "The MRT3's daily ridership fluctuates, and commuters lack information about passenger volume, hindering travel planning. The transportation department relies on historical data spreadsheets which are hard to analyze.", "method": "The study uses time series prediction.", "result": "A time series prediction of daily traffic to anticipate future attendance at a particular station on specific days.", "conclusion": "The study presents a time series prediction of daily traffic."}}
{"id": "2304.06597", "title": "\"What It Wants Me To Say\": Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models", "url": "https://arxiv.org/abs/2304.06597", "pdf": "https://arxiv.org/pdf/2304.06597", "abs": "https://arxiv.org/abs/2304.06597", "authors": ["Michael Xieyang Liu", "Advait Sarkar", "Carina Negreanu", "Ben Zorn", "Jack Williams", "Neil Toronto", "Andrew D. Gordon"], "categories": ["cs.HC"], "comment": null, "summary": "Code-generating large language models translate natural language into code. However, only a small portion of the infinite space of naturalistic utterances is effective at guiding code generation. For non-expert end-user programmers, learning this is the challenge of abstraction matching. We examine this challenge in the specific context of data analysis in spreadsheets, in a system that maps the users natural language query to Python code using the Codex generator, executes the code, and shows the result. We propose grounded abstraction matching, which bridges the abstraction gap by translating the code back into a systematic and predictable naturalistic utterance. In a between-subjects, think-aloud study (n=24), we compare grounded abstraction matching to an ungrounded alternative based on previously established query framing principles. We find that the grounded approach improves end-users' understanding of the scope and capabilities of the code-generating model, and the kind of language needed to use it effectively.", "AI": {"tldr": "This paper introduces grounded abstraction matching to help non-expert programmers use code-generating models for data analysis in spreadsheets by translating code back into natural language.", "motivation": "Non-expert programmers struggle to effectively use natural language to guide code generation due to the challenge of abstraction matching.", "method": "The authors propose grounded abstraction matching, which translates code back into systematic and predictable natural language. They compare this approach to an ungrounded alternative in a between-subjects study.", "result": "The grounded approach improves end-users' understanding of the code-generating model's scope and capabilities, and the language needed to use it effectively.", "conclusion": "Grounded abstraction matching helps bridge the abstraction gap for non-expert programmers using code-generating models."}}
{"id": "2210.13619", "title": "A Simpler Method for Understanding Emergency Shelter Access Patterns", "url": "https://arxiv.org/abs/2210.13619", "pdf": "https://arxiv.org/pdf/2210.13619", "abs": "https://arxiv.org/abs/2210.13619", "authors": ["Geoffrey G. Messier"], "categories": ["cs.CY"], "comment": null, "summary": "The Simplified Access Metric (SAM) is a new approach for characterizing emergency shelter access patterns as a measure of shelter client vulnerability. The goal of SAM is to provide shelter operators with an intuitive way to understand access patterns that can be implemented by non-technical staff using spreadsheet operations. Client data from a large North American shelter will be used to demonstrate that SAM produces similar results to traditional transitional, episodic and chronic client cluster analysis. Since SAM requires less data than cluster analysis, it is also able to generate a real time picture of how shelter access patterns are affected by external factors. Timelines generated from nine years of shelter client data using SAM demonstrate the impact of Housing First programming and the COVID-19 lockdown on how people access shelter. Finally, SAM allows shelter staff to move beyond assigning transitional, episodic and chronic labels and instead use the \"soft\" output of SAM directly as a measure of vulnerability.", "AI": {"tldr": "SAM: a new, intuitive metric for characterizing emergency shelter access, easily implemented by non-technical staff.", "motivation": "To provide shelter operators with an intuitive way to understand access patterns.", "method": "Client data from a large North American shelter is used to demonstrate that SAM produces similar results to traditional transitional, episodic and chronic client cluster analysis.", "result": "SAM produces similar results to traditional methods, requires less data, and generates a real-time picture of how external factors affect shelter access patterns. Demonstrated impact of Housing First and COVID-19 lockdown.", "conclusion": "SAM allows shelter staff to move beyond assigning labels and use SAM output as a measure of vulnerability."}}
{"id": "2302.05482", "title": "Efficient and Compact Spreadsheet Formula Graphs", "url": "https://arxiv.org/abs/2302.05482", "pdf": "https://arxiv.org/pdf/2302.05482", "abs": "https://arxiv.org/abs/2302.05482", "authors": ["Dixin Tang", "Fanchao Chen", "Christopher De Leon", "Tana Wattanawaroon", "Jeaseok Yun", "Srinivasan Seshadri", "Aditya G. Parameswaran"], "categories": ["cs.DB"], "comment": null, "summary": "Spreadsheets are one of the most popular data analysis tools, wherein users can express computation as formulae alongside data. The ensuing dependencies are tracked as formula graphs. Efficiently querying and maintaining these formula graphs is critical for interactivity across multiple settings. Unfortunately, formula graphs are often large and complex such that querying and maintaining them is time-consuming, reducing interactivity. We propose TACO, a framework for efficiently compressing formula graphs, thereby reducing the time for querying and maintenance. The efficiency of TACO stems from a key spreadsheet property: tabular locality, which means that cells close to each other are likely to have similar formula structures. We leverage four such tabular locality-based patterns and develop algorithms for compressing formula graphs using these patterns, directly querying the compressed graph without decompression, and incrementally maintaining the graph during updates. We integrate TACO into an open-source spreadsheet system and show that TACO can significantly reduce formula graph sizes. For querying formula graphs, the speedups of TACO over a baseline implemented in our framework and a commercial spreadsheet system are up to 34,972x and 632x, respectively.", "AI": {"tldr": "This paper introduces TACO, a framework for efficiently compressing spreadsheet formula graphs by leveraging tabular locality, resulting in significant speedups in querying and maintenance.", "motivation": "Formula graphs in spreadsheets are often large and complex, making querying and maintenance time-consuming and reducing interactivity.", "method": "TACO leverages tabular locality-based patterns to compress formula graphs, allowing direct querying and incremental maintenance without decompression.", "result": "TACO significantly reduces formula graph sizes and achieves speedups of up to 34,972x over a baseline and 632x over a commercial spreadsheet system in querying formula graphs.", "conclusion": "TACO effectively addresses the problem of large and complex formula graphs in spreadsheets by utilizing tabular locality for compression, leading to substantial performance improvements."}}
{"id": "2301.11964", "title": "Adversarial Networks and Machine Learning for File Classification", "url": "https://arxiv.org/abs/2301.11964", "pdf": "https://arxiv.org/pdf/2301.11964", "abs": "https://arxiv.org/abs/2301.11964", "authors": ["Ken St. Germain", "Josh Angichiodo"], "categories": ["cs.LG"], "comment": null, "summary": "Correctly identifying the type of file under examination is a critical part of a forensic investigation. The file type alone suggests the embedded content, such as a picture, video, manuscript, spreadsheet, etc. In cases where a system owner might desire to keep their files inaccessible or file type concealed, we propose using an adversarially-trained machine learning neural network to determine a file's true type even if the extension or file header is obfuscated to complicate its discovery. Our semi-supervised generative adversarial network (SGAN) achieved 97.6% accuracy in classifying files across 11 different types. We also compared our network against a traditional standalone neural network and three other machine learning algorithms. The adversarially-trained network proved to be the most precise file classifier especially in scenarios with few supervised samples available. Our implementation of a file classifier using an SGAN is implemented on GitHub (https://ksaintg.github.io/SGAN-File-Classier).", "AI": {"tldr": "\u4f7f\u7528\u5bf9\u6297\u8bad\u7ec3\u7684\u673a\u5668\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\u6765\u786e\u5b9a\u6587\u4ef6\u7684\u771f\u5b9e\u7c7b\u578b\uff0c\u5373\u4f7f\u6269\u5c55\u540d\u6216\u6587\u4ef6\u5934\u88ab\u6df7\u6dc6\u4ee5\u4f7f\u5176\u53d1\u73b0\u590d\u6742\u5316\u3002", "motivation": "\u6b63\u786e\u8bc6\u522b\u6b63\u5728\u68c0\u67e5\u7684\u6587\u4ef6\u7c7b\u578b\u662f\u6cd5\u533b\u8c03\u67e5\u7684\u5173\u952e\u90e8\u5206\u3002\u6587\u4ef6\u7c7b\u578b\u672c\u8eab\u5c31\u6697\u793a\u4e86\u5d4c\u5165\u7684\u5185\u5bb9\uff0c\u4f8b\u5982\u56fe\u7247\u3001\u89c6\u9891\u3001\u624b\u7a3f\u3001\u7535\u5b50\u8868\u683c\u7b49\u3002\u5728\u7cfb\u7edf\u6240\u6709\u8005\u53ef\u80fd\u5e0c\u671b\u4fdd\u6301\u5176\u6587\u4ef6\u4e0d\u53ef\u8bbf\u95ee\u6216\u6587\u4ef6\u7c7b\u578b\u9690\u85cf\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u534a\u76d1\u7763\u751f\u6210\u5bf9\u6297\u7f51\u7edc (SGAN)", "result": "\u5728 11 \u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u6587\u4ef6\u5206\u7c7b\u4e2d\uff0c\u5b9e\u73b0\u4e86 97.6% \u7684\u51c6\u786e\u7387\u3002\u6211\u4eec\u8fd8\u5c06\u6211\u4eec\u7684\u7f51\u7edc\u4e0e\u4f20\u7edf\u7684\u72ec\u7acb\u795e\u7ecf\u7f51\u7edc\u548c\u5176\u4ed6\u4e09\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7ecf\u8fc7\u5bf9\u6297\u8bad\u7ec3\u7684\u7f51\u7edc\u88ab\u8bc1\u660e\u662f\u6700\u7cbe\u786e\u7684\u6587\u4ef6\u5206\u7c7b\u5668\uff0c\u5c24\u5176\u662f\u5728\u53ef\u7528\u76d1\u7763\u6837\u672c\u5f88\u5c11\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u4f7f\u7528 SGAN \u7684\u6587\u4ef6\u5206\u7c7b\u5668\u5b9e\u73b0\u5728\u53ef\u7528\u76d1\u7763\u6837\u672c\u5f88\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u8bc1\u660e\u662f\u6700\u7cbe\u786e\u7684\u6587\u4ef6\u5206\u7c7b\u5668\u3002"}}
{"id": "2208.06032", "title": "CORNET: Learning Table Formatting Rules By Example", "url": "https://arxiv.org/abs/2208.06032", "pdf": "https://arxiv.org/pdf/2208.06032", "abs": "https://arxiv.org/abs/2208.06032", "authors": ["Mukul Singh", "Jos\u00e9 Cambronero", "Sumit Gulwani", "Vu Le", "Carina Negreanu", "Mohammad Raza", "Gust Verbruggen"], "categories": ["cs.AI", "cs.DB", "cs.SE"], "comment": "12 pages content, 2 pages references", "summary": "Spreadsheets are widely used for table manipulation and presentation. Stylistic formatting of these tables is an important property for both presentation and analysis. As a result, popular spreadsheet software, such as Excel, supports automatically formatting tables based on rules. Unfortunately, writing such formatting rules can be challenging for users as it requires knowledge of the underlying rule language and data logic. We present CORNET, a system that tackles the novel problem of automatically learning such formatting rules from user examples in the form of formatted cells. CORNET takes inspiration from advances in inductive programming and combines symbolic rule enumeration with a neural ranker to learn conditional formatting rules. To motivate and evaluate our approach, we extracted tables with over 450K unique formatting rules from a corpus of over 1.8M real worksheets. Since we are the first to introduce conditional formatting, we compare CORNET to a wide range of symbolic and neural baselines adapted from related domains. Our results show that CORNET accurately learns rules across varying evaluation setups. Additionally, we show that CORNET finds shorter rules than those that a user has written and discovers rules in spreadsheets that users have manually formatted.", "AI": {"tldr": "CORNET is a system that learns spreadsheet formatting rules from examples.", "motivation": "Writing formatting rules in spreadsheets is challenging for users.", "method": "CORNET combines symbolic rule enumeration with a neural ranker.", "result": "CORNET accurately learns rules and finds shorter rules than users.", "conclusion": "CORNET can automatically learn formatting rules from user examples."}}
{"id": "2211.04128", "title": "Active Learning with Tabular Language Models", "url": "https://arxiv.org/abs/2211.04128", "pdf": "https://arxiv.org/pdf/2211.04128", "abs": "https://arxiv.org/abs/2211.04128", "authors": ["Martin Ringsquandl", "Aneta Koleva"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "Despite recent advancements in tabular language model research, real-world applications are still challenging. In industry, there is an abundance of tables found in spreadsheets, but acquisition of substantial amounts of labels is expensive, since only experts can annotate the often highly technical and domain-specific tables. Active learning could potentially reduce labeling costs, however, so far there are no works related to active learning in conjunction with tabular language models. In this paper we investigate different acquisition functions in a real-world industrial tabular language model use case for sub-cell named entity recognition. Our results show that cell-level acquisition functions with built-in diversity can significantly reduce the labeling effort, while enforced table diversity is detrimental. We further see open fundamental questions concerning computational efficiency and the perspective of human annotators.", "AI": {"tldr": "This paper explores active learning strategies to reduce labeling costs for tabular language models in industrial settings, specifically for sub-cell named entity recognition.", "motivation": "Reducing the high labeling costs associated with technical, domain-specific tables in industrial spreadsheets.", "method": "Investigates different acquisition functions in a real-world industrial tabular language model use case.", "result": "Cell-level acquisition functions with built-in diversity reduce labeling effort, while enforced table diversity is detrimental.", "conclusion": "Identifies open questions regarding computational efficiency and the human annotator perspective."}}
{"id": "2211.06333", "title": "Excel Spreadsheet Analyzer", "url": "https://arxiv.org/abs/2211.06333", "pdf": "https://arxiv.org/pdf/2211.06333", "abs": "https://arxiv.org/abs/2211.06333", "authors": ["Amir Nassereldine", "Patrick Chen", "Jinjun Xiong"], "categories": ["cs.SE", "cs.PL"], "comment": "10 pages, 9 figures", "summary": "Spreadsheets are widely used in various fields to do large numerical analysis. While several companies have relied on spreadsheets for decades, data scientists are going in the direction of using scientific programming languages such as python to do their data analysis due to the support, community, and vast amount of libraries. While using python to analyze a company's spreadsheets, some information such as the formulas and dependencies of a cell are lost. We propose a tool that creates an abstract intermediate representation (AIR) of a spreadsheet. This representation facilitates the transfer from spreadsheets into scientific programming languages while preserving inter-dependency information about data. In addition to that, we build a python library on top of our tool to perform some data analysis in python.", "AI": {"tldr": "This paper introduces a tool for converting spreadsheets into an abstract intermediate representation (AIR) that preserves cell dependencies, enabling data scientists to analyze spreadsheet data in Python while retaining crucial information.", "motivation": "Data scientists are increasingly using Python for data analysis, but information like formulas and dependencies are lost when converting spreadsheets to Python. The paper aims to bridge this gap.", "method": "The authors propose a tool that creates an abstract intermediate representation (AIR) of a spreadsheet to preserve inter-dependency information about data. They also build a Python library on top of the tool.", "result": "The tool facilitates the transfer from spreadsheets into scientific programming languages while preserving inter-dependency information about data. A python library is built on top of the tool to perform some data analysis in python.", "conclusion": "The proposed tool and Python library enable data scientists to effectively analyze spreadsheet data in Python while retaining important dependency information, addressing the limitations of direct spreadsheet-to-Python conversion."}}
{"id": "2210.09928", "title": "Team OS's System for Dialogue Robot Competition 2022", "url": "https://arxiv.org/abs/2210.09928", "pdf": "https://arxiv.org/pdf/2210.09928", "abs": "https://arxiv.org/abs/2210.09928", "authors": ["Yuki Kubo", "Ryo Yanagimoto", "Hayato Futase", "Mikio Nakano", "Zhaojie Luo", "Kazunori Komatani"], "categories": ["cs.HC"], "comment": "This paper is part of the proceedings of the Dialogue Robot Competition 2022", "summary": "This paper describes our dialogue robot system, OSbot, developed for Dialogue Robot Competition 2022. The dialogue flow is based on state transitions described manually and the transition conditions use the results of keyword extraction and sentiment analysis. The transitions can be easily viewed and edited by managing them on a spreadsheet. The keyword extraction is based on named entity extraction and our predefined keyword set. The sentiment analysis is text-based and uses SVM, which was trained with the multimodal dialogue corpus Hazumi. We quickly checked and edited a dialogue flow by using a logging function. In the competition's preliminary round, our system ended up in third place.", "AI": {"tldr": "OSbot, a dialogue robot, uses state transitions based on keyword extraction and sentiment analysis for dialogue flow, managed via spreadsheet.", "motivation": "To develop a dialogue robot system for the Dialogue Robot Competition 2022.", "method": "Manually defined state transitions with keyword extraction (named entity extraction and predefined keywords) and sentiment analysis (text-based SVM trained on Hazumi corpus). Dialogue flow is managed on a spreadsheet. A logging function was used for quick checking and editing.", "result": "The system achieved third place in the competition's preliminary round.", "conclusion": "The dialogue robot system, OSbot, utilizes a combination of state transitions, keyword extraction, and sentiment analysis, managed through a spreadsheet interface, to achieve a competitive performance in the Dialogue Robot Competition 2022."}}
{"id": "2210.09162", "title": "Table-To-Text generation and pre-training with TabT5", "url": "https://arxiv.org/abs/2210.09162", "pdf": "https://arxiv.org/pdf/2210.09162", "abs": "https://arxiv.org/abs/2210.09162", "authors": ["Ewa Andrejczuk", "Julian Martin Eisenschlos", "Francesco Piccinno", "Syrine Krichene", "Yasemin Altun"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to Findings of EMNLP 2022", "summary": "Encoder-only transformer models have been successfully applied to different table understanding tasks, as in TAPAS (Herzig et al., 2020). A major limitation of these architectures is that they are constrained to classification-like tasks such as cell selection or entailment detection. We present TABT5, an encoder-decoder model that generates natural language text based on tables and textual inputs. TABT5 overcomes the encoder-only limitation by incorporating a decoder component and leverages the input structure with table specific embeddings and pre-training. TABT5 achieves new state-of-the-art results on several domains, including spreadsheet formula prediction with a 15% increase in sequence accuracy, QA with a 2.5% increase in sequence accuracy and data-to-text generation with a 2.5% increase in BLEU.", "AI": {"tldr": "TABT5 is an encoder-decoder model for table understanding that generates natural language text.", "motivation": "Encoder-only models are limited to classification tasks. TABT5 overcomes this limitation by incorporating a decoder component.", "method": "The model leverages the input structure with table specific embeddings and pre-training.", "result": "TABT5 achieves state-of-the-art results on spreadsheet formula prediction, QA, and data-to-text generation.", "conclusion": "TABT5 overcomes the encoder-only limitation and achieves new state-of-the-art results."}}
{"id": "2208.06213", "title": "What is it like to program with artificial intelligence?", "url": "https://arxiv.org/abs/2208.06213", "pdf": "https://arxiv.org/pdf/2208.06213", "abs": "https://arxiv.org/abs/2208.06213", "authors": ["Advait Sarkar", "Andrew D. Gordon", "Carina Negreanu", "Christian Poelitz", "Sruti Srinivasa Ragavan", "Ben Zorn"], "categories": ["cs.HC", "cs.AI", "cs.PL"], "comment": "Proceedings of the 33rd Annual Conference of the Psychology of Programming Interest Group (PPIG 2022)", "summary": "Large language models, such as OpenAI's codex and Deepmind's AlphaCode, can generate code to solve a variety of problems expressed in natural language. This technology has already been commercialised in at least one widely-used programming editor extension: GitHub Copilot.\n  In this paper, we explore how programming with large language models (LLM-assisted programming) is similar to, and differs from, prior conceptualisations of programmer assistance. We draw upon publicly available experience reports of LLM-assisted programming, as well as prior usability and design studies. We find that while LLM-assisted programming shares some properties of compilation, pair programming, and programming via search and reuse, there are fundamental differences both in the technical possibilities as well as the practical experience. Thus, LLM-assisted programming ought to be viewed as a new way of programming with its own distinct properties and challenges.\n  Finally, we draw upon observations from a user study in which non-expert end user programmers use LLM-assisted tools for solving data tasks in spreadsheets. We discuss the issues that might arise, and open research challenges, in applying large language models to end-user programming, particularly with users who have little or no programming expertise.", "AI": {"tldr": "This paper analyzes the similarities and differences between LLM-assisted programming and previous programmer assistance methods, and discusses the challenges of applying LLMs to end-user programming.", "motivation": "Explore the characteristics of LLM-assisted programming and its differences from previous programming assistance methods.", "method": "Analyze publicly available experience reports and user studies of LLM-assisted programming.", "result": "LLM-assisted programming is a new way of programming with its own distinct properties and challenges. Issues and open research challenges in applying LLMs to end-user programming are discussed.", "conclusion": "LLM-assisted programming should be viewed as a new way of programming, especially for non-expert end-user programmers."}}
{"id": "2209.14812", "title": "Named Entity Recognition in Industrial Tables using Tabular Language Models", "url": "https://arxiv.org/abs/2209.14812", "pdf": "https://arxiv.org/pdf/2209.14812", "abs": "https://arxiv.org/abs/2209.14812", "authors": ["Aneta Koleva", "Martin Ringsquandl", "Mark Buckley", "Rakebul Hasan", "Volker Tresp"], "categories": ["cs.AI", "cs.CL"], "comment": "EMNLP 2022 Industry Track", "summary": "Specialized transformer-based models for encoding tabular data have gained interest in academia. Although tabular data is omnipresent in industry, applications of table transformers are still missing. In this paper, we study how these models can be applied to an industrial Named Entity Recognition (NER) problem where the entities are mentioned in tabular-structured spreadsheets. The highly technical nature of spreadsheets as well as the lack of labeled data present major challenges for fine-tuning transformer-based models. Therefore, we develop a dedicated table data augmentation strategy based on available domain-specific knowledge graphs. We show that this boosts performance in our low-resource scenario considerably. Further, we investigate the benefits of tabular structure as inductive bias compared to tables as linearized sequences. Our experiments confirm that a table transformer outperforms other baselines and that its tabular inductive bias is vital for convergence of transformer-based models.", "AI": {"tldr": "This paper explores the application of table transformers to industrial NER on spreadsheets, using data augmentation to overcome challenges.", "motivation": "Applying table transformers to industrial tabular data (spreadsheets) for NER, addressing the gap between academic interest and real-world applications.", "method": "Developing a table data augmentation strategy based on domain-specific knowledge graphs and comparing table transformers with linearized sequence approaches.", "result": "The table transformer outperforms baselines, and its tabular inductive bias is crucial for convergence, especially in low-resource scenarios.", "conclusion": "Table transformers are effective for industrial NER tasks on spreadsheets, with tabular inductive bias and data augmentation being key to success."}}
{"id": "2209.12560", "title": "Hazard Analysis of Collaborative Automation Systems: A Two-layer Approach based on Supervisory Control and Simulation", "url": "https://arxiv.org/abs/2209.12560", "pdf": "https://arxiv.org/pdf/2209.12560", "abs": "https://arxiv.org/abs/2209.12560", "authors": ["Tom P. Huck", "Yuvaraj Selvaraj", "Constantin Cronrath", "Christoph Ledermann", "Martin Fabian", "Bengt Lennartson", "Torsten Kr\u00f6ger"], "categories": ["cs.RO"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Safety critical systems are typically subjected to hazard analysis before commissioning to identify and analyse potentially hazardous system states that may arise during operation. Currently, hazard analysis is mainly based on human reasoning, past experiences, and simple tools such as checklists and spreadsheets. Increasing system complexity makes such approaches decreasingly suitable. Furthermore, testing-based hazard analysis is often not suitable due to high costs or dangers of physical faults. A remedy for this are model-based hazard analysis methods, which either rely on formal models or on simulation models, each with their own benefits and drawbacks. This paper proposes a two-layer approach that combines the benefits of exhaustive analysis using formal methods with detailed analysis using simulation. Unsafe behaviours that lead to unsafe states are first synthesised from a formal model of the system using Supervisory Control Theory. The result is then input to the simulation where detailed analyses using domain-specific risk metrics are performed. Though the presented approach is generally applicable, this paper demonstrates the benefits of the approach on an industrial human-robot collaboration system.", "AI": {"tldr": "This paper introduces a two-layer model-based hazard analysis method that combines formal methods and simulation to identify and analyze potential hazards in safety-critical systems, particularly human-robot collaboration systems.", "motivation": "Current hazard analysis methods are insufficient for increasingly complex systems, and testing-based methods are costly or dangerous.", "method": "A two-layer approach is proposed: first, unsafe behaviors are synthesized from a formal model using Supervisory Control Theory; second, detailed analyses are performed in simulation using domain-specific risk metrics.", "result": "The benefits of the approach are demonstrated on an industrial human-robot collaboration system.", "conclusion": "The proposed two-layer approach effectively combines the benefits of formal methods and simulation for hazard analysis in complex systems."}}
{"id": "2208.04738", "title": "Long-Term Mentoring for Computer Science Researchers", "url": "https://arxiv.org/abs/2208.04738", "pdf": "https://arxiv.org/pdf/2208.04738", "abs": "https://arxiv.org/abs/2208.04738", "authors": ["Emily Ruppel", "Sihang Liu", "Elba Garza", "Sukyoung Ryu", "Alexandra Silva", "Talia Ringer"], "categories": ["cs.CY", "cs.GL", "cs.PL"], "comment": null, "summary": "Early in the pandemic, we -- leaders in the research areas of programming languages (PL) and computer architecture (CA) -- realized that we had a problem: the only way to form new lasting connections in the community was to already have lasting connections in the community. Both of our academic communities had wonderful short-term mentoring programs to address this problem, but it was clear that we needed long-term mentoring programs.\n  Those of us in CA approached this scientifically, making an evidence-backed case for community-wide long-term mentoring. In the meantime, one of us in PL had impulsively launched an unofficial long-term mentoring program, founded on chaos and spreadsheets. In January 2021, the latter grew to an official cross-institutional long-term mentoring program called SIGPLAN-M; in January 2022, the former grew to Computer Architecture Long-term Mentoring (CALM).\n  The impacts have been strong: SIGPLAN-M reaches 328 mentees and 234 mentors across 41 countries, and mentees have described it as \"life changing\" and \"a career saver.\" And while CALM is in its pilot phase -- with 13 mentors and 21 mentees across 7 countries -- it has received very positive feedback. The leaders of SIGPLAN-M and CALM shared our designs, impacts, and challenges along the way. Now, we wish to share those with you. We hope this will kick-start a larger long-term mentoring effort across all of computer science.", "AI": {"tldr": "The authors shared their experience on building the long-term mentoring program in PL and CA, namely SIGPLAN-M and CALM.", "motivation": "To address the problem that the only way to form new lasting connections in the community was to already have lasting connections in the community.", "method": "The authors shared their designs, impacts, and challenges along the way.", "result": "SIGPLAN-M reaches 328 mentees and 234 mentors across 41 countries, and CALM has 13 mentors and 21 mentees across 7 countries, both programs received positive feedback.", "conclusion": "The authors hope this will kick-start a larger long-term mentoring effort across all of computer science."}}
{"id": "2209.05739", "title": "MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization", "url": "https://arxiv.org/abs/2209.05739", "pdf": "https://arxiv.org/pdf/2209.05739", "abs": "https://arxiv.org/abs/2209.05739", "authors": ["Lu Ying", "Xinhuan Shu", "Dazhen Deng", "Yuchen Yang", "Tan Tang", "Lingyun Yu", "Yingcai Wu"], "categories": ["cs.HC"], "comment": null, "summary": "Glyph-based visualization achieves an impressive graphic design when associated with comprehensive visual metaphors, which help audiences effectively grasp the conveyed information through revealing data semantics. However, creating such metaphoric glyph-based visualization (MGV) is not an easy task, as it requires not only a deep understanding of data but also professional design skills. This paper proposes MetaGlyph, an automatic system for generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct a qualitative analysis to understand the design of current MGVs from the perspectives of metaphor embodiment and glyph design. Based on the results, we introduce a novel framework for generating MGVs by metaphoric image selection and an MGV construction. Specifically, MetaGlyph automatically selects metaphors with corresponding images from online resources based on the input data semantics. We then integrate a Monte Carlo tree search algorithm that explores the design of an MGV by associating visual elements with data dimensions given the data importance, semantic relevance, and glyph non-overlap. The system also provides editing feedback that allows users to customize the MGVs according to their design preferences. We demonstrate the use of MetaGlyph through a set of examples, one usage scenario, and validate its effectiveness through a series of expert interviews.", "AI": {"tldr": "MetaGlyph is an automatic system for generating metaphoric glyph-based visualizations (MGVs) from spreadsheets.", "motivation": "Creating metaphoric glyph-based visualization (MGV) is not an easy task, as it requires not only a deep understanding of data but also professional design skills.", "method": "The system selects metaphors with corresponding images from online resources and uses a Monte Carlo tree search algorithm to explore the design of an MGV by associating visual elements with data dimensions given the data importance, semantic relevance, and glyph non-overlap.", "result": "The system provides editing feedback that allows users to customize the MGVs according to their design preferences. The effectiveness of MetaGlyph is validated through a series of expert interviews.", "conclusion": "MetaGlyph, an automatic system for generating MGVs from a spreadsheet is proposed."}}
{"id": "2204.03128", "title": "Sigma Workbook: A Spreadsheet for Cloud Data Warehouses", "url": "https://arxiv.org/abs/2204.03128", "pdf": "https://arxiv.org/pdf/2204.03128", "abs": "https://arxiv.org/abs/2204.03128", "authors": ["James Gale", "Max Seiden", "Deepanshu Utkarsh", "Jason Frantz", "Rob Woollen", "\u00c7a\u011fatay Demiralp"], "categories": ["cs.DB", "cs.HC"], "comment": "VLDB'22 Demonstrations", "summary": "Cloud data warehouses (CDWs) bring large-scale data and compute power closer to users in enterprises. However, existing tools for analyzing data in CDWs are either limited in ad-hoc transformations or difficult to use for business users. Here we introduce Sigma Workbook, a new interactive system that enables business users to easily perform a visual analysis of data in CDWs at scale. For this, Sigma Workbook provides an accessible spreadsheet-like interface for analysis through direct manipulation. Sigma Workbook dynamically constructs matching SQL queries from user interactions, building on the versatility and expressivity of SQL. Constructed queries are directly executed on CDWs, leveraging the superior characteristics of the new generation CDWs, including scalability. We demonstrate Sigma Workbook through 3 real-life use cases -- cohort analysis, sessionization, and data augmentation -- and underline Workbook's ease of use, scalability, and expressivity.", "AI": {"tldr": "Sigma Workbook is a new interactive system that enables business users to easily perform a visual analysis of data in CDWs at scale.", "motivation": "Existing tools for analyzing data in CDWs are either limited in ad-hoc transformations or difficult to use for business users.", "method": "Sigma Workbook provides an accessible spreadsheet-like interface for analysis through direct manipulation and dynamically constructs matching SQL queries from user interactions.", "result": "Demonstrated Sigma Workbook through 3 real-life use cases -- cohort analysis, sessionization, and data augmentation -- and underline Workbook's ease of use, scalability, and expressivity.", "conclusion": "Sigma Workbook enables business users to easily perform a visual analysis of data in CDWs at scale."}}
{"id": "2204.00598", "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language", "url": "https://arxiv.org/abs/2204.00598", "pdf": "https://arxiv.org/pdf/2204.00598", "abs": "https://arxiv.org/abs/2204.00598", "authors": ["Andy Zeng", "Maria Attarian", "Brian Ichter", "Krzysztof Choromanski", "Adrian Wong", "Stefan Welker", "Federico Tombari", "Aveek Purohit", "Michael Ryoo", "Vikas Sindhwani", "Johnny Lee", "Vincent Vanhoucke", "Pete Florence"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "https://socraticmodels.github.io/", "summary": "Large pretrained (e.g., \"foundation\") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.", "AI": {"tldr": "Socratic Models (SMs) leverage multiple pretrained models to achieve new multimodal capabilities without finetuning.", "motivation": "Large pretrained models have different capabilities depending on the domain of data they are trained on, and these domains may only barely overlap.", "method": "A modular framework in which multiple pretrained models are composed zero-shot via multimodal-informed prompting to exchange information with each other.", "result": "SMs are competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, and enable new applications such as answering free-form questions about egocentric video, engaging in multimodal assistive dialogue, and robot perception and planning.", "conclusion": "Diversity in pretrained models is symbiotic and can be leveraged through Socratic Models."}}
{"id": "2201.09745", "title": "Table Pre-training: A Survey on Model Architectures, Pre-training Objectives, and Downstream Tasks", "url": "https://arxiv.org/abs/2201.09745", "pdf": "https://arxiv.org/pdf/2201.09745", "abs": "https://arxiv.org/abs/2201.09745", "authors": ["Haoyu Dong", "Zhoujun Cheng", "Xinyi He", "Mengyu Zhou", "Anda Zhou", "Fan Zhou", "Ao Liu", "Shi Han", "Dongmei Zhang"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted by IJCAI'2022 survey track", "summary": "Since a vast number of tables can be easily collected from web pages, spreadsheets, PDFs, and various other document types, a flurry of table pre-training frameworks have been proposed following the success of text and images, and they have achieved new state-of-the-arts on various tasks such as table question answering, table type recognition, column relation classification, table search, formula prediction, etc. To fully use the supervision signals in unlabeled tables, a variety of pre-training objectives have been designed and evaluated, for example, denoising cell values, predicting numerical relationships, and implicitly executing SQLs. And to best leverage the characteristics of (semi-)structured tables, various tabular language models, particularly with specially-designed attention mechanisms, have been explored. Since tables usually appear and interact with free-form text, table pre-training usually takes the form of table-text joint pre-training, which attracts significant research interests from multiple domains. This survey aims to provide a comprehensive review of different model designs, pre-training objectives, and downstream tasks for table pre-training, and we further share our thoughts and vision on existing challenges and future opportunities.", "AI": {"tldr": "\u7efc\u8ff0\u8bba\u6587\uff0c\u6982\u8ff0\u4e86\u8868\u683c\u9884\u8bad\u7ec3\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5305\u62ec\u6a21\u578b\u8bbe\u8ba1\u3001\u9884\u8bad\u7ec3\u76ee\u6807\u548c\u4e0b\u6e38\u4efb\u52a1\u3002", "motivation": "\u8868\u683c\u53ef\u4ee5\u4ece\u7f51\u9875\u3001\u7535\u5b50\u8868\u683c\u548cPDF\u7b49\u591a\u79cd\u6765\u6e90\u8f7b\u677e\u6536\u96c6\uff0c\u4e3a\u4e86\u5145\u5206\u5229\u7528\u65e0\u6807\u7b7e\u8868\u683c\u4e2d\u7684\u76d1\u7763\u4fe1\u53f7\u3002", "method": "\u56de\u987e\u4e0d\u540c\u7684\u6a21\u578b\u8bbe\u8ba1\uff0c\u9884\u8bad\u7ec3\u76ee\u6807\u548c\u4e0b\u6e38\u4efb\u52a1\u3002", "result": "\u8868\u683c\u9884\u8bad\u7ec3\u5728\u8868\u683c\u95ee\u7b54\u3001\u8868\u683c\u7c7b\u578b\u8bc6\u522b\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684state-of-the-art\u3002", "conclusion": "\u603b\u7ed3\u4e86\u8868\u683c\u9884\u8bad\u7ec3\u7684\u73b0\u6709\u6311\u6218\u548c\u672a\u6765\u673a\u9047\u3002"}}
{"id": "2109.07323", "title": "FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining", "url": "https://arxiv.org/abs/2109.07323", "pdf": "https://arxiv.org/pdf/2109.07323", "abs": "https://arxiv.org/abs/2109.07323", "authors": ["Zhoujun Cheng", "Haoyu Dong", "Ran Jia", "Pengfei Wu", "Shi Han", "Fan Cheng", "Dongmei Zhang"], "categories": ["cs.IR", "cs.LG"], "comment": "Accepted by ACL'22 main track", "summary": "Tables store rich numerical data, but numerical reasoning over tables is still a challenge. In this paper, we find that the spreadsheet formula, which performs calculations on numerical values in tables, is naturally a strong supervision of numerical reasoning. More importantly, large amounts of spreadsheets with expert-made formulae are available on the web and can be obtained easily. FORTAP is the first method for numerical-reasoning-aware table pretraining by leveraging large corpus of spreadsheet formulae. We design two formula pretraining tasks to explicitly guide FORTAP to learn numerical reference and calculation in semi-structured tables. FORTAP achieves state-of-the-art results on two representative downstream tasks, cell type classification and formula prediction, showing great potential of numerical-reasoning-aware pretraining.", "AI": {"tldr": "FORTAP\u4f7f\u7528\u7535\u5b50\u8868\u683c\u516c\u5f0f\u8fdb\u884c\u6570\u503c\u63a8\u7406\u611f\u77e5\u8868\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u5355\u5143\u683c\u7c7b\u578b\u5206\u7c7b\u548c\u516c\u5f0f\u9884\u6d4b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u8868\u683c\u5b58\u50a8\u4e86\u4e30\u5bcc\u7684\u6570\u503c\u6570\u636e\uff0c\u4f46\u5bf9\u8868\u683c\u8fdb\u884c\u6570\u503c\u63a8\u7406\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u7535\u5b50\u8868\u683c\u516c\u5f0f\u662f\u5bf9\u8868\u683c\u4e2d\u6570\u503c\u8fdb\u884c\u8ba1\u7b97\u7684\u5f3a\u76d1\u7763\uff0c\u4e14\u7f51\u4e0a\u6709\u5927\u91cf\u5e26\u6709\u4e13\u5bb6\u5236\u4f5c\u516c\u5f0f\u7684\u7535\u5b50\u8868\u683c\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u516c\u5f0f\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u4ee5\u663e\u5f0f\u5730\u6307\u5bfcFORTAP\u5b66\u4e60\u534a\u7ed3\u6784\u5316\u8868\u683c\u4e2d\u7684\u6570\u503c\u53c2\u8003\u548c\u8ba1\u7b97\u3002", "result": "FORTAP\u5728\u5355\u5143\u683c\u7c7b\u578b\u5206\u7c7b\u548c\u516c\u5f0f\u9884\u6d4b\u8fd9\u4e24\u4e2a\u5177\u6709\u4ee3\u8868\u6027\u7684\u4e0b\u6e38\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u6570\u503c\u63a8\u7406\u611f\u77e5\u9884\u8bad\u7ec3\u5177\u6709\u5f88\u5927\u7684\u6f5c\u529b\u3002"}}
{"id": "2202.13189", "title": "Efficient Specialized Spreadsheet Parsing for Data Science", "url": "https://arxiv.org/abs/2202.13189", "pdf": "https://arxiv.org/pdf/2202.13189", "abs": "https://arxiv.org/abs/2202.13189", "authors": ["Felix Henze", "Haralampos Gavriilidis", "Eleni Tzirita Zacharatou", "Volker Markl"], "categories": ["cs.DB"], "comment": "Accepted at the 24th International Workshop on Design, Optimization, Languages and Analytical Processing of Big Data (DOLAP 2022), March 29, 2022, Edinburgh, UK", "summary": "Spreadsheets are widely used for data exploration. Since spreadsheet systems have limited capabilities, users often need to load spreadsheets to other data science environments to perform advanced analytics. However, current approaches for spreadsheet loading suffer from either high runtime or memory usage, which hinders data exploration on commodity systems. To make spreasheet loading practical on commodity systems, we introduce a novel parser that minimizes memory usage by tightly coupling decompression and parsing. Furthermore, to reduce the runtime, we introduce optimized spreadsheet-specific parsing routines and employ parallelism. To evaluate our approach, we implement a prototype for loading Excel spreadsheets into R environments. Our evaluation shows that our novel approach is up to 3x faster while consuming up to 40x less memory than state-of-the-art approaches.", "AI": {"tldr": "This paper introduces a novel parser for efficiently loading spreadsheets into data science environments like R, addressing limitations of existing methods in terms of speed and memory usage.", "motivation": "Existing spreadsheet loading methods are inefficient in terms of runtime and memory usage, hindering data exploration on commodity systems.", "method": "The paper proposes a new parser that minimizes memory usage by tightly coupling decompression and parsing. It also introduces optimized spreadsheet-specific parsing routines and parallelism to reduce runtime.", "result": "The proposed approach achieves up to 3x faster loading times and consumes up to 40x less memory compared to state-of-the-art methods when loading Excel spreadsheets into R.", "conclusion": "The novel parser makes spreadsheet loading more practical on commodity systems by significantly improving speed and memory efficiency."}}
{"id": "2203.16346", "title": "Enhanced Spreadsheet Computing with Finite-Domain Constraint Satisfaction", "url": "https://arxiv.org/abs/2203.16346", "pdf": "https://arxiv.org/pdf/2203.16346", "abs": "https://arxiv.org/abs/2203.16346", "authors": ["Ezana N. Beyenne", "Hai-Feng Guo"], "categories": ["cs.PL", "cs.AI"], "comment": null, "summary": "The spreadsheet application is among the most widely used computing tools in modern society. It provides excellent usability and usefulness, and it easily enables a non-programmer to perform programming-like tasks in a visual tabular \"pen and paper\" approach. However, spreadsheets are mostly limited to bookkeeping-like applications due to their mono-directional data flow. This paper shows how the spreadsheet computing paradigm is extended to break this limitation for solving constraint satisfaction problems. We present an enhanced spreadsheet system where finite-domain constraint solving is well supported in a visual environment. Furthermore, a spreadsheet-specific constraint language is constructed for general users to specify constraints among data cells in a declarative and scalable way. The new spreadsheet system significantly simplifies the development of many constraint-based applications using a visual tabular interface. Examples are given to illustrate the usability and usefulness of the extended spreadsheet paradigm.\n  KEYWORDS: Spreadsheet computing, Finite-domain constraint satisfaction, Constraint logic programming", "AI": {"tldr": "This paper introduces an enhanced spreadsheet system that supports finite-domain constraint solving in a visual environment, breaking the mono-directional data flow limitation of traditional spreadsheets.", "motivation": "Traditional spreadsheets are limited to bookkeeping-like applications due to their mono-directional data flow. This paper aims to extend the spreadsheet computing paradigm to solve constraint satisfaction problems.", "method": "The paper presents an enhanced spreadsheet system with visual support for finite-domain constraint solving. A spreadsheet-specific constraint language is constructed for users to specify constraints among data cells.", "result": "The new spreadsheet system simplifies the development of constraint-based applications using a visual tabular interface.", "conclusion": "The extended spreadsheet paradigm enhances usability and usefulness, as demonstrated by examples."}}
{"id": "2203.10944", "title": "Spreadsheet computing with Finite Domain Constraint Enhancements", "url": "https://arxiv.org/abs/2203.10944", "pdf": "https://arxiv.org/pdf/2203.10944", "abs": "https://arxiv.org/abs/2203.10944", "authors": ["Ezana N. Beyenne"], "categories": ["cs.AI"], "comment": "2008 Master's thesis", "summary": "Spreadsheet computing is one of the more popular computing methodologies in today's modern society. The spreadsheet application's ease of use and usefulness has enabled non-programmers to perform programming-like tasks in a familiar setting modeled after the tabular \"pen and paper\" approach. However, spreadsheet applications are limited to bookkeeping-like tasks due to their single-direction data flow. This thesis demonstrates an extension of the spreadsheet computing paradigm in overcoming this limitation to solve constraint satisfaction problems. We present a framework seamlessly incorporating a finite constraint solver with the spreadsheet computing paradigm. This framework allows the individual cells in the spreadsheet to be attached to either a finite domain or a constraint specifying the relationship among the cells. The framework provides an interface for constraint solving and further enhances the spreadsheet computing paradigm by providing a set of spreadsheet-specific constraints that will aid in controlling the scalability of large spreadsheet applications implementations. Finally, we provide examples to demonstrate the usability and usefulness of the extended spreadsheet paradigm.\n  Keywords: Spreadsheet computing, Constraint Logic Programming, Constraint satisfaction, Domain-Specific language, Excel, SWI Prolog, C#", "AI": {"tldr": "This thesis introduces a framework extending spreadsheet computing with constraint solving capabilities, allowing spreadsheets to solve constraint satisfaction problems.", "motivation": "Spreadsheet applications are limited by their single-direction data flow, restricting them to bookkeeping tasks.", "method": "A framework is presented that integrates a finite constraint solver with the spreadsheet paradigm, enabling cells to be attached to finite domains or constraints.", "result": "The framework enhances spreadsheet computing by providing spreadsheet-specific constraints to control scalability and demonstrates usability through examples.", "conclusion": "The extended spreadsheet paradigm overcomes limitations of traditional spreadsheets by incorporating constraint solving, expanding its applicability."}}
{"id": "2202.00454", "title": "TableQuery: Querying tabular data with natural language", "url": "https://arxiv.org/abs/2202.00454", "pdf": "https://arxiv.org/pdf/2202.00454", "abs": "https://arxiv.org/abs/2202.00454", "authors": ["Abhijith Neil Abraham", "Fariz Rahman", "Damanpreet Kaur"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "11 pages, 1 figures", "summary": "This paper presents TableQuery, a novel tool for querying tabular data using deep learning models pre-trained to answer questions on free text. Existing deep learning methods for question answering on tabular data have various limitations, such as having to feed the entire table as input into a neural network model, making them unsuitable for most real-world applications. Since real-world data might contain millions of rows, it may not entirely fit into the memory. Moreover, data could be stored in live databases, which are updated in real-time, and it is impractical to serialize an entire database to a neural network-friendly format each time it is updated. In TableQuery, we use deep learning models pre-trained for question answering on free text to convert natural language queries to structured queries, which can be run against a database or a spreadsheet. This method eliminates the need for fitting the entire data into memory as well as serializing databases. Furthermore, deep learning models pre-trained for question answering on free text are readily available on platforms such as HuggingFace Model Hub (7). TableQuery does not require re-training; when a newly trained model for question answering with better performance is available, it can replace the existing model in TableQuery.", "AI": {"tldr": "TableQuery\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u67e5\u8be2\uff0c\u4ee5\u89e3\u51b3\u8868\u683c\u6570\u636e\u67e5\u8be2\u95ee\u9898\uff0c\u65e0\u9700\u5c06\u6574\u4e2a\u8868\u683c\u6570\u636e\u52a0\u8f7d\u5230\u5185\u5b58\u4e2d\u3002", "motivation": "\u73b0\u6709\u8868\u683c\u6570\u636e\u95ee\u7b54\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u9700\u8981\u5c06\u6574\u4e2a\u8868\u683c\u4f5c\u4e3a\u8f93\u5165\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002\u5b9e\u9645\u6570\u636e\u53ef\u80fd\u5305\u542b\u6570\u767e\u4e07\u884c\uff0c\u65e0\u6cd5\u5b8c\u5168\u653e\u5165\u5185\u5b58\u3002\u6570\u636e\u5b58\u50a8\u5728\u5b9e\u65f6\u66f4\u65b0\u7684\u6570\u636e\u5e93\u4e2d\uff0c\u6bcf\u6b21\u66f4\u65b0\u90fd\u5c06\u6574\u4e2a\u6570\u636e\u5e93\u5e8f\u5217\u5316\u4e3a\u795e\u7ecf\u7f51\u7edc\u53cb\u597d\u7684\u683c\u5f0f\u662f\u4e0d\u5207\u5b9e\u9645\u7684\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u67e5\u8be2\uff0c\u8fd9\u4e9b\u67e5\u8be2\u53ef\u4ee5\u5728\u6570\u636e\u5e93\u6216\u7535\u5b50\u8868\u683c\u4e2d\u8fd0\u884c\u3002", "result": "TableQuery\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u5f53\u6709\u6027\u80fd\u66f4\u597d\u7684\u65b0\u578b\u95ee\u7b54\u6a21\u578b\u53ef\u7528\u65f6\uff0c\u53ef\u4ee5\u76f4\u63a5\u66ff\u6362TableQuery\u4e2d\u7684\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "TableQuery\u901a\u8fc7\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u67e5\u8be2\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8868\u683c\u6570\u636e\u95ee\u7b54\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u65e0\u9700\u5c06\u6574\u4e2a\u6570\u636e\u52a0\u8f7d\u5230\u5185\u5b58\u4e2d\uff0c\u4e5f\u65e0\u9700\u5e8f\u5217\u5316\u6570\u636e\u5e93\u3002"}}
{"id": "2201.06337", "title": "PoVRPoint: Authoring Presentations in Mobile Virtual Reality", "url": "https://arxiv.org/abs/2201.06337", "pdf": "https://arxiv.org/pdf/2201.06337", "abs": "https://arxiv.org/abs/2201.06337", "authors": ["Verena Biener", "Travis Gesslein", "Daniel Schneider", "Felix Kawala", "Alexander Otte", "Per Ola Kristensson", "Michel Pahud", "Eyal Ofek", "Cuauhtli Campos", "Matja\u017e Kljun", "Klen \u010copi\u010d Pucihar", "Jens Grubert"], "categories": ["cs.HC"], "comment": "IEEE VR 2022; to appear in IEEE transactions on visualization and computer graphics, 2022", "summary": "Virtual Reality (VR) has the potential to support mobile knowledge workers by complementing traditional input devices with a large three-dimensional output space and spatial input. Previous research on supporting VR knowledge work explored domains such as text entry using physical keyboards and spreadsheet interaction using combined pen and touch input. Inspired by such work, this paper probes the VR design space for authoring presentations in mobile settings. We propose PoVRPoint -- a set of tools coupling pen- and touch-based editing of presentations on mobile devices, such as tablets, with the interaction capabilities afforded by VR. We study the utility of extended display space to, for example, assist users in identifying target slides, supporting spatial manipulation of objects on a slide, creating animations, and facilitating arrangements of multiple, possibly occluded, shapes. Among other things, our results indicate that 1) the wide field of view afforded by VR results in significantly faster target slide identification times compared to a tablet-only interface for visually salient targets; and 2) the three-dimensional view in VR enables significantly faster object reordering in the presence of occlusion compared to two baseline interfaces. A user study further confirmed that the interaction techniques were found to be usable and enjoyable.", "AI": {"tldr": "This paper introduces PoVRPoint, a VR-based presentation authoring tool for mobile settings, enhancing pen and touch interactions with VR capabilities.", "motivation": "To explore the potential of VR in supporting mobile knowledge workers by providing a large 3D output space and spatial input for presentation authoring.", "method": "Developed PoVRPoint, a set of tools coupling pen- and touch-based editing on mobile devices with VR interaction capabilities. Conducted a user study to evaluate the utility of extended display space for target slide identification, spatial object manipulation, animation creation, and shape arrangement.", "result": "VR significantly improves target slide identification speed for visually salient targets and object reordering speed in the presence of occlusion compared to tablet-only interfaces. The interaction techniques were found to be usable and enjoyable.", "conclusion": "VR offers a usable and enjoyable way to improve the presentation authoring experience for mobile knowledge workers."}}
{"id": "2201.01654", "title": "TableParser: Automatic Table Parsing with Weak Supervision from Spreadsheets", "url": "https://arxiv.org/abs/2201.01654", "pdf": "https://arxiv.org/pdf/2201.01654", "abs": "https://arxiv.org/abs/2201.01654", "authors": ["Susie Xi Rao", "Johannes Rausch", "Peter Egger", "Ce Zhang"], "categories": ["cs.CV"], "comment": "accepted in the AAAI-22 Workshop on Scientific Document Understanding at the Thirty-Sixth AAAI Conference on Artificial Intelligence (SDU@AAAI-22)", "summary": "Tables have been an ever-existing structure to store data. There exist now different approaches to store tabular data physically. PDFs, images, spreadsheets, and CSVs are leading examples. Being able to parse table structures and extract content bounded by these structures is of high importance in many applications. In this paper, we devise TableParser, a system capable of parsing tables in both native PDFs and scanned images with high precision. We have conducted extensive experiments to show the efficacy of domain adaptation in developing such a tool. Moreover, we create TableAnnotator and ExcelAnnotator, which constitute a spreadsheet-based weak supervision mechanism and a pipeline to enable table parsing. We share these resources with the research community to facilitate further research in this interesting direction.", "AI": {"tldr": "This paper introduces TableParser, a system for parsing tables in PDFs and scanned images.", "motivation": "Parsing table structures and extracting content is important in many applications.", "method": "The authors devise TableParser and use domain adaptation. They also create TableAnnotator and ExcelAnnotator for weak supervision.", "result": "The paper shows the efficacy of domain adaptation in developing TableParser through experiments.", "conclusion": "The authors share resources to facilitate further research in table parsing."}}
{"id": "2201.07696", "title": "Exploring Spreadsheet Use and Practices in a Technologically Constrained Setting", "url": "https://arxiv.org/abs/2201.07696", "pdf": "https://arxiv.org/pdf/2201.07696", "abs": "https://arxiv.org/abs/2201.07696", "authors": ["Khwima Mckinley Mkamanga", "Simon Thorne"], "categories": ["cs.SE"], "comment": "23 pages, 7 colour figures, 17 Tables and a Sample Questionnaire", "summary": "This paper explores the impacts of spreadsheets on business operations in a water utility parastatal in Malawi, Sub-Saharan Africa. The organisation is a typical example of a semi-government body operating in a technologically underdeveloped country. The study focused on spreadsheet scope of use and life cycle as well as organisational policy and governance. The results will help define future spreadsheet usage by influencing new approaches for managing potential risks associated with spreadsheets in the organization. Generally, findings indicate that the proliferation of spreadsheets in the organization has provided an enabling environment for business automation. The paper also highlights management, technological and human factor issues contributing to high risks associated with the pervasive spreadsheet use. The conclusions drawn from the research confirms that there is ample room for improvement in many areas such as implementation of comprehensive policies and regulations governing spreadsheet development processes and adoption.", "AI": {"tldr": "Explores spreadsheet impacts on a Malawi water utility, revealing benefits and risks.", "motivation": "To understand how spreadsheets affect business operations in a technologically underdeveloped African water utility.", "method": "Focuses on spreadsheet scope, lifecycle, organizational policy, and governance within the utility.", "result": "Spreadsheets enable business automation but introduce management, technological, and human-related risks.", "conclusion": "Confirms the need for better policies and regulations for spreadsheet development and use."}}
{"id": "2110.12829", "title": "Spread2RML: Constructing Knowledge Graphs by Predicting RML Mappings on Messy Spreadsheets", "url": "https://arxiv.org/abs/2110.12829", "pdf": "https://arxiv.org/pdf/2110.12829", "abs": "https://arxiv.org/abs/2110.12829", "authors": ["Markus Schr\u00f6der", "Christian Jilek", "Andreas Dengel"], "categories": ["cs.DB"], "comment": "17 pages, 1 figure, 2 tables, accepted at K-CAP 2021", "summary": "The RDF Mapping Language (RML) allows to map semi-structured data to RDF knowledge graphs. Besides CSV, JSON and XML, this also includes the mapping of spreadsheet tables. Since spreadsheets have a complex data model and can become rather messy, their mapping creation tends to be very time consuming. In order to reduce such efforts, this paper presents Spread2RML which predicts RML mappings on messy spreadsheets. This is done with an extensible set of RML object map templates which are applied for each column based on heuristics. In our evaluation, three datasets are used ranging from very messy synthetic data to spreadsheets from data.gov which are less messy. We obtained first promising results especially with regard to our approach being fully automatic and dealing with rather messy data.", "AI": {"tldr": "Spread2RML predicts RML mappings on messy spreadsheets using RML object map templates and heuristics.", "motivation": "Mapping spreadsheets to RDF knowledge graphs is time-consuming due to their complexity. Spread2RML aims to reduce this effort.", "method": "Applies RML object map templates for each column based on heuristics.", "result": "Promising results, especially regarding the automatic processing of messy data.", "conclusion": "Spread2RML can automatically generate RML mappings for messy spreadsheets, achieving promising results."}}
{"id": "2110.11575", "title": "Methodology for Assessing the State of the Practice for Domain X", "url": "https://arxiv.org/abs/2110.11575", "pdf": "https://arxiv.org/pdf/2110.11575", "abs": "https://arxiv.org/abs/2110.11575", "authors": ["Spencer Smith", "Jacques Carette", "Peter Michalski", "Ao Dong", "Olu Owojaiye"], "categories": ["cs.SE"], "comment": "35 pages, 3 figures", "summary": "To improve software development methods and tools for research software, we first need to understand the current state of the practice. Therefore, we have developed a methodology for assessing the state of the software development practices for a given research software domain. For each domain we wish to answer questions such as: i) What artifacts (documents, code, test cases, etc.) are present? ii) What tools are used? iii) What principles, process and methodologies are used? iv) What are the pain points for developers? v) What actions are used to improve qualities like maintainability and reproducibility? To answer these questions, our methodology prescribes the following steps: i) Identify the domain; ii) Identify a list of candidate software packages; iii) Filter the list to a length of about 30 packages; iv) Gather source code and documentation for each package; v) Collect repository related data on each software package, like number of stars, number of open issues, number of lines of code; vi) Fill in the measurement template (the template consists of 108 questions to assess 9 qualities (including the qualities of installability, usability and visibility)); vii) Interview developers (the interview consists of 20 questions and takes about an hour); viii) Rank the software using the Analytic Hierarchy Process (AHP); and, ix) Analyze the data to answer the questions posed above. A domain expert should be engaged throughout the process, to ensure that implicit information about the domain is properly represented and to assist with conducting an analysis of the commonalities and variabilities between the 30 selected packages. Using our methodology, spreadsheet templates and AHP tool, we estimate (based on our experience with using the process) the time to complete an assessment for a given domain at 173 person hours.", "AI": {"tldr": "The paper introduces a methodology for assessing software development practices in research software domains.", "motivation": "Understanding the current state of software development practices for research software.", "method": "A multi-step methodology involving identifying software packages, gathering data, using a measurement template with 108 questions, interviewing developers, ranking software using AHP, and analyzing data.", "result": "Estimated time to complete an assessment for a given domain is 173 person hours.", "conclusion": "The methodology provides a structured way to assess and improve software development practices in research software."}}
{"id": "2110.08993", "title": "Typed Image-based Programming with Structure Editing", "url": "https://arxiv.org/abs/2110.08993", "pdf": "https://arxiv.org/pdf/2110.08993", "abs": "https://arxiv.org/abs/2110.08993", "authors": ["Jonathan Edwards", "Tomas Petricek"], "categories": ["cs.PL", "cs.SE"], "comment": "Accepted to: Human Aspects of Types and Reasoning Assistants (HATRA'21), Oct 19, 2021, Chicago, US", "summary": "Many beloved programming systems are image-based: self-contained worlds that persist both code and data in a single file. Examples include Smalltalk, LISP, HyperCard, Flash, and spreadsheets. Image-based programming avoids much of the complexity of modern programming technology stacks and encourages more casual and exploratory programming. However conventional file-based programming has better support for collaboration and deployment. These problems have been blamed for the limited commercial success of Smalltalk. We propose to enable collaboration in image-based programming via types and structure editing.\n  We focus on the problem of schema change on persistent data. We turn to static types, which paradoxically require more schema change but also provide a mechanism to express and execute those changes. To determine those changes we turn to structure editing, so that we can capture changes in type definitions with sufficient fidelity to automatically adapt the data to suit. We conjecture that typical schema changes can be handled through structure editing of static types.\n  That positions us to tackle collaboration with what could be called version control for structure editing. We present a theory realizing this idea, which is our main technical contribution. While we focus here on editing types, if we can extend the approach to cover the entire programming experience then it would offer a new way to collaborate in image-based programming.", "AI": {"tldr": "This paper introduces a new approach to enable collaboration in image-based programming through types and structure editing.", "motivation": "Image-based programming has advantages of simplicity but lacks support for collaboration and deployment compared to file-based programming.", "method": "The authors propose using static types and structure editing to manage schema changes and enable collaboration.", "result": "The paper presents a theory for version control of structure editing.", "conclusion": "The proposed approach could offer a new way to collaborate in image-based programming if extended to cover the entire programming experience."}}
{"id": "2109.06630", "title": "Detecting Layout Templates in Complex Multiregion Files", "url": "https://arxiv.org/abs/2109.06630", "pdf": "https://arxiv.org/pdf/2109.06630", "abs": "https://arxiv.org/abs/2109.06630", "authors": ["Gerardo Vitagliano", "Lan Jiang", "Felix Naumann"], "categories": ["cs.IR"], "comment": null, "summary": "Spreadsheets are among the most commonly used file formats for data management, distribution, and analysis. Their widespread employment makes it easy to gather large collections of data, but their flexible canvas-based structure makes automated analysis difficult without heavy preparation. One of the common problems that practitioners face is the presence of multiple, independent regions in a single spreadsheet, possibly separated by repeated empty cells. We define such files as \"multiregion\" files. In collections of various spreadsheets, we can observe that some share the same layout. We present the Mondrian approach to automatically identify layout templates across multiple files and systematically extract the corresponding regions. Our approach is composed of three phases: first, each file is rendered as an image and inspected for elements that could form regions; then, using a clustering algorithm, the identified elements are grouped to form regions; finally, every file layout is represented as a graph and compared with others to find layout templates. We compare our method to state-of-the-art table recognition algorithms on two corpora of real-world enterprise spreadsheets. Our approach shows the best performances in detecting reliable region boundaries within each file and can correctly identify recurring layouts across files.", "AI": {"tldr": "This paper introduces Mondrian, a new approach to automatically identify layout templates across multiple spreadsheet files and systematically extract the corresponding regions.", "motivation": "The widespread use of spreadsheets for data management leads to challenges in automated analysis due to their flexible structure, especially in multiregion files.", "method": "The Mondrian approach involves rendering files as images, identifying potential region elements, clustering these elements to form regions, and representing layouts as graphs for comparison to find templates.", "result": "The Mondrian approach outperforms state-of-the-art table recognition algorithms in detecting reliable region boundaries and identifying recurring layouts.", "conclusion": "The Mondrian approach effectively addresses the problem of identifying layout templates in multiregion spreadsheet files, improving automated analysis capabilities."}}
{"id": "2109.07267", "title": "JUBILEE: Secure Debt Relief and Forgiveness", "url": "https://arxiv.org/abs/2109.07267", "pdf": "https://arxiv.org/pdf/2109.07267", "abs": "https://arxiv.org/abs/2109.07267", "authors": ["David Cerezo S\u00e1nchez"], "categories": ["cs.CR", "cs.GT", "econ.GN"], "comment": null, "summary": "JUBILEE is a securely computed mechanism for debt relief and forgiveness in a frictionless manner without involving trusted third parties, leading to more harmonious debt settlements by incentivising the parties to truthfully reveal their private information. JUBILEE improves over all previous methods:\n  - individually rational, incentive-compatible, truthful/strategy-proof, ex-post efficient, optimal mechanism for debt relief and forgiveness with private information\n  - by the novel introduction of secure computation techniques to debt relief, the \"blessing of the debtor\" is hereby granted for the first time: debt settlements with higher expected profits and a higher probability of success than without using secure computation\n  A simple and practical implementation is included for \"The Secure Spreadsheet\". Another implementation is realised using Raziel smart contracts on a blockchain with Pravuil consensus.", "AI": {"tldr": "JUBILEE: A secure, incentive-compatible debt relief mechanism using secure computation, improving upon previous methods.", "motivation": "To create a debt relief mechanism that is individually rational, incentive-compatible, and efficient without trusted third parties.", "method": "Introducing secure computation techniques to debt relief, implemented via \"The Secure Spreadsheet\" and Raziel smart contracts on a blockchain.", "result": "Achieves higher expected profits and a higher probability of success in debt settlements compared to methods without secure computation.", "conclusion": "JUBILEE provides a practical and optimal mechanism for debt relief and forgiveness with private information."}}
{"id": "2108.11525", "title": "Supercomputing Enabled Deployable Analytics for Disaster Response", "url": "https://arxiv.org/abs/2108.11525", "pdf": "https://arxiv.org/pdf/2108.11525", "abs": "https://arxiv.org/abs/2108.11525", "authors": ["Kaira Samuel", "Jeremy Kepner", "Michael Jones", "Lauren Milechin", "Vijay Gadepally", "William Arcand", "David Bestor", "William Bergeron", "Chansup Byun", "Matthew Hubbell", "Michael Houle", "Anna Klein", "Victor Lopez", "Julie Mullen", "Andrew Prout", "Albert Reuther", "Antonio Rosa", "Sid Samsi", "Charles Yee", "Peter Michaleas"], "categories": ["cs.DB", "cs.DC", "cs.GR", "cs.HC", "cs.MM"], "comment": "5 pages, 11 figures, 17 references, accepted to IEEE HPEC 2021", "summary": "First responders and other forward deployed essential workers can benefit from advanced analytics. Limited network access and software security requirements prevent the usage of standard cloud based microservice analytic platforms that are typically used in industry. One solution is to precompute a wide range of analytics as files that can be used with standard preinstalled software that does not require network access or additional software and can run on a wide range of legacy hardware. In response to the COVID-19 pandemic, this approach was tested for providing geo-spatial census data to allow quick analysis of demographic data for better responding to emergencies. These data were processed using the MIT SuperCloud to create several thousand Google Earth and Microsoft Excel files representative of many advanced analytics. The fast mapping of census data using Google Earth and Microsoft Excel has the potential to give emergency responders a powerful tool to improve emergency preparedness. Our approach displays relevant census data (total population, population under 15, population over 65, median age) per census block, sorted by county, through a Microsoft Excel spreadsheet (xlsx file) and Google Earth map (kml file). The spreadsheet interface includes features that allow users to convert between different longitude and latitude coordinate units. For the Google Earth files, a variety of absolute and relative colors maps of population density have been explored to provide an intuitive and meaningful interface. Using several hundred cores on the MIT SuperCloud, new analytics can be generated in a few minutes.", "AI": {"tldr": "This paper explores a method for providing advanced analytics to first responders and essential workers in situations where network access is limited.", "motivation": "The motivation is to overcome the limitations of standard cloud-based microservice analytic platforms for forward-deployed essential workers due to limited network access and security requirements.", "method": "The method involves precomputing a wide range of analytics as files (Google Earth and Microsoft Excel) that can be used with preinstalled software on legacy hardware. The MIT SuperCloud was used to process geo-spatial census data.", "result": "The result is a system that allows fast mapping of census data using Google Earth and Microsoft Excel, providing emergency responders with a tool to improve emergency preparedness. Relevant census data is displayed per census block, sorted by county.", "conclusion": "The approach of precomputing analytics and making them available through standard software like Google Earth and Microsoft Excel has the potential to empower emergency responders with readily accessible and easily interpretable data, enhancing their ability to prepare for and respond to emergencies."}}
{"id": "2108.00567", "title": "Agile Elicitation of Scalability Requirements for Open Systems: A Case Study", "url": "https://arxiv.org/abs/2108.00567", "pdf": "https://arxiv.org/pdf/2108.00567", "abs": "https://arxiv.org/abs/2108.00567", "authors": ["Gunnar Brataas", "Antonio Martini", "Geir Kjetil Hanssen", "Georg R\u00e6der"], "categories": ["cs.SE", "cs.PF"], "comment": "36 pages, 7 figures, 6 tables, accepted for publication in Journal of Systems and Software", "summary": "Eliciting scalability requirements during agile software development is complicated and poorly described in previous research. This article presents a lightweight artifact for eliciting scalability requirements during agile software development: the ScrumScale model. The ScrumScale model is a simple spreadsheet. The scalability concepts underlying the ScrumScale model are clarified in this design science research, which also utilizes coordination theory. This paper describes the open banking case study, where a legacy banking system becomes open. This challenges the scalability of this legacy system. The first step in understanding this challenge is to elicit the new scalability requirements. In the open banking case study, key stakeholders from TietoEVRY spent 55 hours eliciting TietoEVRY's open banking project's scalability requirements. According to TietoEVRY, the ScrumScale model provided a systematic way of producing scalability requirements. For TietoEVRY, the scalability concepts behind the ScrumScale model also offered significant advantages in dialogues with other stakeholders.", "AI": {"tldr": "The paper introduces the ScrumScale model, a lightweight artifact for eliciting scalability requirements in agile software development, particularly in the context of open banking.", "motivation": "Eliciting scalability requirements in agile development is challenging and lacks clear guidance. Open banking initiatives, like the one at TietoEVRY, further complicate this due to legacy system scalability issues.", "method": "The paper uses design science research and coordination theory to develop and clarify the ScrumScale model. It presents a case study of TietoEVRY's open banking project, where stakeholders used the ScrumScale model to elicit scalability requirements.", "result": "TietoEVRY spent 55 hours using the ScrumScale model to elicit scalability requirements for their open banking project. They found the model systematic and the underlying scalability concepts beneficial for stakeholder communication.", "conclusion": "The ScrumScale model offers a systematic approach to eliciting scalability requirements in agile development, as demonstrated by its successful application in TietoEVRY's open banking project. It also facilitates communication among stakeholders regarding scalability concerns."}}
{"id": "2107.13957", "title": "Towards Semantic Interoperability in Historical Research: Documenting Research Data and Knowledge with Synthesis", "url": "https://arxiv.org/abs/2107.13957", "pdf": "https://arxiv.org/pdf/2107.13957", "abs": "https://arxiv.org/abs/2107.13957", "authors": ["Pavlos Fafalios", "Konstantina Konsolaki", "Lida Charami", "Kostas Petrakis", "Manos Paterakis", "Dimitris Angelakis", "Yannis Tzitzikas", "Chrysoula Bekiari", "Martin Doerr"], "categories": ["cs.DL", "cs.DB"], "comment": "This is a preprint of an article accepted for publication at the 20th International Semantic Web Conference (ISWC 2021)", "summary": "A vast area of research in historical science concerns the documentation and study of artefacts and related evidence. Current practice mostly uses spreadsheets or simple relational databases to organise the information as rows with multiple columns of related attributes. This form offers itself for data analysis and scholarly interpretation, however it also poses problems including i) the difficulty for collaborative but controlled documentation by a large number of users, ii) the lack of representation of the details from which the documented relations are inferred, iii) the difficulty to extend the underlying data structures as well as to combine and integrate data from multiple and diverse information sources, and iv) the limitation to reuse the data beyond the context of a particular research activity. To support historians to cope with these problems, in this paper we describe the Synthesis documentation system and its use by a large number of historians in the context of an ongoing research project in the field of History of Art. The system is Web-based and collaborative, and makes use of existing standards for information documentation and publication (CIDOC-CRM, RDF), focusing on semantic interoperability and the production of data of high value and long-term validity.", "AI": {"tldr": "This paper introduces the Synthesis documentation system to address the challenges of documenting and studying artifacts in historical science.", "motivation": "Current methods using spreadsheets or simple databases have limitations in collaborative documentation, representing inferred relations, extending data structures, integrating diverse sources, and reusing data.", "method": "The paper describes the Synthesis documentation system, a Web-based and collaborative system using CIDOC-CRM and RDF standards.", "result": "The system is used by a large number of historians in an ongoing research project in the field of History of Art, focusing on semantic interoperability and the production of high-value, long-term data.", "conclusion": "The Synthesis system supports historians in overcoming the problems associated with traditional documentation methods by providing a collaborative, Web-based solution that leverages semantic standards for interoperability and data longevity."}}
{"id": "2010.12537", "title": "TUTA: Tree-based Transformers for Generally Structured Table Pre-training", "url": "https://arxiv.org/abs/2010.12537", "pdf": "https://arxiv.org/pdf/2010.12537", "abs": "https://arxiv.org/abs/2010.12537", "authors": ["Zhiruo Wang", "Haoyu Dong", "Ran Jia", "Jia Li", "Zhiyi Fu", "Shi Han", "Dongmei Zhang"], "categories": ["cs.IR", "cs.AI", "cs.DB"], "comment": "KDD'21", "summary": "Tables are widely used with various structures to organize and present data. Recent attempts on table understanding mainly focus on relational tables, yet overlook to other common table structures. In this paper, we propose TUTA, a unified pre-training architecture for understanding generally structured tables. Noticing that understanding a table requires spatial, hierarchical, and semantic information, we enhance transformers with three novel structure-aware mechanisms. First, we devise a unified tree-based structure, called a bi-dimensional coordinate tree, to describe both the spatial and hierarchical information of generally structured tables. Upon this, we propose tree-based attention and position embedding to better capture the spatial and hierarchical information. Moreover, we devise three progressive pre-training objectives to enable representations at the token, cell, and table levels. We pre-train TUTA on a wide range of unlabeled web and spreadsheet tables and fine-tune it on two critical tasks in the field of table structure understanding: cell type classification and table type classification. Experiments show that TUTA is highly effective, achieving state-of-the-art on five widely-studied datasets.", "AI": {"tldr": "TUTA is a unified pre-training architecture for understanding generally structured tables.", "motivation": "Understanding a table requires spatial, hierarchical, and semantic information.", "method": "Enhance transformers with three novel structure-aware mechanisms: a bi-dimensional coordinate tree, tree-based attention and position embedding, and three progressive pre-training objectives.", "result": "Achieves state-of-the-art on five widely-studied datasets.", "conclusion": "TUTA is highly effective."}}
{"id": "2106.15005", "title": "Untidy Data: The Unreasonable Effectiveness of Tables", "url": "https://arxiv.org/abs/2106.15005", "pdf": "https://arxiv.org/pdf/2106.15005", "abs": "https://arxiv.org/abs/2106.15005", "authors": ["Lyn Bartram", "Michael Correll", "Melanie Tory"], "categories": ["cs.HC"], "comment": null, "summary": "Working with data in table form is usually considered a preparatory and tedious step in the sensemaking pipeline; a way of getting the data ready for more sophisticated visualization and analytical tools. But for many people, spreadsheets -- the quintessential table tool -- remain a critical part of their information ecosystem, allowing them to interact with their data in ways that are hidden or abstracted in more complex tools. This is particularly true for data workers: people who work with data as part of their job but do not identify as professional analysts or data scientists. We report on a qualitative study of how these workers interact with and reason about their data. Our findings show that data tables serve a broader purpose beyond data cleanup at the initial stage of a linear analytic flow: users want to see and \"get their hands on\" the underlying data throughout the analytics process, reshaping and augmenting it to support sensemaking. They reorganize, mark up, layer on levels of detail, and spawn alternatives within the context of the base data. These direct interactions and human-readable table representations form a rich and cognitively important part of building understanding of what the data mean and what they can do with it. We argue that interactive tables are an important visualization idiom in their own right; that the direct data interaction they afford offers a fertile design space for visual analytics; and that sense making can be enriched by more flexible human-data interaction than is currently supported in visual analytics tools.", "AI": {"tldr": "This paper investigates how data workers use spreadsheets for sensemaking, highlighting the importance of interactive tables in understanding data.", "motivation": "To understand how data workers interact with and reason about data tables, especially in the context of sensemaking, beyond initial data preparation.", "method": "A qualitative study was conducted to observe and analyze how data workers use spreadsheets.", "result": "Data tables serve a broader purpose than just data cleanup; users actively reshape, augment, and interact with data throughout the analytics process for better understanding.", "conclusion": "Interactive tables are a valuable visualization method that offers a rich design space for visual analytics, and more flexible human-data interaction can enhance sensemaking."}}
{"id": "2008.11015", "title": "Table2Charts: Recommending Charts by Learning Shared Table Representations", "url": "https://arxiv.org/abs/2008.11015", "pdf": "https://arxiv.org/pdf/2008.11015", "abs": "https://arxiv.org/abs/2008.11015", "authors": ["Mengyu Zhou", "Qingtao Li", "Xinyi He", "Yuejiang Li", "Yibo Liu", "Wei Ji", "Shi Han", "Yining Chen", "Daxin Jiang", "Dongmei Zhang"], "categories": ["cs.DB", "cs.CL", "cs.HC"], "comment": "9 + 2(appendix) pages, accepted by KDD'21 conference", "summary": "It is common for people to create different types of charts to explore a multi-dimensional dataset (table). However, to recommend commonly composed charts in real world, one should take the challenges of efficiency, imbalanced data and table context into consideration. In this paper, we propose Table2Charts framework which learns common patterns from a large corpus of (table, charts) pairs. Based on deep Q-learning with copying mechanism and heuristic searching, Table2Charts does table-to-sequence generation, where each sequence follows a chart template. On a large spreadsheet corpus with 165k tables and 266k charts, we show that Table2Charts could learn a shared representation of table fields so that recommendation tasks on different chart types could mutually enhance each other. Table2Charts outperforms other chart recommendation systems in both multi-type task (with doubled recall numbers R@3=0.61 and R@1=0.43) and human evaluations.", "AI": {"tldr": "Table2Charts framework learns common chart patterns from (table, charts) pairs using deep Q-learning and heuristic searching for table-to-sequence generation, outperforming other systems.", "motivation": "Recommending commonly composed charts efficiently, considering imbalanced data and table context.", "method": "Deep Q-learning with copying mechanism and heuristic searching for table-to-sequence generation following chart templates.", "result": "Table2Charts learns a shared representation of table fields, enhancing recommendation tasks and doubling recall numbers compared to other systems.", "conclusion": "Table2Charts effectively learns and recommends charts, outperforming existing systems in both multi-type tasks and human evaluations."}}
{"id": "2106.15339", "title": "SpreadsheetCoder: Formula Prediction from Semi-structured Context", "url": "https://arxiv.org/abs/2106.15339", "pdf": "https://arxiv.org/pdf/2106.15339", "abs": "https://arxiv.org/abs/2106.15339", "authors": ["Xinyun Chen", "Petros Maniatis", "Rishabh Singh", "Charles Sutton", "Hanjun Dai", "Max Lin", "Denny Zhou"], "categories": ["cs.SE", "cs.LG", "cs.PL"], "comment": "Published in ICML 2021", "summary": "Spreadsheet formula prediction has been an important program synthesis problem with many real-world applications. Previous works typically utilize input-output examples as the specification for spreadsheet formula synthesis, where each input-output pair simulates a separate row in the spreadsheet. However, this formulation does not fully capture the rich context in real-world spreadsheets. First, spreadsheet data entries are organized as tables, thus rows and columns are not necessarily independent from each other. In addition, many spreadsheet tables include headers, which provide high-level descriptions of the cell data. However, previous synthesis approaches do not consider headers as part of the specification. In this work, we present the first approach for synthesizing spreadsheet formulas from tabular context, which includes both headers and semi-structured tabular data. In particular, we propose SpreadsheetCoder, a BERT-based model architecture to represent the tabular context in both row-based and column-based formats. We train our model on a large dataset of spreadsheets, and demonstrate that SpreadsheetCoder achieves top-1 prediction accuracy of 42.51%, which is a considerable improvement over baselines that do not employ rich tabular context. Compared to the rule-based system, SpreadsheetCoder assists 82% more users in composing formulas on Google Sheets.", "AI": {"tldr": "This paper introduces SpreadsheetCoder, a BERT-based model for synthesizing spreadsheet formulas from tabular context (headers and semi-structured data).", "motivation": "Existing formula synthesis methods don't fully capture the context of real-world spreadsheets, particularly the relationships between rows/columns and the information provided by headers.", "method": "The paper proposes SpreadsheetCoder, a BERT-based architecture that represents tabular context in both row-based and column-based formats. The model is trained on a large dataset of spreadsheets.", "result": "SpreadsheetCoder achieves a top-1 prediction accuracy of 42.51%, a significant improvement over baselines. It also assists 82% more users in composing formulas on Google Sheets compared to a rule-based system.", "conclusion": "SpreadsheetCoder effectively leverages tabular context to improve spreadsheet formula synthesis, outperforming existing methods."}}
{"id": "2106.13500", "title": "TableSense: Spreadsheet Table Detection with Convolutional Neural Networks", "url": "https://arxiv.org/abs/2106.13500", "pdf": "https://arxiv.org/pdf/2106.13500", "abs": "https://arxiv.org/abs/2106.13500", "authors": ["Haoyu Dong", "Shijie Liu", "Shi Han", "Zhouyu Fu", "Dongmei Zhang"], "categories": ["cs.IR"], "comment": null, "summary": "Spreadsheet table detection is the task of detecting all tables on a given sheet and locating their respective ranges. Automatic table detection is a key enabling technique and an initial step in spreadsheet data intelligence. However, the detection task is challenged by the diversity of table structures and table layouts on the spreadsheet. Considering the analogy between a cell matrix as spreadsheet and a pixel matrix as image, and encouraged by the successful application of Convolutional Neural Networks (CNN) in computer vision, we have developed TableSense, a novel end-to-end framework for spreadsheet table detection. First, we devise an effective cell featurization scheme to better leverage the rich information in each cell; second, we develop an enhanced convolutional neural network model for table detection to meet the domain-specific requirement on precise table boundary detection; third, we propose an effective uncertainty metric to guide an active learning based smart sampling algorithm, which enables the efficient build-up of a training dataset with 22,176 tables on 10,220 sheets with broad coverage of diverse table structures and layouts. Our evaluation shows that TableSense is highly effective with 91.3\\% recall and 86.5\\% precision in EoB-2 metric, a significant improvement over both the current detection algorithm that are used in commodity spreadsheet tools and state-of-the-art convolutional neural networks in computer vision.", "AI": {"tldr": "TableSense is a novel end-to-end framework for spreadsheet table detection that uses CNNs and active learning to achieve high precision and recall.", "motivation": "Automatic table detection is important for spreadsheet data intelligence, but challenging due to the diversity of table structures and layouts.", "method": "The authors developed TableSense, which includes a cell featurization scheme, an enhanced CNN model, and an active learning based smart sampling algorithm.", "result": "TableSense achieves 91.3% recall and 86.5% precision in EoB-2 metric, outperforming existing methods.", "conclusion": "TableSense is a highly effective solution for spreadsheet table detection."}}
{"id": "2106.03096", "title": "TabularNet: A Neural Network Architecture for Understanding Semantic Structures of Tabular Data", "url": "https://arxiv.org/abs/2106.03096", "pdf": "https://arxiv.org/pdf/2106.03096", "abs": "https://arxiv.org/abs/2106.03096", "authors": ["Lun Du", "Fei Gao", "Xu Chen", "Ran Jia", "Junshan Wang", "Jiang Zhang", "Shi Han", "Dongmei Zhang"], "categories": ["cs.LG"], "comment": "10 pages, 7 figures, to be published in the proceedings of KDD 2021", "summary": "Tabular data are ubiquitous for the widespread applications of tables and hence have attracted the attention of researchers to extract underlying information. One of the critical problems in mining tabular data is how to understand their inherent semantic structures automatically. Existing studies typically adopt Convolutional Neural Network (CNN) to model the spatial information of tabular structures yet ignore more diverse relational information between cells, such as the hierarchical and paratactic relationships. To simultaneously extract spatial and relational information from tables, we propose a novel neural network architecture, TabularNet. The spatial encoder of TabularNet utilizes the row/column-level Pooling and the Bidirectional Gated Recurrent Unit (Bi-GRU) to capture statistical information and local positional correlation, respectively. For relational information, we design a new graph construction method based on the WordNet tree and adopt a Graph Convolutional Network (GCN) based encoder that focuses on the hierarchical and paratactic relationships between cells. Our neural network architecture can be a unified neural backbone for different understanding tasks and utilized in a multitask scenario. We conduct extensive experiments on three classification tasks with two real-world spreadsheet data sets, and the results demonstrate the effectiveness of our proposed TabularNet over state-of-the-art baselines.", "AI": {"tldr": "This paper introduces TabularNet, a novel neural network architecture for understanding the semantic structure of tabular data.", "motivation": "Existing methods often overlook diverse relational information between cells, such as hierarchical and paratactic relationships.", "method": "TabularNet uses a spatial encoder with row/column-level pooling and Bi-GRU to capture spatial information and a GCN-based encoder with a new graph construction method based on WordNet to capture relational information.", "result": "Experiments on three classification tasks with two real-world spreadsheet datasets demonstrate the effectiveness of TabularNet over state-of-the-art baselines.", "conclusion": "TabularNet can serve as a unified neural backbone for different understanding tasks and can be utilized in a multi-task scenario."}}
{"id": "2105.13733", "title": "FAST CAT: Collaborative Data Entry and Curation for Semantic Interoperability in Digital Humanities", "url": "https://arxiv.org/abs/2105.13733", "pdf": "https://arxiv.org/pdf/2105.13733", "abs": "https://arxiv.org/abs/2105.13733", "authors": ["Pavlos Fafalios", "Kostas Petrakis", "Georgios Samaritakis", "Korina Doerr", "Athina Kritsotaki", "Yannis Tzitzikas", "Martin Doerr"], "categories": ["cs.DL", "cs.DB"], "comment": "This is a preprint of an article accepted for publication at the ACM Journal on Computing and Cultural Heritage (JOCCH)", "summary": "Descriptive and empirical sciences, such as History, are the sciences that collect, observe and describe phenomena in order to explain them and draw interpretative conclusions about influences, driving forces and impacts under given circumstances. Spreadsheet software and relational database management systems are still the dominant tools for quantitative analysis and overall data management in these these sciences, allowing researchers to directly analyse the gathered data and perform scholarly interpretation. However, this current practice has a set of limitations, including the high dependency of the collected data on the initial research hypothesis, usually useless for other research, the lack of representation of the details from which the registered relations are inferred, and the difficulty to revisit the original data sources for verification, corrections or improvements. To cope with these problems, in this paper we present FAST CAT, a collaborative system for assistive data entry and curation in Digital Humanities and similar forms of empirical research. We describe the related challenges, the overall methodology we follow for supporting semantic interoperability, and discuss the use of FAST CAT in the context of a European (ERC) project of Maritime History, called SeaLiT, which examines economic, social and demographic impacts of the introduction of steamboats in the Mediterranean area between the 1850s and the 1920s.", "AI": {"tldr": "This paper introduces FAST CAT, a collaborative system for data entry and curation in Digital Humanities, addressing limitations of current practices in empirical research.", "motivation": "Current data management practices in descriptive and empirical sciences have limitations such as high dependency on initial research hypothesis, lack of representation of details, and difficulty in revisiting original data sources.", "method": "The paper presents FAST CAT, a collaborative system for assistive data entry and curation, and discusses the methodology for supporting semantic interoperability.", "result": "The paper discusses the use of FAST CAT in the context of a European (ERC) project of Maritime History, called SeaLiT.", "conclusion": "FAST CAT is a collaborative system designed to address the limitations of current data management practices in Digital Humanities and similar empirical research fields. It is being used in the SeaLiT project to study the impacts of steamboats in the Mediterranean area."}}
{"id": "2012.00697", "title": "Sigma Worksheet: Interactive Construction of OLAP Queries", "url": "https://arxiv.org/abs/2012.00697", "pdf": "https://arxiv.org/pdf/2012.00697", "abs": "https://arxiv.org/abs/2012.00697", "authors": ["James Gale", "Max Seiden", "Gretchen Atwood", "Jason Frantz", "Rob Woollen", "\u00c7a\u011fatay Demiralp"], "categories": ["cs.DB", "cs.HC"], "comment": null, "summary": "The new generation of cloud data warehouses (CDWs) brings large amounts of data and compute power closer to users in enterprises. The ability to directly access the warehouse data, interactively analyze and explore it at scale can empower users to improve their decision making cycles. However, existing tools for analyzing data in CDWs are either limited in ad-hoc transformations or difficult to use for business users, the largest user segment in enterprises. Here we introduce Sigma Worksheet, a new interactive system that enables users to easily perform ad-hoc visual analysis of data in CDWs at scale. For this, Sigma Worksheet provides an accessible spreadsheet-like interface for data analysis through direct manipulation. Sigma Worksheet dynamically constructs matching SQL queries from user interactions on this familiar interface, building on the versatility and expressivity of SQL. Sigma Worksheet executes constructed queries directly on CDWs, leveraging the superior characteristics of the new generation CDWs, including scalability. To evaluate Sigma Worksheet, we first demonstrate its expressivity through two real life use cases, cohort analysis and sessionization. We then measure the performance of the Worksheet generated queries with a set of experiments using the TPC-H benchmark. Results show the performance of our compiled SQL queries is comparable to that of the reference queries of the benchmark. Finally, to assess the usefulness of Sigma Worksheet in deployment, we elicit feedback through a 100-person survey followed by a semi-structured interview study with 70 participants. We find that Sigma Worksheet is easier to use and learn, improving the productivity of users. Our findings also suggest Sigma Worksheet can further improve user experience by providing guidance to users at various steps of data analysis.", "AI": {"tldr": "Sigma Worksheet is a new interactive system that enables users to easily perform ad-hoc visual analysis of data in CDWs at scale with a spreadsheet-like interface.", "motivation": "Existing tools for analyzing data in CDWs are either limited in ad-hoc transformations or difficult to use for business users.", "method": "Sigma Worksheet dynamically constructs matching SQL queries from user interactions on a familiar interface and executes constructed queries directly on CDWs.", "result": "The performance of compiled SQL queries is comparable to that of the reference queries of the benchmark. Sigma Worksheet is easier to use and learn, improving the productivity of users.", "conclusion": "Sigma Worksheet can further improve user experience by providing guidance to users at various steps of data analysis."}}
{"id": "2104.13600", "title": "Mapping Spreadsheets to RDF: Supporting Excel in RML", "url": "https://arxiv.org/abs/2104.13600", "pdf": "https://arxiv.org/pdf/2104.13600", "abs": "https://arxiv.org/abs/2104.13600", "authors": ["Markus Schr\u00f6der", "Christian Jilek", "Andreas Dengel"], "categories": ["cs.DB"], "comment": "6 pages, submitted to Second International Workshop on Knowledge Graph Construction", "summary": "The RDF Mapping Language (RML) enables, among other formats, the mapping of tabular data as Comma-Separated Values (CSV) files to RDF graphs. Unfortunately, the widely used spreadsheet format is currently neglected by its specification and well-known implementations. Therefore, we extended one of the tools which is RML Mapper to support Microsoft Excel spreadsheet files and demonstrate its capabilities in an interactive online demo. Our approach allows to access various meta data of spreadsheet cells in typical RML maps. Some experimental features for more specific use cases are also provided. The implementation code is publicly available in a GitHub fork.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6269\u5c55\u4e86RML Mapper\u5de5\u5177\uff0c\u4ee5\u652f\u6301Microsoft Excel\u7535\u5b50\u8868\u683c\u6587\u4ef6\u5230RDF\u56fe\u7684\u6620\u5c04\u3002", "motivation": "\u73b0\u6709RML\u89c4\u8303\u548c\u5b9e\u73b0\u5ffd\u7565\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684\u7535\u5b50\u8868\u683c\u683c\u5f0f\u3002", "method": "\u6269\u5c55RML Mapper\u5de5\u5177\u4ee5\u652f\u6301Microsoft Excel\u7535\u5b50\u8868\u683c\u6587\u4ef6\uff0c\u5e76\u63d0\u4f9b\u5728\u7ebf\u6f14\u793a\u3002", "result": "\u5141\u8bb8\u5728\u5178\u578b\u7684RML\u6620\u5c04\u4e2d\u8bbf\u95ee\u7535\u5b50\u8868\u683c\u5355\u5143\u683c\u7684\u5404\u79cd\u5143\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e9b\u9488\u5bf9\u7279\u5b9a\u7528\u4f8b\u7684\u5b9e\u9a8c\u6027\u529f\u80fd\u3002", "conclusion": "\u8be5\u5b9e\u73b0\u4ee3\u7801\u5728GitHub fork\u4e2d\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2104.13576", "title": "Dataset Generation Patterns for Evaluating Knowledge Graph Construction", "url": "https://arxiv.org/abs/2104.13576", "pdf": "https://arxiv.org/pdf/2104.13576", "abs": "https://arxiv.org/abs/2104.13576", "authors": ["Markus Schr\u00f6der", "Christian Jilek", "Andreas Dengel"], "categories": ["cs.DB"], "comment": "5 pages, submitted to ESWC demo track", "summary": "Confidentiality hinders the publication of authentic, labeled datasets of personal and enterprise data, although they could be useful for evaluating knowledge graph construction approaches in industrial scenarios. Therefore, our plan is to synthetically generate such data in a way that it appears as authentic as possible. Based on our assumption that knowledge workers have certain habits when they produce or manage data, generation patterns could be discovered which can be utilized by data generators to imitate real datasets. In this paper, we initially derived 11 distinct patterns found in real spreadsheets from industry and demonstrate a suitable generator called Data Sprout that is able to reproduce them. We describe how the generator produces spreadsheets in general and what altering effects the implemented patterns have.", "AI": {"tldr": "This paper introduces a method for generating synthetic datasets that mimic real-world spreadsheets to address the lack of authentic, labeled data for evaluating knowledge graph construction.", "motivation": "The motivation is the lack of authentic, labeled datasets of personal and enterprise data due to confidentiality concerns, which hinders the evaluation of knowledge graph construction approaches.", "method": "The method involves discovering and implementing generation patterns found in real spreadsheets from industry, and developing a generator called Data Sprout to reproduce these patterns.", "result": "The paper derives 11 distinct patterns from real spreadsheets and demonstrates that Data Sprout can reproduce them.", "conclusion": "The paper concludes by describing how Data Sprout produces spreadsheets and the effects of the implemented patterns."}}
{"id": "2103.15203", "title": "Mathematics of Digital Hyperspace", "url": "https://arxiv.org/abs/2103.15203", "pdf": "https://arxiv.org/pdf/2103.15203", "abs": "https://arxiv.org/abs/2103.15203", "authors": ["Jeremy Kepner", "Timothy Davis", "Vijay Gadepally", "Hayden Jananthan", "Lauren Milechin"], "categories": ["cs.MS", "cs.DB", "cs.DM", "cs.NE", "math.RA"], "comment": "9 pages, 8 figures, 2 tables, accepted to GrAPL 2021. arXiv admin note: text overlap with arXiv:1807.03165, arXiv:2004.01181, arXiv:1909.05631, arXiv:1708.02937", "summary": "Social media, e-commerce, streaming video, e-mail, cloud documents, web pages, traffic flows, and network packets fill vast digital lakes, rivers, and oceans that we each navigate daily. This digital hyperspace is an amorphous flow of data supported by continuous streams that stretch standard concepts of type and dimension. The unstructured data of digital hyperspace can be elegantly represented, traversed, and transformed via the mathematics of hypergraphs, hypersparse matrices, and associative array algebra. This paper explores a novel mathematical concept, the semilink, that combines pairs of semirings to provide the essential operations for graph analytics, database operations, and machine learning. The GraphBLAS standard currently supports hypergraphs, hypersparse matrices, the mathematics required for semilinks, and seamlessly performs graph, network, and matrix operations. With the addition of key based indices (such as pointers to strings) and semilinks, GraphBLAS can become a richer associative array algebra and be a plug-in replacement for spreadsheets, database tables, and data centric operating systems, enhancing the navigation of unstructured data found in digital hyperspace.", "AI": {"tldr": "This paper introduces the concept of semilinks, a combination of semirings, to enhance graph analytics, database operations, and machine learning within the GraphBLAS framework.", "motivation": "The motivation is to address the challenges of representing, traversing, and transforming unstructured data in digital hyperspace, which is characterized by vast amounts of data with varying types and dimensions.", "method": "The paper explores the semilink, a novel mathematical concept that combines pairs of semirings. It leverages the GraphBLAS standard, which already supports hypergraphs, hypersparse matrices, and the necessary mathematics for semilinks.", "result": "The paper suggests that with the addition of key-based indices and semilinks, GraphBLAS can evolve into a richer associative array algebra.", "conclusion": "The conclusion is that GraphBLAS, enhanced with semilinks, has the potential to replace spreadsheets, database tables, and data-centric operating systems, thereby improving the navigation of unstructured data in digital hyperspace."}}
{"id": "2103.10472", "title": "Dynamic Kernel Matching for Non-conforming Data: A Case Study of T-cell Receptor Datasets", "url": "https://arxiv.org/abs/2103.10472", "pdf": "https://arxiv.org/pdf/2103.10472", "abs": "https://arxiv.org/abs/2103.10472", "authors": ["Jared Ostmeyer", "Scott Christley", "Lindsay Cowell"], "categories": ["q-bio.QM", "cs.LG"], "comment": null, "summary": "Most statistical classifiers are designed to find patterns in data where numbers fit into rows and columns, like in a spreadsheet, but many kinds of data do not conform to this structure. To uncover patterns in non-conforming data, we describe an approach for modifying established statistical classifiers to handle non-conforming data, which we call dynamic kernel matching (DKM). As examples of non-conforming data, we consider (i) a dataset of T-cell receptor (TCR) sequences labelled by disease antigen and (ii) a dataset of sequenced TCR repertoires labelled by patient cytomegalovirus (CMV) serostatus, anticipating that both datasets contain signatures for diagnosing disease. We successfully fit statistical classifiers augmented with DKM to both datasets and report the performance on holdout data using standard metrics and metrics allowing for indeterminant diagnoses. Finally, we identify the patterns used by our statistical classifiers to generate predictions and show that these patterns agree with observations from experimental studies.", "AI": {"tldr": "This paper introduces Dynamic Kernel Matching (DKM), a method for adapting statistical classifiers to non-conforming data, and applies it to T-cell receptor sequence data for disease diagnosis.", "motivation": "To address the challenge of applying standard statistical classifiers to non-conforming data, such as biological sequence data, and to uncover potential diagnostic signatures in T-cell receptor (TCR) sequence data related to disease antigen and CMV serostatus.", "method": "The paper presents Dynamic Kernel Matching (DKM) to modify existing statistical classifiers for non-conforming data. It then applies DKM-augmented classifiers to two TCR sequence datasets: one labeled by disease antigen and the other by patient CMV serostatus.", "result": "The study successfully fits DKM-enhanced statistical classifiers to both TCR sequence datasets. It reports performance on holdout data using standard metrics and metrics that accommodate indeterminate diagnoses.", "conclusion": "The classifiers identify patterns in the data that align with findings from experimental studies, suggesting the potential of DKM for analyzing non-conforming data and discovering biologically relevant patterns."}}
{"id": "2103.03537", "title": "Interactively Constructing Knowledge Graphs from Messy User-Generated Spreadsheets", "url": "https://arxiv.org/abs/2103.03537", "pdf": "https://arxiv.org/pdf/2103.03537", "abs": "https://arxiv.org/abs/2103.03537", "authors": ["Markus Schr\u00f6der", "Christian Jilek", "Michael Schulze", "Andreas Dengel"], "categories": ["cs.DB"], "comment": "15 pages", "summary": "When spreadsheets are filled freely by knowledge workers, they can contain rather unstructured content. For humans and especially machines it becomes difficult to interpret such data properly. Therefore, spreadsheets are often converted to a more explicit, formal and structured form, for example, to a knowledge graph. However, if a data maintenance strategy has been missing and user-generated data becomes \"messy\", the construction of knowledge graphs will be a challenging task. In this paper, we catalog several of those challenges and propose an interactive approach to solve them. Our approach includes a graphical user interface which enables knowledge engineers to bulk-annotate spreadsheet cells with extracted information. Based on the cells' annotations a knowledge graph is ultimately formed. Using five spreadsheets from an industrial scenario, we built a 25k-triple graph during our evaluation. We compared our method with the state-of-the-art RDF Mapping Language (RML) attempt. The comparison highlights contributions of our approach.", "AI": {"tldr": "This paper introduces an interactive approach to convert unstructured spreadsheets into knowledge graphs, addressing challenges in data maintenance and user-generated content.", "motivation": "Converting unstructured spreadsheets to knowledge graphs is challenging due to messy, user-generated data and lack of data maintenance strategies.", "method": "The proposed approach uses a graphical user interface for knowledge engineers to annotate spreadsheet cells, enabling the formation of a knowledge graph.", "result": "The approach was evaluated on five industrial spreadsheets, resulting in a 25k-triple knowledge graph. It was compared favorably to the RDF Mapping Language (RML).", "conclusion": "The interactive approach effectively addresses the challenges of converting unstructured spreadsheets into knowledge graphs, offering improvements over existing methods like RML."}}
{"id": "2102.09461", "title": "Optimization Helps Scheduling Nursing Staff at the Long-Term Care Homes of the City of Toronto", "url": "https://arxiv.org/abs/2102.09461", "pdf": "https://arxiv.org/pdf/2102.09461", "abs": "https://arxiv.org/abs/2102.09461", "authors": ["Manion Anderson", "Merve Bodur", "Scott Rathwell", "Vahid Sarhangian"], "categories": ["cs.CY", "math.OC"], "comment": null, "summary": "The City of Toronto Long Term Care Homes & Services (LTCH&S) division is one of the largest providers of long-term care in the Canadian province of Ontario, providing care to 2,640 residents at 10 homes across Toronto. Our collaboration with LTCH&S was initiated to facilitate the increasingly challenging task of scheduling nursing staff and reduce high absenteeism rate observed among the part-time nurses. We developed a spreadsheet-based scheduling tool to automate the generation of schedules and incorporate nurses' preferences for different shifts into the schedules. At the core of the scheduling tool is a hierarchical optimization model that generates a feasible schedule with the highest total preference score while satisfying the maximum possible demand. Feasible schedules had to abide by a set of complex seniority requirements which prioritized more senior nurses when allocating the available shifts. Our scheduling tool was implemented in a 391-bed home in Toronto. The tool allowed nursing managers to generate feasible schedules within a fraction of an hour, in contrast to the status-quo manual approach which could took up to tens of hours. In addition, the schedules successfully accounted for preferences with on average above 94% of the allocated shifts ranked as most preferred.", "AI": {"tldr": "Developed a spreadsheet-based scheduling tool using a hierarchical optimization model to automate nursing staff scheduling in a long-term care home, reducing scheduling time and incorporating nurses' preferences.", "motivation": "The increasing challenge of scheduling nursing staff and high absenteeism among part-time nurses in Toronto's Long Term Care Homes & Services (LTCH&S).", "method": "A spreadsheet-based scheduling tool with a hierarchical optimization model that generates feasible schedules, maximizing preference scores while satisfying demand and seniority requirements.", "result": "The tool generates feasible schedules in a fraction of an hour, compared to tens of hours manually, and successfully accounts for preferences, with 94% of allocated shifts ranked as most preferred.", "conclusion": "The scheduling tool significantly reduces scheduling time and effectively incorporates nurses' preferences, improving the efficiency of nursing staff scheduling."}}
{"id": "2012.01571", "title": "Online Model Swapping in Architectural Simulation", "url": "https://arxiv.org/abs/2012.01571", "pdf": "https://arxiv.org/pdf/2012.01571", "abs": "https://arxiv.org/abs/2012.01571", "authors": ["Patrick Lavin", "Jeffrey Young", "Rich Vuduc", "Jonathan Beard"], "categories": ["cs.AR"], "comment": null, "summary": "As systems and applications grow more complex, detailed simulation takes an ever increasing amount of time. The prospect of increased simulation time resulting in slower design iteration forces architects to use simpler models, such as spreadsheets, when they want to iterate quickly on a design. However, the task of migrating from a simple simulation to one with more detail often requires multiple executions to find where simple models could be effective, which could be more expensive than running the detailed model in the first place. Also, architects must often rely on intuition to choose these simpler models, further complicating the problem.\n  In this work, we present a method of bridging the gap between simple and detailed simulation by monitoring simulation behavior online and automatically swapping out detailed models with simpler statistical approximations. We demonstrate the potential of our methodology by implementing it in the open-source simulator SVE-Cachesim to swap out the level one data cache (L1D) within a memory hierarchy. This proof of concept demonstrates that our technique can handle a non-trivial use-case in not just approximation of local time-invariant statistics, but also those that vary with time (e.g., the L1D is a form of a time-series function), and downstream side-effects (e.g., the L1D filters accesses for the level two cache). Our simulation swaps out the built-in cache model with only an 8% error in the simulated cycle count while using the approximated cache models for over 90% of the simulation, and our simpler models require two to eight times less computation per \"execution\" of the model", "AI": {"tldr": "This paper introduces a method for automatically swapping detailed simulation models with simpler statistical approximations during runtime to reduce simulation time.", "motivation": "Detailed simulations are time-consuming, forcing architects to use simpler models, but migrating between models is difficult and relies on intuition.", "method": "The paper proposes monitoring simulation behavior online and automatically swapping detailed models with simpler statistical approximations.", "result": "The method was implemented in SVE-Cachesim, swapping the L1D cache model with an 8% error in simulated cycle count, using approximated models for over 90% of the simulation, and reducing computation time by two to eight times.", "conclusion": "The technique can handle time-variant statistics and downstream side-effects, demonstrating its potential to bridge the gap between simple and detailed simulation."}}
{"id": "1909.00855", "title": "Defining and Adopting an End User Computing Policy: A Case Study", "url": "https://arxiv.org/abs/1909.00855", "pdf": "https://arxiv.org/pdf/1909.00855", "abs": "https://arxiv.org/abs/1909.00855", "authors": ["Roger Turner"], "categories": ["cs.HC", "cs.CY"], "comment": "25 Pages, 12 Colour Figures. 1 Table. First presented at the EuSpRIG 2018 Conference at Imperial College, London. Revised and updated following a further presentation at the EuSpRIG 2019 Conference also at Imperial College, London", "summary": "End User Computing carries significant risks if not well controlled. This paper is a case study of the introduction of an updated End User Computing policy at the Wesleyan Assurance Society. The paper outlines the plan and identifies various challenges. The paper explains how these challenges were overcome. We wrote an End User Computing Risk Assessment Application which calculates a risk rating band based on the Complexity, Materiality and Control (or lack of it) pertaining to any given application and the basis of assessment is given in this paper. The policy uses a risk based approach for assessing and mitigating against the highest risks first and obtaining the quickest benefit.", "AI": {"tldr": "This paper introduces an updated End User Computing policy at the Wesleyan Assurance Society.", "motivation": "To address significant risks associated with End User Computing.", "method": "A risk-based approach is used, supported by an End User Computing Risk Assessment Application that calculates a risk rating based on Complexity, Materiality, and Control.", "result": "Challenges in implementing the policy were overcome, and a risk assessment application was developed.", "conclusion": "The policy prioritizes mitigating the highest risks first for the quickest benefit."}}
{"id": "2011.05978", "title": "The Impact of Text Presentation on Translator Performance", "url": "https://arxiv.org/abs/2011.05978", "pdf": "https://arxiv.org/pdf/2011.05978", "abs": "https://arxiv.org/abs/2011.05978", "authors": ["Samuel L\u00e4ubli", "Patrick Simianer", "Joern Wuebker", "Geza Kovacs", "Rico Sennrich", "Spence Green"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted for publication in Target", "summary": "Widely used computer-aided translation (CAT) tools divide documents into segments such as sentences and arrange them in a side-by-side, spreadsheet-like view. We present the first controlled evaluation of these design choices on translator performance, measuring speed and accuracy in three experimental text processing tasks. We find significant evidence that sentence-by-sentence presentation enables faster text reproduction and within-sentence error identification compared to unsegmented text, and that a top-and-bottom arrangement of source and target sentences enables faster text reproduction compared to a side-by-side arrangement. For revision, on the other hand, our results suggest that presenting unsegmented text results in the highest accuracy and time efficiency. Our findings have direct implications for best practices in designing CAT tools.", "AI": {"tldr": "This paper evaluates the design choices of CAT tools, specifically sentence segmentation and layout, on translator performance.", "motivation": "To determine the impact of CAT tool design on translator speed and accuracy.", "method": "Controlled experiment measuring speed and accuracy in text processing tasks with different segmentation and layout conditions.", "result": "Sentence-by-sentence presentation improves speed and error identification for text reproduction. Top-and-bottom layout is faster than side-by-side for text reproduction. Unsegmented text is best for revision.", "conclusion": "The findings inform best practices for CAT tool design, suggesting different designs for different translation tasks."}}
{"id": "2010.09975", "title": "Calliope: Automatic Visual Data Story Generation from a Spreadsheet", "url": "https://arxiv.org/abs/2010.09975", "pdf": "https://arxiv.org/pdf/2010.09975", "abs": "https://arxiv.org/abs/2010.09975", "authors": ["Danqing Shi", "Xinyue Xu", "Fuling Sun", "Yang Shi", "Nan Cao"], "categories": ["cs.HC"], "comment": null, "summary": "Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.", "AI": {"tldr": "This paper introduces Calliope, a system for automatically generating visual data stories from spreadsheets.", "motivation": "Existing tools for creating visual data stories are inefficient and difficult to use, requiring significant user skills in data analysis, visualization, and scripting.", "method": "Calliope uses a logic-oriented Monte Carlo tree search algorithm to explore the data space, generate story pieces, and organize them logically. Data fact importance is measured using information theory, and each fact is visualized with an automatically generated description.", "result": "Evaluations, including example stories, controlled experiments, and expert interviews, demonstrate Calliope's effectiveness in generating visual data stories.", "conclusion": "Calliope is beneficial for efficient visual data story generation."}}
{"id": "2009.03520", "title": "Leam: An Interactive System for In-situ Visual Text Analysis", "url": "https://arxiv.org/abs/2009.03520", "pdf": "https://arxiv.org/pdf/2009.03520", "abs": "https://arxiv.org/abs/2009.03520", "authors": ["Sajjadur Rahman", "Peter Griggs", "\u00c7a\u011fatay Demiralp"], "categories": ["cs.DB", "cs.CL", "cs.HC"], "comment": null, "summary": "With the increase in scale and availability of digital text generated on the web, enterprises such as online retailers and aggregators often use text analytics to mine and analyze the data to improve their services and products alike. Text data analysis is an iterative, non-linear process with diverse workflows spanning multiple stages, from data cleaning to visualization. Existing text analytics systems usually accommodate a subset of these stages and often fail to address challenges related to data heterogeneity, provenance, workflow reusability and reproducibility, and compatibility with established practices. Based on a set of design considerations we derive from these challenges, we propose Leam, a system that treats the text analysis process as a single continuum by combining advantages of computational notebooks, spreadsheets, and visualization tools. Leam features an interactive user interface for running text analysis workflows, a new data model for managing multiple atomic and composite data types, and an expressive algebra that captures diverse sets of operations representing various stages of text analysis and enables coordination among different components of the system, including data, code, and visualizations. We report our current progress in Leam development while demonstrating its usefulness with usage examples. Finally, we outline a number of enhancements to Leam and identify several research directions for developing an interactive visual text analysis system.", "AI": {"tldr": "This paper introduces Leam, a new system for text analysis that combines computational notebooks, spreadsheets, and visualization tools.", "motivation": "Existing text analytics systems often fail to address challenges related to data heterogeneity, provenance, workflow reusability and reproducibility, and compatibility with established practices.", "method": "Leam features an interactive user interface, a new data model, and an expressive algebra.", "result": "The paper reports current progress in Leam development and demonstrates its usefulness with usage examples.", "conclusion": "The paper outlines a number of enhancements to Leam and identifies several research directions for developing an interactive visual text analysis system."}}
{"id": "2008.04543", "title": "Pen-based Interaction with Spreadsheets in Mobile Virtual Reality", "url": "https://arxiv.org/abs/2008.04543", "pdf": "https://arxiv.org/pdf/2008.04543", "abs": "https://arxiv.org/abs/2008.04543", "authors": ["Travis Gesslein", "Verena Biener", "Philipp Gagel", "Daniel Schneider", "Per Ola Kristensson", "Eyal Ofek", "Michel Pahud", "Jens Grubert"], "categories": ["cs.HC"], "comment": "10 pages, 11 figures, ISMAR 2020", "summary": "Virtual Reality (VR) can enhance the display and interaction of mobile knowledge work and in particular, spreadsheet applications. While spreadsheets are widely used yet are challenging to interact with, especially on mobile devices, using them in VR has not been explored in depth. A special uniqueness of the domain is the contrast between the immersive and large display space afforded by VR, contrasted by the very limited interaction space that may be afforded for the information worker on the go, such as an airplane seat or a small work-space. To close this gap, we present a tool-set for enhancing spreadsheet interaction on tablets using immersive VR headsets and pen-based input. This combination opens up many possibilities for enhancing the productivity for spreadsheet interaction. We propose to use the space around and in front of the tablet for enhanced visualization of spreadsheet data and meta-data. For example, extending sheet display beyond the bounds of the physical screen, or easier debugging by uncovering hidden dependencies between sheet's cells. Combining the precise on-screen input of a pen with spatial sensing around the tablet, we propose tools for the efficient creation and editing of spreadsheets functions such as off-the-screen layered menus, visualization of sheets dependencies, and gaze-and-touch-based switching between spreadsheet tabs. We study the feasibility of the proposed tool-set using a video-based online survey and an expert-based assessment of indicative human performance potential.", "AI": {"tldr": "This paper explores the use of VR to enhance spreadsheet interaction on tablets, addressing the challenges of limited interaction space in mobile knowledge work.", "motivation": "The motivation is to improve the usability of spreadsheets, which are widely used but challenging to interact with, especially on mobile devices, by leveraging the immersive display space of VR.", "method": "The paper presents a tool-set for enhancing spreadsheet interaction on tablets using VR headsets and pen-based input. It proposes using the space around the tablet for enhanced visualization and combining pen input with spatial sensing for efficient creation and editing of spreadsheets.", "result": "The feasibility of the proposed tool-set is studied using a video-based online survey and an expert-based assessment of human performance potential.", "conclusion": "The study explores the potential of VR to enhance spreadsheet interaction on tablets, proposing a tool-set that combines VR headsets, pen-based input, and spatial sensing to improve productivity."}}
{"id": "2005.05227", "title": "ObjTables: structured spreadsheets that promote data quality, reuse, and integration", "url": "https://arxiv.org/abs/2005.05227", "pdf": "https://arxiv.org/pdf/2005.05227", "abs": "https://arxiv.org/abs/2005.05227", "authors": ["Jonathan R. Karr", "Wolfram Liebermeister", "Arthur P. Goldberg", "John A. P. Sekar", "Bilal Shaikh"], "categories": ["cs.DB", "q-bio.QM"], "comment": "5 pages, 1 figures, 18 pages of supplementary information, 3 supplementary datasets", "summary": "A central challenge in science is to understand how systems behaviors emerge from complex networks. This often requires aggregating, reusing, and integrating heterogeneous information. Supplementary spreadsheets to articles are a key data source. Spreadsheets are popular because they are easy to read and write. However, spreadsheets are often difficult to reanalyze because they capture data ad hoc without schemas that define the objects, relationships, and attributes that they represent. To help researchers reuse and compose spreadsheets, we developed ObjTables, a toolkit that makes spreadsheets human- and machine-readable by combining spreadsheets with schemas and an object-relational mapping system. ObjTables includes a format for schemas; markup for indicating the class and attribute represented by each spreadsheet and column; numerous data types for scientific information; and high-level software for using schemas to read, write, validate, compare, merge, revision, and analyze spreadsheets. By making spreadsheets easier to reuse, ObjTables could enable unprecedented secondary meta-analyses. By making it easy to build new formats and associated software for new types of data, ObjTables can also accelerate emerging scientific fields.", "AI": {"tldr": "ObjTables is a toolkit that makes spreadsheets human- and machine-readable by combining spreadsheets with schemas and an object-relational mapping system.", "motivation": "Spreadsheets are often difficult to reanalyze because they capture data ad hoc without schemas that define the objects, relationships, and attributes that they represent.", "method": "ObjTables combines spreadsheets with schemas and an object-relational mapping system. It includes a format for schemas; markup for indicating the class and attribute represented by each spreadsheet and column; numerous data types for scientific information; and high-level software for using schemas to read, write, validate, compare, merge, revision, and analyze spreadsheets.", "result": "ObjTables makes spreadsheets easier to reuse, which could enable unprecedented secondary meta-analyses.", "conclusion": "ObjTables can also accelerate emerging scientific fields by making it easy to build new formats and associated software for new types of data."}}
{"id": "2007.00003", "title": "EQUS -- helping to see formulae", "url": "https://arxiv.org/abs/2007.00003", "pdf": "https://arxiv.org/pdf/2007.00003", "abs": "https://arxiv.org/abs/2007.00003", "authors": ["Chris Roast"], "categories": ["cs.HC", "cs.SE"], "comment": "12 Pages, 7 Colour Figures", "summary": "Visualisation is often presented as a means of simplifying information and helping people understand complex data. In this paper we describe the design, development and evaluation of an interactive visualisation for spreadsheet formulae (EQUS). The work is justified on the grounds that these are widely used tools for significant numerical processing and modeling, yet the formula developed can be easily misunderstood. The development process was one of iterative refinement engaging an initial target audience of mid-teen learners, involving re-design and formative evaluation. The resulting visualisation techniques have been found to be broadly relevant to spreadsheet users beyond the initial target audience. EQUS has since been developed as fully integrated plug-in for MS Excel.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8e\u7535\u5b50\u8868\u683c\u516c\u5f0f\u7684\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5de5\u5177EQUS\u7684\u8bbe\u8ba1\u3001\u5f00\u53d1\u548c\u8bc4\u4f30\u3002", "motivation": "\u7535\u5b50\u8868\u683c\u88ab\u5e7f\u6cdb\u7528\u4e8e\u91cd\u8981\u7684\u6570\u503c\u5904\u7406\u548c\u5efa\u6a21\uff0c\u4f46\u516c\u5f0f\u5bb9\u6613\u88ab\u8bef\u89e3\u3002", "method": "\u901a\u8fc7\u8fed\u4ee3\u6539\u8fdb\uff0c\u4e0e\u6700\u521d\u7684\u76ee\u6807\u53d7\u4f17\uff08\u9752\u5c11\u5e74\u5b66\u4e60\u8005\uff09\u4e92\u52a8\uff0c\u8fdb\u884c\u91cd\u65b0\u8bbe\u8ba1\u548c\u5f62\u6210\u6027\u8bc4\u4f30\u3002", "result": "\u53ef\u89c6\u5316\u6280\u672f\u88ab\u53d1\u73b0\u4e0e\u7535\u5b50\u8868\u683c\u7528\u6237\u5e7f\u6cdb\u76f8\u5173\uff0cEQUS\u5df2\u88ab\u5f00\u53d1\u4e3aMS Excel\u7684\u5b8c\u5168\u96c6\u6210\u63d2\u4ef6\u3002", "conclusion": "EQUS\u662f\u4e00\u4e2a\u6709\u6548\u7684\u7535\u5b50\u8868\u683c\u516c\u5f0f\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u5bf9\u5e7f\u6cdb\u7684\u7535\u5b50\u8868\u683c\u7528\u6237\u6709\u76ca\u3002"}}
{"id": "2006.14694", "title": "From webtables to datatables", "url": "https://arxiv.org/abs/2006.14694", "pdf": "https://arxiv.org/pdf/2006.14694", "abs": "https://arxiv.org/abs/2006.14694", "authors": ["M\u00e1ria Csernoch"], "categories": ["cs.SE"], "comment": "22 pages, 34 Formulae & 21 Colour Figures", "summary": "Webtables -- tables and table-like structures on webpages -- are excellent sources for teaching spreadsheeting, in commercial and professional organisations by utilizing and developing knowledge-transfer items, presenting and handling various real-world problems and solutions, discussing and debugging, and in general, developing and utilizing computational thinking skills. In the present paper the conversion process of one of the LOL Boards (League of Legends, Riot Games Inc. 2019) is detailed. After presenting the algorithm of the conversion, two solutions are offered -- one in a word processor, the other purely in a spreadsheet application -- leaving space for discussions, inventing other solutions and combining them.", "AI": {"tldr": "Webtables on webpages can be used for teaching spreadsheeting and computational thinking skills.", "motivation": "Demonstrates how Webtables can be used for teaching spreadsheeting, in commercial and professional organisations by utilizing and developing knowledge-transfer items, presenting and handling various real-world problems and solutions, discussing and debugging, and in general, developing and utilizing computational thinking skills.", "method": "Details the conversion process of a LOL Board, presenting an algorithm and offering two solutions: one in a word processor and one in a spreadsheet application.", "result": "Presents two solutions for converting a LOL Board, one in a word processor and the other purely in a spreadsheet application.", "conclusion": "The conversion process and solutions leave space for discussions, inventing other solutions and combining them."}}
{"id": "2006.08224", "title": "Needles in the 'Sheet'stack: Augmented Analytics to get Insights from Spreadsheets", "url": "https://arxiv.org/abs/2006.08224", "pdf": "https://arxiv.org/pdf/2006.08224", "abs": "https://arxiv.org/abs/2006.08224", "authors": ["Medha Atre", "Anand Deshpande", "Reshma Godse", "Pooja Deokar", "Sandip Moharir", "Dhruva Ray", "Akshay Chitlangia", "Trupti Phadnis", "Yugansh Goyal"], "categories": ["cs.DB", "cs.HC"], "comment": null, "summary": "Business intelligence (BI) tools for database analytics have come a long way and nowadays also provide ready insights or visual query explorations, e.g. QuickInsights by Microsoft Power BI, SpotIQ by ThoughtSpot, Zenvisage, etc. In this demo, we focus on providing insights by examining periodic spreadsheets of different reports (aka views), without prior knowledge of the schema of the database or reports, or data information. Such a solution is targeted at users without the familiarity with the database schema or resources to conduct analytics in the contemporary way.", "AI": {"tldr": "This paper introduces a business intelligence (BI) tool that provides insights from periodic spreadsheets without requiring knowledge of the database schema or data information.", "motivation": "Existing BI tools require familiarity with the database schema or resources to conduct analytics. This paper aims to provide a solution for users without such knowledge or resources.", "method": "The paper focuses on examining periodic spreadsheets of different reports (aka views) to provide insights.", "result": "The paper introduces a BI tool that can provide insights from periodic spreadsheets without requiring prior knowledge of the database schema or data information.", "conclusion": "The proposed solution targets users without the familiarity with the database schema or resources to conduct analytics in the contemporary way."}}
{"id": "2006.05814", "title": "Implementation Strategies for Multidimensional Spreadsheets", "url": "https://arxiv.org/abs/2006.05814", "pdf": "https://arxiv.org/pdf/2006.05814", "abs": "https://arxiv.org/abs/2006.05814", "authors": ["Paul Mireault"], "categories": ["cs.SE"], "comment": "12 Pages, 18 Colour Figures. arXiv admin note: text overlap with arXiv:1801.09777", "summary": "Seasoned Excel developers were invited to participate in a challenge to implement a spreadsheet with multi-dimensional variables. We analyzed their spreadsheet to see the different implement strategies employed. We identified two strategies: most participants used a projection of three or four-dimensional variables on the two-dimensional plane used by Excel. A few participants used a database approach where the multi-dimensional variables are presented in the form of a dataset table with the appropriate primary key. This approach leads to simpler formulas.", "AI": {"tldr": "This paper analyzes how experienced Excel developers handle multi-dimensional variables in spreadsheets.", "motivation": "To understand the different strategies employed when implementing spreadsheets with multi-dimensional variables in Excel.", "method": "Analyzing spreadsheets created by seasoned Excel developers who participated in a challenge.", "result": "Identified two main strategies: projection onto a 2D plane and a database approach using dataset tables.", "conclusion": "The database approach leads to simpler formulas compared to the projection method."}}
{"id": "2006.04794", "title": "Abstracting spreadsheet data flow through hypergraph redrawing", "url": "https://arxiv.org/abs/2006.04794", "pdf": "https://arxiv.org/pdf/2006.04794", "abs": "https://arxiv.org/abs/2006.04794", "authors": ["David Birch", "Nicolai Stawinoga", "Jack Binks", "Bruno Nicoletti", "Paul Kelly"], "categories": ["cs.SE"], "comment": "23 Pages, 12 Colour Figures", "summary": "We believe the error prone nature of traditional spreadsheets is due to their low level of abstraction. End user programmers are forced to construct their data models from low level cells which we define as \"a data container or manipulator linked by user-intent to model their world and positioned to reflect its structure\". Spreadsheet cells are limited in what they may contain (scalar values) and the links between them are inherently hidden. This paper proposes a method of raising the level of abstraction of spreadsheets by \"redrawing the boundary\" of the cell. To expose the hidden linkage structure we transform spreadsheets into fine-grained graphs with operators and values as nodes. \"cells\" are then represented as hypergraph edges by drawing a boundary \"wall\" around a set of operator/data nodes. To extend what cells may contain and to create a higher level model of the spreadsheet we propose that researchers should seek techniques to redraw these boundaries to create higher level \"cells\" which will more faithfully represent the end-user's real world/mental model. We illustrate this approach via common sub-expression identification and the application of sub-tree isomorphisms for the detection of vector (array) operations.", "AI": {"tldr": "This paper proposes a method of raising the level of abstraction of spreadsheets by redrawing the boundary of the cell.", "motivation": "The error prone nature of traditional spreadsheets is due to their low level of abstraction.", "method": "Transform spreadsheets into fine-grained graphs with operators and values as nodes. Cells are then represented as hypergraph edges by drawing a boundary wall around a set of operator/data nodes.", "result": "Illustrate this approach via common sub-expression identification and the application of sub-tree isomorphisms for the detection of vector (array) operations.", "conclusion": "Researchers should seek techniques to redraw these boundaries to create higher level cells which will more faithfully represent the end-user's real world/mental model."}}
{"id": "2006.04793", "title": "Developing Excel Thought Leadership", "url": "https://arxiv.org/abs/2006.04793", "pdf": "https://arxiv.org/pdf/2006.04793", "abs": "https://arxiv.org/abs/2006.04793", "authors": ["David Lyford-Smith"], "categories": ["cs.CY"], "comment": "8 Pages", "summary": "Over a period of five years, the Institute of Chartered Accountants in England and Wales (ICAEW) has developed a suite of three 'thought leadership' papers surrounding good practice in spreadsheet use and spreadsheet work environments. We will review the history of these three papers, the key lessons which each has to teach, and discuss how the process of making them has helped ICAEW to develop its position in the field.", "AI": {"tldr": "ICAEW developed three papers on spreadsheet best practices over five years.", "motivation": "To promote good practice in spreadsheet use and work environments.", "method": "Reviewing the history and key lessons of the three papers.", "result": "ICAEW developed its position in the field.", "conclusion": "The paper discusses how creating the papers helped ICAEW develop its position."}}
{"id": "2004.11113", "title": "Human-Machine Collaboration for Democratizing Data Science", "url": "https://arxiv.org/abs/2004.11113", "pdf": "https://arxiv.org/pdf/2004.11113", "abs": "https://arxiv.org/abs/2004.11113", "authors": ["Cl\u00e9ment Gautrais", "Yann Dauxais", "Stefano Teso", "Samuel Kolb", "Gust Verbruggen", "Luc De Raedt"], "categories": ["cs.AI", "cs.HC"], "comment": "26 pages", "summary": "Everybody wants to analyse their data, but only few posses the data science expertise to to this. Motivated by this observation we introduce a novel framework and system \\textsc{VisualSynth} for human-machine collaboration in data science.\n  It wants to democratize data science by allowing users to interact with standard spreadsheet software in order to perform and automate various data analysis tasks ranging from data wrangling, data selection, clustering, constraint learning, predictive modeling and auto-completion. \\textsc{VisualSynth} relies on the user providing colored sketches, i.e., coloring parts of the spreadsheet, to partially specify data science tasks, which are then determined and executed using artificial intelligence techniques.", "AI": {"tldr": "VisualSynth is a system for human-machine collaboration in data science that allows users to interact with standard spreadsheet software to perform and automate various data analysis tasks.", "motivation": "Democratize data science by allowing users to interact with standard spreadsheet software in order to perform and automate various data analysis tasks.", "method": "Relying on the user providing colored sketches to partially specify data science tasks, which are then determined and executed using artificial intelligence techniques.", "result": "Various data analysis tasks ranging from data wrangling, data selection, clustering, constraint learning, predictive modeling and auto-completion.", "conclusion": "Introduce a novel framework and system VisualSynth for human-machine collaboration in data science."}}
{"id": "2001.01007", "title": "Automated Discovery of Data Transformations for Robotic Process Automation", "url": "https://arxiv.org/abs/2001.01007", "pdf": "https://arxiv.org/pdf/2001.01007", "abs": "https://arxiv.org/abs/2001.01007", "authors": ["Volodymyr Leno", "Marlon Dumas", "Marcello La Rosa", "Fabrizio Maria Maggi", "Artem Polyvyanyy"], "categories": ["cs.AI"], "comment": "8 pages, 5 figures. To be published in proceedings of AAAI-20 workshop on Intelligent Process Automation", "summary": "Robotic Process Automation (RPA) is a technology for automating repetitive routines consisting of sequences of user interactions with one or more applications. In order to fully exploit the opportunities opened by RPA, companies need to discover which specific routines may be automated, and how. In this setting, this paper addresses the problem of analyzing User Interaction (UI) logs in order to discover routines where a user transfers data from one spreadsheet or (Web) form to another. The paper maps this problem to that of discovering data transformations by example - a problem for which several techniques are available. The paper shows that a naive application of a state-of-the-art technique for data transformation discovery is computationally inefficient. Accordingly, the paper proposes two optimizations that take advantage of the information in the UI log and the fact that data transfers across applications typically involve copying alphabetic and numeric tokens separately. The proposed approach and its optimizations are evaluated using UI logs that replicate a real-life repetitive data transfer routine.", "AI": {"tldr": "This paper introduces a method to discover and optimize data transfer routines from UI logs for Robotic Process Automation (RPA).", "motivation": "Companies need to identify and automate repetitive routines, specifically data transfers between spreadsheets or web forms, to fully utilize RPA.", "method": "The paper uses data transformation discovery techniques and optimizes them for UI logs, considering alphabetic and numeric token separation in data transfers.", "result": "The proposed approach and optimizations are evaluated using UI logs that replicate a real-life data transfer routine, demonstrating improved computational efficiency.", "conclusion": "The paper introduces a feasible method to discover data transformations in UI logs."}}
{"id": "1912.09209", "title": "Comprehensive review for common types of errors using spreadsheets", "url": "https://arxiv.org/abs/1912.09209", "pdf": "https://arxiv.org/pdf/1912.09209", "abs": "https://arxiv.org/abs/1912.09209", "authors": ["Ali Aburas"], "categories": ["cs.SE"], "comment": null, "summary": "Thanks to their flexibility and capability to perform different tasks and organize data in the best form and format, spreadsheets are widely used in different organizations and by different end users. Many business organizations rely on spreadsheets to fulfill their various tasks. On the other hand, the number of spreadsheets that contain errors are very high, thus researchers have developed different tools aimed at the prevention, detection, and correction of errors in spreadsheets. This research work is a comprehensive review that describes and classifies approaches on finding and fixing errors in spreadsheets. The paper discusses up-to-date research work approaches in terms of definition, how they work, and kinds of errors they can find in spreadsheets. The paper looks also for the kinds of errors that end users commonly make in spreadsheets.", "AI": {"tldr": "This paper reviews methods for finding and fixing errors in spreadsheets.", "motivation": "Spreadsheets are important but error-prone, impacting business organizations.", "method": "Comprehensive review and classification of existing approaches.", "result": "Discussion of up-to-date research, error definitions, and common user errors.", "conclusion": "Summarizes current research on spreadsheet error detection and correction, also identifying common user errors."}}
{"id": "1910.05685", "title": "A Coding-free Software Framework of Developing Web Data Management Systems", "url": "https://arxiv.org/abs/1910.05685", "pdf": "https://arxiv.org/pdf/1910.05685", "abs": "https://arxiv.org/abs/1910.05685", "authors": ["Can Yang", "Shiying Pan", "Runmin Li", "Yu Liu", "Lizhang Peng"], "categories": ["cs.SE"], "comment": "16pages, 11 figures, 2 tables", "summary": "More and more enterprises recently intend to deploy data management systems in the cloud. Due to the professionalism of software development, it has still been difficult for non-programmers to develop this kind of systems, even a small one. However, the development of SaaS brings forth the more feasibility of coding-free software development than before. Based on the SaaS architecture, this paper presents a set of theory and method for coding-free construction of a data management system, on which our contributions involve in a practical application platform, a set of construction method and a set of interface on data exchange. By abstracting the common features of data management systems, we design a universal web platform to quickly generate and publish customized system instances. Moreover, we propose a kind of method to develop a data management system using a specific requirements table in spreadsheet. The corresponding platform maps the requirements table into a system instance through parsing the table model and implementing the objective system in the running stage. Finally, we implement the proposed framework and deploy it on web. The empirical result demonstrates the feasibility and availability of the coding-free method in developing web data management systems.", "AI": {"tldr": "Unexpected Error", "motivation": "Error code: 429 - [{'error': {'code': 429, 'message': 'Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.', 'status': 'RESOURCE_EXHAUSTED'}}]", "method": "N/A", "result": "N/A", "conclusion": "N/A"}}
{"id": "1712.05944", "title": "Taggle: Combining Overview and Details in Tabular Data Visualizations", "url": "https://arxiv.org/abs/1712.05944", "pdf": "https://arxiv.org/pdf/1712.05944", "abs": "https://arxiv.org/abs/1712.05944", "authors": ["Katarina Furmanova", "Samuel Gratzl", "Holger Stitz", "Thomas Zichner", "Miroslava Jaresova", "Alexander Lex", "Marc Streit"], "categories": ["cs.HC"], "comment": null, "summary": "Most tabular data visualization techniques focus on overviews, yet many practical analysis tasks are concerned with investigating individual items of interest. At the same time, relating an item to the rest of a potentially large table is important. In this work we present Taggle, a tabular visualization technique for exploring and presenting large and complex tables. Taggle takes an item-centric, spreadsheet-like approach, visualizing each row in the source data individually using visual encodings for the cells. At the same time, Taggle introduces data-driven aggregation of data subsets. The aggregation strategy is complemented by interaction methods tailored to answer specific analysis questions, such as sorting based on multiple columns and rich data selection and filtering capabilities. We demonstrate Taggle using a case study conducted by a domain expert on complex genomics data analysis for the purpose of drug discovery.", "AI": {"tldr": "Taggle is a tabular visualization technique for exploring and presenting large and complex tables.", "motivation": "Many practical analysis tasks are concerned with investigating individual items of interest, relating an item to the rest of a potentially large table is important.", "method": "Item-centric, spreadsheet-like approach, visualizing each row in the source data individually using visual encodings for the cells. Data-driven aggregation of data subsets. Interaction methods tailored to answer specific analysis questions, such as sorting based on multiple columns and rich data selection and filtering capabilities.", "result": "Demonstrated Taggle using a case study conducted by a domain expert on complex genomics data analysis for the purpose of drug discovery.", "conclusion": "Taggle is a tabular visualization technique for exploring and presenting large and complex tables."}}
{"id": "1909.07462", "title": "A Case Study of Spreadsheet Use within the Finance and Academic Registry units within a Higher Education Institution", "url": "https://arxiv.org/abs/1909.07462", "pdf": "https://arxiv.org/pdf/1909.07462", "abs": "https://arxiv.org/abs/1909.07462", "authors": ["Simon Thorne", "Jamie Hancock"], "categories": ["cs.CY"], "comment": "15 Pages, 20 Tables", "summary": "This paper presents the findings of a case study of spreadsheet use in a higher education institution in the UK. The paper considers the use of spreadsheets in two units of the organisation, academic registry and finance. Spreadsheet use is explored in terms of importance, training, experience, purpose, techniques deployed, size of spreadsheets created and sharing of spreadsheets. The implications of the results are then considered in terms of accurate reporting to external funding bodies such the funding councils, internal data integrity and internal data efficiencies. The results show a large volume of spreadsheets being created and used, that the profile of spreadsheet developers is typical of other studies of spreadsheet use and the need for the organisation to have clear principles and guidelines for the development of spreadsheet models in the organisation to ensure data integrity, reduce duplication of effort and to optimise the use of spreadsheets to meet the institutions goals.", "AI": {"tldr": "Unexpected Error", "motivation": "Error code: 429 - [{'error': {'code': 429, 'message': 'Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.', 'status': 'RESOURCE_EXHAUSTED'}}]", "method": "N/A", "result": "N/A", "conclusion": "N/A"}}
{"id": "1909.00891", "title": "Structured Spreadsheet Modelling and Implementation with Multiple Dimensions -- Part 2: Implementation", "url": "https://arxiv.org/abs/1909.00891", "pdf": "https://arxiv.org/pdf/1909.00891", "abs": "https://arxiv.org/abs/1909.00891", "authors": ["Paul Mireault"], "categories": ["cs.SE"], "comment": "14 Pages, 12 Colour Figures, 3 Tables. First presented at EuSpRIG 2018, Imperial College, London", "summary": "In Part 1, we showed how to develop a conceptual model of a problem involving variables of multiple dimensions, like Products, Regions, Sectors and Months. The conceptual model is presented as a Formula Diagram, giving a global view of the interaction between all the variables, and a Formula List, giving a precise view of the interaction between the variables. In this paper, we present precise steps to implement a multi-dimensional problem in a way that will produce a spreadsheet that is easy to maintain", "AI": {"tldr": "This paper provides steps to implement a multi-dimensional problem into a maintainable spreadsheet.", "motivation": "Building upon a conceptual model from Part 1 involving multi-dimensional variables (Products, Regions, Sectors, Months).", "method": "Presents precise steps for implementation.", "result": "Produces a maintainable spreadsheet.", "conclusion": "Details the implementation of a multi-dimensional problem for easy maintenance in a spreadsheet."}}
{"id": "1909.00865", "title": "Are digital natives spreadsheet natives?", "url": "https://arxiv.org/abs/1909.00865", "pdf": "https://arxiv.org/pdf/1909.00865", "abs": "https://arxiv.org/abs/1909.00865", "authors": ["Maria Csernoch", "Piroska Bir\u00f3"], "categories": ["cs.HC"], "comment": "13 Pages, 6 Colour Figures, 9 Tables. First Presented at the EuSpRIG 2018 conference, Imperial College, London", "summary": "The present paper reports the results of testing first year students of Informatics on their algorithmic skills and knowledge transfer abilities in spreadsheet environments. The selection of students plays a crucial role in the project. On the one hand, they have officially finished their spreadsheet training - they know everything - while on the other hand, they do not need any training, since they are digital natives, to whom digital skills are assigned by birth. However, we found that the students had serious difficulties in solving the spreadsheet problems presented: so low were their results that it allowed us to form broad tendencies. Considering computational thinking, algorithmic skills, and knowledge transfer abilities, it is clear that those students performed better who used algorithm-based, multilevel array formulas instead of problem specific, unconnected built-in functions. Furthermore, we can conclude that students, regardless of their birth date and digital generation assigned to them, are in great need of official, high-mathability, algorithm-based training with expert teachers.", "AI": {"tldr": "This paper investigates the algorithmic skills and knowledge transfer abilities of first-year Informatics students in spreadsheet environments.", "motivation": "To assess the spreadsheet skills of students who have completed spreadsheet training and are considered digital natives.", "method": "Testing first-year Informatics students on spreadsheet problems and analyzing their solutions.", "result": "Students struggled with spreadsheet problems, with better performance from those using algorithm-based array formulas.", "conclusion": "Students need algorithm-based training with expert teachers, regardless of their digital native status."}}
{"id": "1909.02960", "title": "Real-time stock analysis for blending recipes in industrial plants", "url": "https://arxiv.org/abs/1909.02960", "pdf": "https://arxiv.org/pdf/1909.02960", "abs": "https://arxiv.org/abs/1909.02960", "authors": ["Florin Zamfir", "Nicolae Paraschiv", "Emil Pricop"], "categories": ["cs.OH"], "comment": "Accepted for presentation at 23rd International Conference on System Theory, Control and Computing (ICSTCC 2019), October 9-11, 2019, Sinaia, Romania", "summary": "Many companies use Excel spreadsheets to keep stock records and to calculate process-specific data. These spreadsheets are often hard to understand and track. And if the user does not protect them, there is a risk that the user randomly changes or erase formulas. The paper focuses on the stocks of products used in a blending process with a known recipe. Developing an application that can bring this data in a centralized form and that can assist the operator in decide is a necessity. When a programmer implements an application that uses data from plants he needs to consider one fundamental aspect as reading real-time data from the process. The real-time stock analysis application takes into account all the above elements. The application is easy to use by an operator in the command room of installation because of the planning algorithms integrated into it. The algorithms proposed and implemented in this paper have well-defined goals: identifying the ingredients needed to achieve the blending process for required quantities, determine the quantities of the finished product that can be made with the existing ingredients and determine the optimum quantities of the finished product. The application implemented in C# intensively uses these algorithms and gives the user the ability to build the result step by step.", "AI": {"tldr": "This paper introduces a C# application for real-time stock analysis in blending processes, replacing Excel spreadsheets.", "motivation": "Excel spreadsheets for stock records are hard to track and prone to errors, motivating a centralized application.", "method": "Developing a C# application with planning algorithms to manage stocks and blending process data.", "result": "The application identifies ingredients, determines product quantities, and optimizes blending.", "conclusion": "The application provides a user-friendly interface for operators to manage blending processes efficiently."}}
{"id": "1908.08187", "title": "A CNN toolbox for skin cancer classification", "url": "https://arxiv.org/abs/1908.08187", "pdf": "https://arxiv.org/pdf/1908.08187", "abs": "https://arxiv.org/abs/1908.08187", "authors": ["Fabrizio Nunnari", "Daniel Sonntag"], "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "DFKI Technical Report", "summary": "We describe a software toolbox for the configuration of deep neural networks in the domain of skin cancer classification. The implemented software architecture allows developers to quickly set up new convolutional neural network (CNN) architectures and hyper-parameter configurations. At the same time, the user interface, manageable as a simple spreadsheet, allows non-technical users to explore different configuration settings that need to be explored when switching to different data sets. In future versions, meta leaning frameworks can be added, or AutoML systems that continuously improve over time. Preliminary results, conducted with two CNNs in the context melanoma detection on dermoscopic images, quantify the impact of image augmentation, image resolution, and rescaling filter on the overall detection performance and training time.", "AI": {"tldr": "A software toolbox is developed for configuring CNNs for skin cancer classification, enabling both technical and non-technical users to experiment with different configurations.", "motivation": "The need for a flexible tool to configure CNNs for skin cancer classification and explore hyper-parameter settings.", "method": "A software architecture with a user interface manageable as a spreadsheet is implemented, allowing quick setup of CNN architectures and hyper-parameter configurations.", "result": "Preliminary results quantify the impact of image augmentation, image resolution, and rescaling filter on melanoma detection performance and training time using two CNNs on dermoscopic images.", "conclusion": "The toolbox facilitates CNN configuration and exploration of different settings, with potential for future expansion with meta-learning frameworks or AutoML systems."}}
{"id": "1907.03595", "title": "Recommending Related Tables", "url": "https://arxiv.org/abs/1907.03595", "pdf": "https://arxiv.org/pdf/1907.03595", "abs": "https://arxiv.org/abs/1907.03595", "authors": ["Shuo Zhang", "Krisztian Balog"], "categories": ["cs.IR"], "comment": null, "summary": "Tables are an extremely powerful visual and interactive tool for structuring and manipulating data, making spreadsheet programs one of the most popular computer applications. In this paper we introduce and address the task of recommending related tables: given an input table, identifying and returning a ranked list of relevant tables. One of the many possible application scenarios for this task is to provide users of a spreadsheet program proactively with recommendations for related structured content on the Web. At its core, the related table recommendation task boils down to computing the similarity between a pair of tables. We develop a theoretically sound framework for performing table matching. Our approach hinges on the idea of representing table elements in multiple semantic spaces, and then combining element-level similarities using a discriminative learning model. Using a purpose-built test collection from Wikipedia tables, we demonstrate that the proposed approach delivers state-of-the-art performance.", "AI": {"tldr": "This paper introduces the task of recommending related tables, which involves identifying and ranking relevant tables given an input table, with the goal of providing recommendations for structured content on the Web.", "motivation": "The motivation is to provide users of spreadsheet programs with proactive recommendations for related structured content on the Web.", "method": "The method involves representing table elements in multiple semantic spaces and combining element-level similarities using a discriminative learning model.", "result": "The proposed approach delivers state-of-the-art performance on a purpose-built test collection from Wikipedia tables.", "conclusion": "The paper demonstrates that the proposed approach effectively addresses the task of recommending related tables and achieves state-of-the-art performance."}}
{"id": "1907.04827", "title": "Hillview: A trillion-cell spreadsheet for big data", "url": "https://arxiv.org/abs/1907.04827", "pdf": "https://arxiv.org/pdf/1907.04827", "abs": "https://arxiv.org/abs/1907.04827", "authors": ["Mihai Budiu", "Parikshit Gopalan", "Lalith Suresh", "Udi Wieder", "Han Kruiger", "Marcos K. Aguilera"], "categories": ["cs.DC"], "comment": null, "summary": "Hillview is a distributed spreadsheet for browsing very large datasets that cannot be handled by a single machine. As a spreadsheet, Hillview provides a high degree of interactivity that permits data analysts to explore information quickly along many dimensions while switching visualizations on a whim. To provide the required responsiveness, Hillview introduces visualization sketches, or vizketches, as a simple idea to produce compact data visualizations. Vizketches combine algorithmic techniques for data summarization with computer graphics principles for efficient rendering. While simple, vizketches are effective at scaling the spreadsheet by parallelizing computation, reducing communication, providing progressive visualizations, and offering precise accuracy guarantees. Using Hillview running on eight servers, we can navigate and visualize datasets of tens of billions of rows and trillions of cells, much beyond the published capabilities of competing systems.", "AI": {"tldr": "Hillview is a distributed spreadsheet for big data visualization, enabling interactive exploration of massive datasets.", "motivation": "Existing systems struggle to handle and visualize very large datasets interactively.", "method": "Hillview uses visualization sketches (vizketches) for compact data visualizations, combining data summarization and efficient rendering techniques.", "result": "Hillview can navigate and visualize datasets of tens of billions of rows and trillions of cells using eight servers.", "conclusion": "Vizketches are effective at scaling spreadsheets by parallelizing computation, reducing communication, providing progressive visualizations, and offering precise accuracy guarantees."}}
{"id": "1907.04217", "title": "Streaming 1.9 Billion Hypersparse Network Updates per Second with D4M", "url": "https://arxiv.org/abs/1907.04217", "pdf": "https://arxiv.org/pdf/1907.04217", "abs": "https://arxiv.org/abs/1907.04217", "authors": ["Jeremy Kepner", "Vijay Gadepally", "Lauren Milechin", "Siddharth Samsi", "William Arcand", "David Bestor", "William Bergeron", "Chansup Byun", "Matthew Hubbell", "Michael Houle", "Michael Jones", "Anne Klein", "Peter Michaleas", "Julie Mullen", "Andrew Prout", "Antonio Rosa", "Charles Yee", "Albert Reuther"], "categories": ["cs.DC", "cs.DB", "cs.DS", "cs.IR", "cs.PF"], "comment": "6 pages; 6 figures; accepted to IEEE High Performance Extreme Computing (HPEC) Conference 2019. arXiv admin note: text overlap with arXiv:1807.05308, arXiv:1902.00846", "summary": "The Dynamic Distributed Dimensional Data Model (D4M) library implements associative arrays in a variety of languages (Python, Julia, and Matlab/Octave) and provides a lightweight in-memory database implementation of hypersparse arrays that are ideal for analyzing many types of network data. D4M relies on associative arrays which combine properties of spreadsheets, databases, matrices, graphs, and networks, while providing rigorous mathematical guarantees, such as linearity. Streaming updates of D4M associative arrays put enormous pressure on the memory hierarchy. This work describes the design and performance optimization of an implementation of hierarchical associative arrays that reduces memory pressure and dramatically increases the update rate into an associative array. The parameters of hierarchical associative arrays rely on controlling the number of entries in each level in the hierarchy before an update is cascaded. The parameters are easily tunable to achieve optimal performance for a variety of applications. Hierarchical arrays achieve over 40,000 updates per second in a single instance. Scaling to 34,000 instances of hierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of 1,900,000,000 updates per second. This capability allows the MIT SuperCloud to analyze extremely large streaming network data sets.", "AI": {"tldr": "This paper introduces a hierarchical associative array implementation for the D4M library to improve memory pressure and update rates for streaming network data analysis.", "motivation": "Streaming updates of D4M associative arrays put enormous pressure on the memory hierarchy.", "method": "The paper designs and optimizes a hierarchical associative array implementation, controlling the number of entries in each level of the hierarchy before cascading updates. Parameters are tunable for optimal performance.", "result": "Hierarchical arrays achieve over 40,000 updates per second in a single instance and 1,900,000,000 updates per second on 1,100 server nodes.", "conclusion": "The new implementation enables the MIT SuperCloud to analyze extremely large streaming network datasets."}}
{"id": "1907.02099", "title": "GeoGebra e situa\u00e7\u00f5es que envolvem modela\u00e7\u00e3o numa abordagem STEAM", "url": "https://arxiv.org/abs/1907.02099", "pdf": "https://arxiv.org/pdf/1907.02099", "abs": "https://arxiv.org/abs/1907.02099", "authors": ["J. M. D. S. Dos Santos", "A. P. Silveira", "A. E. S. Trocado"], "categories": ["math.HO", "cs.CY"], "comment": "in Portuguese", "summary": "In order to implement a STEAM approach including the use of technology, namely the use of interactive mathematics software GeoGebra, in mathematics classes, in the lusophone space, the materials presented here were conceived, to be implemented in a first phase among teachers. Later, with the necessary adaptations, these tasks will be applied to the students. The tasks deal with modeling situations, in two- and three-dimensional geometric problems, in order to apply GeoGebra software in its analysis to illustrate its capabilities. The different windows of this software are used, namely the 2D and 3D windows, CAS window, spreadsheet and extra two dimensional windows in order to study cutting planes in solids and some surfaces. The tasks are presented so that any user, regardless of the degree of knowledge they have of the software, can follow them, being supported in scripts with some indications of the tools and commands to use. Designed for the teaching and learning of Mathematics, from a STEAM approach, these tasks allow connections with other Sciences and the Arts, and allow the development of projects using and consolidating relevant mathematical contents. These tasks are part of the proposals of activities of the participants of the Training Courses for Trainers in GeoGebra for Portuguese Speaking Countries, which from 2019 have an impact on the STEAM approach. These courses are carried out with the high sponsorship of the Organization of Ibero-American States for Education, Science and Culture (OEI). Given the interest that the tasks have for the users of the Iberian space, as well as their dissemination at a global level, the materials initially developed in Portuguese language will be adapted for Spanish and English speakers.", "AI": {"tldr": "This paper introduces STEAM materials using GeoGebra for math classes in Lusophone countries, focusing on modeling 2D and 3D geometric problems. The materials are designed for teachers and students, adaptable for different knowledge levels, and will be translated into Spanish and English.", "motivation": "To implement a STEAM approach in mathematics classes using GeoGebra in Lusophone countries.", "method": "Modeling situations in 2D and 3D geometry using GeoGebra software, utilizing various windows like 2D, 3D, CAS, and spreadsheet.", "result": "Tasks designed for users of all GeoGebra knowledge levels, supported by scripts and tool indications. These tasks facilitate connections with other sciences and arts, promoting project development and consolidating mathematical content.", "conclusion": "The materials, initially in Portuguese, will be adapted for Spanish and English speakers, expanding their reach beyond the Iberian space. They are part of training courses impacting the STEAM approach, sponsored by the OEI."}}
{"id": "1906.04011", "title": "Visual Backpropagation", "url": "https://arxiv.org/abs/1906.04011", "pdf": "https://arxiv.org/pdf/1906.04011", "abs": "https://arxiv.org/abs/1906.04011", "authors": ["Roy S. Freedman"], "categories": ["cs.LG", "cs.PL"], "comment": null, "summary": "We show how a declarative functional programming specification of backpropagation yields a visual and transparent implementation within spreadsheets. We call our method Visual Backpropagation. This backpropagation implementation exploits array worksheet formulas, manual calculation, and has a sequential order of computation similar to the processing of a systolic array. The implementation uses no hidden macros nor user-defined functions; there are no loops, assignment statements, or links to any procedural programs written in conventional languages. As an illustration, we compare a Visual Backpropagation solution to a Tensorflow (Python) solution on a standard regression problem.", "AI": {"tldr": "This paper introduces Visual Backpropagation, a spreadsheet-based backpropagation implementation.", "motivation": "To create a visual and transparent backpropagation implementation.", "method": "Using array worksheet formulas and manual calculation in spreadsheets.", "result": "A Visual Backpropagation solution is compared to a Tensorflow solution on a regression problem.", "conclusion": "Visual Backpropagation offers a clear and understandable alternative to traditional backpropagation implementations."}}
{"id": "1905.13072", "title": "Somewhere Around That Number: An Interview Study of How Spreadsheet Users Manage Uncertainty", "url": "https://arxiv.org/abs/1905.13072", "pdf": "https://arxiv.org/pdf/1905.13072", "abs": "https://arxiv.org/abs/1905.13072", "authors": ["Judith Borghouts", "Andrew D. Gordon", "Advait Sarkar", "Kenton P. O'Hara", "Neil Toronto"], "categories": ["cs.HC"], "comment": null, "summary": "Spreadsheet users regularly deal with uncertainty in their data, for example due to errors and estimates. While an insight into data uncertainty can help in making better informed decisions, prior research suggests that people often use informal heuristics to reason with probabilities, which leads to incorrect conclusions. Moreover, people often ignore or simplify uncertainty. To understand how people currently encounter and deal with uncertainty in spreadsheets, we conducted an interview study with 11 spreadsheet users from a range of domains. We found that how people deal with uncertainty is influenced by the role the spreadsheet plays in people's work and the user's aims. Spreadsheets are used as a database, template, calculation tool, notepad and exploration tool. In doing so, participants' aims were to compute and compare different scenarios, understand something about the nature of the uncertainty in their situation, and translate the complexity of data uncertainty into simplified presentations to other people, usually decision-makers. Spreadsheets currently provide limited tools to support these aims, and participants had various workarounds.", "AI": {"tldr": "This paper investigates how spreadsheet users handle uncertainty in their data and the workarounds they employ due to the limitations of current spreadsheet tools.", "motivation": "To understand how people currently encounter and deal with uncertainty in spreadsheets.", "method": "Interview study with 11 spreadsheet users from various domains.", "result": "Identified that how people deal with uncertainty is influenced by the role of the spreadsheet and the user's aims. Spreadsheets are used as databases, templates, calculation tools, notepads, and exploration tools. Participants aim to compute and compare scenarios, understand uncertainty, and simplify presentations for decision-makers.", "conclusion": "Spreadsheets provide limited tools to support these aims, leading to various workarounds."}}
{"id": "1902.00846", "title": "A Billion Updates per Second Using 30,000 Hierarchical In-Memory D4M Databases", "url": "https://arxiv.org/abs/1902.00846", "pdf": "https://arxiv.org/pdf/1902.00846", "abs": "https://arxiv.org/abs/1902.00846", "authors": ["Jeremy Kepner", "Vijay Gadepally", "Lauren Milechin", "Siddharth Samsi", "William Arcand", "David Bestor", "William Bergeron", "Chansup Byun", "Matthew Hubbell", "Micheal Houle", "Micheal Jones", "Anne Klein", "Peter Michaleas", "Julie Mullen", "Andrew Prout", "Antonio Rosa", "Charles Yee", "Albert Reuther"], "categories": ["cs.DB", "cs.DC", "cs.DS", "cs.NI"], "comment": "Northeast Database Data 2019 (MIT)", "summary": "Analyzing large scale networks requires high performance streaming updates of graph representations of these data. Associative arrays are mathematical objects combining properties of spreadsheets, databases, matrices, and graphs, and are well-suited for representing and analyzing streaming network data. The Dynamic Distributed Dimensional Data Model (D4M) library implements associative arrays in a variety of languages (Python, Julia, and Matlab/Octave) and provides a lightweight in-memory database. Associative arrays are designed for block updates. Streaming updates to a large associative array requires a hierarchical implementation to optimize the performance of the memory hierarchy. Running 34,000 instances of a hierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of 1,900,000,000 updates per second. This capability allows the MIT SuperCloud to analyze extremely large streaming network data sets.", "AI": {"tldr": "This paper introduces a hierarchical implementation of D4M associative arrays for high-performance streaming updates of graph representations on large networks.", "motivation": "Analyzing large scale networks requires high performance streaming updates of graph representations.", "method": "The paper uses a hierarchical implementation of D4M associative arrays to optimize the performance of the memory hierarchy.", "result": "Achieved a sustained update rate of 1,900,000,000 updates per second running 34,000 instances on 1,100 server nodes.", "conclusion": "The capability allows the MIT SuperCloud to analyze extremely large streaming network data sets."}}
{"id": "1901.11100", "title": "ExceLint: Automatically Finding Spreadsheet Formula Errors", "url": "https://arxiv.org/abs/1901.11100", "pdf": "https://arxiv.org/pdf/1901.11100", "abs": "https://arxiv.org/abs/1901.11100", "authors": ["Daniel W. Barowy", "Emery D. Berger", "Benjamin Zorn"], "categories": ["cs.PL", "cs.SE"], "comment": "Appeared at OOPSLA 2018", "summary": "Spreadsheets are one of the most widely used programming environments, and are widely deployed in domains like finance where errors can have catastrophic consequences. We present a static analysis specifically designed to find spreadsheet formula errors. Our analysis directly leverages the rectangular character of spreadsheets. It uses an information-theoretic approach to identify formulas that are especially surprising disruptions to nearby rectangular regions. We present ExceLint, an implementation of our static analysis for Microsoft Excel. We demonstrate that ExceLint is fast and effective: across a corpus of 70 spreadsheets, ExceLint takes a median of 5 seconds per spreadsheet, and it significantly outperforms the state of the art analysis.", "AI": {"tldr": "This paper introduces ExceLint, a static analysis tool for detecting errors in spreadsheet formulas by leveraging the rectangular structure of spreadsheets and using an information-theoretic approach to identify surprising disruptions.", "motivation": "To address the widespread use of spreadsheets and the potential for catastrophic consequences due to errors in financial and other domains.", "method": "A static analysis is designed by using information-theoretic approach to identify formulas that are especially surprising disruptions to nearby rectangular regions.", "result": "ExceLint is fast and effective, taking a median of 5 seconds per spreadsheet across a corpus of 70 spreadsheets, and significantly outperforms the state-of-the-art analysis.", "conclusion": "ExceLint, an implementation of static analysis for Microsoft Excel, can effectively detect spreadsheet formula errors."}}
{"id": "1807.00018", "title": "Computer Simulation of Neural Networks Using Spreadsheets: The Dawn of the Age of Camelot", "url": "https://arxiv.org/abs/1807.00018", "pdf": "https://arxiv.org/pdf/1807.00018", "abs": "https://arxiv.org/abs/1807.00018", "authors": ["Serhiy O. Semerikov", "Illia O. Teplytskyi", "Yuliia V. Yechkalo", "Arnold E. Kiv"], "categories": ["cs.CY"], "comment": "26 pages, 8 figures; submitted to the 1st International Workshop on Augmented Reality in Education (AREdu 2018)", "summary": "The article substantiates the necessity to develop training methods of computer simulation of neural networks in the spreadsheet environment. The systematic review of their application to simulating artificial neural networks is performed. The authors distinguish basic approaches to solving the problem of network computer simulation training in the spreadsheet environment, joint application of spreadsheets and tools of neural network simulation, application of third-party add-ins to spreadsheets, development of macros using the embedded languages of spreadsheets; use of standard spreadsheet add-ins for non-linear optimization, creation of neural networks in the spreadsheet environment without add-ins and macros. After analyzing a collection of writings of 1890-1950, the research determines the role of the scientific journal \"Bulletin of Mathematical Biophysics\", its founder Nicolas Rashevsky and the scientific community around the journal in creating and developing models and methods of computational neuroscience. There are identified psychophysical basics of creating neural networks, mathematical foundations of neural computing and methods of neuroengineering (image recognition, in particular). The role of Walter Pitts in combining the descriptive and quantitative theories of training is discussed. It is shown that to acquire neural simulation competences in the spreadsheet environment, one should master the models based on the historical and genetic approach. It is indicated that there are three groups of models, which are promising in terms of developing corresponding methods - the continuous two-factor model of Rashevsky, the discrete model of McCulloch and Pitts, and the discrete-continuous models of Householder and Landahl.", "AI": {"tldr": "This paper discusses using spreadsheets for neural network simulation training, reviewing different approaches and highlighting historical models.", "motivation": "The paper argues for the need to develop training methods for computer simulation of neural networks within spreadsheet environments.", "method": "The paper reviews existing literature on applying spreadsheets to simulate artificial neural networks, identifying different approaches. It also analyzes historical writings to determine the role of a specific journal and scientific community in developing computational neuroscience models.", "result": "The paper identifies basic approaches to network simulation training in spreadsheets and highlights the role of key figures and models in the historical development of neural networks.", "conclusion": "The paper concludes that mastering models based on a historical and genetic approach is crucial for acquiring neural simulation competence in spreadsheet environments, and identifies three promising model groups for developing corresponding methods."}}
{"id": "1810.04542", "title": "On the Refinement of Spreadsheet Smells by means of Structure Information", "url": "https://arxiv.org/abs/1810.04542", "pdf": "https://arxiv.org/pdf/1810.04542", "abs": "https://arxiv.org/abs/1810.04542", "authors": ["Patrick Koch", "Birgit Hofer", "Franz Wotawa"], "categories": ["cs.SE"], "comment": null, "summary": "Spreadsheet users are often unaware of the risks imposed by poorly designed spreadsheets. One way to assess spreadsheet quality is to detect smells which attempt to identify parts of spreadsheets that are hard to comprehend or maintain and which are more likely to be the root source of bugs. Unfortunately, current spreadsheet smell detection techniques suffer from a number of drawbacks that lead to incorrect or redundant smell reports. For example, the same quality issue is often reported for every copy of a cell, which may overwhelm users. To deal with these issues, we propose to refine spreadsheet smells by exploiting inferred structural information for smell detection. We therefore first provide a detailed description of our static analysis approach to infer clusters and blocks of related cells. We then elaborate on how to improve existing smells by providing three example refinements of existing smells that incorporate information about cell groups and computation blocks. Furthermore, we propose three novel smell detection techniques that make use of the inferred spreadsheet structures. Empirical evaluation of the proposed techniques suggests that the refinements successfully reduce the number of incorrectly and redundantly reported smells, and novel deficits are revealed by the newly introduced smells.", "AI": {"tldr": "This paper introduces a novel approach to detect smells in spreadsheets by exploiting inferred structural information, reducing incorrect/redundant reports and revealing new deficits.", "motivation": "Current spreadsheet smell detection techniques have drawbacks that lead to incorrect or redundant smell reports.", "method": "The paper proposes a static analysis approach to infer clusters and blocks of related cells, then refines existing smells and introduces novel smell detection techniques using the inferred structures.", "result": "The refinements reduce the number of incorrectly and redundantly reported smells, and novel deficits are revealed by the newly introduced smells.", "conclusion": "The proposed techniques improve spreadsheet smell detection by reducing incorrect reports and revealing new deficits through structural information."}}
{"id": "1809.03435", "title": "Now You're Thinking With Structures: A Concept for Structure-based Interactions with Spreadsheets", "url": "https://arxiv.org/abs/1809.03435", "pdf": "https://arxiv.org/pdf/1809.03435", "abs": "https://arxiv.org/abs/1809.03435", "authors": ["Patrick Koch"], "categories": ["cs.SE"], "comment": "In Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (arXiv:1808.09174)", "summary": "Spreadsheets are the go-to tool for computerized calculation and modelling, but are hard to comprehend and adapt after reaching a certain complexity. In general, cognition of complex systems is facilitated by having a higher order mental model of the system in question to work with. We therefore present a concept for structure-aware understanding of and interaction with spreadsheets that extends previous work on structure inference in the domain. Following this concept, structural information is used to enrich visualizations, reactively enhance traditional user actions, and provide tools to proactively alter the overall spreadsheet makeup instead of individual cells The intended systems should, in first approximation, not replace common spreadsheet tools, but provide an additional layer of functionality alongside the established interface. In ongoing work, we therefore implemented a tool for structure inference and visualization along the common spreadsheet layout. Based on this framework, we plan to introduce the envisioned proactive and reactive interaction mechanics, and finally provide structure-aware unctionality as an add-in for common spreadsheet processors. We believe that providing the tools for thinking about and interacting with spreadsheets in this manner will benefit users both in terms of productivity and overall spreadsheet quality.", "AI": {"tldr": "This paper introduces a concept for structure-aware interaction with spreadsheets to improve understanding and adaptability of complex spreadsheets.", "motivation": "Spreadsheets become hard to comprehend and adapt when they reach a certain complexity. Cognition of complex systems is facilitated by having a higher order mental model.", "method": "The paper presents a concept for structure-aware understanding and interaction with spreadsheets, using structural information to enrich visualizations, reactively enhance user actions, and proactively alter the spreadsheet makeup. They implemented a tool for structure inference and visualization.", "result": "The intended system should provide an additional layer of functionality alongside the established interface. They plan to introduce proactive and reactive interaction mechanics and provide structure-aware functionality as an add-in for common spreadsheet processors.", "conclusion": "Providing tools for thinking about and interacting with spreadsheets in this manner will benefit users in terms of productivity and overall spreadsheet quality."}}
{"id": "1804.01186", "title": "Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples", "url": "https://arxiv.org/abs/1804.01186", "pdf": "https://arxiv.org/pdf/1804.01186", "abs": "https://arxiv.org/abs/1804.01186", "authors": ["Ashwin Kalyan", "Abhishek Mohta", "Oleksandr Polozov", "Dhruv Batra", "Prateek Jain", "Sumit Gulwani"], "categories": ["cs.AI", "cs.LG", "cs.PL"], "comment": "Published in ICLR 2018, International Conference on Learning Representations (2018)", "summary": "Synthesizing user-intended programs from a small number of input-output examples is a challenging problem with several important applications like spreadsheet manipulation, data wrangling and code refactoring. Existing synthesis systems either completely rely on deductive logic techniques that are extensively hand-engineered or on purely statistical models that need massive amounts of data, and in general fail to provide real-time synthesis on challenging benchmarks. In this work, we propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique that combines the best of both symbolic logic techniques and statistical models. Thus, it produces programs that satisfy the provided specifications by construction and generalize well on unseen examples, similar to data-driven systems. Our technique effectively utilizes the deductive search framework to reduce the learning problem of the neural component to a simple supervised learning setup. Further, this allows us to both train on sparingly available real-world data and still leverage powerful recurrent neural network encoders. We demonstrate the effectiveness of our method by evaluating on real-world customer scenarios by synthesizing accurate programs with up to 12x speed-up compared to state-of-the-art systems.", "AI": {"tldr": "This paper introduces Neural Guided Deductive Search (NGDS), a hybrid synthesis technique combining symbolic logic and statistical models for program synthesis from input-output examples.", "motivation": "Existing synthesis systems rely on either hand-engineered deductive logic or data-hungry statistical models, failing in real-time synthesis on challenging benchmarks.", "method": "NGDS combines deductive search with neural networks, using the former to simplify the learning problem for the latter and enabling training on limited data.", "result": "NGDS synthesizes accurate programs with up to 12x speed-up compared to state-of-the-art systems on real-world customer scenarios.", "conclusion": "NGDS effectively leverages both symbolic logic and statistical models for program synthesis, achieving improved speed and accuracy."}}
{"id": "1809.02746", "title": "Typed Table Transformations", "url": "https://arxiv.org/abs/1809.02746", "pdf": "https://arxiv.org/pdf/1809.02746", "abs": "https://arxiv.org/abs/1809.02746", "authors": ["Martin Erwig"], "categories": ["cs.SE"], "comment": "In Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (arXiv:1808.09174)", "summary": "Spreadsheet tables are often labeled, and these labels effectively constitute types for the data in the table. In such cases tables can be considered to be built from typed data where the placement of values within the table is controlled by the types used for rows and columns. We present a new approach to the transformations of spreadsheet tables that is based on transformations of row and column types. We illustrate the basic idea of type-based table construction and transformation and lay out a series of research questions that should be addressed in future work.", "AI": {"tldr": "This paper introduces a new approach to transforming spreadsheet tables based on row and column type transformations.", "motivation": "Spreadsheet tables are often labeled, and these labels effectively constitute types for the data in the table. The placement of values within the table is controlled by the types used for rows and columns.", "method": "The paper presents a new approach to the transformations of spreadsheet tables that is based on transformations of row and column types.", "result": "The paper illustrates the basic idea of type-based table construction and transformation.", "conclusion": "The paper lays out a series of research questions that should be addressed in future work."}}
{"id": "1809.00025", "title": "Implementing WHERE and ORDER BY as spreadsheet formulas", "url": "https://arxiv.org/abs/1809.00025", "pdf": "https://arxiv.org/pdf/1809.00025", "abs": "https://arxiv.org/abs/1809.00025", "authors": ["Paul Mireault"], "categories": ["cs.SE"], "comment": "In Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (arXiv:1808.09174)", "summary": "The WHERE and ORDER BY clauses of the SQL SELECT statement select a subset of rows in the result of a database query and present the result in the specified order. In a spreadsheet program like Microsoft Excel, one could use the filter and sort buttons, or use its Query or its Pivot Table tools to achieve a similar effect. The disadvantage of using those tools is that they don't react automatically to changes in the calculated values of the spreadsheet. In this paper, we develop spreadsheet formulas that implement SQL's WHERE and ORDER BY clauses.", "AI": {"tldr": "This paper introduces spreadsheet formulas that mimic SQL's WHERE and ORDER BY clauses, providing dynamic filtering and sorting.", "motivation": "Existing spreadsheet tools lack automatic reaction to changes in calculated values when filtering and sorting data.", "method": "Development of spreadsheet formulas to implement SQL's WHERE and ORDER BY clauses.", "result": "The paper develops spreadsheet formulas.", "conclusion": "Spreadsheet formulas can implement SQL's WHERE and ORDER BY clauses."}}
{"id": "1808.10642", "title": "The use of Charts, Pivot Tables, and Array Formulas in two Popular Spreadsheet Corpora", "url": "https://arxiv.org/abs/1808.10642", "pdf": "https://arxiv.org/pdf/1808.10642", "abs": "https://arxiv.org/abs/1808.10642", "authors": ["Bas Jansen", "Felienne Hermans"], "categories": ["cs.SE"], "comment": "In Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (arXiv:1808.09174)", "summary": "The use of spreadsheets in industry is widespread. Companies base decisions on information coming from spreadsheets. Unfortunately, spreadsheets are error-prone and this increases the risk that companies base their decisions on inaccurate information, which can lead to incorrect decisions and loss of money. In general, spreadsheet research is aimed to reduce the error-proneness of spreadsheets. Most research is concentrated on the use of formulas. However, there are other constructions in spreadsheets, like charts, pivot tables, and array formulas, that are also used to present decision support information to the user. There is almost no research about how these constructions are used. To improve spreadsheet quality it is important to understand how spreadsheets are used and to obtain a complete understanding, the use of charts, pivot tables, and array formulas should be included in research. In this paper, we analyze two popular spreadsheet corpora: Enron and EUSES on the use of the aforementioned constructions.", "AI": {"tldr": "This paper analyzes the use of charts, pivot tables, and array formulas in spreadsheets using the Enron and EUSES corpora to improve spreadsheet quality.", "motivation": "Spreadsheets are widely used in industry for decision-making, but they are error-prone, leading to inaccurate information and potential losses. Most research focuses on formulas, neglecting other constructions like charts, pivot tables, and array formulas.", "method": "The paper analyzes the Enron and EUSES spreadsheet corpora to study the use of charts, pivot tables, and array formulas.", "result": "The paper analyzes two popular spreadsheet corpora: Enron and EUSES on the use of the aforementioned constructions.", "conclusion": "To improve spreadsheet quality it is important to understand how spreadsheets are used and to obtain a complete understanding, the use of charts, pivot tables, and array formulas should be included in research."}}
{"id": "1808.10231", "title": "Asheetoxy: A Taxonomy for Classifying Negative Spreadsheet-related Phenomena", "url": "https://arxiv.org/abs/1808.10231", "pdf": "https://arxiv.org/pdf/1808.10231", "abs": "https://arxiv.org/abs/1808.10231", "authors": ["Daniel Kulesz", "Stefan Wagner"], "categories": ["cs.SE"], "comment": "In Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (arXiv:1808.09174)", "summary": "Spreadsheets (sometimes also called Excel programs) are powerful tools which play a business-critical role in many organizations. However, due to faulty spreadsheets many bad decisions have been taken in recent years. Since then, a number of researchers have been studying spreadsheet errors. However, one issue that hinders discussion among researchers and professionals is the lack of a commonly accepted taxonomy.\n  Albeit a number of taxonomies for spreadsheet errors have been proposed in previous work, a major issue is that they use the term error that itself is already ambiguous. Furthermore, to apply most existing taxonomies, detailed knowledge about the underlying process and knowledge about the \"brain state\" of the acting spreadsheet users is required. Due to these limitations, known error-like phenomena in freely available spreadsheet corpora cannot be classified with these taxonomies.\n  We propose Asheetoxy, a simple and phenomenon-oriented taxonomy that avoids the problematic term error altogether. An initial study with 7 participants indicates that even non-spreadsheet researchers similarly classify real-world spreadsheet phenomena using Asheetoxy.", "AI": {"tldr": "This paper introduces Asheetoxy, a new taxonomy for classifying spreadsheet phenomena that avoids the term 'error'.", "motivation": "Existing spreadsheet error taxonomies are ambiguous, require detailed process knowledge, and cannot classify phenomena in available corpora.", "method": "The authors propose a simple, phenomenon-oriented taxonomy called Asheetoxy and conduct a study with 7 participants.", "result": "The study indicates that even non-spreadsheet researchers classify spreadsheet phenomena similarly using Asheetoxy.", "conclusion": "Asheetoxy is a promising taxonomy for classifying spreadsheet phenomena."}}
{"id": "1808.09174", "title": "Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (SEMS'18)", "url": "https://arxiv.org/abs/1808.09174", "pdf": "https://arxiv.org/pdf/1808.09174", "abs": "https://arxiv.org/abs/1808.09174", "authors": ["Birgit Hofer", "Jorge Mendes"], "categories": ["cs.SE"], "comment": null, "summary": "Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (SEMS'18), held on October 1st, 2018, in Lisbon, Portugal, and co-located with the 2018 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC).", "AI": {"tldr": "This is the abstract of the 5th International Workshop on Software Engineering Methods in Spreadsheets (SEMS'18).", "motivation": "To advance research in software engineering methods for spreadsheets.", "method": "Not specified in the abstract.", "result": "Not specified in the abstract.", "conclusion": "Not specified in the abstract."}}
{"id": "1806.04952", "title": "Towards Semantically Enhanced Data Understanding", "url": "https://arxiv.org/abs/1806.04952", "pdf": "https://arxiv.org/pdf/1806.04952", "abs": "https://arxiv.org/abs/1806.04952", "authors": ["Markus Schr\u00f6der", "Christian Jilek", "J\u00f6rn Hees", "Andreas Dengel"], "categories": ["cs.DB", "cs.AI", "cs.HC"], "comment": "4 pages, 3 figures", "summary": "In the field of machine learning, data understanding is the practice of getting initial insights in unknown datasets. Such knowledge-intensive tasks require a lot of documentation, which is necessary for data scientists to grasp the meaning of the data. Usually, documentation is separate from the data in various external documents, diagrams, spreadsheets and tools which causes considerable look up overhead. Moreover, other supporting applications are not able to consume and utilize such unstructured data. That is why we propose a methodology that uses a single semantic model that interlinks data with its documentation. Hence, data scientists are able to directly look up the connected information about the data by simply following links. Equally, they can browse the documentation which always refers to the data. Furthermore, the model can be used by other approaches providing additional support, like searching, comparing, integrating or visualizing data. To showcase our approach we also demonstrate an early prototype.", "AI": {"tldr": "This paper introduces a methodology using a single semantic model that interlinks data with its documentation, enabling data scientists to easily access connected information and supporting various data-related tasks.", "motivation": "Data scientists face challenges due to documentation being separate from data, causing lookup overhead and hindering application support.", "method": "A single semantic model interlinks data with its documentation.", "result": "An early prototype is demonstrated.", "conclusion": "The proposed approach allows data scientists to directly look up the connected information about the data by simply following links. Equally, they can browse the documentation which always refers to the data. Furthermore, the model can be used by other approaches providing additional support, like searching, comparing, integrating or visualizing data."}}
{"id": "1805.10493", "title": "Combining Spreadsheet Smells for Improved Fault Prediction", "url": "https://arxiv.org/abs/1805.10493", "pdf": "https://arxiv.org/pdf/1805.10493", "abs": "https://arxiv.org/abs/1805.10493", "authors": ["Patrick Koch", "Konstantin Schekotihin", "Dietmar Jannach", "Birgit Hofer", "Franz Wotawa"], "categories": ["cs.SE"], "comment": "4 pages, 1 figure, to be published in 40th International Conference on Software Engineering: New Ideas and Emerging Results Track", "summary": "Spreadsheets are commonly used in organizations as a programming tool for business-related calculations and decision making. Since faults in spreadsheets can have severe business impacts, a number of approaches from general software engineering have been applied to spreadsheets in recent years, among them the concept of code smells. Smells can in particular be used for the task of fault prediction. An analysis of existing spreadsheet smells, however, revealed that the predictive power of individual smells can be limited. In this work we therefore propose a machine learning based approach which combines the predictions of individual smells by using an AdaBoost ensemble classifier. Experiments on two public datasets containing real-world spreadsheet faults show significant improvements in terms of fault prediction accuracy.", "AI": {"tldr": "This paper proposes a machine learning approach using AdaBoost to combine spreadsheet smells for improved fault prediction.", "motivation": "Existing spreadsheet smells have limited predictive power for fault prediction.", "method": "A machine learning approach based on AdaBoost ensemble classifier is used to combine predictions of individual smells.", "result": "Experiments on two public datasets show significant improvements in fault prediction accuracy.", "conclusion": "The proposed machine learning approach significantly improves fault prediction accuracy by combining spreadsheet smells."}}
{"id": "1805.06353", "title": "SmartTable: A Spreadsheet Program with Intelligent Assistance", "url": "https://arxiv.org/abs/1805.06353", "pdf": "https://arxiv.org/pdf/1805.06353", "abs": "https://arxiv.org/abs/1805.06353", "authors": ["Shuo Zhang", "Vugar Abdul Zada", "Krisztian Balog"], "categories": ["cs.IR"], "comment": "The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '18)", "summary": "We introduce SmartTable, an online spreadsheet application that is equipped with intelligent assistance capabilities. With a focus on relational tables, describing entities along with their attributes, we offer assistance in two flavors: (i) for populating the table with additional entities (rows) and (ii) for extending it with additional entity attributes (columns). We provide details of our implementation, which is also released as open source. The application is available at http://smarttable.cc.", "AI": {"tldr": "SmartTable is an online spreadsheet application with intelligent assistance for relational tables.", "motivation": "To provide intelligent assistance in populating tables with additional entities and extending them with additional attributes.", "method": "Implementation details are provided and released as open source.", "result": "The application is available online.", "conclusion": "SmartTable offers intelligent assistance for relational tables by adding entities and attributes, and it is available as an open-source application."}}
{"id": "1804.04175", "title": "An Easy & Collaborative RDF Data Entry Method using the Spreadsheet Metaphor", "url": "https://arxiv.org/abs/1804.04175", "pdf": "https://arxiv.org/pdf/1804.04175", "abs": "https://arxiv.org/abs/1804.04175", "authors": ["Markus Schr\u00f6der", "Christian Jilek", "J\u00f6rn Hees", "Sven Hertling", "Andreas Dengel"], "categories": ["cs.SE"], "comment": "15 pages", "summary": "Spreadsheets are widely used by knowledge workers, especially in the industrial sector. Their methodology enables a well understood, easy and fast possibility to enter data. As filling out a spreadsheet is more accessible to common knowledge workers than defining RDF statements, in this paper, we propose an easy-to-use, zero-configuration, web-based spreadsheet editor that simultaneously transfers spreadsheet entries into RDF statements. It enables various kinds of users to easily create semantic data whether they are RDF experts or novices. The typical scenario we address focuses on creating instance data starting with an empty knowledge base that is filled incrementally. In a user study, participants were able to create more statements in shorter time, having similar or even significantly outperforming quality, compared to other approaches.", "AI": {"tldr": "This paper introduces a user-friendly web-based spreadsheet editor that automatically converts spreadsheet entries into RDF statements, allowing users to easily create semantic data without requiring RDF expertise.", "motivation": "To provide a more accessible way for knowledge workers, especially in the industrial sector, to create semantic data compared to defining RDF statements directly.", "method": "The paper proposes a web-based spreadsheet editor with zero-configuration that simultaneously transfers spreadsheet entries into RDF statements.", "result": "A user study showed that participants were able to create more statements in less time with similar or better quality compared to other approaches.", "conclusion": "The proposed spreadsheet editor enables various users to easily create semantic data, whether they are RDF experts or novices, and outperforms other approaches in terms of speed and quality."}}
{"id": "1801.09777", "title": "Structured Spreadsheet Modelling and Implementation with Multiple Dimensions - Part 1: Modelling", "url": "https://arxiv.org/abs/1801.09777", "pdf": "https://arxiv.org/pdf/1801.09777", "abs": "https://arxiv.org/abs/1801.09777", "authors": ["Paul Mireault"], "categories": ["cs.SE"], "comment": "13 Pages, 17 Tables and Figures", "summary": "Dimensions are an integral part of many models we use every day. Without thinking about it, we frequently use the time dimension: many financial and accounting spreadsheets have columns representing months or years. Representing a second dimension is often done by repeating blocs of formulas in a worksheet or creating multiple worksheets with the same structure.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u5728\u7535\u5b50\u8868\u683c\u4e2d\u4f7f\u7528\u7ef4\u5ea6\uff0c\u7279\u522b\u662f\u5728\u8d22\u52a1\u548c\u4f1a\u8ba1\u4e2d\u5e38\u89c1\u7684\u65f6\u95f4\u7ef4\u5ea6\uff0c\u4ee5\u53ca\u5904\u7406\u591a\u7ef4\u6570\u636e\u7684\u65b9\u6cd5\u3002", "motivation": "\u8bf4\u660e\u4e86\u7ef4\u5ea6\u5728\u65e5\u5e38\u6a21\u578b\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u5f53\u524d\u7535\u5b50\u8868\u683c\u5904\u7406\u591a\u7ef4\u6570\u636e\u7684\u4e0d\u8db3\u3002", "method": "\u672a\u63d0\u53ca\u5177\u4f53\u65b9\u6cd5\uff0c\u4f46\u6697\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u662f\u91cd\u590d\u516c\u5f0f\u6216\u521b\u5efa\u591a\u4e2a\u5de5\u4f5c\u8868\u3002", "result": "\u672a\u63d0\u53ca\u5177\u4f53\u7ed3\u679c\u3002", "conclusion": "\u672a\u63d0\u53ca\u5177\u4f53\u7ed3\u8bba\u3002"}}
{"id": "1802.01640", "title": "Mitigating Spreadsheet Risk in Complex Multi-Dimensional Models in Excel", "url": "https://arxiv.org/abs/1802.01640", "pdf": "https://arxiv.org/pdf/1802.01640", "abs": "https://arxiv.org/abs/1802.01640", "authors": ["Steve Litt"], "categories": ["cs.SE"], "comment": "13 Pages, 11 Colour Figures", "summary": "Microsoft Excel is the most ubiquitous analytical tool ever built. Companies around the world leverage it for its power, flexibility and ease of use. However, spreadsheets are manually intensive and prone to error, making it difficult for companies to control spreadsheet risk. The following solution is designed to mitigate spreadsheet risk for a set of problems commonly addressed in a spreadsheet defined as \"complex multi-dimensional models\". \"Complex\" referring to certain types of applications that require functionality such as sophisticated algorithms, challenging hierarchies and database write-back (i.e. planning, forecasting, etc.) and \"multi-dimensional\" referring to providing capabilities such as reporting, data input forms and ad hoc analysis on the different attributes associated with the resulting model. The solution is defined as a \"PivotModel\" because it works similarly to a PivotTable but is designed to leverage the robust capabilities of the Microsoft Excel platform.", "AI": {"tldr": "This paper introduces PivotModel, a solution designed to mitigate spreadsheet risk in complex multi-dimensional models within Microsoft Excel.", "motivation": "Companies rely on Excel but face risks due to manual processes and errors in complex models.", "method": "The paper defines a 'PivotModel' solution that leverages Excel's capabilities.", "result": "The solution aims to mitigate spreadsheet risk.", "conclusion": "PivotModel is designed to work similarly to a PivotTable but is designed to leverage the robust capabilities of the Microsoft Excel platform."}}
{"id": "1802.01628", "title": "Proposed Spreadsheet Transparency Definition and Measures", "url": "https://arxiv.org/abs/1802.01628", "pdf": "https://arxiv.org/pdf/1802.01628", "abs": "https://arxiv.org/abs/1802.01628", "authors": ["Craig Hatmaker"], "categories": ["cs.SE"], "comment": "13 Pages, 12 Screenshots", "summary": "Auditors demand financial models be transparent yet no consensus exists on what that means precisely. Without a clear modeling transparency definition we cannot know when our models are \"transparent\". The financial modeling community debates which methods are more or less transparent as though transparency is a quantifiable entity yet no measures exist. Without a transparency measure modelers cannot objectively evaluate methods and know which improves model transparency.\n  This paper proposes a definition for spreadsheet modeling transparency that is specific enough to create measures and automation tools for auditors to determine if a model meets transparency requirements. The definition also provides modelers the ability to objectively compare spreadsheet modeling methods to select which best meets their goals.", "AI": {"tldr": "This paper defines spreadsheet modeling transparency to allow for measures and tools for auditors and modelers.", "motivation": "Lack of consensus on the definition of financial model transparency hinders objective evaluation and improvement of modeling methods.", "method": "Proposes a specific definition for spreadsheet modeling transparency.", "result": "Enables creation of measures and automation tools for auditors.", "conclusion": "Provides a basis for objectively comparing spreadsheet modeling methods."}}
{"id": "1802.00496", "title": "Edu-Edition Spreadsheet Competency Framework", "url": "https://arxiv.org/abs/1802.00496", "pdf": "https://arxiv.org/pdf/1802.00496", "abs": "https://arxiv.org/abs/1802.00496", "authors": ["Maria Csernoch", "Piroska Bir\u00f3"], "categories": ["cs.CY"], "comment": null, "summary": "Based on the Spreadsheet Competency Framework for finance professionals, in the present paper we introduce the Edu-Edition of the Spreadsheet Competency Framework (E2SCF). We claim that building spreadsheet competences should start in education, as early as possible, and this process is a lot more effective if support arrives from expert teachers. The main feature of E2SCF is high mathability computer-supported real world problem solving. This approach is based on - from the very beginning of training - a two-directional knowledge transfer, data and error analysis and handling, and the programming aspect of spreadsheets. Based on these features, E2SCF is set up for basic and general users to build up firm spreadsheet knowledge and to develop transferable problem solving skills and competences.", "AI": {"tldr": "This paper introduces the Edu-Edition of the Spreadsheet Competency Framework (E2SCF) for finance professionals.", "motivation": "Building spreadsheet competences should start in education, as early as possible, and this process is a lot more effective if support arrives from expert teachers.", "method": "Introducing the Edu-Edition of the Spreadsheet Competency Framework (E2SCF). The main feature of E2SCF is high mathability computer-supported real world problem solving. This approach is based on a two-directional knowledge transfer, data and error analysis and handling, and the programming aspect of spreadsheets.", "result": "E2SCF is set up for basic and general users to build up firm spreadsheet knowledge and to develop transferable problem solving skills and competences.", "conclusion": "E2SCF is effective for building spreadsheet competence."}}
{"id": "1802.00484", "title": "Alternative Spreadsheet Model Designs for an Operations Management Model Embedded in a Periodic Business Process", "url": "https://arxiv.org/abs/1802.00484", "pdf": "https://arxiv.org/pdf/1802.00484", "abs": "https://arxiv.org/abs/1802.00484", "authors": ["Thomas A. Grossman", "Vijay Mehrotra", "Mouwafac Sidaoui"], "categories": ["cs.SE"], "comment": "12 Pages, 10 Colour Figures", "summary": "We present a widely-used operations management model used in supply and distribution planning, that is typically embedded in a periodic business process that necessitates model modification and reuse. We consider three alternative spreadsheet implementations, a data-driven design, a canonical (textbook) design, and a novel (table-driven) technical design. We evaluate each regarding suitability for accuracy, modification, analysis, and transfer. We consider the degree of training and technical sophistication required to utilize each design. The data-driven design provides insight into poor spreadsheet practices by na\u00efve modelers. The technical design can be modified for new data and new structural elements without manual writing or editing of cell formulas, thus speeding modification and reducing risk of error. The technical design has potential for use with other classes of models. We identify opportunities for future research.", "AI": {"tldr": "This paper analyzes three spreadsheet implementations of an operations management model for supply and distribution planning.", "motivation": "To evaluate different spreadsheet designs for accuracy, modification, analysis, and transfer in the context of a periodic business process.", "method": "Comparing a data-driven design, a canonical design, and a novel table-driven design based on their suitability and required expertise.", "result": "The technical design can be modified for new data and structural elements without manual formula editing, improving modification speed and reducing errors. The data-driven design highlights poor spreadsheet practices.", "conclusion": "The technical design shows potential for other model classes and identifies opportunities for future research."}}
{"id": "1801.03829", "title": "Characterizing Scalability Issues in Spreadsheet Software using Online Forums", "url": "https://arxiv.org/abs/1801.03829", "pdf": "https://arxiv.org/pdf/1801.03829", "abs": "https://arxiv.org/abs/1801.03829", "authors": ["Kelly Mack", "John Lee", "Kevin Chang", "Karrie Karahalios", "Aditya Parameswaran"], "categories": ["cs.HC"], "comment": null, "summary": "In traditional usability studies, researchers talk to users of tools to understand their needs and challenges. Insights gained via such interviews offer context, detail, and background. Due to costs in time and money, we are beginning to see a new form of tool interrogation that prioritizes scale, cost, and breadth by utilizing existing data from online forums. In this case study, we set out to apply this method of using online forum data to a specific issue---challenges that users face with Excel spreadsheets. Spreadsheets are a versatile and powerful processing tool if used properly. However, with versatility and power come errors, from both users and the software, which make using spreadsheets less effective. By scraping posts from the website Reddit, we collected a dataset of questions and complaints about Excel. Specifically, we explored and characterized the issues users were facing with spreadsheet software in general, and in particular, as resulting from a large amount of data in their spreadsheets. We discuss the implications of our findings on the design of next-generation spreadsheet software.", "AI": {"tldr": "This paper explores challenges users face with Excel spreadsheets by analyzing Reddit posts.", "motivation": "To understand user needs and challenges with Excel spreadsheets at scale and low cost.", "method": "The authors collected a dataset of questions and complaints about Excel from Reddit and analyzed them.", "result": "The study identifies issues users face with spreadsheet software, particularly when dealing with large amounts of data.", "conclusion": "The findings have implications for the design of next-generation spreadsheet software."}}
{"id": "1801.10249", "title": "The Reification of an Incorrect and Inappropriate Spreadsheet Model", "url": "https://arxiv.org/abs/1801.10249", "pdf": "https://arxiv.org/pdf/1801.10249", "abs": "https://arxiv.org/abs/1801.10249", "authors": ["Grenville J. Croll"], "categories": ["cs.HC"], "comment": "14 Pages, 4 Colour Figures, 2 Tables", "summary": "Once information is loaded into a spreadsheet, it acquires properties that it may not deserve. These properties include believability, correctness, appropriateness, concreteness, integrity, tangibility, objectivity and authority. The information becomes reified. We describe a case study through which we were able to observe at close hand the reification of a demonstrably incorrect and inappropriate spreadsheet model within a small non profit organisation.", "AI": {"tldr": "\u7535\u5b50\u8868\u683c\u4e2d\u7684\u4fe1\u606f\u4f1a\u88ab\u8fc7\u5ea6\u4fe1\u4efb\u3002", "motivation": "\u63cf\u8ff0\u4e86\u7535\u5b50\u8868\u683c\u4e2d\u7684\u4fe1\u606f\u4f1a\u83b7\u5f97\u4e0d\u5e94\u6709\u7684\u5c5e\u6027\uff0c\u4f8b\u5982\u53ef\u4fe1\u5ea6\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u89c2\u5bdf\u3002", "result": "\u89c2\u5bdf\u5230\u4e00\u4e2a\u5c0f\u578b\u975e\u8425\u5229\u7ec4\u7ec7\u4e2d\u660e\u663e\u4e0d\u6b63\u786e\u548c\u4e0d\u9002\u5f53\u7684\u7535\u5b50\u8868\u683c\u6a21\u578b\u7684\u5177\u4f53\u5316\u3002", "conclusion": "\u7535\u5b50\u8868\u683c\u4e2d\u7684\u4fe1\u606f\u4f1a\u88ab\u8fc7\u5ea6\u4fe1\u4efb\uff0c\u5373\u4f7f\u5b83\u4eec\u662f\u4e0d\u6b63\u786e\u7684\u3002"}}
{"id": "1801.10231", "title": "The Future of Spreadsheets in the Big Data Era", "url": "https://arxiv.org/abs/1801.10231", "pdf": "https://arxiv.org/pdf/1801.10231", "abs": "https://arxiv.org/abs/1801.10231", "authors": ["David Birch", "David Lyford-Smith", "Yike Guo"], "categories": ["cs.CY"], "comment": "13 Pages, 1 Table", "summary": "The humble spreadsheet is the most widely used data storage, manipulation and modelling tool. Its ubiquity over the past 30 years has seen its successful application in every area of life. Surprisingly the spreadsheet has remained fundamentally unchanged over the past three decades. As spreadsheet technology enters its 4th decade a number of drivers of change are beginning to impact upon the spreadsheet. The rise of Big Data, increased end-user computing and mobile computing will undoubtedly increasingly shape the evolution and use of spreadsheet technology.\n  To explore the future of spreadsheet technology a workshop was convened with the aim of \"bringing together academia and industry to examine the future direction of spreadsheet technology and the consequences for users\". This paper records the views of the participants on the reasons for the success of the spreadsheet, the trends driving change and the likely directions of change for the spreadsheet. We then set out key directions for further research in the evolution and use of spreadsheets. Finally we look at the implications of these trends for the end users who after all are the reason for the remarkable success of the spreadsheet.", "AI": {"tldr": "This paper discusses the future of spreadsheet technology, considering the impact of Big Data, increased end-user computing, and mobile computing.", "motivation": "To explore the future of spreadsheet technology and its consequences for users.", "method": "A workshop was convened with academia and industry to discuss the future direction of spreadsheet technology.", "result": "The paper records the views of the participants on the reasons for the success of the spreadsheet, the trends driving change and the likely directions of change for the spreadsheet. Key directions for further research in the evolution and use of spreadsheets are set out.", "conclusion": "The paper looks at the implications of these trends for the end users."}}
{"id": "1801.09771", "title": "Mitigating Spreadsheet Model Risk with Python Open Source Infrastructure", "url": "https://arxiv.org/abs/1801.09771", "pdf": "https://arxiv.org/pdf/1801.09771", "abs": "https://arxiv.org/abs/1801.09771", "authors": ["Oliver Beavers"], "categories": ["cs.SE"], "comment": "14 Pages, 15 Colour Diagrams", "summary": "Across an aggregation of EuSpRIG presentation papers, two maxims hold true: spreadsheets models are akin to software, yet spreadsheet developers are not software engineers. As such, the lack of traditional software engineering tools and protocols invites a higher rate of error in the end result. This paper lays ground work for spreadsheet modelling professionals to develop reproducible audit tools using freely available, open source packages built with the Python programming language, enabling stakeholders to develop clearly defined model \"oracles\" with which to test and audit spreadsheet calculations against.", "AI": {"tldr": "This paper introduces open-source Python tools for auditing spreadsheets, addressing the issue of errors due to the lack of software engineering practices in spreadsheet development.", "motivation": "Spreadsheet models, like software, are prone to errors due to the absence of standard software engineering tools and practices.", "method": "The paper proposes using open-source Python packages to create reproducible audit tools for spreadsheet models.", "result": "The paper lays the groundwork for professionals to develop model oracles for testing and auditing spreadsheet calculations.", "conclusion": "The research enables stakeholders to develop clearly defined model oracles to test and audit spreadsheet calculations."}}
{"id": "1801.08603", "title": "Structuring Spreadsheets with the \"Lish\" Data Model", "url": "https://arxiv.org/abs/1801.08603", "pdf": "https://arxiv.org/pdf/1801.08603", "abs": "https://arxiv.org/abs/1801.08603", "authors": ["Alan Hall", "Michel Wermelinger", "Tony Hirst", "Santi Phithakkitnukoon"], "categories": ["cs.SE"], "comment": "4 colour figures", "summary": "A spreadsheet is remarkably flexible in representing various forms of structured data, but the individual cells have no knowledge of the larger structures of which they may form a part. This can hamper comprehension and increase formula replication, increasing the risk of error on both scores. We explore a novel data model (called the \"lish\") that could form an alternative to the traditional grid in a spreadsheet-like environment. Its aim is to capture some of these higher structures while preserving the simplicity that makes a spreadsheet so attractive. It is based on cells organised into nested lists, in each of which the user may optionally employ a template to prototype repeating structures. These template elements can be likened to the marginal \"cells\" in the borders of a traditional worksheet, but are proper members of the sheet and may themselves contain internal structure. A small demonstration application shows the \"lish\" in operation.", "AI": {"tldr": "This paper introduces \"lish\", a novel data model for spreadsheets that uses nested lists and templates to capture higher-level structures, aiming to improve comprehension and reduce errors compared to the traditional grid-based approach.", "motivation": "The motivation is that traditional spreadsheets lack awareness of higher-level structures, leading to comprehension difficulties, formula replication, and increased error risk.", "method": "The paper proposes a new data model called \"lish\" based on nested lists and templates. Users can employ templates to prototype repeating structures within these lists. A demonstration application is used to showcase \"lish\" in operation.", "result": "The paper presents a demonstration application to show the \"lish\" in operation.", "conclusion": "The paper introduces the \"lish\" data model as a potential alternative to the traditional spreadsheet grid, designed to capture higher-level structures while maintaining simplicity."}}
{"id": "1801.07782", "title": "The Role of Spreadsheets in Clinical Decision Support: A Survey of the Medical Algorithms Company User Community", "url": "https://arxiv.org/abs/1801.07782", "pdf": "https://arxiv.org/pdf/1801.07782", "abs": "https://arxiv.org/abs/1801.07782", "authors": ["Simon Thorne"], "categories": ["cs.CY"], "comment": "13 pages, 6 Colour Figures", "summary": "This paper presents and discusses the results of a small scoping survey of Clinical Decision Support System (CDSS) users from the Medical Algorithms Company website which hosts 24,000 different CDSS. These results are analysed, discussed, and compared with other similar studies and contribute to the wider understanding of how CDSS impact on clinical practice. The results show that CDSS provided by Medal are being used by clinical professionals in a variety of settings, both as an operational tool and as a research and reference tool. Whilst these tools are implemented and executed in a database, the initial logic is worked out on a spreadsheet. The paper describes that process and examines some of the results of the survey.", "AI": {"tldr": "A survey of CDSS users shows they're used in various settings for both operations and research, with initial logic often developed in spreadsheets.", "motivation": "To understand how CDSS impacts clinical practice.", "method": "A survey of CDSS users from the Medical Algorithms Company website.", "result": "CDSS are used in a variety of settings as operational, research, and reference tools. Initial logic is often worked out on a spreadsheet.", "conclusion": "CDSS provided by Medal are used by clinical professionals in a variety of settings."}}
{"id": "1712.09797", "title": "Automated Refactoring of Nested-IF Formulae in Spreadsheets", "url": "https://arxiv.org/abs/1712.09797", "pdf": "https://arxiv.org/pdf/1712.09797", "abs": "https://arxiv.org/abs/1712.09797", "authors": ["Jie Zhang", "Shi Han", "Dan Hao", "Lu Zhang", "Dongmei Zhang"], "categories": ["cs.SE"], "comment": null, "summary": "Spreadsheets are the most popular end-user programming software, where formulae act like programs and also have smells. One well recognized common smell of spreadsheet formulae is nest-IF expressions, which have low readability and high cognitive cost for users, and are error-prone during reuse or maintenance. However, end users usually lack essential programming language knowledge and skills to tackle or even realize the problem. The previous research work has made very initial attempts in this aspect, while no effective and automated approach is currently available.\n  This paper firstly proposes an AST-based automated approach to systematically refactoring nest-IF formulae. The general idea is two-fold. First, we detect and remove logic redundancy on the AST. Second, we identify higher-level semantics that have been fragmented and scattered, and reassemble the syntax using concise built-in functions. A comprehensive evaluation has been conducted against a real-world spreadsheet corpus, which is collected in a leading IT company for research purpose. The results with over 68,000 spreadsheets with 27 million nest-IF formulae reveal that our approach is able to relieve the smell of over 99\\% of nest-IF formulae. Over 50% of the refactorings have reduced nesting levels of the nest-IFs by more than a half. In addition, a survey involving 49 participants indicates that for most cases the participants prefer the refactored formulae, and agree on that such automated refactoring approach is necessary and helpful.", "AI": {"tldr": "This paper introduces an automated approach to refactor nested-IF formulae in spreadsheets, improving readability and reducing errors.", "motivation": "Nested-IF expressions in spreadsheets are hard to read, error-prone, and difficult to maintain, but users lack the skills to fix them.", "method": "The approach uses AST-based logic redundancy removal and semantic reassembly with concise built-in functions.", "result": "The approach relieved the smell of over 99% of nest-IF formulae, reducing nesting levels by more than half in over 50% of cases. A user survey indicated preference for refactored formulae.", "conclusion": "The automated refactoring approach is effective, necessary, and helpful for improving spreadsheet formula quality."}}
{"id": "1612.03813", "title": "Spreadsheet Guardian: An Approach to Protecting Semantic Correctness throughout the Evolution of Spreadsheets", "url": "https://arxiv.org/abs/1612.03813", "pdf": "https://arxiv.org/pdf/1612.03813", "abs": "https://arxiv.org/abs/1612.03813", "authors": ["Daniel Kulesz", "Verena K\u00e4fer", "Stefan Wagner"], "categories": ["cs.SE", "cs.PL"], "comment": "30 pages, 15 figures, 4 tables", "summary": "Spreadsheets are powerful tools which play a business-critical role in many organizations. However, many bad decisions taken due to faulty spreadsheets show that these tools need serious quality assurance. Furthermore, while collaboration on spreadsheets for maintenance tasks is common, there has been almost no support for ensuring that the spreadsheets remain correct during this process.\n  We have developed an approach named Spreadsheet Guardian which separates the specification of spreadsheet test rules from their execution. By automatically executing user-defined test rules, our approach is able to detect semantic faults. It also protects all collaborating spreadsheet users from introducing faults during maintenance, even if only few end-users specify test rules. To evaluate Spreadsheet Guardian, we implemented a representative testing technique as an add-in for Microsoft Excel.\n  We evaluated the testing technique in two empirical evaluations with 29 end-users and 42 computer science students. The results indicate that the technique is easy to learn and to apply. Furthermore, after finishing maintenance, participants with spreadsheets \"protected\" by the technique are more realistic about the correctness of their spreadsheets than participants who employ only \"classic\", non-interactive test rules based on static analysis techniques. Hence, we believe Spreadsheet Guardian can be of use for business-critical spreadsheets.", "AI": {"tldr": "Spreadsheet Guardian: detects semantic faults and protects users during maintenance.", "motivation": "Spreadsheets are critical but prone to errors, lacking support for collaborative maintenance.", "method": "Separates test rule specification from execution, automatically executing user-defined rules.", "result": "Easy to learn and apply, improves user realism about spreadsheet correctness after maintenance.", "conclusion": "Spreadsheet Guardian is useful for business-critical spreadsheets."}}
