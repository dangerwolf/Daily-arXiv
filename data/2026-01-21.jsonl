{"id": "2512.10165", "title": "BookReconciler: An Open-Source Tool for Metadata Enrichment and Work-Level Clustering", "url": "https://arxiv.org/abs/2512.10165", "pdf": "https://arxiv.org/pdf/2512.10165", "abs": "https://arxiv.org/abs/2512.10165", "authors": ["Matt Miller", "Dan Sinykin", "Melanie Walsh"], "categories": ["cs.DL", "cs.IR"], "comment": "Published in the proceedings of the Joint Conference on Digital Libraries (JCDL) 2025, Resources", "summary": "We present BookReconciler, an open-source tool for enhancing and clustering book data. BookReconciler allows users to take spreadsheets with minimal metadata, such as book title and author, and automatically 1) add authoritative, persistent identifiers like ISBNs 2) and cluster related Expressions and Manifestations of the same Work, e.g., different translations or editions. This enhancement makes it easier to combine related collections and analyze books at scale. The tool is currently designed as an extension for OpenRefine -- a popular software application -- and connects to major bibliographic services including the Library of Congress, VIAF, OCLC, HathiTrust, Google Books, and Wikidata. Our approach prioritizes human judgment. Through an interactive interface, users can manually evaluate matches and define the contours of a Work (e.g., to include translations or not). We evaluate reconciliation performance on datasets of U.S. prize-winning books and contemporary world fiction. BookReconciler achieves near-perfect accuracy for U.S. works but lower performance for global texts, reflecting structural weaknesses in bibliographic infrastructures for non-English and global literature. Overall, BookReconciler supports the reuse of bibliographic data across domains and applications, contributing to ongoing work in digital libraries and digital humanities."}
{"id": "2601.11546", "title": "RelServe: Fast LLM Inference Serving on Relational Data", "url": "https://arxiv.org/abs/2601.11546", "pdf": "https://arxiv.org/pdf/2601.11546", "abs": "https://arxiv.org/abs/2601.11546", "authors": ["Xin Zhang", "Shihong Gao", "Yanyan Shen", "Haoyang Li", "Lei Chen"], "categories": ["cs.DB"], "comment": "Paper Under Review", "summary": "The use of Large Language Models (LLMs) for querying relational data has given rise to relQuery, a workload pattern that applies templated LLM calls to structured tables. As relQuery services become more widely adopted in applications such as AI-powered spreadsheets, fast response times under concurrent query loads are increasingly important. Unfortunately, current LLM engines face severe latency bottlenecks from Head-of-Line (HoL) blocking across three comparable inference phases: waiting, core running, and tail running. Existing static priority scheduling methods only address HoL blocking during the waiting phase, leaving two critical problems unsolved. First, the absence of a priority update mechanism causes inaccurate prioritization and continued HoL blocking during core execution. Second, suboptimal prefill-decode batching exacerbates HoL blocking in tail execution and worsens latency trade-offs between running and waiting relQueries. To address these problems, we propose RelServe, an optimized LLM engine for low-latency relQuery serving. RelServe features two core innovations: a Dynamic Priority Updater that continuously adjusts priorities while minimizing overhead via statistical approximations, and an Adaptive Batch Arranger that quantitatively evaluates candidate prefill and decode batches to minimize projected average latency. Extensive experiments on four real-world datasets using LLMs ranging from 13B to 70B parameters show that RelServe reduces average serving latency by up to 3.1x compared to vLLM."}
{"id": "2509.26557", "title": "The Invisible Mentor: Inferring User Actions from Screen Recordings to Recommend Better Workflows", "url": "https://arxiv.org/abs/2509.26557", "pdf": "https://arxiv.org/pdf/2509.26557", "abs": "https://arxiv.org/abs/2509.26557", "authors": ["Litao Yan", "Andrew Head", "Ken Milne", "Vu Le", "Sumit Gulwani", "Chris Parnin", "Emerson Murphy-Hill"], "categories": ["cs.HC"], "comment": "15 pages, 6 figures", "summary": "Many users struggle to notice when a more efficient workflow exists in feature-rich tools like Excel. Existing AI assistants offer help only after users describe their goals or problems, which can be effortful and imprecise. We present InvisibleMentor, a system that turns screen recordings of task completion into vision-grounded reflections on tasks. It detects issues such as repetitive edits and recommends more efficient alternatives based on observed behavior. Unlike prior systems that rely on logs, APIs, or user prompts, InvisibleMentor operates directly on screen recordings. It uses a two-stage pipeline: a vision-language model reconstructs actions and context, and a language model generates structured, high-fidelity suggestions. In evaluation, InvisibleMentor accurately identified inefficient workflows, and participants found its suggestions more actionable, tailored, and more helpful for learning and improvement compared to a prompt-based spreadsheet assistant."}
{"id": "2509.26331", "title": "AI Playing Business Games: Benchmarking Large Language Models on Managerial Decision-Making in Dynamic Simulations", "url": "https://arxiv.org/abs/2509.26331", "pdf": "https://arxiv.org/pdf/2509.26331", "abs": "https://arxiv.org/abs/2509.26331", "authors": ["Berdymyrat Ovezmyradov"], "categories": ["cs.AI"], "comment": "34 pages, 7 figures, 3 tables", "summary": "The rapid advancement of LLMs sparked significant interest in their potential to augment or automate managerial functions. One of the most recent trends in AI benchmarking is performance of Large Language Models (LLMs) over longer time horizons. While LLMs excel at tasks involving natural language and pattern recognition, their capabilities in multi-step, strategic business decision-making remain largely unexplored. Few studies demonstrated how results can be different from benchmarks in short-term tasks, as Vending-Bench revealed. Meanwhile, there is a shortage of alternative benchmarks for long-term coherence. This research analyses a novel benchmark using a business game for the decision making in business. The research contributes to the recent literature on AI by proposing a reproducible, open-access management simulator to the research community for LLM benchmarking. This novel framework is used for evaluating the performance of five leading LLMs available in free online interface: Gemini, ChatGPT, Meta AI, Mistral AI, and Grok. LLM makes decisions for a simulated retail company. A dynamic, month-by-month management simulation provides transparently in spreadsheet model as experimental environment. In each of twelve months, the LLMs are provided with a structured prompt containing a full business report from the previous period and are tasked with making key strategic decisions: pricing, order size, marketing budget, hiring, dismissal, loans, training expense, R&D expense, sales forecast, income forecast The methodology is designed to compare the LLMs on quantitative metrics: profit, revenue, and market share, and other KPIs. LLM decisions are analyzed in their strategic coherence, adaptability to market changes, and the rationale provided for their decisions. This approach allows to move beyond simple performance metrics for assessment of the long-term decision-making."}
