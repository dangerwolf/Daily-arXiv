<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.HC](#cs.HC) [Total: 5]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Vision Language Models for Spreadsheet Understanding: Challenges and Opportunities](https://arxiv.org/abs/2405.16234)
*Shiyu Xia,Junyu Xiong,Haoyu Dong,Jianbo Zhao,Yuzhang Tian,Mengyu Zhou,Yeye He,Shi Han,Dongmei Zhang*

Main category: cs.CV

TL;DR: 本文研究了视觉语言模型（VLM）在电子表格理解方面的能力，提出了一种新的评估框架，包括三个自监督任务（OCR、空间感知、格式识别）和一个集成任务（表格检测），并设计了三种新的场景（列宽调整、样式更改、地址增强）来更精细地探测VLM。


<details>
  <summary>Details</summary>
Motivation: 为了全面评估视觉语言模型（VLM）在电子表格理解方面的能力，并为未来改进VLM在该领域的表现提供方向。

Method: 提出三个自监督的挑战任务（OCR、空间感知、格式识别）及相应的评估指标，并利用电子表格表格检测任务来评估VLM的整体性能。此外，还提出了三种电子表格到图像的设置（列宽调整、样式更改、地址增强）以及相应的提示词变体，以更精细地探测VLM。在表格边界检测任务中，为了利用VLM在理解文本方面的优势，提出了解码表格四个边界的单元格值。

Result: VLM在OCR方面表现出潜力，但在单元格遗漏和错位方面存在不足。VLM在空间感知和格式识别方面表现不佳。

Conclusion: VLM在电子表格理解方面仍有很大提升空间，尤其是在空间感知和格式识别能力方面。未来的研究应致力于利用本文提出的方法，生成各种设置下的电子表格-图像对，以增强VLM的电子表格数据理解能力。

Abstract: This paper explores capabilities of Vision Language Models on spreadsheet comprehension. We propose three self-supervised challenges with corresponding evaluation metrics to comprehensively evaluate VLMs on Optical Character Recognition (OCR), spatial perception, and visual format recognition. Additionally, we utilize the spreadsheet table detection task to assess the overall performance of VLMs by integrating these challenges. To probe VLMs more finely, we propose three spreadsheet-to-image settings: column width adjustment, style change, and address augmentation. We propose variants of prompts to address the above tasks in different settings. Notably, to leverage the strengths of VLMs in understanding text rather than two-dimensional positioning, we propose to decode cell values on the four boundaries of the table in spreadsheet boundary detection. Our findings reveal that VLMs demonstrate promising OCR capabilities but produce unsatisfactory results due to cell omission and misalignment, and they notably exhibit insufficient spatial and format recognition skills, motivating future work to enhance VLMs' spreadsheet data comprehension capabilities using our methods to generate extensive spreadsheet-image pairs in various settings.

</details>


### [2] [High-Throughput Phenotyping using Computer Vision and Machine Learning](https://arxiv.org/abs/2407.06354)
*Vivaan Singhvi,Langalibalele Lunga,Pragya Nidhi,Chris Keum,Varrun Prakash*

Main category: cs.CV

TL;DR: 该研究提出了一种结合光学字符识别（OCR）、图像分割和机器学习的高通量植物表型分析方法，以处理包含物理标签的Populus Trichocarpa图像数据集。


<details>
  <summary>Details</summary>
Motivation: 为了提高高通量植物表型分析的效率和准确性，尤其是在处理包含物理标签的大型数据集时。

Method: 使用OCR技术读取标签信息，结合图像分割和机器学习进行形态学分类，并利用机器学习模型预测植物处理（对照或干旱）条件。此外，还分析了EXIF标签以提取叶片大小和表型相关性信息。

Result: OCR模型在非空文本提取方面达到94.31%的准确率。分类模型在识别叶片形状、颜色和褐斑程度方面平均准确率为62.82%，预测植物处理的准确率为60.08%。研究发现EXIF标签缺失关键信息，阻碍了叶片大小和表型-条件相关性的评估。

Conclusion: 该研究成功展示了OCR、图像分割和机器学习在植物表型分析中的潜力，尽管存在数据限制，但为未来的研究提供了改进方向。

Abstract: High-throughput phenotyping refers to the non-destructive and efficient evaluation of plant phenotypes. In recent years, it has been coupled with machine learning in order to improve the process of phenotyping plants by increasing efficiency in handling large datasets and developing methods for the extraction of specific traits. Previous studies have developed methods to advance these challenges through the application of deep neural networks in tandem with automated cameras; however, the datasets being studied often excluded physical labels. In this study, we used a dataset provided by Oak Ridge National Laboratory with 1,672 images of Populus Trichocarpa with white labels displaying treatment (control or drought), block, row, position, and genotype. Optical character recognition (OCR) was used to read these labels on the plants, image segmentation techniques in conjunction with machine learning algorithms were used for morphological classifications, machine learning models were used to predict treatment based on those classifications, and analyzed encoded EXIF tags were used for the purpose of finding leaf size and correlations between phenotypes. We found that our OCR model had an accuracy of 94.31% for non-null text extractions, allowing for the information to be accurately placed in a spreadsheet. Our classification models identified leaf shape, color, and level of brown splotches with an average accuracy of 62.82%, and plant treatment with an accuracy of 60.08%. Finally, we identified a few crucial pieces of information absent from the EXIF tags that prevented the assessment of the leaf size. There was also missing information that prevented the assessment of correlations between phenotypes and conditions. However, future studies could improve upon this to allow for the assessment of these features.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [3] [General Table Question Answering via Answer-Formula Joint Generation](https://arxiv.org/abs/2503.12345)
*Zhongyuan Wang,Richong Zhang,Zhijie Nie,Hangyu Mao*

Main category: cs.CL

TL;DR: LLMs 在表格问答 (TableQA) 中虽有进展，但缺乏通用性。本文提出使用电子表格公式作为可执行表示，构建了 FormulaQA 数据集，并提出了 TabAF 框架，实现了跨多种表格和问题类型的通用性，并在 WikiTableQuestion, HiTab, 和 TabFact 数据集上取得了新的 SOTA 性能。


<details>
  <summary>Details</summary>
Motivation: 现有 TableQA 方法主要依赖 LLM 生成文本或代码，但缺乏处理不同表格结构和问题类型的通用性。电子表格公式作为一种成熟的表格操作语言，尚未被充分利用于 TableQA。

Method: 本文首次尝试使用电子表格公式作为解决复杂表格问答的可执行表示，构建了 FormulaQA 数据集，并提出了 TabAF 框架，该框架使用单一 LLM 核心来同时解码答案和公式，以解决多种表格和问题类型。

Result: TabAF 在 WikiTableQuestion, HiTab, 和 TabFact 数据集上取得了新的 SOTA 性能，证明了其在不同表格结构和问题类型上的通用性和泛化能力。

Conclusion: TabAF 框架通过引入电子表格公式作为可执行表示，显著提升了 TableQA 的通用性和性能，并在多个基准测试中创下新纪录。

Abstract: Advanced table question answering (TableQA) methods prompt large language models (LLMs) to generate answer text, SQL query, Python code, or custom operation, which impressively improve the complex reasoning problems in the TableQA task. However, these methods lack the versatility to cope with specific question types or table structures. In contrast, the Spreadsheet Formula, the widely used and well-defined operation language for tabular data, has not been thoroughly explored to solve TableQA. In this paper, we first attempt to use the Formula as the executable representation for solving complex reasoning on tables with different structures. Specifically, we construct \texttt{FromulaQA}, a large Formula-annotated TableQA dataset from existing datasets. In addition, we propose \texttt{TabAF}, a general table answering framework to solve multiple types of tasks over multiple types of tables simultaneously, which decodes answers and Formulas with a single LLM backbone. Extensive experiments demonstrate the versatility and generalization of \texttt{TabAF}. Under the same model size, \texttt{TabAF} achieves new state-of-the-art performance on the WikiTableQuestion, HiTab, and TabFact.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [4] [Consensus-Free Spreadsheet Integration](https://arxiv.org/abs/2209.14457)
*Brandon Baylor,Eric Daimler,James Hansen,Esteban Montero,Ryan Wisnesky*

Main category: cs.DB

TL;DR: 提出了一种利用范畴论中的上极限、提升和Kan扩张构造来合并多个电子表格的方法。


<details>
  <summary>Details</summary>
Motivation: 提出该方法是为了解决工程模型合并时，各模型作者之间无法达成共识的问题，因为该方法通过保持语义的映射来解决此问题。

Method: 通过将每个工作表的公式表示为其代数（方程）理论，以及将每个工作表的值表示为其模型，将工作表之间的重叠表示为理论和模型态射，然后执行上极限、提升和Kan扩张构造来计算一个规范的、普遍的集成理论和模型。

Result: 通过一个实际的石油和天然气计算案例研究，展示了该方法在整合两个独立工程师构建的非交互式井筒压力测试（MASP）计算电子表格中的应用。同时，还讨论了与验证映射的语义保持以及结果集成表的保守性/一致性相关的自动化定理证明的负担。

Conclusion: 阐述了如何将该方法应用于企业范围内扩大工程规模的思考。

Abstract: We describe a method for merging multiple spreadsheets into one sheet, and/or exchanging data among the sheets, by expressing each sheet's formulae as an algebraic (equational) theory and each sheet's values as a model of its theory, expressing the overlap between the sheets as theory and model morphisms, and then performing colimit, lifting, and Kan-extension constructions from category theory to compute a canonically universal integrated theory and model, which can then be expressed as a spreadsheet. Our motivation is to find methods of merging engineering models that do not require consensus (agreement) among the authors of the models being merged, a condition fulfilled by our method because theory and model morphisms are semantics-preserving. We describe a case study of this methodology on a real-world oil and gas calculation at a major energy company, describing the theories and models that arise when integrating two different casing pressure test (MASP) calculation spreadsheets constructed by two non-interacting engineers. We also describe the automated theorem proving burden associated with both verifying the semantics preservation of the overlap mappings as well as verifying the conservativity/consistency of the resulting integrated sheet. We conclude with thoughts on how to apply the methodology to scale engineering efforts across the enterprise.

</details>


### [5] [A System and Benchmark for LLM-based Q&A on Heterogeneous Data](https://arxiv.org/abs/2409.05735)
*Achille Fokoue,Srideepika Jayaraman,Elham Khabiri,Jeffrey O. Kephart,Yingjie Li,Dhruv Shah,Youssef Drissi,Fenno F. Heath,Anu Bhamidipaty,Fateh A. Tipu,Robert J. Baseman*

Main category: cs.DB

TL;DR: siwarex 平台实现了对数据库和 API 的无缝自然语言访问，以解决工业环境中数据源异构性问题。


<details>
  <summary>Details</summary>
Motivation: 在工业环境中，用户希望查询结构化数据（如电子表格、数据库、API 或它们的组合），但常常不知道如何识别或访问正确的数据源，尤其是在需要整合多个（可能孤立的）数据源才能获得答案的情况下。现有的 Text-to-SQL 应用虽然能处理自然语言查询，但在面对工业环境中典型的数据源异构性时仍显不足。

Method: 引入 siwarex 平台，实现对数据库和 API 的无缝自然语言访问。通过扩展 Spider 数据集和基准测试，用数据检索 API 替换部分表，来演示 siwarex 的有效性。

Result: siwarex 能够很好地处理数据源异构性问题。

Conclusion: siwarex 平台有效解决了工业环境中数据源异构性带来的挑战，实现了对异构数据源的无缝自然语言访问。

Abstract: In many industrial settings, users wish to ask questions whose answers may be found in structured data sources such as a spreadsheets, databases, APIs, or combinations thereof. Often, the user doesn't know how to identify or access the right data source. This problem is compounded even further if multiple (and potentially siloed) data sources must be assembled to derive the answer. Recently, various Text-to-SQL applications that leverage Large Language Models (LLMs) have addressed some of these problems by enabling users to ask questions in natural language. However, these applications remain impractical in realistic industrial settings because they fail to cope with the data source heterogeneity that typifies such environments. In this paper, we address heterogeneity by introducing the siwarex platform, which enables seamless natural language access to both databases and APIs. To demonstrate the effectiveness of siwarex, we extend the popular Spider dataset and benchmark by replacing some of its tables by data retrieval APIs. We find that siwarex does a good job of coping with data source heterogeneity. Our modified Spider benchmark will soon be available to the research community

</details>


### [6] [Auditable and reusable crosswalks for fast, scaled integration of scattered tabular data](https://arxiv.org/abs/2409.01517)
*Gavin Chait*

Main category: cs.DB

TL;DR: 本研究提出了一个开源策展工具包，用于生成结构良好且可互操作的数据。


<details>
  <summary>Details</summary>
Motivation: 策展工作被分解为离散的组件，以模式为中心，对复杂的、分散的表格数据进行可审计的重组，以符合目标模式。

Method: 该工具包支持任务分离，允许在没有源数据的情况下进行软件开发和分析。转换被捕获为描述模式到模式映射的高级顺序脚本，从而降低了复杂性和资源需求。

Result: 该工具包能够将任何符合模式定义的数据通过交叉表进行重组。

Conclusion: 该工具包既可以作为 Python 包使用，也可以作为‘无代码’的可视化 Web 应用程序使用，并通过一个来自纵向研究的视觉示例进行了说明，该研究将来自数百个地方委员会的分散源数据整合到一个数据库中。

Abstract: This paper presents an open-source curatorial toolkit intended to produce well-structured and interoperable data. Curation is divided into discrete components, with a schema-centric focus for auditable restructuring of complex and scattered tabular data to conform to a destination schema. Task separation allows development of software and analysis without source data being present. Transformations are captured as high-level sequential scripts describing schema-to-schema mappings, reducing complexity and resource requirements. Ultimately, data are transformed, but the objective is that any data meeting a schema definition can be restructured using a crosswalk. The toolkit is available both as a Python package, and as a 'no-code' visual web application. A visual example is presented, derived from a longitudinal study where scattered source data from hundreds of local councils are integrated into a single database.

</details>


### [7] [Auto-Formula: Recommend Formulas in Spreadsheets using Contrastive Learning for Table Representations](https://arxiv.org/abs/2404.12608)
*Sibei Chen,Yeye He,Weiwei Cui,Ju Fan,Song Ge,Haidong Zhang,Dongmei Zhang,Surajit Chaudhuri*

Main category: cs.DB

TL;DR: Auto-Formula系统通过学习和适应相似电子表格中已有的公式，来预测用户在目标单元格中想要输入的公式，解决了非技术用户在编写复杂公式时的痛点。


<details>
  <summary>Details</summary>
Motivation: 电子表格是流行的编程工具，但非技术用户在编写复杂公式时存在困难。

Method: 利用组织内相似电子表格中存在的相似计算逻辑，通过借鉴计算机视觉中的“相似人脸识别”的对比学习技术，来学习和适应这些现有公式，从而预测用户想要输入的公式。

Result: 在从真实企业电子表格中提取的2K多个测试公式上进行的大量评估显示，Auto-Formula的有效性优于其他方法。

Conclusion: Auto-Formula系统能够有效帮助用户更轻松地创建复杂公式。

Abstract: Spreadsheets are widely recognized as the most popular end-user programming tools, which blend the power of formula-based computation, with an intuitive table-based interface. Today, spreadsheets are used by billions of users to manipulate tables, most of whom are neither database experts nor professional programmers.
  Despite the success of spreadsheets, authoring complex formulas remains challenging, as non-technical users need to look up and understand non-trivial formula syntax. To address this pain point, we leverage the observation that there is often an abundance of similar-looking spreadsheets in the same organization, which not only have similar data, but also share similar computation logic encoded as formulas. We develop an Auto-Formula system that can accurately predict formulas that users want to author in a target spreadsheet cell, by learning and adapting formulas that already exist in similar spreadsheets, using contrastive-learning techniques inspired by "similar-face recognition" from compute vision.
  Extensive evaluations on over 2K test formulas extracted from real enterprise spreadsheets show the effectiveness of Auto-Formula over alternatives. Our benchmark data is available at https://github.com/microsoft/Auto-Formula to facilitate future research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [Oh That Looks Familiar: A Novel Similarity Measure for Spreadsheet Template Discovery](https://arxiv.org/abs/2511.06973)
*Anand Krishnakumar,Vengadesh Ravikumaran*

Main category: cs.LG

TL;DR:  提出了一种结合语义嵌入、数据类型和空间位置的混合距离度量方法，用于量化电子表格之间的相似性，并通过实验证明其在无监督聚类和模板重建方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation:  传统的电子表格相似性识别方法未能捕捉定义模板的空间布局和类型模式。

Method:  将电子表格转换为单元格级别的嵌入，并使用 Chamfer 和 Hausdorff 距离等聚合技术来计算电子表格相似性。

Result:  在 FUSTE 数据集上，与基于图的 Mondrian 基线相比，在无监督聚类方面表现更优，模板重建达到完美（调整兰德指数为 1.00 对比 0.90）。

Conclusion:  该方法有助于大规模自动化模板发现，并支持检索增强生成、模型训练和批量数据清理等下游应用。

Abstract: Traditional methods for identifying structurally similar spreadsheets fail to capture the spatial layouts and type patterns defining templates. To quantify spreadsheet similarity, we introduce a hybrid distance metric that combines semantic embeddings, data type information, and spatial positioning. In order to calculate spreadsheet similarity, our method converts spreadsheets into cell-level embeddings and then uses aggregation techniques like Chamfer and Hausdorff distances. Experiments across template families demonstrate superior unsupervised clustering performance compared to the graph-based Mondrian baseline, achieving perfect template reconstruction (Adjusted Rand Index of 1.00 versus 0.90) on the FUSTE dataset. Our approach facilitates large-scale automated template discovery, which in turn enables downstream applications such as retrieval-augmented generation over tabular collections, model training, and bulk data cleaning.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [9] [Knowledge engineering for open science: Building and deploying knowledge bases for metadata standards](https://arxiv.org/abs/2507.22391)
*Mark A. Musen,Martin J. O'Connor,Josef Hardi,Marcos Martinez-Romero*

Main category: cs.DL

TL;DR: 科学家们努力将数据集发布到开放存储库，以实现FAIR（可查找、可访问、可互操作、可重用）原则。CEDAR中心开发了技术，通过创建元数据模板来帮助科学家注释数据，这些模板枚举了不同实验的属性，捕获了数据描述的偏好以及第三方理解数据集所需的信息。


<details>
  <summary>Details</summary>
Motivation: 使数据集符合FAIR原则，特别是通过标准化和丰富的元数据注释，以促进开放科学。

Method: 开发CEDAR（中心扩展数据注释和检索）技术，该技术允许将元数据标准编码为模板，这些模板枚举了不同类型实验的属性，并捕获了关于数据描述和第三方理解数据集所需的知识。

Result: CEDAR模板已被用于标准化各种科学联盟的元数据，并已成为数据注释系统的基础，这些系统通过Web表单或电子表格获取元数据，并有助于确保元数据符合标准。

Conclusion: CEDAR模板能够以符号形式捕获知识，并将其应用于各种设置，使科学界能够创建共享的元数据标准，为其应用编码偏好，并在各种智能系统中部署这些标准，从而促进开放科学。

Abstract: Scientists strive to make their datasets available in open repositories, with the goal that they be findable, accessible, interoperable, and reusable (FAIR). Although it is hard for most investigators to remember all the guiding principles associated with FAIR data, there is one overarching requirement: The data need to be annotated with rich, discipline-specific, standardized metadata. The Center for Expanded Data Annotation and Retrieval (CEDAR) builds technology that enables scientists to encode metadata standards as templates that enumerate the attributes of different kinds of experiments. These metadata templates capture preferences regarding how data should be described and what a third party needs to know to make sense of the datasets. CEDAR templates describing community metadata preferences have been used to standardize metadata for a variety of scientific consortia. They have been used as the basis for data-annotation systems that acquire metadata through Web forms or through spreadsheets, and they can help correct metadata to ensure adherence to standards. Like the declarative knowledge bases that underpinned intelligent systems decades ago, CEDAR templates capture the knowledge in symbolic form, and they allow that knowledge to be applied in a variety of settings. They provide a mechanism for scientific communities to create shared metadata standards and to encode their preferences for the application of those standards, and for deploying those standards in a range of intelligent systems to promote open science.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [10] [Billion-files File Systems (BfFS): A Comparison](https://arxiv.org/abs/2408.01805)
*Sohail Shaikh*

Main category: cs.PF

TL;DR: As data grows exponentially, this paper analyzes popular Linux filesystems (EXT4, XFS, BtrFS, ZFS, F2FS) by creating, storing, and reading one billion files. It measures read/write throughput, storage usage, and performance degradation to inform system designers.


<details>
  <summary>Details</summary>
Motivation: The exponential increase in data necessitates processing data closer to compute devices to reduce latency, leading to a need to understand local filesystem performance and limitations.

Method: The study involved creating, storing, and reading back one billion files from five popular Linux filesystems (EXT4, XFS, BtrFS, ZFS, F2FS). Key metrics such as read/write throughput, storage block usage, disk space utilization, overheads, and performance degradation were captured and analyzed.

Result: The analysis provides insights into the performance metrics (throughput, storage efficiency, overheads) and potential degradation issues of EXT4, XFS, BtrFS, ZFS, and F2FS when handling a massive number of files.

Conclusion: This research offers valuable data and analysis for system designers and integrators concerned with local filesystem performance and scalability in the face of large datasets.

Abstract: As the volume of data being produced is increasing at an exponential rate that needs to be processed quickly, it is reasonable that the data needs to be available very close to the compute devices to reduce transfer latency. Due to this need, local filesystems are getting close attention to understand their inner workings, performance, and more importantly their limitations. This study analyzes few popular Linux filesystems: EXT4, XFS, BtrFS, ZFS, and F2FS by creating, storing, and then reading back one billion files from the local filesystem. The study also captured and analyzed read/write throughput, storage blocks usage, disk space utilization and overheads, and other metrics useful for system designers and integrators. Furthermore, the study explored other side effects such as filesystem performance degradation during and after these large numbers of files and folders are created.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [11] [The Paradox of Spreadsheet Self-Efficacy: Social Incentives for Informal Knowledge Sharing in End-User Programming](https://arxiv.org/abs/2408.08068)
*Qing,Xia,Advait Sarkar,Duncan P. Brumby,Anna Cox*

Main category: cs.HC

TL;DR: 个人（自我效能感）、社会（声誉收益、同事间的信任）和软件相关（编纂工作量）变量影响电子表格知识共享意愿。


<details>
  <summary>Details</summary>
Motivation: 了解个人、社会和软件相关变量如何影响电子表格知识共享意愿。

Method: 对来自行政和财务部门的电子表格用户（n=100）进行的调查数据进行多元回归分析。

Result: 较高的电子表格自我效能感和认为共享会带来声誉收益的看法可以预测较高的知识共享意愿，但那些认为知识编纂工作量大的个体则表现出较低的知识共享意愿。此外，无论何种职业，用户倾向于报告较低的总体电子表格熟练度自我效能感，尽管他们在工作相关的背景下使用电子表格时表现出较高的自我效能感。

Conclusion: 承认并设计这些社会和个人变量可以帮助避免有经验的个人不必要地避免分享，这对电子表格设计有影响。

Abstract: Informal Knowledge Sharing (KS) is vital for end-user programmers to gain expertise. To better understand how personal (self-efficacy), social (reputational gains, trust between colleagues), and software-related (codification effort) variables influence spreadsheet KS intention, we conducted a multiple regressions analysis based on survey data from spreadsheet users (n=100) in administrative and finance roles. We found that high levels of spreadsheet self-efficacy and a perception that sharing would result in reputational gains predicted higher KS intention, but individuals who found knowledge codification effortful showed lower KS intention. We also observed that regardless of occupation, users tended to report a lower sense of self-efficacy in their general spreadsheet proficiency, despite also reporting high self-efficacy in spreadsheet use for job-related contexts. Our findings suggest that acknowledging and designing for these social and personal variables can help avoid situations where experienced individuals refrain unnecessarily from sharing, with implications for spreadsheet design.

</details>


### [12] [Subject integration with spreadsheets -- Ignoring education is the greatest risk ever](https://arxiv.org/abs/2409.12975)
*Mária Csernoch,Ádám Gulácsi,Júlia Csernoch*

Main category: cs.HC

TL;DR: 在技术教学内容知识的框架内，学科整合是通过引入有意义的数字化和数字化来解决的，其中任何学科都可以通过数字支持进行教学，信息学课程可以情境化，并且可以最小化“严肃信息学”和“数字素养”之间的差距。


<details>
  <summary>Details</summary>
Motivation: 学科整合是技术教学内容知识框架下引入有意义的数字化和数字化的一种可能解决方案。

Method: 本文详细介绍了如何在电子表格中解决三个传统的三年级任务，以及可以培养教师和学生的哪些技能、能力和计算机科学知识。

Result: 在电子表格中解决任务，分析、理解、规划和讨论任务与在电子表格中的活动一样重要。

Conclusion: 在电子表格中解决任务的过程在为学生未来的职业做准备方面起着至关重要的作用。

Abstract: Within the framework of Technological Pedagogical and Content Knowledge, subject integration is one possible solution for the introduction of meaningful digitalization and digitization in schools. This process incorporates that any school subject can be taught with digital support, informatics (computer) classes can be contextualized, and the gap between 'serious informatics' and 'digital literacy' can be minimized. The present paper details how three traditional Grade 3 tasks can be solved in spreadsheets, what skills, competencies, and computer science knowledge of both teachers and students can be developed. The solutions also reveal that analysing, understanding, planning, and discussing tasks is as important as the activity in the spreadsheets, which process plays a crucial role in the preparation of students for their future jobs.

</details>


### [13] [Data Guards: Challenges and Solutions for Fostering Trust in Data](https://arxiv.org/abs/2407.14042)
*Nicole Sultanum,Dennis Bromley,Michael Correll*

Main category: cs.HC

TL;DR: 数据造假威胁数据驱动决策的有效性，建立信任或验证是必要的。本文通过访谈数据生产者和消费者，探讨了建立数据信任的策略和障碍，并提出了一套数据保护措施。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的决策受到数据有效性威胁，因此需要建立对数据的信任或进行验证。

Method: 通过访谈数据生产者和消费者，了解他们建立数据信任的策略和面临的障碍。

Result: 数据消费者尤其缺乏数据验证和核实的标准，但又存在这方面的迫切需求。

Conclusion: 提出了一套数据保护措施（data guards），以期为数据产品建立信任。

Abstract: From dirty data to intentional deception, there are many threats to the validity of data-driven decisions. Making use of data, especially new or unfamiliar data, therefore requires a degree of trust or verification. How is this trust established? In this paper, we present the results of a series of interviews with both producers and consumers of data artifacts (outputs of data ecosystems like spreadsheets, charts, and dashboards) aimed at understanding strategies and obstacles to building trust in data. We find a recurring need, but lack of existing standards, for data validation and verification, especially among data consumers. We therefore propose a set of data guards: methods and tools for fostering trust in data artifacts.

</details>


### [14] [Smart Portable Computer](https://arxiv.org/abs/2405.05292)
*Niladri Das*

Main category: cs.HC

TL;DR: 疫情期间，PC价格昂贵且配置不足，催生了便携式智能计算机，该设备性能媲美台式机，便携、节能、经济高效，支持编程。


<details>
  <summary>Details</summary>
Motivation: 疫情期间，学生难以获得配置 adequate 的 PC，且传统笔记本电脑对需要依赖笔记本电脑工作的人来说很麻烦。

Method: 开发了一款名为“便携式智能计算机”的创新设备，该设备在紧凑、节能且成本效益高的封装中提供与传统台式计算机相媲美的速度和性能。

Result: 该设备提供了无缝的桌面体验，支持文档编辑、多标签浏览、电子表格管理、演示文稿创建以及 Python、C、C++ 等编程语言和 Keil、Xilinx 等编译器的运行。

Conclusion: 便携式智能计算机是一种便携、节能、经济高效的解决方案，可提供无缝的桌面体验，并满足程序员的需求。

Abstract: Amidst the COVID-19 pandemic, with many organizations, schools, colleges, and universities transitioning to virtual platforms, students encountered difficulties in acquiring PCs such as desktops or laptops. The starting prices, around 15,000 INR, often failed to offer adequate system specifications, posing a challenge for consumers. Additionally, those reliant on laptops for work found the conventional approach cumbersome. Enter the "Portable Smart Computer," a leap into the future of computing. This innovative device boasts speed and performance comparable to traditional desktops but in a compact, energy-efficient, and cost-effective package. It delivers a seamless desktop experience, whether one is editing documents, browsing multiple tabs, managing spreadsheets, or creating presentations. Moreover, it supports programming languages like Python, C, C++, as well as compilers such as Keil and Xilinx, catering to the needs of programmers.

</details>


### [15] ["My toxic trait is thinking I'll remember this": gaps in the learner experience of video tutorials for feature-rich software](https://arxiv.org/abs/2404.07114)
*Ian Drosos,Advait Sarkar,Andrew D. Gordon*

Main category: cs.HC

TL;DR: 学习者在使用视频教程学习软件时会遇到“沟壑”问题，影响学习效果。本研究旨在识别、分类这些沟壑，分析其对学习的阻碍，并提出解决方案。


<details>
  <summary>Details</summary>
Motivation: 视频教程是一种流行的学习方式，但学习者在实践时常遇到阻碍，影响学习效果。本研究旨在识别和解决这些问题。

Method: 通过分析360条YouTube、TikTok和Instagram上的Excel视频教程评论，构建沟壑理论和分类法。访谈了8位有影响力的教程创作者，了解他们遇到的沟壑、解决方法、创作过程和遇到的挫折。最后，向创作者展示了旨在解决沟壑的设计方案，并收集反馈和新的设计想法。

Result: 识别并分类了视频教程学习中存在的沟壑，分析了其阻碍学习的机制，了解了创作者的创作过程和面临的挑战，并获得了关于如何改进教程设计的初步反馈。

Conclusion: 本研究识别了视频教程学习中的关键障碍（沟壑），并为改进教程设计、提升学习体验提供了理论基础和实践指导。

Abstract: Video tutorials are a popular medium for informal and formal learning. However, when learners attempt to view and follow along with these tutorials, they encounter what we call gaps, that is, issues that can prevent learning. We examine the gaps encountered by users of video tutorials for feature-rich software, such as spreadsheets. We develop a theory and taxonomy of such gaps, identifying how they act as barriers to learning, by collecting and analyzing 360 viewer comments from 90 Microsoft Excel video tutorials published by 43 creators across YouTube, TikTok, and Instagram. We conducted contextual interviews with 8 highly influential tutorial creators to investigate the gaps their viewers experience and how they address them. Further, we obtain insights into their creative process and frustrations when creating video tutorials. Finally, we present creators with two designs that aim to address gaps identified in the comment analysis for feedback and alternative design ideas.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [16] [Excel: Automated Ledger or Analytics IDE?](https://arxiv.org/abs/2409.12976)
*Andrew Kumiega*

Main category: cs.CY

TL;DR: Excel 已从简单的账本自动化工具演变为分析的集成开发环境 (IDE)，但其风险管理框架并未跟上这一演变，需要进行扩展以应对新的风险。


<details>
  <summary>Details</summary>
Motivation: 许多人没有注意到 Excel 已经演变成一个功能齐全的数据库、OLAP 引擎、多种统计编程语言、第三方软件库、动态图表和实时数据连接器的集成开发环境 (IDE)。因此，有必要建立一个全面的风险框架来管理这种独特的开发环境。

Method: 解释了当前的电子表格风险框架需要如何扩展，以管理将 Excel 用作分析 IDE 所带来的日益增长的风险。

Result: Excel 已经从一个桌面应用程序演变为一个分析 IDE。

Conclusion: 承认 Excel 已转变为分析 IDE，并强调了为管理这种独特的开发环境建立全面的风险框架的重要性。

Abstract: Since the inception of VisiCalc over four decades ago, spreadsheets have undergone a gradual transformation, evolving from simple ledger automation tools to the current state of Excel, which can be described as an Integrated Development Environment (IDE) for analytics. The slow evolution of Excel from an automation tool for ledgers to an IDE for analytics explains why many people have not noticed that Excel includes a fully functional database, an OLAP Engine, multiple statistical programming languages, multiple third-party software libraries, dynamic charts, and real time data connectors. The simplicity of accessing these multiple tools is a low-code framework controlled from the Excel tool that is effectively an IDE. Once we acknowledge Excel's shift from a desk top application to an IDE for analytics, the importance of establishing a comprehensive risk framework for managing this distinctive development environment becomes clear. In this paper we will explain how the current risk framework for spreadsheets needs to be expanded to manage the growing risks of using Excel as an IDE for analytics.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.05587)
*Junjie Xing,Yeye He,Mengyu Zhou,Haoyu Dong,Shi Han,Lingjiao Chen,Dongmei Zhang,Surajit Chaudhuri,H. V. Jagadish*

Main category: cs.AI

TL;DR: LLMs在处理表格数据方面取得了显著进展，但目前缺乏全面的基准测试来评估它们在专业用户级别的能力。本研究提出了MMTU，一个包含25个真实表格任务、超过28000个问题的基准测试，旨在全面评估模型理解、推理和操作表格的能力。评估结果表明，即使是像OpenAI GPT-5和DeepSeek R1这样的前沿模型，在MMTU上的表现也远非完美，凸显了在结构化数据处理和分析领域基础模型发展的重要性和潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言处理基准测试忽视了表格相关任务，尤其是在专业用户面临的复杂表格任务方面，这限制了我们对模型在该领域能力的理解和进步。

Method: 构建了一个大规模基准测试MMTU，包含25个真实世界表格任务和超过28000个问题，旨在全面评估模型在表格理解、推理和操作方面的专家级能力。

Result: MMTU基准测试显示，包括OpenAI GPT-5和DeepSeek R1在内的前沿模型在处理这些任务时仍面临挑战，准确率分别为69%和57%，表明在结构化数据处理方面仍有很大的改进空间。

Conclusion: MMTU是一个全面的基准测试，可以推动基础模型在结构化数据处理和分析方面的进一步发展。

Abstract: Tables and table-based use cases play a crucial role in many important real-world applications, such as spreadsheets, databases, and computational notebooks, which traditionally require expert-level users like data engineers, data analysts, and database administrators to operate. Although LLMs have shown remarkable progress in working with tables (e.g., in spreadsheet and database copilot scenarios), comprehensive benchmarking of such capabilities remains limited. In contrast to an extensive and growing list of NLP benchmarks, evaluations of table-related tasks are scarce, and narrowly focus on tasks like NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks that professional users face. This gap limits our understanding and model progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 28K questions across 25 real-world table tasks, designed to comprehensively evaluate models ability to understand, reason, and manipulate real tables at the expert-level. These tasks are drawn from decades' worth of computer science research on tabular data, with a focus on complex table tasks faced by professional users. We show that MMTU require a combination of skills -- including table understanding, reasoning, and coding -- that remain challenging for today's frontier models, where even frontier reasoning models like OpenAI GPT-5 and DeepSeek R1 score only around 69\% and 57\% respectively, suggesting significant room for improvement. We highlight key findings in our evaluation using MMTU and hope that this benchmark drives further advances in understanding and developing foundation models for structured data processing and analysis.
  Our code and data are available at https://github.com/MMTU-Benchmark/MMTU and https://huggingface.co/datasets/MMTU-benchmark/MMTU.

</details>


### [18] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: 本篇论文的主要内容是关于关系学习的现状和发展。


<details>
  <summary>Details</summary>
Motivation: 当前的人工智能研究过于侧重于像素和词语的模拟，而忽略了对现实世界中实体及其关系进行建模。作者认为，我们应该关注对实体的建模，而不是对它们的感知或描述。

Method: 本文旨在探讨关系学习未能占据主导地位的原因，并提出使其获得应有地位的必要措施。

Result: 作者指出，目前只有在关系受限的情况下，关系学习才能取得成功。

Conclusion: 文章认为，关系学习在人工智能领域具有重要潜力，但需要进一步的研究和发展才能充分发挥其作用。

Abstract: Artificial intelligence seems to be taking over the world with systems that model pixels, words, and phonemes. The world is arguably made up, not of pixels, words, and phonemes but of entities (objects, things, including events) with properties and relations among them. Surely we should model these, not the perception or description of them. You might suspect that concentrating on modeling words and pixels is because all of the (valuable) data in the world is in terms of text and images. If you look into almost any company you will find their most valuable data is in spreadsheets, databases and other relational formats. These are not the form that are studied in introductory machine learning, but are full of product numbers, student numbers, transaction numbers and other identifiers that can't be interpreted naively as numbers. The field that studies this sort of data has various names including relational learning, statistical relational AI, and many others. This paper explains why relational learning is not taking over the world -- except in a few cases with restricted relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [19] [TableTalk: Scaffolding Spreadsheet Development with a Language Agent](https://arxiv.org/abs/2502.09787)
*Jenny T. Liang,Aayush Kumar,Yasharth Bajpai,Sumit Gulwani,Vu Le,Chris Parnin,Arjun Radhakrishna,Ashish Tiwari,Emerson Murphy-Hill,Guastavo Soares*

Main category: cs.SE

TL;DR: TableTalk是一个表格编程代理，通过脚手架、灵活性和渐进性原则，帮助用户更轻松地创建电子表格。


<details>
  <summary>Details</summary>
Motivation: 电子表格编程具有挑战性，而语言代理在创建电子表格方面显示出潜力。本研究旨在探索一种新的代理方法。

Method: TableTalk代理基于对7位电子表格程序员和85个Excel模板的研究，采用了脚手架、灵活性和渐进性三个设计原则。它通过结构化计划、生成下一步操作以及使用预定义工具来渐进式构建电子表格。

Result: 在20位程序员的对照研究中，TableTalk生成的电子表格质量更高，并且用户偏好度是以往的2.3倍。此外，它还降低了12.6%的认知负荷和思考时间。

Conclusion: 本研究提出了用于代理电子表格编程工具的设计指南，并探讨了其在电子表格编程、最终用户编程、AI辅助编程和人机协作方面的影响。

Abstract: Spreadsheet programming is challenging. Programmers use spreadsheet programming knowledge (e.g., formulas) and problem-solving skills to combine actions into complex tasks. Advancements in large language models have introduced language agents that observe, plan, and perform tasks, showing promise for spreadsheet creation. We present TableTalk, a spreadsheet programming agent embodying three design principles -- scaffolding, flexibility, and incrementality -- derived from studies with seven spreadsheet programmers and 85 Excel templates. TableTalk guides programmers through structured plans based on professional workflows, generating three potential next steps to adapt plans to programmer needs. It uses pre-defined tools to generate spreadsheet components and incrementally build spreadsheets. In a study with 20 programmers, TableTalk produced higher-quality spreadsheets 2.3 times more likely to be preferred than the baseline. It reduced cognitive load and thinking time by 12.6%. From this, we derive design guidelines for agentic spreadsheet programming tools and discuss implications on spreadsheet programming, end-user programming, AI-assisted programming, and human-agent collaboration.

</details>


### [20] [MaxMind: A Memory Loop Network to Enhance Software Productivity based on Large Language Models](https://arxiv.org/abs/2408.03841)
*Yuchen Dong,XiaoXiang Fang,Yuchen Hu,Renshuang Jiang,Zhe Jiang*

Main category: cs.SE

TL;DR: 大型语言模型在软件开发和工具生成（SOTG）中的应用，通过引入记忆循环网络和价值区分的RAG机制，并以MaxMind模型和MaxMind4Sheet系统为例，证明了其在提高任务成功率和效率方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前研究忽视了将实时任务经验转化为系统记忆以及区分现有知识价值的重要性，而这对于AI在软件操作和工具生成（SOTG）领域的进步至关重要。

Method: 提出了记忆循环网络（Memory-Loop Networks）用于及时记忆和经验引用，并增强了具有知识精度分割的检索增强生成（RAG）机制，以根据价值区分来利用记忆。设计了MaxMind模型来处理SOTG任务。

Result: MaxMind4Sheet系统在与SheetCopilot的对比实验中，任务记忆的积累和回收使任务成功率稳步提高约3%-6%，任务执行效率最多可提高25%。该方法还通过记忆转移解决了大型语言模型在处理专业任务时的再训练问题。

Conclusion: MaxMind模型在SOTG领域具有显著潜力，能够增强大型语言模型系统的能力和生产力，尤其是在记忆积累、价值区分利用和任务效率提升方面。

Abstract: The application of large language models to facilitate automated software operations and tool generation (SOTG), thus augmenting software productivity, mirrors the early stages of human evolution when the ability to create and use tools accelerated the progress of civilization. These complex tasks require AI to continuously summarize and improve. Current research often overlooks the importance of converting real-time task experiences into system memory and differentiating the value of existing knowledge for future reference. This paper addresses these issues by evolving external memory models into Memory-Loop Networks for timely memorization and experience referencing. We also enhance a RAG mechanism with knowledge precision segmentation to utilize memory based on value differentiation, and design the MaxMind model for SOTG accordingly.To demonstrate our approach, we developed MaxMind4Sheet, an electronic spreadsheet processing system aligned with the MaxMind philosophy. Comparative experiments with SheetCopilot have demonstrated that the accumulation and recycling of task memories lead to a steady enhancement in task success rate, with an improvement rate of approximately 3%-6% per round in this implementation example. Note that as the memories continue to grow, this cumulative improvement may be substantial. The inclusion of memory recycling can also boost the system's task execution efficiency by up to 25%, and it can address the retraining issue faced by LLMs when handling specialized tasks through memories transfer.These suggest that MaxMind has significant potential to enhance the capabilities and productivity of LLM systems in SOTG.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [21] [Solving Data-centric Tasks using Large Language Models](https://arxiv.org/abs/2402.11734)
*Shraddha Barke,Christian Poelitz,Carina Suzana Negreanu,Benjamin Zorn,José Cambronero,Andrew D. Gordon,Vu Le,Elnaz Nouri,Nadia Polikarpova,Advait Sarkar,Brian Slininger,Neil Toronto,Jack Williams*

Main category: cs.PL

TL;DR: LLM在处理数据相关任务时，通过引入


<details>
  <summary>Details</summary>
Motivation: LLM正逐渐取代StackOverflow等论坛，在数据处理任务上特别有助于非专业程序员和终端用户，但如何有效地在提示中包含数据以解决这些任务是一个挑战。

Method: 首先，创建了一个包含真实世界自然语言到代码的数据集，该数据集是从StackOverflow帖子中挖掘的，用于操作表格数据。其次，引入了一种聚类后选择的提示技术，该技术将输入数据中最具代表性的行添加到LLM提示中。

Result: 实验表明，LLM在提示中包含的数据量对其性能有显著影响。对于输入表中具有大量语法变动的任务，所提出的聚类后选择技术优于随机选择基线。

Conclusion: LLM在处理数据相关任务时，通过引入代表性数据行到提示中，可以提升其性能，并且所提出的聚类后选择技术是一种有效的选择数据的方法。

Abstract: Large language models (LLMs) are rapidly replacing help forums like StackOverflow, and are especially helpful for non-professional programmers and end users. These users are often interested in data-centric tasks, such as spreadsheet manipulation and data wrangling, which are hard to solve if the intent is only communicated using a natural-language description, without including the data. But how do we decide how much data and which data to include in the prompt? This paper makes two contributions towards answering this question. First, we create a dataset of real-world NL-to-code tasks manipulating tabular data, mined from StackOverflow posts. Second, we introduce a cluster-then-select prompting technique, which adds the most representative rows from the input data to the LLM prompt. Our experiments show that LLM performance is indeed sensitive to the amount of data passed in the prompt, and that for tasks with a lot of syntactic variation in the input table, our cluster-then-select technique outperforms a random selection baseline.

</details>
