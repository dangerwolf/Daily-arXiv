<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 5]
- [cs.CL](#cs.CL) [Total: 12]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.SE](#cs.SE) [Total: 40]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.MS](#cs.MS) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.HC](#cs.HC) [Total: 31]
- [cs.DL](#cs.DL) [Total: 5]
- [stat.OT](#stat.OT) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [math.HO](#math.HO) [Total: 2]
- [cs.DB](#cs.DB) [Total: 24]
- [cs.OH](#cs.OH) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.PL](#cs.PL) [Total: 7]
- [cs.CY](#cs.CY) [Total: 12]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.IR](#cs.IR) [Total: 6]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Image deidentification in the XNAT ecosystem: use cases and solutions](https://arxiv.org/abs/2504.20657)
*Alex Michie,Simon J Doran*

Main category: cs.CV

TL;DR: 该论文详细描述了一个使用XNAT及其生态系统工具的DICOM数据去识别工作流程，并报告了其在医学图像去识别基准（MIDI-B）挑战中的表现，从最初的97.91%提升至99.61%，并讨论了未来在地址识别和图像像素去识别方面的改进方向。


<details>
  <summary>Details</summary>
Motivation: 在学术界，为了管理和策展大量的DICOM图像数据库用于研究项目，需要一个服务器端的数据管理平台。鉴于此，本文旨在描述一个针对DICOM数据的去识别工作流程，并评估其在实际挑战中的有效性，以满足不同场景下对去识别的需求。

Method: 本研究使用XNAT平台及其独立的“生态系统”工具来描述DICOM数据的去识别工作流程。研究团队基于先前的经验，列举了需要去识别的不同情境。参与医学图像去识别基准（MIDI-B）挑战时，他们调整了现有的本地方法。去识别方法包括一个完全基于规则的方法来处理姓名相关信息，并尝试使用已发布的机器学习模型来去除地址数据。

Result: 在MIDI-B挑战的测试阶段，由于与挑战平台的兼容性问题，初始结果为97.91%。通过后期反馈，分数显著提升至99.61%。完全基于规则的方法成功移除了所有姓名相关信息，但在处理地址数据时出现失败。初步使用机器学习模型移除地址部分成功，但模型对其他类型的自由文本数据“过度激进”，导致整体性能略微下降至99.54%。目前，在MIDI-B测试语料库中，估计的真实去识别失败率为0.19%。

Conclusion: XNAT及其工具可以有效地进行DICOM数据去识别，但地址识别仍然是一个挑战，需要进一步开发，包括改进地址识别能力和更好地去除图像像素中烧录的可识别数据。未来的工作将专注于这些改进，以达到更完善的去识别效果。

Abstract: XNAT is a server-based data management platform widely used in academia for curating large databases of DICOM images for research projects. We describe in detail a deidentification workflow for DICOM data using facilities in XNAT, together with independent tools in the XNAT "ecosystem". We list different contexts in which deidentification might be needed, based on our prior experience. The starting point for participation in the Medical Image De-Identification Benchmark (MIDI-B) challenge was a set of pre-existing local methodologies, which were adapted during the validation phase of the challenge. Our result in the test phase was 97.91\%, considerably lower than our peers, due largely to an arcane technical incompatibility of our methodology with the challenge's Synapse platform, which prevented us receiving feedback during the validation phase. Post-submission, additional discrepancy reports from the organisers and via the MIDI-B Continuous Benchmarking facility, enabled us to improve this score significantly to 99.61\%. An entirely rule-based approach was shown to be capable of removing all name-related information in the test corpus, but exhibited failures in dealing fully with address data. Initial experiments using published machine-learning models to remove addresses were partially successful but showed the models to be "over-aggressive" on other types of free-text data, leading to a slight overall degradation in performance to 99.54\%. Future development will therefore focus on improving address-recognition capabilities, but also on better removal of identifiable data burned into the image pixels. Several technical aspects relating to the "answer key" are still under discussion with the challenge organisers, but we estimate that our percentage of genuine deidentification failures on the MIDI-B test corpus currently stands at 0.19\%. (Abridged from original for arXiv submission)

</details>


### [2] [Vision Language Models for Spreadsheet Understanding: Challenges and Opportunities](https://arxiv.org/abs/2405.16234)
*Shiyu Xia,Junyu Xiong,Haoyu Dong,Jianbo Zhao,Yuzhang Tian,Mengyu Zhou,Yeye He,Shi Han,Dongmei Zhang*

Main category: cs.CV

TL;DR: 本文探讨了视觉语言模型（VLMs）在电子表格理解方面的能力，发现VLMs在光学字符识别（OCR）方面表现出潜力，但在空间感知和格式识别方面表现不足。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）在电子表格理解方面，尤其是在空间感知和视觉格式识别方面存在局限性。

Method: 提出了三个自监督挑战（光学字符识别、空间感知、视觉格式识别）及相应的评估指标。利用电子表格检测任务评估VLM的整体性能。提出了三种电子表格到图像的设置：列宽调整、样式更改和地址增强，并设计了不同的提示变体。在电子表格边界检测中，通过解码表格四个边界上的单元格值来利用VLMs的文本理解优势。

Result: VLMs在OCR方面表现出有希望的能力，但由于单元格遗漏和错位导致结果不尽如人意，并且在空间和格式识别技能方面表现不足。

Conclusion: 未来的工作应利用本文提出的方法在各种设置中生成大量的电子表格-图像对，以增强VLMs的电子表格数据理解能力。

Abstract: This paper explores capabilities of Vision Language Models on spreadsheet comprehension. We propose three self-supervised challenges with corresponding evaluation metrics to comprehensively evaluate VLMs on Optical Character Recognition (OCR), spatial perception, and visual format recognition. Additionally, we utilize the spreadsheet table detection task to assess the overall performance of VLMs by integrating these challenges. To probe VLMs more finely, we propose three spreadsheet-to-image settings: column width adjustment, style change, and address augmentation. We propose variants of prompts to address the above tasks in different settings. Notably, to leverage the strengths of VLMs in understanding text rather than two-dimensional positioning, we propose to decode cell values on the four boundaries of the table in spreadsheet boundary detection. Our findings reveal that VLMs demonstrate promising OCR capabilities but produce unsatisfactory results due to cell omission and misalignment, and they notably exhibit insufficient spatial and format recognition skills, motivating future work to enhance VLMs' spreadsheet data comprehension capabilities using our methods to generate extensive spreadsheet-image pairs in various settings.

</details>


### [3] [High-Throughput Phenotyping using Computer Vision and Machine Learning](https://arxiv.org/abs/2407.06354)
*Vivaan Singhvi,Langalibalele Lunga,Pragya Nidhi,Chris Keum,Varrun Prakash*

Main category: cs.CV

TL;DR: 该研究将高通量表型分析与机器学习结合，使用OCR、图像分割和分类技术，分析带有物理标签的毛果杨图像。OCR模型准确率达94.31%，性状和处理分类准确率约为60%，但EXIF数据限制了叶片大小和表型关联的评估。


<details>
  <summary>Details</summary>
Motivation: 高通量表型分析结合机器学习在处理大数据集和提取特定性状方面需要改进。以往研究常缺少物理标签，本研究旨在通过使用带有物理标签的数据集来解决这一问题。

Method: 本研究使用了一个包含1,672张带有物理标签（包含处理、区块、行、位置和基因型信息）的毛果杨图像数据集。通过光学字符识别（OCR）读取标签，结合图像分割技术和机器学习算法进行形态分类，并使用机器学习模型根据分类预测处理。同时，分析了编码的EXIF标签以寻找叶片大小和表型之间的相关性。

Result: OCR模型在非空文本提取方面的准确率为94.31%。分类模型在识别叶片形状、颜色和褐斑程度方面的平均准确率为62.82%。植物处理预测的准确率为60.08%。然而，EXIF标签中缺少关键信息，阻碍了对叶片大小以及表型与条件之间相关性的评估。

Conclusion: 本研究成功地将物理标签与高通量表型分析相结合，利用OCR和机器学习进行形态分类和处理预测。尽管OCR和分类显示出合理的准确性，但EXIF数据中的限制阻碍了对叶片大小和表型相关性的全面评估。未来的研究可以在数据收集方面进行改进以解决这些问题。

Abstract: High-throughput phenotyping refers to the non-destructive and efficient evaluation of plant phenotypes. In recent years, it has been coupled with machine learning in order to improve the process of phenotyping plants by increasing efficiency in handling large datasets and developing methods for the extraction of specific traits. Previous studies have developed methods to advance these challenges through the application of deep neural networks in tandem with automated cameras; however, the datasets being studied often excluded physical labels. In this study, we used a dataset provided by Oak Ridge National Laboratory with 1,672 images of Populus Trichocarpa with white labels displaying treatment (control or drought), block, row, position, and genotype. Optical character recognition (OCR) was used to read these labels on the plants, image segmentation techniques in conjunction with machine learning algorithms were used for morphological classifications, machine learning models were used to predict treatment based on those classifications, and analyzed encoded EXIF tags were used for the purpose of finding leaf size and correlations between phenotypes. We found that our OCR model had an accuracy of 94.31% for non-null text extractions, allowing for the information to be accurately placed in a spreadsheet. Our classification models identified leaf shape, color, and level of brown splotches with an average accuracy of 62.82%, and plant treatment with an accuracy of 60.08%. Finally, we identified a few crucial pieces of information absent from the EXIF tags that prevented the assessment of the leaf size. There was also missing information that prevented the assessment of correlations between phenotypes and conditions. However, future studies could improve upon this to allow for the assessment of these features.

</details>


### [4] [Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language](https://arxiv.org/abs/2204.00598)
*Andy Zeng,Maria Attarian,Brian Ichter,Krzysztof Choromanski,Adrian Wong,Stefan Welker,Federico Tombari,Aveek Purohit,Michael Ryoo,Vikas Sindhwani,Johnny Lee,Vincent Vanhoucke,Pete Florence*

Main category: cs.CV

TL;DR: Socratic Models (SMs)通过多模态提示零样本组合不同的预训练模型（如视觉语言模型和大型语言模型），以利用它们多样化的知识，实现新的多模态能力，并在多种应用中展现出竞争力，无需微调。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型（如视觉语言模型和大型语言模型）在不同数据领域训练，存储着不同形式的常识知识。本研究旨在利用这种多样性。

Method: Socratic Models (SMs) 框架：通过多模态提示零样本组合多个预训练模型，使它们能够相互交换信息。

Result: Socratic Models (SMs) 在零样本图像字幕和视频到文本检索方面与最先进的技术具有竞争力。它们还支持新应用，例如回答关于以自我为中心的视频的自由形式问题、进行多模态辅助对话以及机器人感知和规划。

Conclusion: 预训练模型的知识多样性是共生的，可以通过Socratic Models框架利用，以零样本方式实现新的多模态能力和广泛的应用，而无需进行微调。

Abstract: Large pretrained (e.g., "foundation") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.

</details>


### [5] [TableParser: Automatic Table Parsing with Weak Supervision from Spreadsheets](https://arxiv.org/abs/2201.01654)
*Susie Xi Rao,Johannes Rausch,Peter Egger,Ce Zhang*

Main category: cs.CV

TL;DR: 本文提出了TableParser系统，能够高精度地解析原生PDF和扫描图像中的表格。该系统利用领域适应，并引入了TableAnnotator和ExcelAnnotator进行弱监督。


<details>
  <summary>Details</summary>
Motivation: 表格是存储数据的常用结构，从PDF、图像等多种格式中解析表格结构并提取内容在许多应用中至关重要。

Method: 开发了TableParser系统，用于解析原生PDF和扫描图像中的表格。该系统通过领域适应来提高效率，并创建了TableAnnotator和ExcelAnnotator作为基于电子表格的弱监督机制和表格解析流程。

Result: 通过广泛的实验证明了领域适应在开发此类工具中的有效性。

Conclusion: 将TableAnnotator和ExcelAnnotator资源分享给研究社区，以促进该方向的进一步研究。

Abstract: Tables have been an ever-existing structure to store data. There exist now different approaches to store tabular data physically. PDFs, images, spreadsheets, and CSVs are leading examples. Being able to parse table structures and extract content bounded by these structures is of high importance in many applications. In this paper, we devise TableParser, a system capable of parsing tables in both native PDFs and scanned images with high precision. We have conducted extensive experiments to show the efficacy of domain adaptation in developing such a tool. Moreover, we create TableAnnotator and ExcelAnnotator, which constitute a spreadsheet-based weak supervision mechanism and a pipeline to enable table parsing. We share these resources with the research community to facilitate further research in this interesting direction.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [6] [An Empirical Study of Validating Synthetic Data for Formula Generation](https://arxiv.org/abs/2407.10657)
*Usneek Singh,José Cambronero,Sumit Gulwani,Aditya Kanade,Anirudh Khatry,Vu Le,Mukul Singh,Gust Verbruggen*

Main category: cs.CL

TL;DR: 针对稀缺的电子表格公式LLM训练资源，本文通过验证合成的自然语言训练数据，显著提升了四种模型的性能，并增加了模型解决复杂问题的能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在电子表格公式编写方面应用受限，因为相关资源稀缺，影响了预训练模型的基础性能和微调能力。使用模型生成合成自然语言数据进行微调时，需要验证其准确性以确保益处。

Method: 本文采用代理目标来评估合成自然语言注释的准确性，并提供了验证这些合成训练示例影响的实证结果。

Result: 经验结果表明，经过验证的数据集在四种模型（两种开源，两种闭源）上的性能均优于原始数据。尽管验证倾向于筛选掉更具挑战性的示例，但它能提升模型在微调后解决复杂问题的能力。

Conclusion: 验证合成训练示例的准确性对于提升大型语言模型在电子表格公式任务上的表现至关重要，甚至能让模型处理更复杂的挑战。

Abstract: Large language models (LLMs) can be leveraged to help with writing formulas in spreadsheets, but resources on these formulas are scarce, impacting both the base performance of pre-trained models and limiting the ability to fine-tune them. Given a corpus of formulas, we can use a(nother) model to generate synthetic natural language utterances for fine-tuning. However, it is important to validate whether the NL generated by the LLM is indeed accurate to be beneficial for fine-tuning. In this paper, we provide empirical results on the impact of validating these synthetic training examples with surrogate objectives that evaluate the accuracy of the synthetic annotations. We demonstrate that validation improves performance over raw data across four models (2 open and 2 closed weight). Interestingly, we show that although validation tends to prune more challenging examples, it increases the complexity of problems that models can solve after being fine-tuned on validated data.

</details>


### [7] [TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios](https://arxiv.org/abs/2403.19318)
*Xiaokang Zhang,Sijia Luo,Bohan Zhang,Zeyao Ma,Jing Zhang,Yang Li,Guanlin Li,Zijun Yao,Kangli Xu,Jinchang Zhou,Daniel Zhang-Li,Jifan Yu,Shu Zhao,Juanzi Li,Jie Tang*

Main category: cs.CL

TL;DR: TableLLM是一个80亿参数的LLM，专门用于处理文档和电子表格中的表格数据操作，通过远距离监督方法训练，并在基准测试中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在处理现实办公场景中嵌入文档或电子表格内的表格数据操作任务时存在不足，因此需要一个专用的解决方案。

Method: 我们提出了TableLLM，一个80亿参数的LLM，专门处理表格数据操作。训练方法采用远距离监督，包含推理过程扩展策略和交叉验证策略以提高数据质量和模型推理能力。为评估TableLLM性能，我们创建了针对文档和电子表格格式的定制基准测试及评估流程。

Result: 通过全面评估，TableLLM在处理表格数据任务方面优于现有的通用型和专注于表格数据的LLM。

Conclusion: TableLLM为处理现实办公场景中的表格数据操作任务提供了一个强大的解决方案，并且其模型、代码和基准已公开发布，促进了该领域的研究和应用。

Abstract: We introduce TableLLM, a robust large language model (LLM) with 8 billion parameters, purpose-built for proficiently handling tabular data manipulation tasks, whether they are embedded within documents or spreadsheets, catering to real-world office scenarios. We propose a distant supervision method for training, which comprises a reasoning process extension strategy, aiding in training LLMs to understand reasoning patterns more effectively as well as a cross-way validation strategy, ensuring the quality of the automatically generated data. To evaluate the performance of TableLLM, we have crafted benchmarks tailored to address both document and spreadsheet formats as well as constructed a well-organized evaluation pipeline capable of handling both scenarios. Thorough evaluations underscore the advantages of TableLLM when compared to various existing general-purpose and tabular data-focused LLMs. We have publicly released the model checkpoint, source code, benchmarks, and a web application for user interaction. Our codes and data are publicly available at https://github.com/TableLLM/TableLLM.

</details>


### [8] [GiesKaNe: Bridging Past and Present in Grammatical Theory and Practical Application](https://arxiv.org/abs/2502.05113)
*Volker Emmrich*

Main category: cs.CL

TL;DR: 本文探讨了GiesKaNe项目（一个历史性、句法深度标注的参考语料库）的语料库编译要求，该项目通过人机协作和利用现有基础设施，平衡了创新与标准。


<details>
  <summary>Details</summary>
Motivation: GiesKaNe项目旨在建立历史语料库与当代语料库之间的联系，确保其在时间和语言背景下的相关性，并满足项目内部目标及更广泛的研究社区利益。

Method: 该项目通过人机协作的方式，解决了语料库编译中的方法学复杂性。具体方法包括探讨了分词、标准化、句子定义、标注、句法分析和标注者间一致性等基础问题，并深入比较了语法模型、标注方案和事实标准。此外，还提出了一种新的机器辅助文本分类方法和一种从现有标注中推导事实标准标注的方法。工作流程基于利用现有研究基础设施，特别是简单的电子表格。

Result: 文章展示了即使是GiesKaNe这样宏大的项目，也可以有效利用现有研究基础设施实施，无需专门的标注工具。它还提出了一种新的机器辅助文本分类方法和一种推导事实标准标注的方法。

Conclusion: GiesKaNe项目成功地平衡了创新与标准化，利用现有基础设施和人机协作，高效地构建了一个复杂的历史树库，证明了雄心勃勃的项目也能有效实施。

Abstract: This article explores the requirements for corpus compilation within the GiesKaNe project (University of Giessen and Kassel, Syntactic Basic Structures of New High German). The project is defined by three central characteristics: it is a reference corpus, a historical corpus, and a syntactically deeply annotated treebank. As a historical corpus, GiesKaNe aims to establish connections with both historical and contemporary corpora, ensuring its relevance across temporal and linguistic contexts. The compilation process strikes the balance between innovation and adherence to standards, addressing both internal project goals and the broader interests of the research community. The methodological complexity of such a project is managed through a complementary interplay of human expertise and machine-assisted processes. The article discusses foundational topics such as tokenization, normalization, sentence definition, tagging, parsing, and inter-annotator agreement, alongside advanced considerations. These include comparisons between grammatical models, annotation schemas, and established de facto annotation standards as well as the integration of human and machine collaboration. Notably, a novel method for machine-assisted classification of texts along the continuum of conceptual orality and literacy is proposed, offering new perspectives on text selection. Furthermore, the article introduces an approach to deriving de facto standard annotations from existing ones, mediating between standardization and innovation. In the course of describing the workflow the article demonstrates that even ambitious projects like GiesKaNe can be effectively implemented using existing research infrastructure, requiring no specialized annotation tools. Instead, it is shown that the workflow can be based on the strategic use of a simple spreadsheet and integrates the capabilities of the existing infrastructure.

</details>


### [9] [MiMoTable: A Multi-scale Spreadsheet Benchmark with Meta Operations for Table Reasoning](https://arxiv.org/abs/2412.11711)
*Zheng Li,Yang Du,Mao Zheng,Mingyang Song*

Main category: cs.CL

TL;DR: 该论文提出了MiMoTable，一个多尺度电子表格基准，旨在解决现有LLM表格推理基准与现实世界复杂表格和问题之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型表格推理基准未能充分反映现实世界中表格和用户查询的复杂性和多样性，导致现实应用中存在显著的性能差距。

Method: 提出MiMoTable基准，该基准包含来自七个领域的真实世界电子表格。同时，定义了一个包含六类元操作的新标准来衡量问题和现有基准的难度。

Result: Claude-3.5-Sonnet在MiMoTable上取得了77.4%的准确率，表明LLMs仍有很大提升空间。实验结果也证明了所提出新难度标准的有效性，即LLMs的性能随基准难度的增加而下降。

Conclusion: MiMoTable基准和新的难度衡量标准有效弥补了现有基准与现实世界应用之间的差距，为LLM在表格推理方面的未来研究指明了方向。

Abstract: Extensive research has been conducted to explore the capability of Large Language Models (LLMs) for table reasoning and has significantly improved the performance on existing benchmarks. However, tables and user questions in real-world applications are more complex and diverse, presenting an unignorable gap compared to the existing benchmarks. To fill the gap, we propose a \textbf{M}ult\textbf{i}-scale spreadsheet benchmark with \textbf{M}eta \textbf{o}perations for \textbf{Table} reasoning, named as MiMoTable. Specifically, MiMoTable incorporates two key features. First, the tables in MiMoTable are all spreadsheets used in real-world scenarios, which cover seven domains and contain different types. Second, we define a new criterion with six categories of meta operations for measuring the difficulty of each question in MiMoTable, simultaneously as a new perspective for measuring the difficulty of the existing benchmarks. Experimental results show that Claude-3.5-Sonnet achieves the best performance with 77.4\% accuracy, indicating that there is still significant room to improve for LLMs on MiMoTable. Furthermore, we grade the difficulty of existing benchmarks according to our new criteria. Experiments have shown that the performance of LLMs decreases as the difficulty of benchmarks increases, thereby proving the effectiveness of our proposed new criterion.

</details>


### [10] [SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation](https://arxiv.org/abs/2406.14991)
*Zeyao Ma,Bohan Zhang,Jing Zhang,Jifan Yu,Xiaokang Zhang,Xiaohan Zhang,Sijia Luo,Xi Wang,Jie Tang*

Main category: cs.CL

TL;DR: 本文介绍了SpreadsheetBench，一个基于真实场景的电子表格操作基准，旨在评估大型语言模型（LLMs）在处理复杂电子表格任务时的表现。该基准揭示了当前LLMs与人类表现之间存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准依赖于合成查询和简化电子表格文件，未能真实反映用户需求。本研究旨在通过构建一个基于真实世界场景的新基准来解决这一问题。

Method: SpreadsheetBench包含912个从在线Excel论坛收集的真实问题，以及包含多种复杂表格数据和非文本元素的电子表格。此外，本文提出了一种更可靠的评估指标，即为每条指令创建多个电子表格文件作为测试用例，以确保评估解决方案的鲁棒性。

Result: 对各种LLMs在单轮和多轮推理设置下的综合评估表明，最先进的模型与人类表现之间存在巨大差距，突显了该基准的难度。

Conclusion: SpreadsheetBench揭示了当前最先进的LLMs在真实世界电子表格操作任务中与人类表现之间存在显著差距，表明该领域仍有很大的改进空间。

Abstract: We introduce SpreadsheetBench, a challenging spreadsheet manipulation benchmark exclusively derived from real-world scenarios, designed to immerse current large language models (LLMs) in the actual workflow of spreadsheet users. Unlike existing benchmarks that rely on synthesized queries and simplified spreadsheet files, SpreadsheetBench is built from 912 real questions gathered from online Excel forums, which reflect the intricate needs of users. The associated spreadsheets from the forums contain a variety of tabular data such as multiple tables, non-standard relational tables, and abundant non-textual elements. Furthermore, we propose a more reliable evaluation metric akin to online judge platforms, where multiple spreadsheet files are created as test cases for each instruction, ensuring the evaluation of robust solutions capable of handling spreadsheets with varying values. Our comprehensive evaluation of various LLMs under both single-round and multi-round inference settings reveals a substantial gap between the state-of-the-art (SOTA) models and human performance, highlighting the benchmark's difficulty.

</details>


### [11] [NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries](https://arxiv.org/abs/2402.14853)
*Wei Zhao,Zhitao Hou,Siyuan Wu,Yan Gao,Haoyu Dong,Yao Wan,Hongyu Zhang,Yulei Sui,Haidong Zhang*

Main category: cs.CL

TL;DR: 本文介绍了一个名为NL2Formula的新基准任务，旨在根据自然语言查询生成可执行的电子表格公式。为此，我们构建了一个包含70,799个配对NL查询和对应电子表格公式的综合数据集，并实现了一个名为fCoder的序列到序列基线实现。实验结果验证了fCoder的有效性，并指出NL2Formula任务中存在的潜在挑战。


<details>
  <summary>Details</summary>
Motivation: 在电子表格上编写公式对于许多终端用户来说是一项繁琐且容易出错的任务，尤其是在处理复杂操作时。

Method: 提出了NL2Formula基准任务，旨在根据自然语言（NL）查询生成基于电子表格的可执行公式。构建了一个包含70,799个配对NL查询和对应电子表格公式的综合数据集，涵盖21,670个表格和37种公式函数类型。通过提供一个名为fCoder的序列到序列基线实现来完成NL2Formula任务。

Result: fCoder的实验结果验证了其有效性，与基线模型相比，fCoder表现出卓越的性能。此外，还将fCoder与初始的GPT-3.5模型（即text-davinci-003）进行了比较。

Conclusion: fCoder是有效的，但NL2Formula任务中仍然存在潜在挑战，需要进一步研究。

Abstract: Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets, is a widespread practice among users performing data analysis. However, crafting formulas on spreadsheets remains a tedious and error-prone task for many end-users, particularly when dealing with complex operations. To alleviate the burden associated with writing spreadsheet formulas, this paper introduces a novel benchmark task called NL2Formula, with the aim to generate executable formulas that are grounded on a spreadsheet table, given a Natural Language (NL) query as input. To accomplish this, we construct a comprehensive dataset consisting of 70,799 paired NL queries and corresponding spreadsheet formulas, covering 21,670 tables and 37 types of formula functions. We realize the NL2Formula task by providing a sequence-to-sequence baseline implementation called fCoder. Experimental results validate the effectiveness of fCoder, demonstrating its superior performance compared to the baseline models. Furthermore, we also compare fCoder with an initial GPT-3.5 model (i.e., text-davinci-003). Lastly, through in-depth error analysis, we identify potential challenges in the NL2Formula task and advocate for further investigation.

</details>


### [12] [InstructExcel: A Benchmark for Natural Language Instruction in Excel](https://arxiv.org/abs/2310.14495)
*Justin Payan,Swaroop Mishra,Mukul Singh,Carina Negreanu,Christian Poelitz,Chitta Baral,Subhro Roy,Rasika Chakravarthy,Benjamin Van Durme,Elnaz Nouri*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型（LLMs）根据自然语言指令生成Excel OfficeScripts代码的能力，并提出了一个新的大规模基准测试InstructExcel。结果显示，对于GPT-4等先进模型而言，这是一个具有挑战性的任务。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的发展，它们能够解决包括电子表格在内各个领域日益复杂的自然语言处理任务。本研究旨在探究LLMs是否能够根据用户的自然语言指令，生成解决Excel特定任务的代码（Excel OfficeScripts）。

Method: 为此，我们引入了一个新的大规模基准测试InstructExcel。该基准通过利用Excel的“自动化”功能，根据用户操作自动生成OfficeScripts来创建。InstructExcel包含超过1万个样本，涵盖了2,000个公开可用的Excel电子表格中的170多种Excel操作。

Result: 在各种零样本和少样本设置下的实验表明，InstructExcel对于像GPT-4这样的最先进模型来说是一个艰巨的基准。我们观察到，(1) 使用GPT-4而非GPT-3.5，(2) 提供更多上下文示例，以及 (3) 动态提示可以帮助提高在此基准上的性能。

Conclusion: InstructExcel基准测试对当前先进模型（如GPT-4）来说是一个艰巨的挑战，但通过升级模型、增加上下文示例和采用动态提示等方法，有望提升LLMs在根据自然语言指令生成Excel OfficeScripts方面的表现。

Abstract: With the evolution of Large Language Models (LLMs) we can solve increasingly more complex NLP tasks across various domains, including spreadsheets. This work investigates whether LLMs can generate code (Excel OfficeScripts, a TypeScript API for executing many tasks in Excel) that solves Excel specific tasks provided via natural language user instructions. To do so we introduce a new large-scale benchmark, InstructExcel, created by leveraging the 'Automate' feature in Excel to automatically generate OfficeScripts from users' actions. Our benchmark includes over 10k samples covering 170+ Excel operations across 2,000 publicly available Excel spreadsheets. Experiments across various zero-shot and few-shot settings show that InstructExcel is a hard benchmark for state of the art models like GPT-4. We observe that (1) using GPT-4 over GPT-3.5, (2) providing more in-context examples, and (3) dynamic prompting can help improve performance on this benchmark.

</details>


### [13] [Active Learning with Tabular Language Models](https://arxiv.org/abs/2211.04128)
*Martin Ringsquandl,Aneta Koleva*

Main category: cs.CL

TL;DR: 本论文研究了在工业场景下，表格语言模型结合主动学习进行子单元格命名实体识别，发现内置多样性的单元格级别采集函数能显著降低标注成本，而强制的表格多样性则不利。


<details>
  <summary>Details</summary>
Motivation: 尽管表格语言模型研究取得了进展，但其实际应用仍面临挑战，尤其是在工业领域，由于表格技术性和领域特异性高，获取大量标注成本昂贵。主动学习有望降低标注成本，但目前尚无将主动学习与表格语言模型结合的研究。

Method: 本文研究了在真实工业表格语言模型用例中，针对子单元格命名实体识别的不同采集函数。

Result: 结果表明，内置多样性的单元格级别采集函数能显著减少标注工作量，而强制的表格多样性则会产生负面影响。同时，还存在关于计算效率和人类标注者视角的基本问题。

Conclusion: 本文表明，内置多样性的单元格级别采集函数在表格语言模型的主动学习中能有效降低标注成本，并提出了关于计算效率和人类标注者的进一步研究问题。

Abstract: Despite recent advancements in tabular language model research, real-world applications are still challenging. In industry, there is an abundance of tables found in spreadsheets, but acquisition of substantial amounts of labels is expensive, since only experts can annotate the often highly technical and domain-specific tables. Active learning could potentially reduce labeling costs, however, so far there are no works related to active learning in conjunction with tabular language models. In this paper we investigate different acquisition functions in a real-world industrial tabular language model use case for sub-cell named entity recognition. Our results show that cell-level acquisition functions with built-in diversity can significantly reduce the labeling effort, while enforced table diversity is detrimental. We further see open fundamental questions concerning computational efficiency and the perspective of human annotators.

</details>


### [14] [Table-To-Text generation and pre-training with TabT5](https://arxiv.org/abs/2210.09162)
*Ewa Andrejczuk,Julian Martin Eisenschlos,Francesco Piccinno,Syrine Krichene,Yasemin Altun*

Main category: cs.CL

TL;DR: TABT5是一个新的编码器-解码器模型，通过结合解码器和表格特定嵌入，在表格理解任务上，特别是公式预测、问答和数据到文本生成方面，取得了最先进的成果，超越了传统的仅编码器模型。


<details>
  <summary>Details</summary>
Motivation: 现有的仅编码器Transformer模型在表格理解任务中存在局限性，仅限于分类任务（如单元格选择或蕴涵检测）。

Method: 本文提出了TABT5，一个基于表格和文本输入生成自然语言文本的编码器-解码器模型。它通过引入解码器组件，并利用表格特定的嵌入和预训练来处理输入结构，克服了仅编码器模型的限制。

Result: TABT5在多个领域取得了新的最先进结果，包括电子表格公式预测的序列准确率提高了15%，问答的序列准确率提高了2.5%，以及数据到文本生成的BLEU值提高了2.5%。

Conclusion: TABT5通过克服仅编码器模型的限制，并在多项表格理解任务中取得了显著的性能提升，证明了其在生成自然语言文本方面的有效性。

Abstract: Encoder-only transformer models have been successfully applied to different table understanding tasks, as in TAPAS (Herzig et al., 2020). A major limitation of these architectures is that they are constrained to classification-like tasks such as cell selection or entailment detection. We present TABT5, an encoder-decoder model that generates natural language text based on tables and textual inputs. TABT5 overcomes the encoder-only limitation by incorporating a decoder component and leverages the input structure with table specific embeddings and pre-training. TABT5 achieves new state-of-the-art results on several domains, including spreadsheet formula prediction with a 15% increase in sequence accuracy, QA with a 2.5% increase in sequence accuracy and data-to-text generation with a 2.5% increase in BLEU.

</details>


### [15] [Table Pre-training: A Survey on Model Architectures, Pre-training Objectives, and Downstream Tasks](https://arxiv.org/abs/2201.09745)
*Haoyu Dong,Zhoujun Cheng,Xinyi He,Mengyu Zhou,Anda Zhou,Fan Zhou,Ao Liu,Shi Han,Dongmei Zhang*

Main category: cs.CL

TL;DR: 这篇综述旨在全面回顾表格预训练的不同模型设计、预训练目标和下游任务，并分享对现有挑战和未来机遇的看法和展望。


<details>
  <summary>Details</summary>
Motivation: 由于网络页面、电子表格、PDF等多种文档类型中可以轻松收集到大量表格，并且表格预训练框架在文本和图像成功后也取得了显著进展，在表格问答、表格类型识别等各种任务上达到了最新水平。

Method: 本文通过对表格预训练的不同模型设计、预训练目标和下游任务进行全面的综述。

Result: 表格预训练框架在表格问答、表格类型识别、列关系分类、表格搜索、公式预测等各种任务上取得了最新的SOTA表现，并探索了各种预训练目标（如去噪单元格值、预测数值关系、隐式执行SQL）和表格语言模型（特别是带有特殊设计注意力机制的）。

Conclusion: 本文分享了对现有挑战和未来机遇的思考和展望，以期促进表格预训练领域的发展。

Abstract: Since a vast number of tables can be easily collected from web pages, spreadsheets, PDFs, and various other document types, a flurry of table pre-training frameworks have been proposed following the success of text and images, and they have achieved new state-of-the-arts on various tasks such as table question answering, table type recognition, column relation classification, table search, formula prediction, etc. To fully use the supervision signals in unlabeled tables, a variety of pre-training objectives have been designed and evaluated, for example, denoising cell values, predicting numerical relationships, and implicitly executing SQLs. And to best leverage the characteristics of (semi-)structured tables, various tabular language models, particularly with specially-designed attention mechanisms, have been explored. Since tables usually appear and interact with free-form text, table pre-training usually takes the form of table-text joint pre-training, which attracts significant research interests from multiple domains. This survey aims to provide a comprehensive review of different model designs, pre-training objectives, and downstream tasks for table pre-training, and we further share our thoughts and vision on existing challenges and future opportunities.

</details>


### [16] [TableQuery: Querying tabular data with natural language](https://arxiv.org/abs/2202.00454)
*Abhijith Neil Abraham,Fariz Rahman,Damanpreet Kaur*

Main category: cs.CL

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 503 - [{'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: This paper presents TableQuery, a novel tool for querying tabular data using deep learning models pre-trained to answer questions on free text. Existing deep learning methods for question answering on tabular data have various limitations, such as having to feed the entire table as input into a neural network model, making them unsuitable for most real-world applications. Since real-world data might contain millions of rows, it may not entirely fit into the memory. Moreover, data could be stored in live databases, which are updated in real-time, and it is impractical to serialize an entire database to a neural network-friendly format each time it is updated. In TableQuery, we use deep learning models pre-trained for question answering on free text to convert natural language queries to structured queries, which can be run against a database or a spreadsheet. This method eliminates the need for fitting the entire data into memory as well as serializing databases. Furthermore, deep learning models pre-trained for question answering on free text are readily available on platforms such as HuggingFace Model Hub (7). TableQuery does not require re-training; when a newly trained model for question answering with better performance is available, it can replace the existing model in TableQuery.

</details>


### [17] [The Impact of Text Presentation on Translator Performance](https://arxiv.org/abs/2011.05978)
*Samuel Läubli,Patrick Simianer,Joern Wuebker,Geza Kovacs,Rico Sennrich,Spence Green*

Main category: cs.CL

TL;DR: 该研究首次受控评估了计算机辅助翻译（CAT）工具的设计选择对译者表现的影响，发现分句呈现和上下布局有助于提高文本复现和句内错误识别的速度，而修订任务中非分段文本能提高准确性和效率。研究结果对CAT工具的设计实践有直接指导意义。


<details>
  <summary>Details</summary>
Motivation: 评估广泛使用的计算机辅助翻译（CAT）工具的设计选择（如文档分段和布局方式）对译者性能（速度和准确性）的影响，这是首次进行此类受控评估。

Method: 通过受控实验评估，测量译者在三种文本处理任务中的速度和准确性。比较了分句呈现与非分段文本，以及源文本和目标文本的上下排列与并排排列。

Result: 分句呈现能提高文本复现和句内错误识别的速度。源文本和目标文本的上下排列比并排排列能更快地进行文本复现。然而，对于修订任务，非分段文本能带来最高的准确性和时间效率。

Conclusion: 研究结果对CAT工具的设计最佳实践具有直接指导意义。

Abstract: Widely used computer-aided translation (CAT) tools divide documents into segments such as sentences and arrange them in a side-by-side, spreadsheet-like view. We present the first controlled evaluation of these design choices on translator performance, measuring speed and accuracy in three experimental text processing tasks. We find significant evidence that sentence-by-sentence presentation enables faster text reproduction and within-sentence error identification compared to unsegmented text, and that a top-and-bottom arrangement of source and target sentences enables faster text reproduction compared to a side-by-side arrangement. For revision, on the other hand, our results suggest that presenting unsegmented text results in the highest accuracy and time efficiency. Our findings have direct implications for best practices in designing CAT tools.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [18] [Using a Catenary Trajectory to Reduce Wellbore Friction in Horizontal Extended Reach Drilling](https://arxiv.org/abs/2309.12317)
*Vu Nguyen*

Main category: cs.RO

TL;DR: 该项目旨在通过引入悬链线概念来减少钻井中的井筒摩擦，并将其与传统二维圆弧设计进行比较，以降低总成本。


<details>
  <summary>Details</summary>
Motivation: 井筒摩擦是钻井中最大的担忧之一，因为它与总成本相关。悬链线概念被引入以减少井筒摩擦，但它需要详细的分析，本项目旨在填补这一空白。

Method: 本项目通过案例研究，比较了悬链线轨迹设计和传统二维圆弧设计的效果。悬链线形状使得钻杆在井筒内自由悬挂，旨在实现井眼与钻杆之间无接触，从而消除摩擦。悬链线轨迹和二维圆弧设计的计算程序可在MS Excel电子表格中找到。

Result: 通过悬链线轨迹设计，扭矩和阻力应该被最小化。MS Excel电子表格提供了一种易于使用且可靠的方法，用于设计延伸井的悬链线井筒轨迹。

Conclusion: 悬链线轨迹设计是一种有效减少井筒摩擦、扭矩和阻力的方法，特别是对于大位移井，且提供了实用的设计工具。

Abstract: Wellbore friction is one of the biggest concerns when drilling due to its relation to the total cost. The catenary concept was introduced to reduce wellbore friction, but it requires detailed analyses. This project would fill this gap. A catenary shape is simply the natural shape of a rope, chain, or drill string. The drill string will then hang freely inside the wellbore. Perfectly, there should be no contact between the hole and the string, and thus no friction. Torque and drag should be minimized this way. A case study is introduced to examine the outcome between Catenary Trajectory Design and traditional 2D Arc design. The calculation procedure of Catenary Trajectory and 2D Arc Design can be found in an MS Excel spreadsheet which is easy to use and reliable for designing catenary well trajectories for extended-reach wells.

</details>


### [19] [Hazard Analysis of Collaborative Automation Systems: A Two-layer Approach based on Supervisory Control and Simulation](https://arxiv.org/abs/2209.12560)
*Tom P. Huck,Yuvaraj Selvaraj,Constantin Cronrath,Christoph Ledermann,Martin Fabian,Bengt Lennartson,Torsten Kröger*

Main category: cs.RO

TL;DR: 该论文提出了一种双层模型化危险分析方法，结合形式化方法（监督控制理论）和仿真，用于安全关键系统，并在人机协作系统上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 当前危险分析方法在系统复杂性增加的情况下变得不适用，基于测试的分析成本高昂或存在危险。现有的模型化方法各有优缺点。

Method: 提出双层方法：第一层使用监督控制理论从形式化模型中综合不安全行为；第二层将这些不安全行为输入仿真模型进行详细分析，并应用领域特定的风险指标。

Result: 该方法结合了形式化方法的穷尽性分析和仿真方法的详细分析的优点，并在工业人机协作系统上展示了其益处。

Conclusion: 该双层方法为安全关键系统的危险分析提供了一个通用且全面的解决方案，通过结合形式化方法和仿真，提供了详尽而深入的分析。

Abstract: Safety critical systems are typically subjected to hazard analysis before commissioning to identify and analyse potentially hazardous system states that may arise during operation. Currently, hazard analysis is mainly based on human reasoning, past experiences, and simple tools such as checklists and spreadsheets. Increasing system complexity makes such approaches decreasingly suitable. Furthermore, testing-based hazard analysis is often not suitable due to high costs or dangers of physical faults. A remedy for this are model-based hazard analysis methods, which either rely on formal models or on simulation models, each with their own benefits and drawbacks. This paper proposes a two-layer approach that combines the benefits of exhaustive analysis using formal methods with detailed analysis using simulation. Unsafe behaviours that lead to unsafe states are first synthesised from a formal model of the system using Supervisory Control Theory. The result is then input to the simulation where detailed analyses using domain-specific risk metrics are performed. Though the presented approach is generally applicable, this paper demonstrates the benefits of the approach on an industrial human-robot collaboration system.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [20] [Hillview: A trillion-cell spreadsheet for big data](https://arxiv.org/abs/1907.04827)
*Mihai Budiu,Parikshit Gopalan,Lalith Suresh,Udi Wieder,Han Kruiger,Marcos K. Aguilera*

Main category: cs.DC

TL;DR: Hillview是一个分布式电子表格系统，用于浏览单机无法处理的超大数据集。它通过引入可视化草图（vizketches）来提供高度交互性和响应式的数据探索，结合了数据概括算法和高效渲染技术。该系统能够处理比现有竞争系统大得多的数据集。


<details>
  <summary>Details</summary>
Motivation: 处理单机无法应对的超大数据集，并提供高度交互性，使数据分析师能够快速探索多维度信息并随意切换可视化。

Method: Hillview引入了可视化草图（vizketches），这是一种生成紧凑数据可视化的简单方法。Vizketches结合了数据概括的算法技术和高效渲染的计算机图形学原理。它们通过并行计算、减少通信、提供渐进式可视化和提供精确的准确性保证来实现电子表格的扩展。

Result: 使用运行在八台服务器上的Hillview，可以导航和可视化数百亿行和数万亿个单元格的数据集，这远远超出了现有竞争系统已公布的能力。

Conclusion: Hillview及其vizketches有效地扩展了分布式电子表格，以处理数百亿行和数万亿个单元格的数据，在交互式管理超大数据集方面优于现有系统。

Abstract: Hillview is a distributed spreadsheet for browsing very large datasets that cannot be handled by a single machine. As a spreadsheet, Hillview provides a high degree of interactivity that permits data analysts to explore information quickly along many dimensions while switching visualizations on a whim. To provide the required responsiveness, Hillview introduces visualization sketches, or vizketches, as a simple idea to produce compact data visualizations. Vizketches combine algorithmic techniques for data summarization with computer graphics principles for efficient rendering. While simple, vizketches are effective at scaling the spreadsheet by parallelizing computation, reducing communication, providing progressive visualizations, and offering precise accuracy guarantees. Using Hillview running on eight servers, we can navigate and visualize datasets of tens of billions of rows and trillions of cells, much beyond the published capabilities of competing systems.

</details>


### [21] [Streaming 1.9 Billion Hypersparse Network Updates per Second with D4M](https://arxiv.org/abs/1907.04217)
*Jeremy Kepner,Vijay Gadepally,Lauren Milechin,Siddharth Samsi,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Michael Houle,Michael Jones,Anne Klein,Peter Michaleas,Julie Mullen,Andrew Prout,Antonio Rosa,Charles Yee,Albert Reuther*

Main category: cs.DC

TL;DR: D4M库在流式更新时面临内存压力，本文提出并优化了分层关联数组（hierarchical associative arrays），显著提高了更新速率，单实例可达40,000次/秒，集群可达1,900,000,000次/秒，从而能处理超大型流式网络数据。


<details>
  <summary>Details</summary>
Motivation: D4M关联数组的流式更新对内存层级造成巨大压力，限制了其处理大规模流式网络数据的能力。

Method: 设计并优化了分层关联数组的实现，通过控制层级中每个条目的数量来减少内存压力并提高更新速率，且参数易于调整以达到最佳性能。

Result: 单个实例的分层数组更新速率超过40,000次/秒。在MIT SuperCloud上，34,000个分层D4M关联数组实例在1,100个服务器节点上实现了每秒1,900,000,000次更新的持续速率。

Conclusion: 这种能力使得MIT SuperCloud能够分析极其庞大的流式网络数据集。

Abstract: The Dynamic Distributed Dimensional Data Model (D4M) library implements associative arrays in a variety of languages (Python, Julia, and Matlab/Octave) and provides a lightweight in-memory database implementation of hypersparse arrays that are ideal for analyzing many types of network data. D4M relies on associative arrays which combine properties of spreadsheets, databases, matrices, graphs, and networks, while providing rigorous mathematical guarantees, such as linearity. Streaming updates of D4M associative arrays put enormous pressure on the memory hierarchy. This work describes the design and performance optimization of an implementation of hierarchical associative arrays that reduces memory pressure and dramatically increases the update rate into an associative array. The parameters of hierarchical associative arrays rely on controlling the number of entries in each level in the hierarchy before an update is cascaded. The parameters are easily tunable to achieve optimal performance for a variety of applications. Hierarchical arrays achieve over 40,000 updates per second in a single instance. Scaling to 34,000 instances of hierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of 1,900,000,000 updates per second. This capability allows the MIT SuperCloud to analyze extremely large streaming network data sets.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [22] [Data Encryption Battlefield: A Deep Dive into the Dynamic Confrontations in Ransomware Attacks](https://arxiv.org/abs/2504.20681)
*Arash Mahboubi,Hamed Aboutorab,Seyit Camtepe,Hang Thanh Bui,Khanh Luong,Keyvan Ansari,Shenlu Wang,Bazara Barry*

Main category: cs.CR

TL;DR: 本研究探讨了在线增量机器学习算法在预测文件加密活动方面的应用，以应对不断演变的网络勒索软件威胁，并评估了不同机器学习分类器在检测各种加密策略方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 网络勒索软件对网络安全构成重大挑战，攻击者利用复杂的加密方法（如Base64编码的熵减和部分/间歇性加密）来逃避传统检测。本研究旨在探索对抗这些不断发展加密策略的先进对策。

Method: 本研究利用一个包含32.6 GB数据（11,928个文件，包括多种格式）的广泛数据集，这些文件由75个不同的勒索软件家族加密。研究调查了在线增量机器学习算法的应用，旨在预测文件加密活动。

Result: 霍夫丁树算法在增量学习能力方面表现出色，特别是在检测传统和AES-Base64加密方法方面。而具有热启动功能的随机森林分类器则擅长识别间歇性加密方法。

Conclusion: 为了对抗复杂的勒索软件策略，需要定制化的机器学习解决方案。

Abstract: In the rapidly evolving landscape of cybersecurity threats, ransomware represents a significant challenge. Attackers increasingly employ sophisticated encryption methods, such as entropy reduction through Base64 encoding, and partial or intermittent encryption to evade traditional detection methods. This study explores the dynamic battle between adversaries who continuously refine encryption strategies and defenders developing advanced countermeasures to protect vulnerable data. We investigate the application of online incremental machine learning algorithms designed to predict file encryption activities despite adversaries evolving obfuscation techniques. Our analysis utilizes an extensive dataset of 32.6 GB, comprising 11,928 files across multiple formats, including Microsoft Word documents (doc), PowerPoint presentations (ppt), Excel spreadsheets (xlsx), image formats (jpg, jpeg, png, tif, gif), PDFs (pdf), audio (mp3), and video (mp4) files. These files were encrypted by 75 distinct ransomware families, facilitating a robust empirical evaluation of machine learning classifiers effectiveness against diverse encryption tactics. Results highlight the Hoeffding Tree algorithms superior incremental learning capability, particularly effective in detecting traditional and AES-Base64 encryption methods employed to lower entropy. Conversely, the Random Forest classifier with warm-start functionality excels at identifying intermittent encryption methods, demonstrating the necessity of tailored machine learning solutions to counter sophisticated ransomware strategies.

</details>


### [23] [JUBILEE: Secure Debt Relief and Forgiveness](https://arxiv.org/abs/2109.07267)
*David Cerezo Sánchez*

Main category: cs.CR

TL;DR: JUBILEE是一种安全的计算机制，用于无摩擦地实现债务减免和豁免，无需信任第三方，通过激励各方真实披露私人信息，从而实现更和谐的债务解决方案。它在所有先前方法的基础上进行了改进，提供了最优机制，并首次通过安全计算技术实现了更高的预期利润和成功概率。


<details>
  <summary>Details</summary>
Motivation: 当前债务解决方案存在不足，需要一种更有效、更公平且能激励各方真实披露信息的新机制，以实现和谐的债务和解并提高成功率。

Method: JUBILEE通过引入安全计算技术来处理债务减免和豁免。它提供了一种个体理性、激励兼容、真实/策略证明、事后高效且最优的机制。具体实现包括“安全电子表格”和基于区块链（Raziel智能合约与Pravuil共识）的实现。

Result: JUBILEE是一种个体理性、激励兼容、真实/策略证明、事后高效且最优的私人信息债务减免和豁免机制。通过引入安全计算，它首次实现了“债务人的祝福”，即与不使用安全计算相比，债务解决方案具有更高的预期利润和更高的成功概率。

Conclusion: JUBILEE通过安全计算技术，提供了一种优于现有方法的、无需信任第三方的债务减免和豁免机制，显著提高了债务解决方案的效率、公平性和成功率。

Abstract: JUBILEE is a securely computed mechanism for debt relief and forgiveness in a frictionless manner without involving trusted third parties, leading to more harmonious debt settlements by incentivising the parties to truthfully reveal their private information. JUBILEE improves over all previous methods:
  - individually rational, incentive-compatible, truthful/strategy-proof, ex-post efficient, optimal mechanism for debt relief and forgiveness with private information
  - by the novel introduction of secure computation techniques to debt relief, the "blessing of the debtor" is hereby granted for the first time: debt settlements with higher expected profits and a higher probability of success than without using secure computation
  A simple and practical implementation is included for "The Secure Spreadsheet". Another implementation is realised using Raziel smart contracts on a blockchain with Pravuil consensus.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [24] [Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE](https://arxiv.org/abs/2506.17330)
*Simon Thorne*

Main category: cs.SE

TL;DR: 大型语言模型在处理复杂电子表格任务时表现不佳，需要更强的逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在电子表格相关任务中的有效性尚未得到充分探索，需要一个全面的基准框架来评估其性能。

Method: 引入了一个全面的基准框架（FLARE），用于评估领先的大型语言模型在执行电子表格函数、公式生成和数据操作任务方面的性能。

Result: 大型语言模型在直接任务中表现熟练，但在复杂的、多步骤操作中经常出错，产生看似合理但实际上不正确的输出。

Conclusion: 目前的LLM在处理需要精确逻辑推理的电子表格任务时存在局限性，需要将符号推理能力整合到LLM架构中。为此，引入了FLARE基准。

Abstract: Large Language Models (LLMs) have demonstrated some significant capabilities across various domains; however, their effectiveness in spreadsheet related tasks remains underexplored. This study introduces a foundation for a comprehensive benchmark framework to evaluate the performance of leading LLMs in executing spreadsheet functions, formula generation and data manipulation tasks. The benchmark encompasses tasks ranging from basic formula creation to complex, real world spreadsheet scenarios. Our findings reveal that while LLMs exhibit proficiency in straightforward tasks, they often falter in complex, multi step operations, frequently producing plausible yet incorrect outputs. These results underscore the limitations of current LLMs in handling spreadsheet tasks that require precise logical reasoning and highlight the need for integrating symbolic reasoning capabilities into LLM architectures. To support this, we introduce FLARE (Formula Logic, Auditing, Reasoning and Evaluation) a new benchmark for evaluating LLM performance on real-world spreadsheet logic, auditing, and reasoning tasks.

</details>


### [25] [Overcoming Vision Language Model Challenges in Diagram Understanding: A Proof-of-Concept with XML-Driven Large Language Models Solutions](https://arxiv.org/abs/2502.04389)
*Shue Shiinoki,Ryo Koshihara,Hayato Motegi,Masumi Morishige*

Main category: cs.SE

TL;DR: 本研究提出了一种文本驱动的方法来理解图表，通过从可编辑的源文件（如xlsx、pptx或docx）中提取文本元数据，并使用大型语言模型（LLMs）进行分析，从而绕过视觉语言模型（VLMs）的视觉识别限制，在需要详细理解图表结构的问题上取得了更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型（VLMs）在图像理解任务上取得了进展，但在业务文档中，准确识别和提取图表中描绘的结构和关系仍然面临挑战。

Method: 该研究提出了一种文本驱动的方法，不依赖于VLMs的视觉识别能力。它利用可编辑的源文件（如xlsx、pptx或docx），将图表元素作为文本元数据保留。通过从xlsx文件提取图表信息并将其转换为LLM的文本输入，进行关系分析并回答业务问题。

Result: 与基于VLM的方法相比，所提出的文本驱动框架在需要详细理解图表结构的问题上产生了更准确的答案。该方法不仅限于.xlsx文件，还可以扩展到其他带有源文件的文档（如Office pptx和docx格式）。

Conclusion: 通过从原始源文件直接进行文本提取，规避VLM的限制是可行的。这种方法通过LLMs实现了强大的图表理解，为现实世界业务场景中提高工作流程效率和信息分析提供了有前景的途径。

Abstract: Diagrams play a crucial role in visually conveying complex relationships and processes within business documentation. Despite recent advances in Vision-Language Models (VLMs) for various image understanding tasks, accurately identifying and extracting the structures and relationships depicted in diagrams continues to pose significant challenges. This study addresses these challenges by proposing a text-driven approach that bypasses reliance on VLMs' visual recognition capabilities. Instead, it utilizes the editable source files--such as xlsx, pptx or docx--where diagram elements (e.g., shapes, lines, annotations) are preserved as textual metadata. In our proof-of-concept, we extracted diagram information from xlsx-based system design documents and transformed the extracted shape data into textual input for Large Language Models (LLMs). This approach allowed the LLM to analyze relationships and generate responses to business-oriented questions without the bottleneck of image-based processing. Experimental comparisons with a VLM-based method demonstrated that the proposed text-driven framework yielded more accurate answers for questions requiring detailed comprehension of diagram structures.The results obtained in this study are not limited to the tested .xlsx files but can also be extended to diagrams in other documents with source files, such as Office pptx and docx formats. These findings highlight the feasibility of circumventing VLM constraints through direct textual extraction from original source files. By enabling robust diagram understanding through LLMs, our method offers a promising path toward enhanced workflow efficiency and information analysis in real-world business scenarios.

</details>


### [26] [On the Workflows and Smells of Leaderboard Operations (LBOps): An Exploratory Study of Foundation Model Leaderboards](https://arxiv.org/abs/2407.04065)
*Zhimin Zhao,Abdul Ali Bangash,Filipe Roseiro Côgo,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本研究旨在理解基础模型（FM）排行榜的运作方式并识别其潜在问题，以提高透明度并促进更有效的FM选择。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏基础模型评估和比较的标准化指南，导致FM排行榜的透明度受到威胁，并限制了利益相关者有效选择FM的能力。

Method: 从五个来源（GitHub、Hugging Face Spaces、Papers With Code、电子表格和独立平台）收集了1045个FM排行榜，通过检查文档并与排行榜操作者沟通，识别了五种独特的工作流模式并开发了领域模型，然后识别了八种独特的排行榜“异味”。

Result: 识别了五种不同的工作流模式，开发了一个捕捉这些工作流中关键组件及其交互的领域模型，并确定了八种独特的排行榜“异味”。

Conclusion: 通过减轻这些“异味”，软件工程团队可以提高当前排行榜操作实践的透明度、问责制和协作性，从而为FM的比较和选择建立一个更稳健和负责任的生态系统。

Abstract: Foundation models (FM), such as large language models (LLMs), which are large-scale machine learning (ML) models, have demonstrated remarkable adaptability in various downstream software engineering (SE) tasks, such as code completion, code understanding, and software development. As a result, FM leaderboards have become essential tools for SE teams to compare and select the best third-party FMs for their specific products and purposes. However, the lack of standardized guidelines for FM evaluation and comparison threatens the transparency of FM leaderboards and limits stakeholders' ability to perform effective FM selection. As a first step towards addressing this challenge, our research focuses on understanding how these FM leaderboards operate in real-world scenarios ("leaderboard operations") and identifying potential pitfalls and areas for improvement ("leaderboard smells"). In this regard, we collect up to 1,045 FM leaderboards from five different sources: GitHub, Hugging Face Spaces, Papers With Code, spreadsheet and independent platform, to examine their documentation and engage in direct communication with leaderboard operators to understand their workflows. Through card sorting and negotiated agreement, we identify five distinct workflow patterns and develop a domain model that captures the key components and their interactions within these workflows. We then identify eight unique types of leaderboard smells in LBOps. By mitigating these smells, SE teams can improve transparency, accountability, and collaboration in current LBOps practices, fostering a more robust and responsible ecosystem for FM comparison and selection.

</details>


### [27] [MaxMind: A Memory Loop Network to Enhance Software Productivity based on Large Language Models](https://arxiv.org/abs/2408.03841)
*Yuchen Dong,XiaoXiang Fang,Yuchen Hu,Renshuang Jiang,Zhe Jiang*

Main category: cs.SE

TL;DR: 本文提出了一种名为MaxMind的模型，通过内存循环网络和增强的RAG机制，改进了大型语言模型在自动化软件操作和工具生成（SOTG）中的表现，特别是通过任务记忆的积累和循环利用，显著提高了任务成功率和执行效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动化软件操作和工具生成（SOTG）方面的应用对软件生产力至关重要，但当前研究忽略了将实时任务经验转化为系统记忆以及区分现有知识价值的重要性。

Method: 通过将外部记忆模型演变为记忆循环网络，用于及时记忆和经验参考；通过知识精度分割增强RAG机制，实现基于价值区分的记忆利用；并据此设计了MaxMind模型。通过MaxMind4Sheet（电子表格处理系统）进行了验证。

Result: 与SheetCopilot的对比实验表明，任务记忆的积累和循环利用使任务成功率稳步提高，每轮改进约3%-6%；记忆循环可将系统任务执行效率提高高达25%；通过记忆传输解决了大型语言模型在处理专业任务时面临的再训练问题。

Conclusion: MaxMind在增强大型语言模型系统在SOTG方面的能力和生产力方面具有巨大潜力。

Abstract: The application of large language models to facilitate automated software operations and tool generation (SOTG), thus augmenting software productivity, mirrors the early stages of human evolution when the ability to create and use tools accelerated the progress of civilization. These complex tasks require AI to continuously summarize and improve. Current research often overlooks the importance of converting real-time task experiences into system memory and differentiating the value of existing knowledge for future reference. This paper addresses these issues by evolving external memory models into Memory-Loop Networks for timely memorization and experience referencing. We also enhance a RAG mechanism with knowledge precision segmentation to utilize memory based on value differentiation, and design the MaxMind model for SOTG accordingly.To demonstrate our approach, we developed MaxMind4Sheet, an electronic spreadsheet processing system aligned with the MaxMind philosophy. Comparative experiments with SheetCopilot have demonstrated that the accumulation and recycling of task memories lead to a steady enhancement in task success rate, with an improvement rate of approximately 3%-6% per round in this implementation example. Note that as the memories continue to grow, this cumulative improvement may be substantial. The inclusion of memory recycling can also boost the system's task execution efficiency by up to 25%, and it can address the retraining issue faced by LLMs when handling specialized tasks through memories transfer.These suggest that MaxMind has significant potential to enhance the capabilities and productivity of LLM systems in SOTG.

</details>


### [28] [Will Dynamic Arrays finally change the way Models are built?](https://arxiv.org/abs/2006.14706)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: 本文探讨了在专业开发环境中采用Excel动态数组如何提高电子表格的完整性、减少错误，并取代传统技术，使其适用于严肃的分析。


<details>
  <summary>Details</summary>
Motivation: 电子表格虽然普及但容易出错。之前的研究使用名称来提高透明度，但CSE数组公式操作繁琐。Excel新引入的动态数组为提高解决方案完整性和减少错误提供了新机遇。

Method: 本文旨在论证在更专业的开发环境中采用动态数组，可以取代传统技术来确保解决方案的完整性。

Result: 采用完全动态模型的主要优势是它们需要较少的人工干预来保持更新，因此有潜力减少伴随的错误和风险。

Conclusion: 动态数组在专业开发环境中，能够提高电子表格的完整性，减少人工干预以及错误和风险，使其更适合严肃的分析或建模任务。

Abstract: Spreadsheets offer a supremely successful and intuitive means of processing and exchanging numerical content. Its intuitive ad-hoc nature makes it hugely popular for use in diverse areas including business and engineering, yet these very same characteristics make it extraordinarily error-prone; many would question whether it is suitable for serious analysis or modelling tasks. A previous EuSpRIG paper examined the role of Names in increasing solution transparency and providing a readable notation to forge links with the problem domain. Extensive use was made of CSE array formulas, but it is acknowledged that their use makes spreadsheet development a distinctly cumbersome task. Since that time, the new dynamic arrays have been introduced and array calculation is now the default mode of operation for Excel. This paper examines the thesis that their adoption within a more professional development environment could replace traditional techniques where solution integrity is important. A major advantage of fully dynamic models is that they require less manual intervention to keep them updated and so have the potential to reduce the attendant errors and risk.

</details>


### [29] [A Structured Approach to the development of Solutions in Excel](https://arxiv.org/abs/1704.01142)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: 该论文讨论了Excel解决方案中缺乏超越单单元格公式的结构，这可能导致错误，并提出使用非常规技术来创建结构化的解决方案。


<details>
  <summary>Details</summary>
Motivation: 电子表格普及了数字处理，但缺乏允许其在不增加错误的情况下进行扩展的结构元素，而传统代码中存在这种结构。

Method: 本文考虑使用“有争议或鲜为人知的技术”来创建“连贯的解决方案策略”，其中问题通过“类似于编程语言步骤的一系列公式”来解决。

Result: 摘要中没有明确说明结果，因为它“考虑使用”这些技术，这暗示它是一个提议或探索，而不是报告的结果。

Conclusion: 该论文提出了一种将类似于编程语言的结构元素引入电子表格以解决问题的方法。

Abstract: Spreadsheets offer a supremely successful democratisation platform, placing the manipulation and presentation of numbers within the grasp of users that have little or no mathematical expertise or IT experience. What appears to be almost completely lacking within a "normal" solution built using Excel default settings is the deployment of any structure that extends beyond a single-cell formula. The structural elements that allow conventional code to scale without escalating errors appear to be absent. This paper considers the use of controversial or lesser-used techniques to create a coherent solution strategy in which the problem is solved by a sequence of formulas resembling the steps of a programmed language.

</details>


### [30] [Spreadsheet-based Configuration of Families of Real-Time Specifications](https://arxiv.org/abs/2310.20395)
*José Proença,David Pereira,Giann Spilere Nandi,Sina Borrami,Jonas Melchert*

Main category: cs.SE

TL;DR: 实时系统模型检测因细节和状态爆炸问题而复杂。本研究利用模型和需求的变异性，通过与阿尔斯通的合作，使用Excel表格配置形式规范的变异性，并由原型工具自动处理以生成实例并运行模型检测器。工作扩展了之前的方法，通过分析有效特征组合来保持基于电子表格的简单界面。


<details>
  <summary>Details</summary>
Motivation: 实时系统模型检测因需要权衡细节以避免状态爆炸而复杂。本研究旨在通过利用形式模型和被检查需求的变异性，简化实时规范变体的模型检测。

Method: 利用形式模型和需求的变异性。将形式规范的变异性配置在MS Excel电子表格中，并通过原型工具自动处理这些表格，以生成实例并运行模型检测器。本研究通过利用对特征有效组合的分析来扩展之前的工作，同时保持基于电子表格的简洁界面。

Result: 该研究成果是一个原型工具和方法，它来源于学术界与铁路公司阿尔斯通在VALU3S欧洲项目背景下的合作，旨在促进实时规范变体的模型检测，并已有一个具体的用例。

Conclusion: 本研究提出了对其先前工作的扩展，即通过利用对特征有效组合的分析，来促进实时系统变体的模型检测，同时保持了基于电子表格界面的简单性。

Abstract: Model checking real-time systems is complex, and requires a careful trade-off between including enough detail to be useful and not too much detail to avoid state explosion. This work exploits variability of the formal model being analysed and the requirements being checked, to facilitate the model-checking of variations of real-time specifications.  This work results from the collaboration between academics and Alstom, a railway company with a concrete use-case, in the context of the VALU3S European project. The configuration of the variability of the formal specifications is described in MS Excel spreadsheets with a particular structure, making it easy to use also by developers. These spreadsheets are processed automatically by our prototype tool that generates instances and runs the model checker.  We propose the extension of our previous work by exploiting analysis over valid combination of features, while preserving the simplicity of a spreadsheet-based interface with the model checker.

</details>


### [31] [SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models](https://arxiv.org/abs/2305.19308)
*Hongxin Li,Jingran Su,Yuntao Chen,Qing Li,Zhaoxiang Zhang*

Main category: cs.SE

TL;DR: SheetCopilot是一个利用LLMs通过自然语言控制电子表格的智能体，它通过原子操作和状态机框架，在221个任务中取得了44.3%的完成率，显著优于代码生成基线。


<details>
  <summary>Details</summary>
Motivation: 计算机终端用户在处理电子表格等日常任务上耗费了大量时间，这些任务重复且易错，但多数用户缺乏自动化技能。大型语言模型（LLMs）的兴起使得通过自然语言指令控制软件成为可能。

Method: 提出SheetCopilot智能体，通过自然语言指令控制电子表格。设计了一套原子操作作为电子表格功能的抽象，并构建了一个基于状态机的任务规划框架，使LLMs能稳健地与电子表格交互。同时，策划了一个包含221个任务的代表性数据集，并建立了全自动评估流程。

Result: SheetCopilot在单次生成中正确完成了44.3%的任务，明显优于强大的代码生成基线。

Conclusion: SheetCopilot展示了LLMs通过自然语言控制电子表格的有效性和潜力，为自动化繁琐的电子表格任务提供了一个稳健的解决方案。

Abstract: Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill to automate these burdensome works. With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal. In this work, we propose a SheetCopilot agent that takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for LLMs to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of LLMs in software control tasks. Our SheetCopilot correctly completes 44.3\% of tasks for a single generation, outperforming the strong code generation baseline by a wide margin. Our project page:https://sheetcopilot.github.io/.

</details>


### [32] [Excel as a Turing-complete Functional Programming Environment](https://arxiv.org/abs/2309.00115)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: Excel在2018年引入动态数组后，电子表格的构建方式发生了巨大变化。本文探讨了如何用更接近编程的结构化方法取代传统的临时性用户实践，并讨论了Excel社区中出现的新趋势。


<details>
  <summary>Details</summary>
Motivation: Excel动态数组的引入彻底改变了电子表格的构建方式，使得传统的临时性方法可以被更类似于正式编程的方法所取代。本文旨在探讨这些变化和新兴趋势。

Method: 本文将展示如何用与正式编程更相似的激进方法取代传统电子表格的临时性终端用户实践，并将讨论Excel社区中正在出现的先驱性工作趋势。

Result: Excel的新功能允许用更接近编程的方法构建电子表格解决方案，并且Excel社区内正在出现新的趋势。然而，新功能在商业和工程社区中的采纳程度及其对风险的影响尚待观察。

Conclusion: Excel动态数组的引入标志着电子表格构建方式的重大转变，促进了向更结构化、类编程方法的过渡。尽管其全面影响和采纳程度尚不明朗，但新的发展趋势正在涌现。

Abstract: Since the calculation engine of Excel was the subject of a major upgrade to accommodate Dynamic Arrays in 2018 there has been a series of seismic changes to the art of building spreadsheet solutions. This paper will show the ad-hoc end user practices of traditional spreadsheets can be replaced by radically different approaches that have far more in common with formal programming. It is too early to guess the extent to which the new functionality will be adopted by the business and engineering communities and the impact that may have upon risk. Nevertheless, some trends are emerging from pioneering work within the Excel community which we will discuss here.

</details>


### [33] [A Use Case-Engineering Resources Taxonomy for Analytical Spreadsheet Models](https://arxiv.org/abs/2309.00104)
*Thomas A. Grossman,Vijay Mehrotra*

Main category: cs.SE

TL;DR: 本文提出了一种分析型电子表格模型的分类法，将之前的三个类型扩展到九个类型，区分了“分析解决方案”和“工业级分析型电子表格模型”，并提供了一个理解电子表格开发、错误、风险和演变的框架。


<details>
  <summary>Details</summary>
Motivation: 本文旨在考虑电子表格的用例和开发资源，对分析型电子表格模型进行分类，以扩展现有分类法并连接不同的研究文献。它还旨在帮助识别有用的开发指南，查看错误/风险，并理解电子表格随时间的变化。

Method: 本文将之前的三种类型分类法扩展到九种电子表格模型类型。它连接了不同的研究文献，以区分“分析解决方案”和“工业级分析型电子表格模型”。它探讨了每种类型的性质，提出了定义，将其与文献关联，并假设了它们的产生方式。

Result: 提出了一个包含九种类型的分析型电子表格模型分类法。该分类法有助于识别最有效的电子表格开发指南，为观察电子表格错误和风险提供了视角，并提供了一个理解电子表格如何随时间变化的结构。

Conclusion: 所开发的分类法有助于理解电子表格的开发、风险和演变，并开启了许多有趣的研究问题。

Abstract: This paper presents a taxonomy for analytical spreadsheet models. It considers both the use case that a spreadsheet is meant to serve, and the engineering resources devoted to its development. We extend a previous three-type taxonomy, to identify nine types of spreadsheet models, that encompass the many analytical spreadsheet models seen in the literature. We connect disparate research literature to distinguish between an "analytical solution" and an "industrial-quality analytical spreadsheet model". We explore the nature of each of the nine types, propose definitions for some, relate them to the literature, and hypothesize on how they might arise. The taxonomy aids in identifying where various spreadsheet development guidelines are most useful, provides a lens for viewing spreadsheet errors and risk, and offers a structure for understanding how spreadsheets change over time. This taxonomy opens the door to many interesting research questions, including refinements to itself.

</details>


### [34] [Experimenting with ChatGPT for Spreadsheet Formula Generation: Evidence of Risk in AI Generated Spreadsheets](https://arxiv.org/abs/2309.00095)
*Simon Thorne*

Main category: cs.SE

TL;DR: 本文探讨了ChatGPT在生成有效电子表格公式方面的能力，发现其在信息充足时表现良好，但在信息有限或问题复杂时准确性下降并可能出现“幻觉”。


<details>
  <summary>Details</summary>
Motivation: LLM（如ChatGPT）能够通过解释自然语言创建复杂的计算机程序，使编程变得普及。本文旨在探究ChatGPT生成电子表格公式并进行推理、推断和问题解决的能力。

Method: 本文通过一系列针对ChatGPT的实验，探索其在需要推断、推理和解决问题的场景中，生成有效电子表格公式和相关计算输出的能力。

Result: 实验结果显示，在特定情况下，ChatGPT能生成正确的电子表格公式，并展现出正确的推理、演绎和推断能力。然而，当信息有限、不确定或问题过于复杂时，ChatGPT的准确性及其推理、推断和演绎能力会下降，并可能导致错误陈述和“幻觉”。

Conclusion: 尽管ChatGPT在生成电子表格公式方面具有潜力，但在面对信息不足或复杂问题时，其准确性和可靠性会受到限制，这可能干扰电子表格公式的创建过程。

Abstract: Large Language Models (LLM) have become sophisticated enough that complex computer programs can be created through interpretation of plain English sentences and implemented in a variety of modern languages such as Python, Java Script, C++ and Spreadsheets. These tools are powerful and relatively accurate and therefore provide broad access to computer programming regardless of the background or knowledge of the individual using them. This paper presents a series of experiments with ChatGPT to explore the tool's ability to produce valid spreadsheet formulae and related computational outputs in situations where ChatGPT has to deduce, infer and problem solve the answer. The results show that in certain circumstances, ChatGPT can produce correct spreadsheet formulae with correct reasoning, deduction and inference. However, when information is limited, uncertain or the problem is too complex, the accuracy of ChatGPT breaks down as does its ability to reason, infer and deduce. This can also result in false statements and "hallucinations" that all subvert the process of creating spreadsheet formulae.

</details>


### [35] [Demonstration of CORNET: A System For Learning Spreadsheet Formatting Rules By Example](https://arxiv.org/abs/2308.07357)
*Mukul Singh,Jose Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Gust Verbruggen*

Main category: cs.SE

TL;DR: CORNET是一个系统，它能根据用户提供的示例，自动从电子表格中学习条件格式规则。


<details>
  <summary>Details</summary>
Motivation: 在电子表格软件中，用户需要手动编写条件格式规则，这是一项常用但繁琐的功能。

Method: CORNET受归纳程序合成启发，结合了基于半监督聚类和迭代决策树学习的符号规则枚举与神经网络排序器。

Result: CORNET可以根据用户提供的一两个格式化单元格作为示例，生成准确的格式化规则建议。系统以Microsoft Excel插件的形式进行演示。

Conclusion: CORNET自动化了条件格式规则的创建，简化了用户操作。

Abstract: Data management and analysis tasks are often carried out using spreadsheet software. A popular feature in most spreadsheet platforms is the ability to define data-dependent formatting rules. These rules can express actions such as "color red all entries in a column that are negative" or "bold all rows not containing error or failure." Unfortunately, users who want to exercise this functionality need to manually write these conditional formatting (CF) rules. We introduce CORNET, a system that automatically learns such conditional formatting rules from user examples. CORNET takes inspiration from inductive program synthesis and combines symbolic rule enumeration, based on semi-supervised clustering and iterative decision tree learning, with a neural ranker to produce accurate conditional formatting rules. In this demonstration, we show CORNET in action as a simple add-in to Microsoft Excel. After the user provides one or two formatted cells as examples, CORNET generates formatting rule suggestions for the user to apply to the spreadsheet.

</details>


### [36] [Excel Spreadsheet Analyzer](https://arxiv.org/abs/2211.06333)
*Amir Nassereldine,Patrick Chen,Jinjun Xiong*

Main category: cs.SE

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 503 - [{'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Spreadsheets are widely used in various fields to do large numerical analysis. While several companies have relied on spreadsheets for decades, data scientists are going in the direction of using scientific programming languages such as python to do their data analysis due to the support, community, and vast amount of libraries. While using python to analyze a company's spreadsheets, some information such as the formulas and dependencies of a cell are lost. We propose a tool that creates an abstract intermediate representation (AIR) of a spreadsheet. This representation facilitates the transfer from spreadsheets into scientific programming languages while preserving inter-dependency information about data. In addition to that, we build a python library on top of our tool to perform some data analysis in python.

</details>


### [37] [Exploring Spreadsheet Use and Practices in a Technologically Constrained Setting](https://arxiv.org/abs/2201.07696)
*Khwima Mckinley Mkamanga,Simon Thorne*

Main category: cs.SE

TL;DR: 本文探讨了电子表格对马拉维一家国有水务公司业务运营的影响，既强调了其带来的业务自动化益处，也指出了管理、技术和人为因素带来的高风险，并建议在政策和治理方面进行改进。


<details>
  <summary>Details</summary>
Motivation: 探索电子表格对马拉维某国有水务公司业务运营的影响，该公司是技术不发达国家半政府机构的典型代表。

Method: 研究侧重于电子表格的使用范围、生命周期、组织政策和治理。

Result: 研究结果表明，电子表格的普及为业务自动化提供了有利环境，但同时也凸显了与广泛使用电子表格相关的高风险（管理、技术和人为因素问题）。

Conclusion: 研究结论证实，在许多领域有很大的改进空间，例如实施全面的政策和法规来管理电子表格的开发流程和应用。

Abstract: This paper explores the impacts of spreadsheets on business operations in a water utility parastatal in Malawi, Sub-Saharan Africa. The organisation is a typical example of a semi-government body operating in a technologically underdeveloped country. The study focused on spreadsheet scope of use and life cycle as well as organisational policy and governance. The results will help define future spreadsheet usage by influencing new approaches for managing potential risks associated with spreadsheets in the organization. Generally, findings indicate that the proliferation of spreadsheets in the organization has provided an enabling environment for business automation. The paper also highlights management, technological and human factor issues contributing to high risks associated with the pervasive spreadsheet use. The conclusions drawn from the research confirms that there is ample room for improvement in many areas such as implementation of comprehensive policies and regulations governing spreadsheet development processes and adoption.

</details>


### [38] [Methodology for Assessing the State of the Practice for Domain X](https://arxiv.org/abs/2110.11575)
*Spencer Smith,Jacques Carette,Peter Michalski,Ao Dong,Olu Owojaiye*

Main category: cs.SE

TL;DR: 该论文提出了一种评估研究软件开发实践现状的方法，包括识别软件、收集数据、访谈开发者和分析结果等步骤。


<details>
  <summary>Details</summary>
Motivation: 为了改进研究软件的开发方法和工具，首先需要了解当前的实践现状。

Method: 开发了一种包含九个步骤的方法论：识别领域、识别候选软件包、筛选约30个软件包、收集源代码和文档、收集代码库相关数据、填写测量模板（108个问题评估9个质量）、访谈开发者（20个问题）、使用层次分析法（AHP）对软件进行排名，并分析数据。整个过程需要领域专家的参与。

Result: 使用该方法论、电子表格模板和AHP工具，估计完成一个特定领域的评估需要173个人时。

Conclusion: 该论文提供了一个全面的方法论，用于评估研究软件开发实践，以识别需要改进的领域。

Abstract: To improve software development methods and tools for research software, we first need to understand the current state of the practice. Therefore, we have developed a methodology for assessing the state of the software development practices for a given research software domain. For each domain we wish to answer questions such as: i) What artifacts (documents, code, test cases, etc.) are present? ii) What tools are used? iii) What principles, process and methodologies are used? iv) What are the pain points for developers? v) What actions are used to improve qualities like maintainability and reproducibility? To answer these questions, our methodology prescribes the following steps: i) Identify the domain; ii) Identify a list of candidate software packages; iii) Filter the list to a length of about 30 packages; iv) Gather source code and documentation for each package; v) Collect repository related data on each software package, like number of stars, number of open issues, number of lines of code; vi) Fill in the measurement template (the template consists of 108 questions to assess 9 qualities (including the qualities of installability, usability and visibility)); vii) Interview developers (the interview consists of 20 questions and takes about an hour); viii) Rank the software using the Analytic Hierarchy Process (AHP); and, ix) Analyze the data to answer the questions posed above. A domain expert should be engaged throughout the process, to ensure that implicit information about the domain is properly represented and to assist with conducting an analysis of the commonalities and variabilities between the 30 selected packages. Using our methodology, spreadsheet templates and AHP tool, we estimate (based on our experience with using the process) the time to complete an assessment for a given domain at 173 person hours.

</details>


### [39] [Agile Elicitation of Scalability Requirements for Open Systems: A Case Study](https://arxiv.org/abs/2108.00567)
*Gunnar Brataas,Antonio Martini,Geir Kjetil Hanssen,Georg Ræder*

Main category: cs.SE

TL;DR: 本文介绍了一个轻量级模型 ScrumScale，用于在敏捷软件开发中获取可扩展性需求，并在一个开放银行案例研究中得到了验证。


<details>
  <summary>Details</summary>
Motivation: 在敏捷软件开发中获取可扩展性需求是复杂且研究不足的。

Method: 本文提出了 ScrumScale 模型，一个基于设计科学研究和协调理论的简单电子表格，并在一个开放银行案例研究中进行了应用。

Result: 在开放银行案例研究中，ScrumScale 模型提供了一种系统化的方法来生成可扩展性需求，并在与利益相关者的对话中带来了显著优势。

Conclusion: ScrumScale 模型有助于系统地获取可扩展性需求，并改善利益相关者之间的沟通。

Abstract: Eliciting scalability requirements during agile software development is complicated and poorly described in previous research. This article presents a lightweight artifact for eliciting scalability requirements during agile software development: the ScrumScale model. The ScrumScale model is a simple spreadsheet. The scalability concepts underlying the ScrumScale model are clarified in this design science research, which also utilizes coordination theory. This paper describes the open banking case study, where a legacy banking system becomes open. This challenges the scalability of this legacy system. The first step in understanding this challenge is to elicit the new scalability requirements. In the open banking case study, key stakeholders from TietoEVRY spent 55 hours eliciting TietoEVRY's open banking project's scalability requirements. According to TietoEVRY, the ScrumScale model provided a systematic way of producing scalability requirements. For TietoEVRY, the scalability concepts behind the ScrumScale model also offered significant advantages in dialogues with other stakeholders.

</details>


### [40] [SpreadsheetCoder: Formula Prediction from Semi-structured Context](https://arxiv.org/abs/2106.15339)
*Xinyun Chen,Petros Maniatis,Rishabh Singh,Charles Sutton,Hanjun Dai,Max Lin,Denny Zhou*

Main category: cs.SE

TL;DR: 该论文提出了SpreadsheetCoder，一个基于BERT的模型，它利用表格上下文（包括表头和半结构化表格数据）来合成电子表格公式，相比现有方法显著提高了准确性并能辅助更多用户。


<details>
  <summary>Details</summary>
Motivation: 以往的电子表格公式预测方法未能充分利用真实世界电子表格的丰富上下文，例如表格的组织方式、行与列的依赖关系以及表头信息。

Method: 我们提出了SpreadsheetCoder，这是一个基于BERT的模型架构，用于以基于行和基于列的格式表示表格上下文。我们在一个大型电子表格数据集上训练了该模型。

Result: SpreadsheetCoder的top-1预测准确率达到42.51%，相较于不使用丰富表格上下文的基线模型有显著提升。与基于规则的系统相比，SpreadsheetCoder在Google Sheets上帮助用户编写公式的比例提高了82%。

Conclusion: SpreadsheetCoder首次提出了利用表格上下文（包括表头和表格数据）合成电子表格公式的方法，显著优于现有方法，并能有效辅助用户。

Abstract: Spreadsheet formula prediction has been an important program synthesis problem with many real-world applications. Previous works typically utilize input-output examples as the specification for spreadsheet formula synthesis, where each input-output pair simulates a separate row in the spreadsheet. However, this formulation does not fully capture the rich context in real-world spreadsheets. First, spreadsheet data entries are organized as tables, thus rows and columns are not necessarily independent from each other. In addition, many spreadsheet tables include headers, which provide high-level descriptions of the cell data. However, previous synthesis approaches do not consider headers as part of the specification. In this work, we present the first approach for synthesizing spreadsheet formulas from tabular context, which includes both headers and semi-structured tabular data. In particular, we propose SpreadsheetCoder, a BERT-based model architecture to represent the tabular context in both row-based and column-based formats. We train our model on a large dataset of spreadsheets, and demonstrate that SpreadsheetCoder achieves top-1 prediction accuracy of 42.51%, which is a considerable improvement over baselines that do not employ rich tabular context. Compared to the rule-based system, SpreadsheetCoder assists 82% more users in composing formulas on Google Sheets.

</details>


### [41] [From webtables to datatables](https://arxiv.org/abs/2006.14694)
*Mária Csernoch*

Main category: cs.SE

TL;DR: 该论文详细介绍了将LOL论坛的Web表格转换为可用于教授电子表格的流程。


<details>
  <summary>Details</summary>
Motivation: Web表格是教授电子表格和计算思维技能的绝佳来源，可在商业和专业组织中利用和开发知识转移项目，处理各种现实问题和解决方案。

Method: 详细介绍了LOL论坛的Web表格的转换过程，包括算法和两种解决方案（一种在文字处理器中，另一种纯粹在电子表格应用程序中）。

Result: 提供了两种将表格转换为电子表格的解决方案。

Conclusion: 该论文为讨论、发明其他解决方案以及组合它们留下了空间。

Abstract: Webtables -- tables and table-like structures on webpages -- are excellent sources for teaching spreadsheeting, in commercial and professional organisations by utilizing and developing knowledge-transfer items, presenting and handling various real-world problems and solutions, discussing and debugging, and in general, developing and utilizing computational thinking skills. In the present paper the conversion process of one of the LOL Boards (League of Legends, Riot Games Inc. 2019) is detailed. After presenting the algorithm of the conversion, two solutions are offered -- one in a word processor, the other purely in a spreadsheet application -- leaving space for discussions, inventing other solutions and combining them.

</details>


### [42] [Implementation Strategies for Multidimensional Spreadsheets](https://arxiv.org/abs/2006.05814)
*Paul Mireault*

Main category: cs.SE

TL;DR: 经验丰富的Excel开发者在实现多维变量电子表格时，主要采用两种策略：多数人使用二维平面投影，少数人使用数据库方法。数据库方法能带来更简洁的公式。


<details>
  <summary>Details</summary>
Motivation: 旨在分析经验丰富的Excel开发者在Excel中实现多维变量的不同策略。

Method: 邀请经验丰富的Excel开发者参与一项挑战，实现一个具有多维变量的电子表格，并分析他们的实现策略。

Result: 识别出两种主要策略：大多数参与者将三维或四维变量投影到Excel的二维平面上；少数参与者采用数据库方法，将多维变量以带有适当主键的数据集表形式呈现。数据库方法能使公式更简单。

Conclusion: 在Excel中实现多维变量时，数据库方法能使公式更简洁，是一种更有效的策略。

Abstract: Seasoned Excel developers were invited to participate in a challenge to implement a spreadsheet with multi-dimensional variables. We analyzed their spreadsheet to see the different implement strategies employed. We identified two strategies: most participants used a projection of three or four-dimensional variables on the two-dimensional plane used by Excel. A few participants used a database approach where the multi-dimensional variables are presented in the form of a dataset table with the appropriate primary key. This approach leads to simpler formulas.

</details>


### [43] [Abstracting spreadsheet data flow through hypergraph redrawing](https://arxiv.org/abs/2006.04794)
*David Birch,Nicolai Stawinoga,Jack Binks,Bruno Nicoletti,Paul Kelly*

Main category: cs.SE

TL;DR: 该论文提出通过重新定义“单元格”的边界来提高电子表格的抽象级别，将其表示为包含操作符和值节点的超图边，以减少错误并更好地反映用户的心智模型，并通过检测向量操作进行了说明。


<details>
  <summary>Details</summary>
Motivation: 传统电子表格由于抽象级别低，且单元格（仅含标量值）之间的链接隐藏，导致易错，迫使终端用户从低级单元格构建数据模型。

Method: 将电子表格转换为细粒度图，其中操作符和值作为节点。然后通过在操作符/数据节点集周围绘制边界“墙”，将“单元格”表示为超图边。通过常见子表达式识别和子树同构的应用来检测向量（数组）操作，从而说明了这种方法。

Result: 该方法通过常见子表达式识别和子树同构来检测向量（数组）操作，从而展示了提高抽象级别的方法。

Conclusion: 研究人员应寻求技术来重新绘制单元格边界，以创建更高级别的“单元格”，使其更能忠实地代表终端用户的真实世界/心智模型。

Abstract: We believe the error prone nature of traditional spreadsheets is due to their low level of abstraction. End user programmers are forced to construct their data models from low level cells which we define as "a data container or manipulator linked by user-intent to model their world and positioned to reflect its structure". Spreadsheet cells are limited in what they may contain (scalar values) and the links between them are inherently hidden. This paper proposes a method of raising the level of abstraction of spreadsheets by "redrawing the boundary" of the cell. To expose the hidden linkage structure we transform spreadsheets into fine-grained graphs with operators and values as nodes. "cells" are then represented as hypergraph edges by drawing a boundary "wall" around a set of operator/data nodes. To extend what cells may contain and to create a higher level model of the spreadsheet we propose that researchers should seek techniques to redraw these boundaries to create higher level "cells" which will more faithfully represent the end-user's real world/mental model. We illustrate this approach via common sub-expression identification and the application of sub-tree isomorphisms for the detection of vector (array) operations.

</details>


### [44] [Comprehensive review for common types of errors using spreadsheets](https://arxiv.org/abs/1912.09209)
*Ali Aburas*

Main category: cs.SE

TL;DR: 该研究全面回顾并分类了电子表格中错误查找和修复的方法。


<details>
  <summary>Details</summary>
Motivation: 电子表格因其灵活性和多功能性而被广泛使用，但其中错误率高，因此研究人员开发了多种工具来预防、检测和纠正电子表格错误。

Method: 该论文通过综合性综述，描述并分类了现有关于电子表格错误查找和修复的方法，讨论了这些方法的定义、工作原理以及它们能发现的错误类型，并探讨了最终用户常犯的错误类型。

Result: 对电子表格错误查找和修复方法进行了全面的综述，详细介绍了各种方法的定义、工作原理、可发现的错误类型以及最终用户常见的错误类型。

Conclusion: 该综述为理解电子表格错误查找和修复的现有方法及其局限性提供了基础。

Abstract: Thanks to their flexibility and capability to perform different tasks and organize data in the best form and format, spreadsheets are widely used in different organizations and by different end users. Many business organizations rely on spreadsheets to fulfill their various tasks. On the other hand, the number of spreadsheets that contain errors are very high, thus researchers have developed different tools aimed at the prevention, detection, and correction of errors in spreadsheets. This research work is a comprehensive review that describes and classifies approaches on finding and fixing errors in spreadsheets. The paper discusses up-to-date research work approaches in terms of definition, how they work, and kinds of errors they can find in spreadsheets. The paper looks also for the kinds of errors that end users commonly make in spreadsheets.

</details>


### [45] [A Coding-free Software Framework of Developing Web Data Management Systems](https://arxiv.org/abs/1910.05685)
*Can Yang,Shiying Pan,Runmin Li,Yu Liu,Lizhang Peng*

Main category: cs.SE

TL;DR: 本文提出了一种基于SaaS架构的无代码数据管理系统构建方法和平台，使得非程序员也能快速开发和部署数据管理系统。


<details>
  <summary>Details</summary>
Motivation: 企业希望在云端部署数据管理系统，但非程序员开发此类系统仍面临困难，而SaaS的发展使得无代码开发成为可能。

Method: 基于SaaS架构，提出了一套无代码构建数据管理系统的理论和方法；设计了一个通用Web平台，通过抽象数据管理系统的通用特征，快速生成和发布定制的系统实例；提出了一种使用电子表格中的需求表来开发数据管理系统的方法，平台通过解析表格模型并在运行阶段实现目标系统来将需求表映射为系统实例。

Result: 实现了所提出的框架并将其部署在Web上。

Conclusion: 实证结果表明，所提出的无代码方法在开发Web数据管理系统方面具有可行性和可用性。

Abstract: More and more enterprises recently intend to deploy data management systems in the cloud. Due to the professionalism of software development, it has still been difficult for non-programmers to develop this kind of systems, even a small one. However, the development of SaaS brings forth the more feasibility of coding-free software development than before. Based on the SaaS architecture, this paper presents a set of theory and method for coding-free construction of a data management system, on which our contributions involve in a practical application platform, a set of construction method and a set of interface on data exchange. By abstracting the common features of data management systems, we design a universal web platform to quickly generate and publish customized system instances. Moreover, we propose a kind of method to develop a data management system using a specific requirements table in spreadsheet. The corresponding platform maps the requirements table into a system instance through parsing the table model and implementing the objective system in the running stage. Finally, we implement the proposed framework and deploy it on web. The empirical result demonstrates the feasibility and availability of the coding-free method in developing web data management systems.

</details>


### [46] [Structured Spreadsheet Modelling and Implementation with Multiple Dimensions -- Part 2: Implementation](https://arxiv.org/abs/1909.00891)
*Paul Mireault*

Main category: cs.SE

TL;DR: 本文介绍了如何实现多维问题，以生成易于维护的电子表格。


<details>
  <summary>Details</summary>
Motivation: 在第一部分展示了如何开发涉及产品、区域、行业和月份等多维变量问题的概念模型之后，本文旨在提供精确的步骤来实施多维问题。

Method: 提出实现多维问题的精确步骤。

Result: 生成一个易于维护的电子表格。

Conclusion: 通过精确的实施步骤，可以构建一个易于维护的多维问题电子表格。

Abstract: In Part 1, we showed how to develop a conceptual model of a problem involving variables of multiple dimensions, like Products, Regions, Sectors and Months. The conceptual model is presented as a Formula Diagram, giving a global view of the interaction between all the variables, and a Formula List, giving a precise view of the interaction between the variables. In this paper, we present precise steps to implement a multi-dimensional problem in a way that will produce a spreadsheet that is easy to maintain

</details>


### [47] [On the Refinement of Spreadsheet Smells by means of Structure Information](https://arxiv.org/abs/1810.04542)
*Patrick Koch,Birgit Hofer,Franz Wotawa*

Main category: cs.SE

TL;DR: 该论文通过利用推断的结构信息（如单元格簇和块）来改进电子表格气味检测，以减少错误或冗余的报告，并引入新的检测技术。


<details>
  <summary>Details</summary>
Motivation: 目前的电子表格气味检测技术存在缺陷，会产生不正确或冗余的报告，例如对同一问题重复报告，导致用户不堪重负。

Method: 作者提出了一种静态分析方法来推断相关单元格的簇和块。然后，他们利用这些结构信息改进了三种现有的气味检测技术，并提出了三种利用推断结构的新型气味检测技术。

Result: 实证评估表明，改进后的技术成功减少了错误和冗余报告的气味数量，并且新引入的气味揭示了新的缺陷。

Conclusion: 利用推断的结构信息能够显著提高电子表格气味检测的准确性和全面性，解决了现有方法的局限性。

Abstract: Spreadsheet users are often unaware of the risks imposed by poorly designed spreadsheets. One way to assess spreadsheet quality is to detect smells which attempt to identify parts of spreadsheets that are hard to comprehend or maintain and which are more likely to be the root source of bugs. Unfortunately, current spreadsheet smell detection techniques suffer from a number of drawbacks that lead to incorrect or redundant smell reports. For example, the same quality issue is often reported for every copy of a cell, which may overwhelm users. To deal with these issues, we propose to refine spreadsheet smells by exploiting inferred structural information for smell detection. We therefore first provide a detailed description of our static analysis approach to infer clusters and blocks of related cells. We then elaborate on how to improve existing smells by providing three example refinements of existing smells that incorporate information about cell groups and computation blocks. Furthermore, we propose three novel smell detection techniques that make use of the inferred spreadsheet structures. Empirical evaluation of the proposed techniques suggests that the refinements successfully reduce the number of incorrectly and redundantly reported smells, and novel deficits are revealed by the newly introduced smells.

</details>


### [48] [Now You're Thinking With Structures: A Concept for Structure-based Interactions with Spreadsheets](https://arxiv.org/abs/1809.03435)
*Patrick Koch*

Main category: cs.SE

TL;DR: 该论文提出了一个概念，旨在通过结构感知理解和交互来改善复杂电子表格的使用体验，该方法不替代现有工具，而是作为附加功能层，以提高用户生产力和电子表格质量。


<details>
  <summary>Details</summary>
Motivation: 电子表格在达到一定复杂程度后难以理解和适应，而对复杂系统的认知通常需要更高阶的心理模型。

Method: 该概念利用结构信息来丰富可视化，增强传统用户操作，并提供工具主动改变电子表格的整体结构。目前正在实现一个用于结构推断和可视化的工具，并计划引入主动和被动交互机制。

Result: 正在进行的工作包括实现一个结构推断和可视化工具，并计划将结构感知功能作为插件提供给常见的电子表格处理器。

Conclusion: 提供以结构感知方式思考和交互电子表格的工具，将有利于用户提高生产力和整体电子表格质量。

Abstract: Spreadsheets are the go-to tool for computerized calculation and modelling, but are hard to comprehend and adapt after reaching a certain complexity. In general, cognition of complex systems is facilitated by having a higher order mental model of the system in question to work with. We therefore present a concept for structure-aware understanding of and interaction with spreadsheets that extends previous work on structure inference in the domain. Following this concept, structural information is used to enrich visualizations, reactively enhance traditional user actions, and provide tools to proactively alter the overall spreadsheet makeup instead of individual cells The intended systems should, in first approximation, not replace common spreadsheet tools, but provide an additional layer of functionality alongside the established interface. In ongoing work, we therefore implemented a tool for structure inference and visualization along the common spreadsheet layout. Based on this framework, we plan to introduce the envisioned proactive and reactive interaction mechanics, and finally provide structure-aware unctionality as an add-in for common spreadsheet processors. We believe that providing the tools for thinking about and interacting with spreadsheets in this manner will benefit users both in terms of productivity and overall spreadsheet quality.

</details>


### [49] [Typed Table Transformations](https://arxiv.org/abs/1809.02746)
*Martin Erwig*

Main category: cs.SE

TL;DR: 该论文提出了一种基于行和列类型转换的电子表格转换新方法，并阐述了其基本思想，同时提出了未来的研究问题。


<details>
  <summary>Details</summary>
Motivation: 电子表格通常带有标签，这些标签有效地构成了数据类型，控制着表格中数值的放置。因此，可以利用这些类型进行表格转换。

Method: 提出了一种基于行和列类型转换的电子表格转换新方法。

Result: 阐释了基于类型的表格构建和转换的基本思想，并提出了一系列需要在未来工作中解决的研究问题。

Conclusion: 基于类型的电子表格转换具有潜力，但仍需进一步研究。

Abstract: Spreadsheet tables are often labeled, and these labels effectively constitute types for the data in the table. In such cases tables can be considered to be built from typed data where the placement of values within the table is controlled by the types used for rows and columns. We present a new approach to the transformations of spreadsheet tables that is based on transformations of row and column types. We illustrate the basic idea of type-based table construction and transformation and lay out a series of research questions that should be addressed in future work.

</details>


### [50] [Implementing WHERE and ORDER BY as spreadsheet formulas](https://arxiv.org/abs/1809.00025)
*Paul Mireault*

Main category: cs.SE

TL;DR: 本文开发了可在电子表格中实现SQL WHERE和ORDER BY子句的公式，解决了现有工具无法自动响应计算值变化的缺点。


<details>
  <summary>Details</summary>
Motivation: 现有的电子表格工具（如筛选、排序、查询或数据透视表）无法自动响应电子表格中计算值的变化，这是一个缺点。

Method: 开发实现SQL的WHERE和ORDER BY子句的电子表格公式。

Result: 开发出能够实现SQL WHERE和ORDER BY子句的电子表格公式。

Conclusion: 摘要中未明确给出结论，但暗示这些公式将提供一种优于现有工具的解决方案。

Abstract: The WHERE and ORDER BY clauses of the SQL SELECT statement select a subset of rows in the result of a database query and present the result in the specified order. In a spreadsheet program like Microsoft Excel, one could use the filter and sort buttons, or use its Query or its Pivot Table tools to achieve a similar effect. The disadvantage of using those tools is that they don't react automatically to changes in the calculated values of the spreadsheet. In this paper, we develop spreadsheet formulas that implement SQL's WHERE and ORDER BY clauses.

</details>


### [51] [The use of Charts, Pivot Tables, and Array Formulas in two Popular Spreadsheet Corpora](https://arxiv.org/abs/1808.10642)
*Bas Jansen,Felienne Hermans*

Main category: cs.SE

TL;DR: 电子表格在工业中广泛使用但容易出错，而现有研究主要关注公式，忽视了图表、数据透视表和数组公式等其他重要结构。本文旨在分析Enron和EUSES这两个常用电子表格语料库中这些结构的使用情况，以期提高电子表格质量。


<details>
  <summary>Details</summary>
Motivation: 电子表格在工业决策中普遍使用，但其高错误率导致公司可能基于不准确的信息做出错误决策并造成经济损失。当前研究多集中于公式，却忽略了图表、数据透视表和数组公式等同样用于决策支持的关键构造。为了全面理解电子表格的使用并提高其质量，有必要研究这些被忽视的构造。

Method: 本文将分析Enron和EUSES这两个流行的电子表格语料库中图表、数据透视表和数组公式的使用情况。

Result: 摘要中未提供具体的研究结果。

Conclusion: 全面理解电子表格中包括图表、数据透视表和数组公式在内的各种构造的使用方式，对于提高电子表格质量至关重要。

Abstract: The use of spreadsheets in industry is widespread. Companies base decisions on information coming from spreadsheets. Unfortunately, spreadsheets are error-prone and this increases the risk that companies base their decisions on inaccurate information, which can lead to incorrect decisions and loss of money. In general, spreadsheet research is aimed to reduce the error-proneness of spreadsheets. Most research is concentrated on the use of formulas. However, there are other constructions in spreadsheets, like charts, pivot tables, and array formulas, that are also used to present decision support information to the user. There is almost no research about how these constructions are used. To improve spreadsheet quality it is important to understand how spreadsheets are used and to obtain a complete understanding, the use of charts, pivot tables, and array formulas should be included in research. In this paper, we analyze two popular spreadsheet corpora: Enron and EUSES on the use of the aforementioned constructions.

</details>


### [52] [Asheetoxy: A Taxonomy for Classifying Negative Spreadsheet-related Phenomena](https://arxiv.org/abs/1808.10231)
*Daniel Kulesz,Stefan Wagner*

Main category: cs.SE

TL;DR: 本文提出了一种名为 Asheetoxy 的新分类法，用于分类电子表格中的“类错误现象”，该分类法避免了“错误”这一模糊术语，并且易于应用。


<details>
  <summary>Details</summary>
Motivation: 现有的电子表格错误分类法存在问题，因为它们使用了模糊的术语（“错误”），并且需要对底层过程和用户“思维状态”有详细了解，这使得它们难以应用于现实世界的电子表格语料库，从而阻碍了研究人员和专业人士之间的讨论。

Method: 提出了一种名为 Asheetoxy 的简单、面向现象的分类法，并进行了一项有 7 名参与者参与的初步研究。

Result: 初步研究表明，即使是非电子表格研究人员也能使用 Asheetoxy 对现实世界的电子表格现象进行类似分类。

Conclusion: Asheetoxy 提供了一种简单、面向现象的电子表格“类错误现象”分类法，该分类法易于应用并避免了“错误”一词的模糊性。

Abstract: Spreadsheets (sometimes also called Excel programs) are powerful tools which play a business-critical role in many organizations. However, due to faulty spreadsheets many bad decisions have been taken in recent years. Since then, a number of researchers have been studying spreadsheet errors. However, one issue that hinders discussion among researchers and professionals is the lack of a commonly accepted taxonomy.
  Albeit a number of taxonomies for spreadsheet errors have been proposed in previous work, a major issue is that they use the term error that itself is already ambiguous. Furthermore, to apply most existing taxonomies, detailed knowledge about the underlying process and knowledge about the "brain state" of the acting spreadsheet users is required. Due to these limitations, known error-like phenomena in freely available spreadsheet corpora cannot be classified with these taxonomies.
  We propose Asheetoxy, a simple and phenomenon-oriented taxonomy that avoids the problematic term error altogether. An initial study with 7 participants indicates that even non-spreadsheet researchers similarly classify real-world spreadsheet phenomena using Asheetoxy.

</details>


### [53] [Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (SEMS'18)](https://arxiv.org/abs/1808.09174)
*Birgit Hofer,Jorge Mendes*

Main category: cs.SE

TL;DR: 该文本是关于2018年10月1日在葡萄牙里斯本举行的第五届电子表格软件工程方法国际研讨会（SEMS'18）的会议记录。该研讨会与2018年IEEE可视化语言和以人为中心计算研讨会（VL/HCC）同期举行。


<details>
  <summary>Details</summary>
Motivation: 该文本是对研讨会记录的描述，而非研究论文，因此不涉及研究动机。

Method: 该文本是对研讨会记录的描述，而非研究论文，因此不涉及研究方法。

Result: 该文本是对研讨会记录的描述，而非研究论文，因此不涉及研究结果。

Conclusion: 该文本是对研讨会记录的描述，而非研究论文，因此不涉及研究结论。

Abstract: Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (SEMS'18), held on October 1st, 2018, in Lisbon, Portugal, and co-located with the 2018 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC).

</details>


### [54] [Combining Spreadsheet Smells for Improved Fault Prediction](https://arxiv.org/abs/1805.10493)
*Patrick Koch,Konstantin Schekotihin,Dietmar Jannach,Birgit Hofer,Franz Wotawa*

Main category: cs.SE

TL;DR: 该论文提出了一种基于机器学习的方法，使用AdaBoost集成分类器结合单个异味预测，以提高电子表格中错误预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 电子表格中的错误可能对业务产生严重影响，且现有单个“异味”的预测能力有限。

Method: 提出了一种基于机器学习的方法，该方法利用AdaBoost集成分类器结合了单个“异味”的预测。

Result: 在两个包含真实世界电子表格错误的数据集上进行的实验表明，错误预测准确性显著提高。

Conclusion: 通过结合机器学习和AdaBoost集成分类器，可以有效提升电子表格错误预测的准确性。

Abstract: Spreadsheets are commonly used in organizations as a programming tool for business-related calculations and decision making. Since faults in spreadsheets can have severe business impacts, a number of approaches from general software engineering have been applied to spreadsheets in recent years, among them the concept of code smells. Smells can in particular be used for the task of fault prediction. An analysis of existing spreadsheet smells, however, revealed that the predictive power of individual smells can be limited. In this work we therefore propose a machine learning based approach which combines the predictions of individual smells by using an AdaBoost ensemble classifier. Experiments on two public datasets containing real-world spreadsheet faults show significant improvements in terms of fault prediction accuracy.

</details>


### [55] [An Easy & Collaborative RDF Data Entry Method using the Spreadsheet Metaphor](https://arxiv.org/abs/1804.04175)
*Markus Schröder,Christian Jilek,Jörn Hees,Sven Hertling,Andreas Dengel*

Main category: cs.SE

TL;DR: 一个将电子表格条目转换为RDF语句的零配置、基于网络的电子表格编辑器，旨在让所有用户更轻松地创建语义数据。


<details>
  <summary>Details</summary>
Motivation: 知识工作者广泛使用电子表格进行数据输入，但定义RDF语句对普通知识工作者来说很困难。

Method: 我们提出了一个易于使用、零配置、基于网络的电子表格编辑器，可以同时将电子表格条目转换为RDF语句。

Result: 用户研究表明，与现有方法相比，参与者能够更快地创建更多语句，并且质量相似或显著优于现有方法。

Conclusion: 该编辑器简化了各种用户的语义数据创建，从而能够有效地增量填充知识库。

Abstract: Spreadsheets are widely used by knowledge workers, especially in the industrial sector. Their methodology enables a well understood, easy and fast possibility to enter data. As filling out a spreadsheet is more accessible to common knowledge workers than defining RDF statements, in this paper, we propose an easy-to-use, zero-configuration, web-based spreadsheet editor that simultaneously transfers spreadsheet entries into RDF statements. It enables various kinds of users to easily create semantic data whether they are RDF experts or novices. The typical scenario we address focuses on creating instance data starting with an empty knowledge base that is filled incrementally. In a user study, participants were able to create more statements in shorter time, having similar or even significantly outperforming quality, compared to other approaches.

</details>


### [56] [Structured Spreadsheet Modelling and Implementation with Multiple Dimensions - Part 1: Modelling](https://arxiv.org/abs/1801.09777)
*Paul Mireault*

Main category: cs.SE

TL;DR: 维度是日常模型的重要组成部分，如电子表格中的时间维度。表示第二个维度通常需要重复公式块或创建多个结构相同的表格。


<details>
  <summary>Details</summary>
Motivation: 摘要强调了维度在日常模型中的普遍性，并指出了在电子表格中表示第二个维度时存在的重复性和复杂性问题，这可能暗示了改进多维度表示方法的动机。

Method: 摘要中未提及具体方法。

Result: 摘要中未提及研究结果。

Conclusion: 摘要中未提及结论。

Abstract: Dimensions are an integral part of many models we use every day. Without thinking about it, we frequently use the time dimension: many financial and accounting spreadsheets have columns representing months or years. Representing a second dimension is often done by repeating blocs of formulas in a worksheet or creating multiple worksheets with the same structure.

</details>


### [57] [Mitigating Spreadsheet Risk in Complex Multi-Dimensional Models in Excel](https://arxiv.org/abs/1802.01640)
*Steve Litt*

Main category: cs.SE

TL;DR: 提出了一种名为“PivotModel”的解决方案，旨在通过利用Excel的强大功能，缓解复杂多维模型中电子表格的风险，其工作方式类似于数据透视表。


<details>
  <summary>Details</summary>
Motivation: Microsoft Excel是最普遍的分析工具，但其手动操作和易出错的特性导致了电子表格风险，尤其是在需要复杂算法、层级和数据库回写等功能的复杂多维模型中。

Method: 提出“PivotModel”解决方案，旨在缓解复杂多维模型（如规划、预测等，涉及复杂算法、层级和数据库回写）的电子表格风险。它提供报告、数据输入表单和即席分析等功能，工作方式类似于数据透视表，但利用了Microsoft Excel平台的强大功能。

Result: 该解决方案旨在减轻在复杂多维模型中使用Excel所带来的电子表格风险。

Conclusion: “PivotModel”是一种利用Excel平台强大功能、类似于数据透视表的解决方案，专门用于减轻复杂多维模型中的电子表格风险。

Abstract: Microsoft Excel is the most ubiquitous analytical tool ever built. Companies around the world leverage it for its power, flexibility and ease of use. However, spreadsheets are manually intensive and prone to error, making it difficult for companies to control spreadsheet risk. The following solution is designed to mitigate spreadsheet risk for a set of problems commonly addressed in a spreadsheet defined as "complex multi-dimensional models". "Complex" referring to certain types of applications that require functionality such as sophisticated algorithms, challenging hierarchies and database write-back (i.e. planning, forecasting, etc.) and "multi-dimensional" referring to providing capabilities such as reporting, data input forms and ad hoc analysis on the different attributes associated with the resulting model. The solution is defined as a "PivotModel" because it works similarly to a PivotTable but is designed to leverage the robust capabilities of the Microsoft Excel platform.

</details>


### [58] [Proposed Spreadsheet Transparency Definition and Measures](https://arxiv.org/abs/1802.01628)
*Craig Hatmaker*

Main category: cs.SE

TL;DR: 当前缺乏对财务建模透明度的明确定义和衡量标准，本文提出了一个电子表格建模透明度的具体定义。


<details>
  <summary>Details</summary>
Motivation: 审计师要求财务模型透明，但目前缺乏对此的精确定义和衡量标准，导致无法客观评估模型透明度及方法。

Method: 本文提出了一个针对电子表格建模透明度的具体定义。

Result: 该定义足够具体，可以创建衡量标准和自动化工具，帮助审计师判断模型是否符合透明度要求，并使建模人员能够客观比较不同的电子表格建模方法。

Conclusion: 通过提出一个具体的电子表格建模透明度定义，本文旨在解决当前缺乏明确标准的问题，从而实现模型的客观评估和方法比较。

Abstract: Auditors demand financial models be transparent yet no consensus exists on what that means precisely. Without a clear modeling transparency definition we cannot know when our models are "transparent". The financial modeling community debates which methods are more or less transparent as though transparency is a quantifiable entity yet no measures exist. Without a transparency measure modelers cannot objectively evaluate methods and know which improves model transparency.
  This paper proposes a definition for spreadsheet modeling transparency that is specific enough to create measures and automation tools for auditors to determine if a model meets transparency requirements. The definition also provides modelers the ability to objectively compare spreadsheet modeling methods to select which best meets their goals.

</details>


### [59] [Alternative Spreadsheet Model Designs for an Operations Management Model Embedded in a Periodic Business Process](https://arxiv.org/abs/1802.00484)
*Thomas A. Grossman,Vijay Mehrotra,Mouwafac Sidaoui*

Main category: cs.SE

TL;DR: 本文比较了三种电子表格实现方案（数据驱动、规范、技术）在操作管理模型中的适用性，发现技术设计在修改和减少错误方面有优势。


<details>
  <summary>Details</summary>
Motivation: 操作管理模型在周期性业务流程中需要频繁修改和重用。

Method: 本文提出了三种替代电子表格实现方案：数据驱动设计、规范设计和新型（表格驱动）技术设计。我们评估了每种设计在准确性、修改、分析和传输方面的适用性，并考虑了使用每种设计所需的培训和技术复杂程度。

Result: 数据驱动设计揭示了新手建模者的不良电子表格实践。技术设计无需手动编写或编辑单元格公式即可修改新数据和新的结构元素，从而加快修改速度并降低错误风险。技术设计还具有用于其他类型模型的潜力。

Conclusion: 技术设计在操作管理模型的电子表格实现中具有显著优势，且具有推广到其他模型类的潜力。本文还指出了未来的研究机会。

Abstract: We present a widely-used operations management model used in supply and distribution planning, that is typically embedded in a periodic business process that necessitates model modification and reuse. We consider three alternative spreadsheet implementations, a data-driven design, a canonical (textbook) design, and a novel (table-driven) technical design. We evaluate each regarding suitability for accuracy, modification, analysis, and transfer. We consider the degree of training and technical sophistication required to utilize each design. The data-driven design provides insight into poor spreadsheet practices by naïve modelers. The technical design can be modified for new data and new structural elements without manual writing or editing of cell formulas, thus speeding modification and reducing risk of error. The technical design has potential for use with other classes of models. We identify opportunities for future research.

</details>


### [60] [Mitigating Spreadsheet Model Risk with Python Open Source Infrastructure](https://arxiv.org/abs/1801.09771)
*Oliver Beavers*

Main category: cs.SE

TL;DR: 本文提出使用Python开源包开发可重现的审计工具，以帮助电子表格建模专业人员测试和审计电子表格计算，从而减少错误。


<details>
  <summary>Details</summary>
Motivation: 电子表格模型类似于软件，但开发人员不是软件工程师，导致缺乏传统软件工程工具和协议，从而增加了错误率。

Method: 使用Python编程语言构建的免费、开源软件包，为电子表格建模专业人员开发可重现的审计工具奠定基础。

Result: 使利益相关者能够开发清晰定义的模型“预言机”，用于测试和审计电子表格计算。

Conclusion: 通过提供基于Python的审计工具，提高电子表格计算的准确性和可靠性。

Abstract: Across an aggregation of EuSpRIG presentation papers, two maxims hold true: spreadsheets models are akin to software, yet spreadsheet developers are not software engineers. As such, the lack of traditional software engineering tools and protocols invites a higher rate of error in the end result. This paper lays ground work for spreadsheet modelling professionals to develop reproducible audit tools using freely available, open source packages built with the Python programming language, enabling stakeholders to develop clearly defined model "oracles" with which to test and audit spreadsheet calculations against.

</details>


### [61] [Structuring Spreadsheets with the "Lish" Data Model](https://arxiv.org/abs/1801.08603)
*Alan Hall,Michel Wermelinger,Tony Hirst,Santi Phithakkitnukoon*

Main category: cs.SE

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\nPlease retry in 31.033390086s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-flash', 'location': 'global'}, 'quotaValue': '250'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '31s'}]}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: A spreadsheet is remarkably flexible in representing various forms of structured data, but the individual cells have no knowledge of the larger structures of which they may form a part. This can hamper comprehension and increase formula replication, increasing the risk of error on both scores. We explore a novel data model (called the "lish") that could form an alternative to the traditional grid in a spreadsheet-like environment. Its aim is to capture some of these higher structures while preserving the simplicity that makes a spreadsheet so attractive. It is based on cells organised into nested lists, in each of which the user may optionally employ a template to prototype repeating structures. These template elements can be likened to the marginal "cells" in the borders of a traditional worksheet, but are proper members of the sheet and may themselves contain internal structure. A small demonstration application shows the "lish" in operation.

</details>


### [62] [Automated Refactoring of Nested-IF Formulae in Spreadsheets](https://arxiv.org/abs/1712.09797)
*Jie Zhang,Shi Han,Dan Hao,Lu Zhang,Dongmei Zhang*

Main category: cs.SE

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\nPlease retry in 17.599467158s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '250'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '17s'}]}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Spreadsheets are the most popular end-user programming software, where formulae act like programs and also have smells. One well recognized common smell of spreadsheet formulae is nest-IF expressions, which have low readability and high cognitive cost for users, and are error-prone during reuse or maintenance. However, end users usually lack essential programming language knowledge and skills to tackle or even realize the problem. The previous research work has made very initial attempts in this aspect, while no effective and automated approach is currently available.
  This paper firstly proposes an AST-based automated approach to systematically refactoring nest-IF formulae. The general idea is two-fold. First, we detect and remove logic redundancy on the AST. Second, we identify higher-level semantics that have been fragmented and scattered, and reassemble the syntax using concise built-in functions. A comprehensive evaluation has been conducted against a real-world spreadsheet corpus, which is collected in a leading IT company for research purpose. The results with over 68,000 spreadsheets with 27 million nest-IF formulae reveal that our approach is able to relieve the smell of over 99\% of nest-IF formulae. Over 50% of the refactorings have reduced nesting levels of the nest-IFs by more than a half. In addition, a survey involving 49 participants indicates that for most cases the participants prefer the refactored formulae, and agree on that such automated refactoring approach is necessary and helpful.

</details>


### [63] [Spreadsheet Guardian: An Approach to Protecting Semantic Correctness throughout the Evolution of Spreadsheets](https://arxiv.org/abs/1612.03813)
*Daniel Kulesz,Verena Käfer,Stefan Wagner*

Main category: cs.SE

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\nPlease retry in 10.834323132s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '250'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '10s'}]}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Spreadsheets are powerful tools which play a business-critical role in many organizations. However, many bad decisions taken due to faulty spreadsheets show that these tools need serious quality assurance. Furthermore, while collaboration on spreadsheets for maintenance tasks is common, there has been almost no support for ensuring that the spreadsheets remain correct during this process.
  We have developed an approach named Spreadsheet Guardian which separates the specification of spreadsheet test rules from their execution. By automatically executing user-defined test rules, our approach is able to detect semantic faults. It also protects all collaborating spreadsheet users from introducing faults during maintenance, even if only few end-users specify test rules. To evaluate Spreadsheet Guardian, we implemented a representative testing technique as an add-in for Microsoft Excel.
  We evaluated the testing technique in two empirical evaluations with 29 end-users and 42 computer science students. The results indicate that the technique is easy to learn and to apply. Furthermore, after finishing maintenance, participants with spreadsheets "protected" by the technique are more realistic about the correctness of their spreadsheets than participants who employ only "classic", non-interactive test rules based on static analysis techniques. Hence, we believe Spreadsheet Guardian can be of use for business-critical spreadsheets.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [64] [Dynamic Kernel Matching for Non-conforming Data: A Case Study of T-cell Receptor Datasets](https://arxiv.org/abs/2103.10472)
*Jared Ostmeyer,Scott Christley,Lindsay Cowell*

Main category: q-bio.QM

TL;DR: 本文提出动态核匹配（DKM）方法，以使现有统计分类器能够处理非规范数据。该方法在T细胞受体（TCR）序列和库数据上进行了验证，显示其在疾病诊断方面的有效性，并且其识别的模式与实验观察结果一致。


<details>
  <summary>Details</summary>
Motivation: 大多数统计分类器设计用于处理结构化数据，但在许多领域，数据不符合这种结构。本文旨在解决这一问题，通过在非规范数据中发现模式，特别是为了疾病诊断。

Method: 本文描述了一种名为动态核匹配（DKM）的方法，用于修改现有统计分类器以处理非规范数据。作者将此方法应用于两种非规范数据集：(i) 带有疾病抗原标签的T细胞受体（TCR）序列数据集，以及 (ii) 带有患者巨细胞病毒（CMV）血清状态标签的TCR库序列数据集。

Result: 将增强了DKM的统计分类器成功地应用于这两个数据集。在保留数据上，使用标准和允许不确定诊断的指标报告了性能。此外，分类器用于生成预测的模式与实验研究的观察结果一致。

Conclusion: 动态核匹配（DKM）是一种有效的方法，可以将统计分类器应用于非规范数据，例如TCR序列和库，以实现疾病诊断，并且其识别出的模式得到了实验证据的支持。

Abstract: Most statistical classifiers are designed to find patterns in data where numbers fit into rows and columns, like in a spreadsheet, but many kinds of data do not conform to this structure. To uncover patterns in non-conforming data, we describe an approach for modifying established statistical classifiers to handle non-conforming data, which we call dynamic kernel matching (DKM). As examples of non-conforming data, we consider (i) a dataset of T-cell receptor (TCR) sequences labelled by disease antigen and (ii) a dataset of sequenced TCR repertoires labelled by patient cytomegalovirus (CMV) serostatus, anticipating that both datasets contain signatures for diagnosing disease. We successfully fit statistical classifiers augmented with DKM to both datasets and report the performance on holdout data using standard metrics and metrics allowing for indeterminant diagnoses. Finally, we identify the patterns used by our statistical classifiers to generate predictions and show that these patterns agree with observations from experimental studies.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [65] [Mathematics of Digital Hyperspace](https://arxiv.org/abs/2103.15203)
*Jeremy Kepner,Timothy Davis,Vijay Gadepally,Hayden Jananthan,Lauren Milechin*

Main category: cs.MS

TL;DR: 本文介绍了一种名为“半链接”（semilink）的新数学概念，它通过结合GraphBLAS标准，旨在为数字超空间中的非结构化数据提供更强大、更灵活的表示、遍历和转换方法。


<details>
  <summary>Details</summary>
Motivation: 当前数字超空间中充斥着大量的非结构化数据（如社交媒体、电子商务、流媒体等），这些数据挑战了传统的类型和维度概念，需要更优雅的方式来表示、遍历和转换。

Method: 论文提出了一种新颖的数学概念——半链接（semilink），它结合了半环对，为图分析、数据库操作和机器学习提供基本运算。该概念将与GraphBLAS标准结合。

Result: 通过在GraphBLAS中添加基于键的索引和半链接，可以将其扩展成为更丰富的关联数组代数。这将使其能够替代电子表格、数据库表和以数据为中心的操作系统的插件。

Conclusion: 半链接和GraphBLAS的结合将显著增强对数字超空间中非结构化数据的导航和处理能力。

Abstract: Social media, e-commerce, streaming video, e-mail, cloud documents, web pages, traffic flows, and network packets fill vast digital lakes, rivers, and oceans that we each navigate daily. This digital hyperspace is an amorphous flow of data supported by continuous streams that stretch standard concepts of type and dimension. The unstructured data of digital hyperspace can be elegantly represented, traversed, and transformed via the mathematics of hypergraphs, hypersparse matrices, and associative array algebra. This paper explores a novel mathematical concept, the semilink, that combines pairs of semirings to provide the essential operations for graph analytics, database operations, and machine learning. The GraphBLAS standard currently supports hypergraphs, hypersparse matrices, the mathematics required for semilinks, and seamlessly performs graph, network, and matrix operations. With the addition of key based indices (such as pointers to strings) and semilinks, GraphBLAS can become a richer associative array algebra and be a plug-in replacement for spreadsheets, database tables, and data centric operating systems, enhancing the navigation of unstructured data found in digital hyperspace.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [66] [Radif Corpus: A Symbolic Dataset for Non-Metric Iranian Classical Music](https://arxiv.org/abs/2507.10456)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.SD

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 503 - [{'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Non-metric music forms the core of the repertoire in Iranian classical music. Dastgahi music serves as the underlying theoretical system for both Iranian art music and certain folk traditions. At the heart of Iranian classical music lies the radif, a foundational repertoire that organizes melodic material central to performance and pedagogy.
  In this study, we introduce a digital corpus representing the complete non-metrical radif repertoire, covering all 13 existing components of this repertoire. We provide MIDI files (about 281 minutes in total) and data spreadsheets describing notes, note durations, intervals, and hierarchical structures for 228 pieces of music. We faithfully represent the tonality including quarter-tones, and the non-metric aspect. Furthermore, we provide supporting basic statistics, and measures of complexity and similarity over the corpus.
  Our corpus provides a platform for computational studies of Iranian classical music. Researchers might employ it in studying melodic patterns, investigating improvisational styles, or for other tasks in music information retrieval, music theory, and computational (ethno)musicology.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [67] [Buckaroo: A Direct Manipulation Visual Data Wrangler](https://arxiv.org/abs/2507.16073)
*Annabelle Warner,Andrew McNutt,Paul Rosen,El Kindi Rezig*

Main category: cs.HC

TL;DR: Buckaroo是一个可视化系统，旨在通过自动识别异常数据、建议清洗操作以及支持可视化数据操作来解决数据清洗过程中耗时且易出错的问题。


<details>
  <summary>Details</summary>
Motivation: 数据清洗是数据科学开发的关键阶段，耗时占项目总时间的80%以上，且传统方法（如手动编码或使用电子表格）费力且易出错，可能导致数据质量问题。

Method: Buckaroo是一个可视化系统，它(1)自动发现异常数据组并推荐检查；(2)推荐用户可选择的清洗操作来修复异常；(3)通过显示清洗操作的效果并提供撤销/重做功能，支持用户可视化操作数据。

Result: Buckaroo能够帮助用户识别数据中的异常，提供修复建议，并支持迭代式的数据清洗过程，从而提高数据清洗的效率和准确性。

Conclusion: Buckaroo提供了一种更高效、更少出错的数据清洗方法，通过可视化系统和自动化建议来简化数据准备工作。

Abstract: Preparing datasets -- a critical phase known as data wrangling -- constitutes the dominant phase of data science development, consuming upwards of 80% of the total project time. This phase encompasses a myriad of tasks: parsing data, restructuring it for analysis, repairing inaccuracies, merging sources, eliminating duplicates, and ensuring overall data integrity. Traditional approaches, typically through manual coding in languages such as Python or using spreadsheets, are not only laborious but also error-prone. These issues range from missing entries and formatting inconsistencies to data type inaccuracies, all of which can affect the quality of downstream tasks if not properly corrected. To address these challenges, we present Buckaroo, a visualization system to highlight discrepancies in data and enable on-the-spot corrections through direct manipulations of visual objects. Buckaroo (1) automatically finds "interesting" data groups that exhibit anomalies compared to the rest of the groups and recommends them for inspection; (2) suggests wrangling actions that the user can choose to repair the anomalies; and (3) allows users to visually manipulate their data by displaying the effects of their wrangling actions and offering the ability to undo or redo these actions, which supports the iterative nature of data wrangling. A video companion is available at https://youtu.be/iXdCYbvpQVE

</details>


### [68] [SheetMind: An End-to-End LLM-Powered Multi-Agent Framework for Spreadsheet Automation](https://arxiv.org/abs/2506.12339)
*Ruiyan Zhu,Xi Cheng,Ke Liu,Brian Zhu,Daniel Jin,Neeraj Parihar,Zhoutian Xu,Oliver Gao*

Main category: cs.HC

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 503 - [{'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: We present SheetMind, a modular multi-agent framework powered by large language models (LLMs) for spreadsheet automation via natural language instructions. The system comprises three specialized agents: a Manager Agent that decomposes complex user instructions into subtasks; an Action Agent that translates these into structured commands using a Backus Naur Form (BNF) grammar; and a Reflection Agent that validates alignment between generated actions and the user's original intent. Integrated into Google Sheets via a Workspace extension, SheetMind supports real-time interaction without requiring scripting or formula knowledge. Experiments on benchmark datasets demonstrate an 80 percent success rate on single step tasks and approximately 70 percent on multi step instructions, outperforming ablated and baseline variants. Our results highlight the effectiveness of multi agent decomposition and grammar based execution for bridging natural language and spreadsheet functionalities.

</details>


### [69] ["How do you even know that stuff?": Barriers to expertise sharing among spreadsheet users](https://arxiv.org/abs/2506.09216)
*Qing,Xia,Advait Sarkar,Duncan Brumby,Anna Cox*

Main category: cs.HC

TL;DR: 本研究发现，尽管电子表格协作对于学习和专业知识共享很重要，但由于社会规范、个人对专业知识的评估以及对协作中断的担忧，专业知识共享受到阻碍。这反映了功能丰富的软件设计与长期学习之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 以往研究表明，电子表格专家往往未能传播他们的知识。本研究提出，关于电子表格使用价值的社会规范和信念显著影响用户参与分享行为。

Method: 我们对来自两个独立样本的31名专业电子表格用户进行了半结构化访谈。

Result: 研究发现，电子表格提供者在调整高度个性化的策略以适应主观标准、评估分享的恰当社会时机方面面临挑战。此外，对自身电子表格专业知识的冲突性自我评估、对这种知识价值的轻蔑性规范信念以及对协作可能带来的干扰的担忧，会进一步阻碍分享。

Conclusion: 这些观察结果反映了功能丰富的软件在以初始可学习性为主要设计目标时，在长期学习中面临的挑战。本研究为应对这种张力提供了设计方面的启示，并展示了技术设计与社会动态之间复杂的相互作用如何塑造功能丰富软件背景下的协作学习行为。

Abstract: Spreadsheet collaboration provides valuable opportunities for learning and expertise sharing between colleagues. Sharing expertise is essential for the retention of important technical skillsets within organisations, but previous studies suggest that spreadsheet experts often fail to disseminate their knowledge to others. We suggest that social norms and beliefs surrounding the value of spreadsheet use significantly influence user engagement in sharing behaviours. To explore this, we conducted 31 semi-structured interviews with professional spreadsheet users from two separate samples. We found that spreadsheet providers face challenges in adapting highly personalised strategies to often subjective standards and evaluating the appropriate social timing of sharing. In addition, conflicted self-evaluations of one's spreadsheet expertise, dismissive normative beliefs about the value of this knowledge, and concerns about the potential disruptions associated with collaboration can further deter sharing. We suggest these observations reflect the challenges of long-term learning in feature-rich software designed primarily with initial learnability in mind. We therefore provide implications for design to navigate this tension. Overall, our findings demonstrate how the complex interaction between technology design and social dynamics can shape collaborative learning behaviours in the context of feature-rich software.

</details>


### [70] [Prompting in the Dark: Assessing Human Performance in Prompt Engineering for Data Labeling When Gold Labels Are Absent](https://arxiv.org/abs/2502.11267)
*Zeyu He,Saniya Naphade,Ting-Hao 'Kenneth' Huang*

Main category: cs.HC

TL;DR: 该论文调查了在没有黄金标准标签的情况下，用户迭代提示大型语言模型（LLM）进行数据标注的有效性，发现这种“在黑暗中提示”的方法非常不可靠。


<details>
  <summary>Details</summary>
Motivation: 当没有黄金标准标签来衡量进展时，用户在提示工程方面的表现如何？用户是否能通过多次迭代提示来更接近其期望结果？这些问题对于LLM驱动的数据标注场景至关重要。

Method: 开发了一个名为PromptingSheet的Google Sheets插件，允许用户通过电子表格编写、修改和迭代标注数据。通过一项有20名参与者参与的研究进行了调查，并测试了像DSPy这样的自动化提示优化工具在缺乏黄金标签时的表现。

Result: 研究发现“在黑暗中提示”的效率非常不可靠——在四次或更多次迭代后，只有9名参与者的标注准确性有所提高。自动化提示优化工具在黄金标签较少时也表现不佳。

Conclusion: 研究结果强调了黄金标签的重要性，以及在人工提示工程中自动化支持的需求和风险，为未来的工具设计提供了见解。

Abstract: Millions of users prompt large language models (LLMs) for various tasks, but how good are people at prompt engineering? Do users actually get closer to their desired outcome over multiple iterations of their prompts? These questions are crucial when no gold-standard labels are available to measure progress. This paper investigates a scenario in LLM-powered data labeling, "prompting in the dark," where users iteratively prompt LLMs to label data without using manually-labeled benchmarks. We developed PromptingSheet, a Google Sheets add-on that enables users to compose, revise, and iteratively label data through spreadsheets. Through a study with 20 participants, we found that prompting in the dark was highly unreliable -- only 9 participants improved labeling accuracy after four or more iterations. Automated prompt optimization tools like DSPy also struggled when few gold labels were available. Our findings highlight the importance of gold labels and the needs, as well as the risks, of automated support in human prompt engineering, providing insights for future tool design.

</details>


### [71] [When Copilot Becomes Autopilot: Generative AI's Critical Risk to Knowledge Work and a Critical Solution](https://arxiv.org/abs/2412.15030)
*Advait Sarkar,Xiaotong,Xu,Neil Toronto,Ian Drosos,Christian Poelitz*

Main category: cs.HC

TL;DR: 生成式AI在知识工作中带来机遇的同时也存在风险（如错误和批判性思维的退化）。本文提出通过设计AI界面来培养批判性思维，并展示了一个原型系统，该系统能为电子表格中的AI生成标准提供“启发式批评”。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在知识工作（特别是电子表格等复杂工作流）中既带来了错误和批判性思维退化的风险，也提供了赋能非专业人士的机遇。主要动机是解决AI委派可能导致人类批判性思维退化的风险，并利用AI的潜力同时减轻其负面影响。

Method: 本文讨论了一个用于电子表格中“批判性筛选”的原型系统。该系统使用生成式AI来建议筛选标准，并应用这些标准对电子表格中的行进行排序。此外，它还生成“启发式批评”：简短的文本片段，批判AI生成的标准，突出风险、缺点和替代方案。

Result: 该原型系统展示了AI辅助知识工作中批判性思维工具的一个全新设计空间。它展示了AI如何充当批评者或煽动者，促使使用者进行批判性思考。

Conclusion: 本文概述了将AI作为批评者或煽动者的研究议程，探讨了“启发式批评”的出现时机、形式、内容以及潜在的设计权衡等问题。该系统开启了一个丰富且未被探索的设计空间。

Abstract: Generative AI, with its tendency to "hallucinate" incorrect results, may pose a risk to knowledge work by introducing errors. On the other hand, it may also provide unprecedented opportunities for users, particularly non-experts, to learn and apply advanced software features and greatly increase the scope and complexity of tasks they can successfully achieve.
  As an example of a complex knowledge workflow that is subject to risks and opportunities from generative AI, we consider the spreadsheet. AI hallucinations are an important challenge, but they are not the greatest risk posed by generative AI to spreadsheet workflows. Rather, as more work can be safely delegated to AI, the risk is that human critical thinking -- the ability to holistically and rigorously evaluate a problem and its solutions -- is degraded in the process. The solution is to design the interfaces of generative AI systems deliberately to foster and encourage critical thinking in knowledge work, building primarily on a long history of research on critical thinking tools for education.
  We discuss a prototype system for the activity of critical shortlisting in spreadsheets. The system uses generative AI to suggest shortlisting criteria and applies these criteria to sort rows in a spreadsheet. It also generates "provocations": short text snippets that critique the AI-generated criteria, highlighting risks, shortcomings, and alternatives. Our prototype opens up a rich and completely unexplored design space of critical thinking tools for modern AI-assisted knowledge work. We outline a research agenda for AI as a critic or provocateur, including questions about where and when provocations should appear, their form and content, and potential design trade-offs.

</details>


### [72] [Understanding and Evaluating Trust in Generative AI and Large Language Models for Spreadsheets](https://arxiv.org/abs/2412.14062)
*Simon Thorne*

Main category: cs.HC

TL;DR: 本文提出了一个可信度框架，用于评估生成式AI和LLMs自动创建的电子表格公式的透明度和可靠性，以解决其幻觉、偏见和用户技能差异带来的不可信赖问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和大型语言模型在自动化电子表格公式创建方面前景广阔，但由于幻觉、偏见和用户技能差异，其输出可能不准确或不可信赖。

Method: 提出一个可信度框架，通过评估公式的透明度（可解释性和可见性）和可靠性（一致性、准确性、伦理考量）来解决这些挑战。

Result: 文章探讨了幻觉、训练数据偏差和不佳的提示如何影响这些可信度指标。

Conclusion: 文章还考虑了对技术不信任的例子并探讨了其后果。

Abstract: Generative AI and Large Language Models (LLMs) hold promise for automating spreadsheet formula creation. However, due to hallucinations, bias and variable user skill, outputs obtained from generative AI cannot be assumed to be accurate or trustworthy. To address these challenges, a trustworthiness framework is proposed based on evaluating the transparency and dependability of the formula. The transparency of the formula is explored through explainability (understanding the formula's reasoning) and visibility (inspecting the underlying algorithms). The dependability of the generated formula is evaluated in terms of reliability (consistency and accuracy) and ethical considerations (bias and fairness). The paper also examines the drivers to these metrics in the form of hallucinations, training data bias and poorly constructed prompts. Finally, examples of mistrust in technology are considered and the consequences explored.

</details>


### [73] [Dynamic Prompt Middleware: Contextual Prompt Refinement Controls for Comprehension Tasks](https://arxiv.org/abs/2412.02357)
*Ian Drosos,Jack Williams,Advait Sarkar,Nicholas Wilson*

Main category: cs.HC

TL;DR: 本研究探讨了动态提示细化控制（Dynamic PRC）和静态提示细化控制（Static PRC）两种提示中间件方法，以帮助用户更好地控制生成式AI的解释，尤其是在理解任务中。结果显示Dynamic PRC因提供更多控制、降低上下文提供障碍而受到青睐。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的有效提示对许多用户来说具有挑战性，尤其是在表达理解任务的上下文方面。现有提示中间件在帮助构建提示方面仍存在不足，用户难以充分表达控制，以获得符合其偏好的AI响应。

Method: 进行了一项形成性调查（n=38）以了解用户对AI生成解释的控制需求，发现标准化与适应性支持之间存在权衡。据此，实现了两种提示中间件方法：动态提示细化控制（Dynamic PRC）和静态提示细化控制（Static PRC）。Dynamic PRC根据用户提示和需求生成上下文特定的UI元素进行提示细化，而Static PRC提供预设的通用细化列表。随后，通过一项受控用户研究（n=16）评估了这两种方法。

Result: 研究结果显示用户偏爱Dynamic PRC方法，因为它提供了更多的控制，降低了提供上下文的障碍，并鼓励了任务的探索和反思。然而，用户仍觉得推理不同生成控件对最终输出的影响具有挑战性。

Conclusion: 动态提示中间件可以通过提供更大的控制并引导用户获得更好的AI响应，从而改善生成式AI工作流程的用户体验。研究结果对未来动态PRC系统的设计具有指导意义，以增强用户对AI响应的控制。

Abstract: Effective prompting of generative AI is challenging for many users, particularly in expressing context for comprehension tasks such as explaining spreadsheet formulas, Python code, and text passages. Prompt middleware aims to address this barrier by assisting in prompt construction, but barriers remain for users in expressing adequate control so that they can receive AI-responses that match their preferences.
  We conduct a formative survey (n=38) investigating user needs for control over AI-generated explanations in comprehension tasks, which uncovers a trade-off between standardized but predictable support for prompting, and adaptive but unpredictable support tailored to the user and task. To explore this trade-off, we implement two prompt middleware approaches: Dynamic Prompt Refinement Control (Dynamic PRC) and Static Prompt Refinement Control (Static PRC). The Dynamic PRC approach generates context-specific UI elements that provide prompt refinements based on the user's prompt and user needs from the AI, while the Static PRC approach offers a preset list of generally applicable refinements.
  We evaluate these two approaches with a controlled user study (n=16) to assess the impact of these approaches on user control of AI responses for crafting better explanations. Results show a preference for the Dynamic PRC approach as it afforded more control, lowered barriers to providing context, and encouraged exploration and reflection of the tasks, but that reasoning about the effects of different generated controls on the final output remains challenging. Drawing on participant feedback, we discuss design implications for future Dynamic PRC systems that enhance user control of AI responses. Our findings suggest that dynamic prompt middleware can improve the user experience of generative AI workflows by affording greater control and guide users to a better AI response.

</details>


### [74] [Subject integration with spreadsheets -- Ignoring education is the greatest risk ever](https://arxiv.org/abs/2409.12975)
*Mária Csernoch,Ádám Gulácsi,Júlia Csernoch*

Main category: cs.HC

TL;DR: 本文在TPACK框架下，提出将数字化融入学科教学（特别是三年级任务，使用电子表格），以实现学校有意义的数字化，弥合信息学与数字素养之间的鸿沟，并培养师生相关技能。


<details>
  <summary>Details</summary>
Motivation: 在TPACK框架下，学科整合被视为在学校引入有意义的数字化和数字化的一个可行方案，旨在通过数字支持的教学、信息学课程的语境化，以及缩小“严肃信息学”与“数字素养”之间的差距。

Method: 本文详细阐述了如何使用电子表格解决三个传统三年级任务，并探讨了教师和学生能够发展哪些技能、能力和计算机科学知识。

Result: 解决方案表明，分析、理解、规划和讨论任务与在电子表格中的操作同样重要。这一过程在培养学生未来就业方面发挥着关键作用，同时也揭示了师生可发展的技能、能力和计算机科学知识。

Conclusion: 将数字工具（如电子表格）融入传统学科任务（如三年级任务）可以实现有意义的数字化，培养关键技能（包括分析和规划能力），弥合信息学与数字素养之间的差距，并为学生的未来就业做好准备。

Abstract: Within the framework of Technological Pedagogical and Content Knowledge, subject integration is one possible solution for the introduction of meaningful digitalization and digitization in schools. This process incorporates that any school subject can be taught with digital support, informatics (computer) classes can be contextualized, and the gap between 'serious informatics' and 'digital literacy' can be minimized. The present paper details how three traditional Grade 3 tasks can be solved in spreadsheets, what skills, competencies, and computer science knowledge of both teachers and students can be developed. The solutions also reveal that analysing, understanding, planning, and discussing tasks is as important as the activity in the spreadsheets, which process plays a crucial role in the preparation of students for their future jobs.

</details>


### [75] [Exploring Higher Education Competencies through Spreadsheet Self-Assessment and Time](https://arxiv.org/abs/2409.12974)
*Maria Csernoch,Judit T. Kiss,Viktor Takács,Domicián Máté*

Main category: cs.HC

TL;DR: 本研究发现，大学生对电子表格能力的自我评估往往不准确，且在纸质任务中表现优于Excel，这挑战了关于数字原住民天生具备技术能力的假设。


<details>
  <summary>Details</summary>
Motivation: 旨在通过自我评估和实际问题解决练习，探讨高等教育学生在电子表格方面的能力和可靠性，并验证数字原住民在Excel表现上是否优于纸质任务的假设。

Method: 通过自我评估和实际问题解决练习（包括纸质和Excel任务）来评估学生的电子表格能力。

Result: 研究发现，学生倾向于不准确地评估自己的电子表格能力；在数字环境中，学生需要两倍的时间才能达到与纸质任务相同的分数；学生在Excel中的表现并未优于纸质任务，这与数字原住民天生具备计算机技能的假设相悖。

Conclusion: 强调了在高等教育背景下，尤其是在技术驱动的学科中，准确的自我评估在数字技能发展和时间管理方面的重要性。

Abstract: The present paper aims to explore higher education students' spreadsheet competencies and reliability through self-assessment and real-world problem-solving practices. Digital natives alleged skills and competences allowed us to hypothesize that students perform better in Excel than on paper, but the findings cannot confirm this hypothesis. However, our results indicate that students tend to inaccurately assess their spreadsheet competencies compared to their actual performance in both paper-based and Excel tasks. It has also be found that students need at least twice as much time to achieve the same high scores in the digital environment as they do on paper. The results violated the widely accepted assumption that digital native students do not need computer science education, since they are born with it. This study highlights the importance of accurate self-assessment in digital skill development and time management within higher education contexts, particularly in technology-driven disciplines.

</details>


### [76] [Data Guards: Challenges and Solutions for Fostering Trust in Data](https://arxiv.org/abs/2407.14042)
*Nicole Sultanum,Dennis Bromley,Michael Correll*

Main category: cs.HC

TL;DR: 该论文调查了数据信任的建立方式，发现数据验证和核查存在普遍需求但缺乏标准，并提出了“数据卫士”来增强对数据制品的信任。


<details>
  <summary>Details</summary>
Motivation: 数据驱动决策面临着数据有效性的威胁，因此数据的使用需要信任或验证。本研究旨在理解这种信任是如何建立的。

Method: 通过对数据制品（如电子表格、图表和仪表板）的生产者和消费者进行一系列访谈。

Result: 发现数据验证和核查存在普遍需求但缺乏现有标准，尤其是在数据消费者中。

Conclusion: 提出了“数据卫士”：一种培养对数据制品信任的方法和工具。

Abstract: From dirty data to intentional deception, there are many threats to the validity of data-driven decisions. Making use of data, especially new or unfamiliar data, therefore requires a degree of trust or verification. How is this trust established? In this paper, we present the results of a series of interviews with both producers and consumers of data artifacts (outputs of data ecosystems like spreadsheets, charts, and dashboards) aimed at understanding strategies and obstacles to building trust in data. We find a recurring need, but lack of existing standards, for data validation and verification, especially among data consumers. We therefore propose a set of data guards: methods and tools for fostering trust in data artifacts.

</details>


### [77] [Smart Portable Computer](https://arxiv.org/abs/2405.05292)
*Niladri Das*

Main category: cs.HC

TL;DR: 在COVID-19大流行期间，学生和工作者难以负担或获得性能不足的传统电脑。本文提出了一种“便携式智能电脑”，它提供与台式机相当的性能，但更紧凑、节能且经济高效，同时支持多种办公和编程任务。


<details>
  <summary>Details</summary>
Motivation: 在COVID-19大流行期间，许多组织转向虚拟平台，学生难以获得价格合理且配置足够的电脑（台式机或笔记本电脑）。传统笔记本电脑对需要移动办公的用户来说也较为笨重。

Method: 提出并开发“便携式智能电脑”。

Result: “便携式智能电脑”具有与传统台式机媲美的速度和性能，但体积紧凑、能效高且成本效益好。它能提供无缝的桌面体验，支持文档编辑、多标签浏览、电子表格管理、演示文稿创建，并支持Python、C、C++等编程语言以及Keil和Xilinx等编译器。

Conclusion: “便携式智能电脑”为用户提供了一个创新、高效且经济的计算解决方案，满足了在疫情期间及以后对便携、高性能和多功能电脑的需求，特别是对程序员而言。

Abstract: Amidst the COVID-19 pandemic, with many organizations, schools, colleges, and universities transitioning to virtual platforms, students encountered difficulties in acquiring PCs such as desktops or laptops. The starting prices, around 15,000 INR, often failed to offer adequate system specifications, posing a challenge for consumers. Additionally, those reliant on laptops for work found the conventional approach cumbersome. Enter the "Portable Smart Computer," a leap into the future of computing. This innovative device boasts speed and performance comparable to traditional desktops but in a compact, energy-efficient, and cost-effective package. It delivers a seamless desktop experience, whether one is editing documents, browsing multiple tabs, managing spreadsheets, or creating presentations. Moreover, it supports programming languages like Python, C, C++, as well as compilers such as Keil and Xilinx, catering to the needs of programmers.

</details>


### [78] ["My toxic trait is thinking I'll remember this": gaps in the learner experience of video tutorials for feature-rich software](https://arxiv.org/abs/2404.07114)
*Ian Drosos,Advait Sarkar,Andrew D. Gordon*

Main category: cs.HC

TL;DR: 本研究探讨了视频教程中阻碍学习的“鸿沟”问题，通过分析用户评论和访谈创作者，提出了鸿沟的理论和分类，并探索了解决这些鸿沟的设计方案。


<details>
  <summary>Details</summary>
Motivation: 视频教程作为一种流行的学习媒介，学习者在观看和实践时常遇到阻碍学习的“鸿沟”问题，本研究旨在识别并理解这些鸿沟。

Method: 收集并分析了来自YouTube、TikTok和Instagram上90个Microsoft Excel视频教程的360条用户评论。对8位有影响力的教程创作者进行了情境访谈，以了解他们观众遇到的鸿沟、创作者的应对策略、创作过程和痛点。最后，向创作者展示了两种设计方案以获取反馈和替代设计思路。

Result: 开发了关于视频教程学习鸿沟的理论和分类法，并识别了它们如何成为学习障碍。获得了关于创作者创作过程和挫折感的见解。

Conclusion: 提出了旨在解决视频教程中学习鸿沟的两种设计方案，并征求创作者的反馈和替代设计想法。

Abstract: Video tutorials are a popular medium for informal and formal learning. However, when learners attempt to view and follow along with these tutorials, they encounter what we call gaps, that is, issues that can prevent learning. We examine the gaps encountered by users of video tutorials for feature-rich software, such as spreadsheets. We develop a theory and taxonomy of such gaps, identifying how they act as barriers to learning, by collecting and analyzing 360 viewer comments from 90 Microsoft Excel video tutorials published by 43 creators across YouTube, TikTok, and Instagram. We conducted contextual interviews with 8 highly influential tutorial creators to investigate the gaps their viewers experience and how they address them. Further, we obtain insights into their creative process and frustrations when creating video tutorials. Finally, we present creators with two designs that aim to address gaps identified in the comment analysis for feedback and alternative design ideas.

</details>


### [79] [Supporting Annotators with Affordances for Efficiently Labeling Conversational Data](https://arxiv.org/abs/2403.07762)
*Austin Z. Henley,David Piorkowski*

Main category: cs.HC

TL;DR: 本文介绍了一种名为 CAL 的新型数据标注界面，旨在解决众包标注耗时且昂贵的问题。用户研究表明，CAL 降低了认知负荷，没有增加任务时间，并且比传统电子表格更容易使用和更受欢迎。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统严重依赖大量正确标注的数据，然而，当前众包标注方法耗时且成本高昂。

Method: 研究设计并实现了一个名为 CAL 的新型数据标注界面，其主要设计特点包括：防止选择不当标签、在需要时指导用户选择合适标签、将标注文档整合到界面中以及提供高效查看历史标签的方法。通过用户研究将 CAL 与标准电子表格进行了比较。

Result: 用户使用 CAL 时报告认知负荷更低，任务时间没有增加，用户认为 CAL 更易于使用，并且用户更喜欢 CAL 而不是电子表格。

Conclusion: CAL 界面能够有效提升数据标注效率和用户体验，相较于传统方法具有显著优势。

Abstract: Without well-labeled ground truth data, machine learning-based systems would not be as ubiquitous as they are today, but these systems rely on substantial amounts of correctly labeled data. Unfortunately, crowdsourced labeling is time consuming and expensive. To address the concerns of effort and tedium, we designed CAL, a novel interface to aid in data labeling. We made several key design decisions for CAL, which include preventing inapt labels from being selected, guiding users in selecting an appropriate label when they need assistance, incorporating labeling documentation into the interface, and providing an efficient means to view previous labels. We implemented a production-quality implementation of CAL and report a user-study evaluation that compares CAL to a standard spreadsheet. Key findings of our study include users using CAL reported lower cognitive load, did not increase task time, users rated CAL to be easier to use, and users preferred CAL over the spreadsheet.

</details>


### [80] [Prompting for Discovery: Flexible Sense-Making for AI Art-Making with Dreamsheets](https://arxiv.org/abs/2310.09985)
*Shm Garanganao Almeda,J. D. Zamfirescu-Pereira,Kyu Won Kim,Pradeep Mani Rathnam,Bjoern Hartmann*

Main category: cs.HC

TL;DR: DreamSheets是一个基于LLM的电子表格界面，通过辅助提示构建和结果显示，帮助用户在文生图模型中探索设计空间，并揭示了用户策略以指导未来界面设计。


<details>
  <summary>Details</summary>
Motivation: 文生图（TTI）模型的设计空间探索面临巨大且不透明的输出空间，以及微小输入调整导致结果差异大的挑战，需要界面支持用户可靠地引导提示空间探索以获得有趣的结果。

Method: 研究提出了设计探针DreamSheets，它是一个基于LLM功能的电子表格界面，用于辅助提示构建和同时显示生成结果。通过初步实验室研究和与五位专家艺术家的长期研究，探索用户工作流。

Result: 研究揭示了用户应对TTI设计空间探索挑战的策略，以及支持这些策略所需的界面功能（如使用文本生成定义局部“探索轴”）。

Conclusion: 将这些洞察提炼成一个UI模型，以指导未来文生图界面的设计。

Abstract: Design space exploration (DSE) for Text-to-Image (TTI) models entails navigating a vast, opaque space of possible image outputs, through a commensurately vast input space of hyperparameters and prompt text. Minor adjustments to prompt input can surface unexpectedly disparate images. How can interfaces support end-users in reliably steering prompt-space explorations towards interesting results? Our design probe, DreamSheets, supports exploration strategies with LLM-based functions for assisted prompt construction and simultaneous display of generated results, hosted in a spreadsheet interface. The flexible layout and novel generative functions enable experimentation with user-defined workflows. Two studies, a preliminary lab study and a longitudinal study with five expert artists, revealed a set of strategies participants use to tackle the challenges of TTI design space exploration, and the interface features required to support them - like using text-generation to define local "axes" of exploration. We distill these insights into a UI mockup to guide future interfaces.

</details>


### [81] [Does Using ChatGPT Result in Human Cognitive Augmentation?](https://arxiv.org/abs/2401.11042)
*Ron Fulbright,Miranda Morrison*

Main category: cs.HC

TL;DR: 该论文调查了使用ChatGPT是否能增强人类认知能力，发现并非总是如此，甚至可能误导用户。


<details>
  <summary>Details</summary>
Motivation: 工具可以增强人类认知能力，而最近的深度学习系统甚至超越了人类。ChatGPT作为一个新的认知系统，其对人类认知增强的影响需要被研究。

Method: 通过两项实验，比较使用ChatGPT和不使用ChatGPT创建的回答。

Result: 使用ChatGPT并不总能带来认知增强，也无法取代人类的判断力、洞察力和评估。在某些任务中，ChatGPT甚至会误导用户，导致负面认知增强。

Conclusion: ChatGPT未能始终增强人类认知能力，有时甚至可能产生负面影响，表明它尚未完全取代人类的判断。

Abstract: Human cognitive performance is enhanced by the use of tools. For example, a human can produce a much greater, and more accurate, volume of mathematical calculation in a unit of time using a calculator or a spreadsheet application on a computer. Such tools have taken over the burden of lower level cognitive grunt work but the human still serves the role of the expert performing higher level thinking and reasoning. Recently, however, unsupervised, deep, machine learning has produced cognitive systems able to outperform humans in several domains. When humans use these tools in a human cog ensemble, the cognitive ability of the human is augmented. In some cases, even non experts can achieve, and even exceed, the performance of experts in a particular domain, synthetic expertise. A new cognitive system, ChatGPT, has burst onto the scene during the past year. This paper investigates human cognitive augmentation due to using ChatGPT by presenting the results of two experiments comparing responses created using ChatGPT with results created not using ChatGPT. We find using ChatGPT does not always result in cognitive augmentation and does not yet replace human judgement, discernment, and evaluation in certain types of tasks. In fact, ChatGPT was observed to result in misleading users resulting in negative cognitive augmentation.

</details>


### [82] [Co-audit: tools to help humans double-check AI-generated content](https://arxiv.org/abs/2310.01297)
*Andrew D. Gordon,Carina Negreanu,José Cambronero,Rasika Chakravarthy,Ian Drosos,Hao Fang,Bhaskar Mitra,Hannah Richardson,Advait Sarkar,Stephanie Simmons,Jack Williams,Ben Zorn*

Main category: cs.HC

TL;DR: 随着AI生成内容复杂性增加，用户难以核验其质量。本文介绍了“协同审计工具”作为一种解决方案，用于帮助用户检查AI输出，并提出了协同审计的原则和研究挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）和其他生成模型生成的内容日益复杂，用户难以有效审计或评估其质量和正确性，特别是在AI生成内容质量重要且错误后果严重的场景（如电子表格计算）。

Method: 本文描述了协同审计工具在由生成模型驱动的电子表格计算中的最新研究，通过工具辅助用户核查AI生成的内容，补充了提示工程技术。

Result: 论文提出了协同审计的初步原则列表。

Conclusion: 强调了协同审计体验对于任何生成式AI应用（在质量重要且错误后果严重的情况下）的必要性，并概述了未来的研究挑战。

Abstract: Users are increasingly being warned to check AI-generated content for correctness. Still, as LLMs (and other generative models) generate more complex output, such as summaries, tables, or code, it becomes harder for the user to audit or evaluate the output for quality or correctness. Hence, we are seeing the emergence of tool-assisted experiences to help the user double-check a piece of AI-generated content. We refer to these as co-audit tools. Co-audit tools complement prompt engineering techniques: one helps the user construct the input prompt, while the other helps them check the output response. As a specific example, this paper describes recent research on co-audit tools for spreadsheet computations powered by generative models. We explain why co-audit experiences are essential for any application of generative AI where quality is important and errors are consequential (as is common in spreadsheet computations). We propose a preliminary list of principles for co-audit, and outline research challenges.

</details>


### [83] ["What It Wants Me To Say": Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models](https://arxiv.org/abs/2304.06597)
*Michael Xieyang Liu,Advait Sarkar,Carina Negreanu,Ben Zorn,Jack Williams,Neil Toronto,Andrew D. Gordon*

Main category: cs.HC

TL;DR: 代码生成大型语言模型在非专业用户中存在抽象匹配难题。本文提出“接地抽象匹配”方法，将生成的代码翻译回系统性自然语言，以提高用户理解和有效使用代码生成模型进行电子表格数据分析的能力。


<details>
  <summary>Details</summary>
Motivation: 代码生成大型语言模型在将自然语言转换为代码时，非专业用户难以学习如何有效指导代码生成，即面临“抽象匹配”的挑战。本文在电子表格数据分析场景下，使用Codex生成Python代码并执行，探讨了这一挑战。

Method: 提出“接地抽象匹配”方法，通过将生成的Python代码翻译回系统化、可预测的自然语言，来弥合抽象鸿沟。通过一项受试者间、有声思维研究（n=24），将此方法与基于既有查询框架原则的非接地替代方案进行比较。

Result: 接地方法提高了最终用户对代码生成模型的范围和能力的理解，以及有效使用该模型所需的语言类型。

Conclusion: 接地抽象匹配方法能有效帮助非专业用户理解代码生成模型的功能范围和所需指令类型，从而更有效地使用代码生成大型语言模型。

Abstract: Code-generating large language models translate natural language into code. However, only a small portion of the infinite space of naturalistic utterances is effective at guiding code generation. For non-expert end-user programmers, learning this is the challenge of abstraction matching. We examine this challenge in the specific context of data analysis in spreadsheets, in a system that maps the users natural language query to Python code using the Codex generator, executes the code, and shows the result. We propose grounded abstraction matching, which bridges the abstraction gap by translating the code back into a systematic and predictable naturalistic utterance. In a between-subjects, think-aloud study (n=24), we compare grounded abstraction matching to an ungrounded alternative based on previously established query framing principles. We find that the grounded approach improves end-users' understanding of the scope and capabilities of the code-generating model, and the kind of language needed to use it effectively.

</details>


### [84] [Team OS's System for Dialogue Robot Competition 2022](https://arxiv.org/abs/2210.09928)
*Yuki Kubo,Ryo Yanagimoto,Hayato Futase,Mikio Nakano,Zhaojie Luo,Kazunori Komatani*

Main category: cs.HC

TL;DR: OSbot是一个为2022年对话机器人竞赛开发的对话机器人系统，其对话流程基于手动状态转换、关键词提取和情感分析结果，并在竞赛预赛中获得第三名。


<details>
  <summary>Details</summary>
Motivation: 开发OSbot对话机器人系统以参加2022年对话机器人竞赛。

Method: 对话流程基于手动描述的状态转换，转换条件利用关键词提取和情感分析结果，并通过电子表格进行管理。关键词提取基于命名实体提取和预定义关键词集。情感分析是基于文本的，使用SVM并利用Hazumi语料库进行训练。系统还通过日志功能进行快速检查和编辑。

Result: 在竞赛预赛中获得第三名。

Conclusion: 本论文描述了OSbot系统的开发及其在对话机器人竞赛中的表现，该系统在预赛中获得了第三名。

Abstract: This paper describes our dialogue robot system, OSbot, developed for Dialogue Robot Competition 2022. The dialogue flow is based on state transitions described manually and the transition conditions use the results of keyword extraction and sentiment analysis. The transitions can be easily viewed and edited by managing them on a spreadsheet. The keyword extraction is based on named entity extraction and our predefined keyword set. The sentiment analysis is text-based and uses SVM, which was trained with the multimodal dialogue corpus Hazumi. We quickly checked and edited a dialogue flow by using a logging function. In the competition's preliminary round, our system ended up in third place.

</details>


### [85] [What is it like to program with artificial intelligence?](https://arxiv.org/abs/2208.06213)
*Advait Sarkar,Andrew D. Gordon,Carina Negreanu,Christian Poelitz,Sruti Srinivasa Ragavan,Ben Zorn*

Main category: cs.HC

TL;DR: 大型语言模型（LLM）辅助编程正在形成一种新的编程范式，具有独特的特性和挑战，特别是对于非专业用户。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM辅助编程与现有编程辅助方式的异同，并将其视为一种新的编程方式；讨论将大型语言模型应用于终端用户编程（特别是编程经验很少或没有的用户的）时可能出现的问题和开放研究挑战。

Method: 作者借鉴了公开的LLM辅助编程经验报告、先前的可用性和设计研究，以及一项用户研究的观察结果，该研究让非专业终端用户程序员使用LLM辅助工具解决电子表格中的数据任务。

Result: LLM辅助编程与编译、结对编程、通过搜索和重用进行编程共享一些特性，但在技术可能性和实践经验上存在根本性差异。它应被视为一种具有自身独特属性和挑战的新编程方式。在将大型语言模型应用于终端用户编程时，尤其对于编程专业知识很少或没有的用户，会出现各种问题和开放研究挑战。

Conclusion: LLM辅助编程代表了一种新的编程范式，具有独特的特点和挑战，尤其是在非专业程序员使用时，需要进一步研究。

Abstract: Large language models, such as OpenAI's codex and Deepmind's AlphaCode, can generate code to solve a variety of problems expressed in natural language. This technology has already been commercialised in at least one widely-used programming editor extension: GitHub Copilot.
  In this paper, we explore how programming with large language models (LLM-assisted programming) is similar to, and differs from, prior conceptualisations of programmer assistance. We draw upon publicly available experience reports of LLM-assisted programming, as well as prior usability and design studies. We find that while LLM-assisted programming shares some properties of compilation, pair programming, and programming via search and reuse, there are fundamental differences both in the technical possibilities as well as the practical experience. Thus, LLM-assisted programming ought to be viewed as a new way of programming with its own distinct properties and challenges.
  Finally, we draw upon observations from a user study in which non-expert end user programmers use LLM-assisted tools for solving data tasks in spreadsheets. We discuss the issues that might arise, and open research challenges, in applying large language models to end-user programming, particularly with users who have little or no programming expertise.

</details>


### [86] [MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization](https://arxiv.org/abs/2209.05739)
*Lu Ying,Xinhuan Shu,Dazhen Deng,Yuchen Yang,Tan Tang,Lingyun Yu,Yingcai Wu*

Main category: cs.HC

TL;DR: 本文提出MetaGlyph，一个自动生成基于隐喻字形可视化（MGV）的系统，它能从电子表格数据中生成可视化，并通过案例、使用场景和专家访谈验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 创建基于隐喻的字形可视化（MGV）具有挑战性，需要深入的数据理解和专业的设计技能。

Method: 首先，对当前MGV的设计进行定性分析，以理解隐喻体现和字形设计。在此基础上，引入了一个通过隐喻图像选择和MGV构建来生成MGV的新框架。具体而言，MetaGlyph根据输入数据语义自动从在线资源中选择带有相应图像的隐喻。然后，它整合了蒙特卡洛树搜索算法，该算法通过结合数据重要性、语义相关性和字形不重叠来探索将视觉元素与数据维度关联的MGV设计。系统还提供编辑反馈，允许用户根据设计偏好自定义MGV。

Result: MetaGlyph系统能够自动生成MGV。通过一组示例、一个使用场景进行演示。

Conclusion: 通过一系列专家访谈验证了其有效性，并允许用户根据设计偏好自定义MGV。

Abstract: Glyph-based visualization achieves an impressive graphic design when associated with comprehensive visual metaphors, which help audiences effectively grasp the conveyed information through revealing data semantics. However, creating such metaphoric glyph-based visualization (MGV) is not an easy task, as it requires not only a deep understanding of data but also professional design skills. This paper proposes MetaGlyph, an automatic system for generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct a qualitative analysis to understand the design of current MGVs from the perspectives of metaphor embodiment and glyph design. Based on the results, we introduce a novel framework for generating MGVs by metaphoric image selection and an MGV construction. Specifically, MetaGlyph automatically selects metaphors with corresponding images from online resources based on the input data semantics. We then integrate a Monte Carlo tree search algorithm that explores the design of an MGV by associating visual elements with data dimensions given the data importance, semantic relevance, and glyph non-overlap. The system also provides editing feedback that allows users to customize the MGVs according to their design preferences. We demonstrate the use of MetaGlyph through a set of examples, one usage scenario, and validate its effectiveness through a series of expert interviews.

</details>


### [87] [PoVRPoint: Authoring Presentations in Mobile Virtual Reality](https://arxiv.org/abs/2201.06337)
*Verena Biener,Travis Gesslein,Daniel Schneider,Felix Kawala,Alexander Otte,Per Ola Kristensson,Michel Pahud,Eyal Ofek,Cuauhtli Campos,Matjaž Kljun,Klen Čopič Pucihar,Jens Grubert*

Main category: cs.HC

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 503 - [{'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Virtual Reality (VR) has the potential to support mobile knowledge workers by complementing traditional input devices with a large three-dimensional output space and spatial input. Previous research on supporting VR knowledge work explored domains such as text entry using physical keyboards and spreadsheet interaction using combined pen and touch input. Inspired by such work, this paper probes the VR design space for authoring presentations in mobile settings. We propose PoVRPoint -- a set of tools coupling pen- and touch-based editing of presentations on mobile devices, such as tablets, with the interaction capabilities afforded by VR. We study the utility of extended display space to, for example, assist users in identifying target slides, supporting spatial manipulation of objects on a slide, creating animations, and facilitating arrangements of multiple, possibly occluded, shapes. Among other things, our results indicate that 1) the wide field of view afforded by VR results in significantly faster target slide identification times compared to a tablet-only interface for visually salient targets; and 2) the three-dimensional view in VR enables significantly faster object reordering in the presence of occlusion compared to two baseline interfaces. A user study further confirmed that the interaction techniques were found to be usable and enjoyable.

</details>


### [88] [Untidy Data: The Unreasonable Effectiveness of Tables](https://arxiv.org/abs/2106.15005)
*Lyn Bartram,Michael Correll,Melanie Tory*

Main category: cs.HC

TL;DR: 该论文通过一项定性研究，探讨了数据工作者如何使用表格进行数据交互和理解。研究发现，数据表格在整个分析过程中都扮演着关键角色，不仅仅是数据准备阶段的工具，更是用户直接操作、重塑和增强数据以进行意义建构的重要方式。论文认为交互式表格本身就是一种重要的可视化形式，其提供的直接数据交互为视觉分析提供了广阔的设计空间，并且更灵活的人机数据交互可以丰富意义建构过程。


<details>
  <summary>Details</summary>
Motivation: 数据表格在传统的意义建构流程中常被视为准备性、繁琐的步骤，用于为更复杂的分析工具准备数据。然而，对于许多数据工作者（非专业分析师或数据科学家）而言，电子表格等表格工具是其信息生态系统的重要组成部分，允许他们以复杂工具中隐藏或抽象的方式与数据进行交互。论文的动机是理解这些数据工作者如何利用表格与数据交互并进行推理。

Method: 本研究采用定性研究方法，调查了数据工作者如何与他们的数据进行交互和推理。

Result: 研究结果表明，数据表格的用途远不止线性分析流程初始阶段的数据清理。用户在整个分析过程中都希望能够查看并“亲手操作”基础数据，对其进行重塑和增强以支持意义建构。他们会在基础数据背景下进行重新组织、标注、分层增加细节以及生成替代方案。这些直接交互和人类可读的表格表示形式，是理解数据含义和如何使用数据的丰富且认知重要的部分。

Conclusion: 论文认为，交互式表格本身就是一种重要的可视化表达方式；它们所提供的直接数据交互为视觉分析提供了富有成效的设计空间；并且，通过比当前视觉分析工具支持的更灵活的人机数据交互，可以丰富意义建构过程。

Abstract: Working with data in table form is usually considered a preparatory and tedious step in the sensemaking pipeline; a way of getting the data ready for more sophisticated visualization and analytical tools. But for many people, spreadsheets -- the quintessential table tool -- remain a critical part of their information ecosystem, allowing them to interact with their data in ways that are hidden or abstracted in more complex tools. This is particularly true for data workers: people who work with data as part of their job but do not identify as professional analysts or data scientists. We report on a qualitative study of how these workers interact with and reason about their data. Our findings show that data tables serve a broader purpose beyond data cleanup at the initial stage of a linear analytic flow: users want to see and "get their hands on" the underlying data throughout the analytics process, reshaping and augmenting it to support sensemaking. They reorganize, mark up, layer on levels of detail, and spawn alternatives within the context of the base data. These direct interactions and human-readable table representations form a rich and cognitively important part of building understanding of what the data mean and what they can do with it. We argue that interactive tables are an important visualization idiom in their own right; that the direct data interaction they afford offers a fertile design space for visual analytics; and that sense making can be enriched by more flexible human-data interaction than is currently supported in visual analytics tools.

</details>


### [89] [Defining and Adopting an End User Computing Policy: A Case Study](https://arxiv.org/abs/1909.00855)
*Roger Turner*

Main category: cs.HC

TL;DR: 本文是一个关于在Wesleyan Assurance Society引入更新的终端用户计算（EUC）政策的案例研究，重点介绍了计划、挑战、克服挑战的方法，以及一个基于复杂性、重要性和控制的EUC风险评估应用的开发。


<details>
  <summary>Details</summary>
Motivation: 终端用户计算（EUC）如果控制不当会带来重大风险，因此需要引入一个更新的政策来管理这些风险。

Method: 本文采用案例研究的方法，概述了引入更新EUC政策的计划，识别并克服了各种挑战。此外，还开发了一个EUC风险评估应用程序，该程序根据复杂性、重要性和控制情况计算风险等级带，并基于风险的方法评估和缓解最高风险。

Result: 成功开发了一个终端用户计算风险评估应用程序，该应用程序能根据应用的复杂性、重要性及其控制情况计算风险等级。政策的引入过程中所遇到的挑战被克服，并且该政策采用了基于风险的方法优先处理并缓解最高风险以获得最快效益。

Conclusion: 通过引入更新的EUC政策并开发一个风险评估应用，可以有效管理和降低终端用户计算所带来的风险，并通过风险导向方法实现快速收益。

Abstract: End User Computing carries significant risks if not well controlled. This paper is a case study of the introduction of an updated End User Computing policy at the Wesleyan Assurance Society. The paper outlines the plan and identifies various challenges. The paper explains how these challenges were overcome. We wrote an End User Computing Risk Assessment Application which calculates a risk rating band based on the Complexity, Materiality and Control (or lack of it) pertaining to any given application and the basis of assessment is given in this paper. The policy uses a risk based approach for assessing and mitigating against the highest risks first and obtaining the quickest benefit.

</details>


### [90] [Calliope: Automatic Visual Data Story Generation from a Spreadsheet](https://arxiv.org/abs/2010.09975)
*Danqing Shi,Xinyue Xu,Fuling Sun,Yang Shi,Nan Cao*

Main category: cs.HC

TL;DR: Calliope是一个新的可视化数据故事生成系统，它能从电子表格自动创建故事，并支持在线编辑。


<details>
  <summary>Details</summary>
Motivation: 可视化数据故事在数据叙事中很有用，但其生成存在技术障碍（如数据分析、可视化、脚本编写），现有工具效率低下且难以使用。

Method: 论文介绍了Calliope系统，它通过逻辑导向的蒙特卡洛树搜索算法，从输入电子表格中自动生成数据事实并按逻辑顺序组织。数据事实的重要性基于信息论衡量，并以图表和自动生成的描述进行可视化。系统还提供在线故事编辑器。

Result: 通过三个示例故事、两次对照实验和对10位领域专家的访谈评估，Calliope系统被证明有利于高效生成可视化数据故事。

Conclusion: Calliope系统能够有效且高效地生成可视化数据故事，解决了现有工具的难题。

Abstract: Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.

</details>


### [91] [Pen-based Interaction with Spreadsheets in Mobile Virtual Reality](https://arxiv.org/abs/2008.04543)
*Travis Gesslein,Verena Biener,Philipp Gagel,Daniel Schneider,Per Ola Kristensson,Eyal Ofek,Michel Pahud,Jens Grubert*

Main category: cs.HC

TL;DR: 该论文提出了一种利用VR头戴设备和笔式输入在平板电脑上增强电子表格交互的工具集，以解决移动知识工作者在有限物理空间中操作电子表格的挑战。


<details>
  <summary>Details</summary>
Motivation: 电子表格在移动设备上交互困难，而VR虽然提供沉浸式大显示空间，但移动工作者的物理交互空间有限，本文旨在弥合这一差距，提升VR环境下电子表格的交互体验。

Method: 提出一套工具集，通过沉浸式VR头戴设备和笔式输入增强平板电脑上的电子表格交互。具体方法包括利用平板周围和前方的空间进行数据和元数据可视化（如扩展显示、调试隐藏依赖），并结合精确的笔式输入和空间感知来高效创建和编辑函数（如屏幕外分层菜单、依赖可视化、眼动和触摸切换标签页）。通过基于视频的在线调查和专家评估来研究其可行性。

Result: 本文提出了一个增强电子表格交互的工具集，并对其可行性进行了研究，结果表明该工具集具有提升人类绩效的潜力。

Conclusion: 通过结合VR的沉浸式显示和笔式输入，该提出的工具集有望显著提高移动知识工作者在VR环境中进行电子表格交互的生产力。

Abstract: Virtual Reality (VR) can enhance the display and interaction of mobile knowledge work and in particular, spreadsheet applications. While spreadsheets are widely used yet are challenging to interact with, especially on mobile devices, using them in VR has not been explored in depth. A special uniqueness of the domain is the contrast between the immersive and large display space afforded by VR, contrasted by the very limited interaction space that may be afforded for the information worker on the go, such as an airplane seat or a small work-space. To close this gap, we present a tool-set for enhancing spreadsheet interaction on tablets using immersive VR headsets and pen-based input. This combination opens up many possibilities for enhancing the productivity for spreadsheet interaction. We propose to use the space around and in front of the tablet for enhanced visualization of spreadsheet data and meta-data. For example, extending sheet display beyond the bounds of the physical screen, or easier debugging by uncovering hidden dependencies between sheet's cells. Combining the precise on-screen input of a pen with spatial sensing around the tablet, we propose tools for the efficient creation and editing of spreadsheets functions such as off-the-screen layered menus, visualization of sheets dependencies, and gaze-and-touch-based switching between spreadsheet tabs. We study the feasibility of the proposed tool-set using a video-based online survey and an expert-based assessment of indicative human performance potential.

</details>


### [92] [EQUS -- helping to see formulae](https://arxiv.org/abs/2007.00003)
*Chris Roast*

Main category: cs.HC

TL;DR: 本文描述并评估了一个名为EQUS的交互式电子表格公式可视化工具的设计、开发和评估，旨在简化复杂数据的理解。


<details>
  <summary>Details</summary>
Motivation: 电子表格公式被广泛用于重要的数值处理和建模，但容易被误解。可视化被认为是简化信息和帮助人们理解复杂数据的一种方法。

Method: 开发过程采用了迭代改进，初期目标受众是青少年学习者，包括重新设计和形成性评估。

Result: 所开发的可视化技术被发现对超出初始目标受众的电子表格用户也普遍适用。EQUS后来被开发成为MS Excel的一个完全集成的插件。

Conclusion: EQUS作为一个交互式电子表格公式可视化工具，成功地简化了公式的理解，并对广泛的用户有效，现已集成到MS Excel中。

Abstract: Visualisation is often presented as a means of simplifying information and helping people understand complex data. In this paper we describe the design, development and evaluation of an interactive visualisation for spreadsheet formulae (EQUS). The work is justified on the grounds that these are widely used tools for significant numerical processing and modeling, yet the formula developed can be easily misunderstood. The development process was one of iterative refinement engaging an initial target audience of mid-teen learners, involving re-design and formative evaluation. The resulting visualisation techniques have been found to be broadly relevant to spreadsheet users beyond the initial target audience. EQUS has since been developed as fully integrated plug-in for MS Excel.

</details>


### [93] [Taggle: Combining Overview and Details in Tabular Data Visualizations](https://arxiv.org/abs/1712.05944)
*Katarina Furmanova,Samuel Gratzl,Holger Stitz,Thomas Zichner,Miroslava Jaresova,Alexander Lex,Marc Streit*

Main category: cs.HC

TL;DR: Taggle 是一种表格可视化技术，用于探索和展示大型复杂表格，它采用以项目为中心的方法，对单个行进行可视化，并引入数据驱动的聚合策略和交互方法。


<details>
  <summary>Details</summary>
Motivation: 大多数表格数据可视化技术侧重于概览，但许多实际分析任务需要调查感兴趣的单个项目，同时将项目与可能很大的表格的其余部分关联起来也很重要。

Method: Taggle 采用以项目为中心、类似电子表格的方法，使用单元格的视觉编码单独可视化源数据中的每一行。同时，Taggle 引入了数据子集的数据驱动聚合，并辅以交互方法，如基于多列的排序以及丰富的数据选择和过滤功能。

Result: 通过一个领域专家对复杂基因组数据分析（用于药物发现）的案例研究，展示了 Taggle 的有效性。

Conclusion: Taggle 提供了一种有效的表格可视化技术，用于探索和展示大型复杂表格，特别适用于以项目为中心的分析和将单个项目与大型数据集关联起来。

Abstract: Most tabular data visualization techniques focus on overviews, yet many practical analysis tasks are concerned with investigating individual items of interest. At the same time, relating an item to the rest of a potentially large table is important. In this work we present Taggle, a tabular visualization technique for exploring and presenting large and complex tables. Taggle takes an item-centric, spreadsheet-like approach, visualizing each row in the source data individually using visual encodings for the cells. At the same time, Taggle introduces data-driven aggregation of data subsets. The aggregation strategy is complemented by interaction methods tailored to answer specific analysis questions, such as sorting based on multiple columns and rich data selection and filtering capabilities. We demonstrate Taggle using a case study conducted by a domain expert on complex genomics data analysis for the purpose of drug discovery.

</details>


### [94] [Are digital natives spreadsheet natives?](https://arxiv.org/abs/1909.00865)
*Maria Csernoch,Piroska Biró*

Main category: cs.HC

TL;DR: 本文报告了对计算机科学一年级学生在电子表格环境中算法技能和知识迁移能力测试的结果。发现学生在解决电子表格问题时遇到严重困难，那些使用基于算法的多级数组公式的学生表现优于使用特定问题内置函数的学生。结论是学生需要由专家教师提供的、基于算法的高数学能力培训。


<details>
  <summary>Details</summary>
Motivation: 本文旨在测试计算机科学一年级学生在电子表格环境中的算法技能和知识迁移能力，并挑战他们作为“数字原住民”无需额外培训的假设。

Method: 对计算机科学一年级学生进行了测试，评估他们在电子表格环境中的算法技能和知识迁移能力。观察并分析了学生在解决问题时遇到的困难，并比较了使用基于算法的多级数组公式与使用特定问题内置函数的两种解决策略的表现。

Result: 学生在解决电子表格问题时表现出严重困难，成绩很低。使用基于算法、多级数组公式的学生表现优于使用特定问题、不相关内置函数的学生。

Conclusion: 学生，无论其出生日期和所属的数字代际如何，都非常需要由专家教师提供的、高数学能力、基于算法的正式培训。

Abstract: The present paper reports the results of testing first year students of Informatics on their algorithmic skills and knowledge transfer abilities in spreadsheet environments. The selection of students plays a crucial role in the project. On the one hand, they have officially finished their spreadsheet training - they know everything - while on the other hand, they do not need any training, since they are digital natives, to whom digital skills are assigned by birth. However, we found that the students had serious difficulties in solving the spreadsheet problems presented: so low were their results that it allowed us to form broad tendencies. Considering computational thinking, algorithmic skills, and knowledge transfer abilities, it is clear that those students performed better who used algorithm-based, multilevel array formulas instead of problem specific, unconnected built-in functions. Furthermore, we can conclude that students, regardless of their birth date and digital generation assigned to them, are in great need of official, high-mathability, algorithm-based training with expert teachers.

</details>


### [95] [Somewhere Around That Number: An Interview Study of How Spreadsheet Users Manage Uncertainty](https://arxiv.org/abs/1905.13072)
*Judith Borghouts,Andrew D. Gordon,Advait Sarkar,Kenton P. O'Hara,Neil Toronto*

Main category: cs.HC

TL;DR: 本研究通过访谈调查了11名电子表格用户如何处理数据不确定性。发现用户处理不确定性的方式受电子表格角色和个人目标影响，现有工具支持不足，用户常采用变通方法。


<details>
  <summary>Details</summary>
Motivation: 电子表格用户经常处理不确定性数据，但这常常导致错误的结论或被忽略。本研究旨在理解用户当前在电子表格中如何遭遇和处理不确定性。

Method: 对来自不同领域的11名电子表格用户进行了访谈研究。

Result: 用户处理不确定性的方式受到电子表格在其工作中的角色以及用户目标（如计算和比较情景、理解不确定性本质、简化呈现给决策者）的影响。现有电子表格工具对此支持有限，用户常使用各种变通方法。

Conclusion: 电子表格目前在支持用户处理数据不确定性方面存在不足，用户有明确的需求和应对策略，这表明需要开发更有效的工具来辅助他们。

Abstract: Spreadsheet users regularly deal with uncertainty in their data, for example due to errors and estimates. While an insight into data uncertainty can help in making better informed decisions, prior research suggests that people often use informal heuristics to reason with probabilities, which leads to incorrect conclusions. Moreover, people often ignore or simplify uncertainty. To understand how people currently encounter and deal with uncertainty in spreadsheets, we conducted an interview study with 11 spreadsheet users from a range of domains. We found that how people deal with uncertainty is influenced by the role the spreadsheet plays in people's work and the user's aims. Spreadsheets are used as a database, template, calculation tool, notepad and exploration tool. In doing so, participants' aims were to compute and compare different scenarios, understand something about the nature of the uncertainty in their situation, and translate the complexity of data uncertainty into simplified presentations to other people, usually decision-makers. Spreadsheets currently provide limited tools to support these aims, and participants had various workarounds.

</details>


### [96] [Characterizing Scalability Issues in Spreadsheet Software using Online Forums](https://arxiv.org/abs/1801.03829)
*Kelly Mack,John Lee,Kevin Chang,Karrie Karahalios,Aditya Parameswaran*

Main category: cs.HC

TL;DR: 本文通过分析Reddit论坛数据，识别并归纳了用户在使用Excel电子表格时遇到的挑战，尤其是在处理大量数据时的问题，并探讨了这对未来电子表格软件设计的影响。


<details>
  <summary>Details</summary>
Motivation: 传统的可用性研究成本高昂且耗时。本文旨在利用在线论坛数据这种新兴且经济高效的方法来理解用户需求，特别关注Excel电子表格用户面临的挑战，因为电子表格虽功能强大但易出错。

Method: 通过抓取Reddit网站上关于Excel的问题和投诉帖子，收集了相关数据集。然后，对用户在使用电子表格软件时遇到的问题，特别是由于数据量大而引起的问题进行了探索和表征。

Result: 研究探索并归纳了用户在使用电子表格软件时遇到的问题，尤其是在处理大量数据时的问题。

Conclusion: 本文讨论了研究结果对下一代电子表格软件设计的启示。

Abstract: In traditional usability studies, researchers talk to users of tools to understand their needs and challenges. Insights gained via such interviews offer context, detail, and background. Due to costs in time and money, we are beginning to see a new form of tool interrogation that prioritizes scale, cost, and breadth by utilizing existing data from online forums. In this case study, we set out to apply this method of using online forum data to a specific issue---challenges that users face with Excel spreadsheets. Spreadsheets are a versatile and powerful processing tool if used properly. However, with versatility and power come errors, from both users and the software, which make using spreadsheets less effective. By scraping posts from the website Reddit, we collected a dataset of questions and complaints about Excel. Specifically, we explored and characterized the issues users were facing with spreadsheet software in general, and in particular, as resulting from a large amount of data in their spreadsheets. We discuss the implications of our findings on the design of next-generation spreadsheet software.

</details>


### [97] [The Reification of an Incorrect and Inappropriate Spreadsheet Model](https://arxiv.org/abs/1801.10249)
*Grenville J. Croll*

Main category: cs.HC

TL;DR: 电子表格中的信息，即使是不正确或不恰当的，也可能获得不应有的可信度、正确性等属性，从而被具象化。


<details>
  <summary>Details</summary>
Motivation: 描述了信息一旦被加载到电子表格中，即使不应有，也会获得可信性、正确性等属性，从而被具象化。文章旨在通过一个案例研究，近距离观察在一个小型非营利组织中，一个明显不正确和不恰当的电子表格模型是如何被具象化的。

Method: 进行了一个案例研究，观察在一个小型非营利组织中，一个明显不正确和不恰当的电子表格模型的具象化过程。

Result: 在一个小型非营利组织中，观察到了一个明显不正确和不恰当的电子表格模型的具象化现象。

Conclusion: 通过案例研究表明，电子表格中的信息即使存在明显错误或不当之处，也可能获得不应有的权威性和真实性，从而被具象化。

Abstract: Once information is loaded into a spreadsheet, it acquires properties that it may not deserve. These properties include believability, correctness, appropriateness, concreteness, integrity, tangibility, objectivity and authority. The information becomes reified. We describe a case study through which we were able to observe at close hand the reification of a demonstrably incorrect and inappropriate spreadsheet model within a small non profit organisation.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [98] [Trapped in Transformative Agreements? A Multifaceted Analysis of >1,000 Contracts](https://arxiv.org/abs/2409.20224)
*Laura Rothfritz,W. Benedikt Schmal,Ulrich Herb*

Main category: cs.DL

TL;DR: 研究机构似乎陷入了转型协议的困境，未能完全过渡到开放获取，反而使传统出版商获得了巨大的市场权力，增加了成本并降低了竞争。


<details>
  <summary>Details</summary>
Motivation: 转型协议在学术出版中普遍存在，ESAC数据库列出了1000多个合同，但这些数据尚未被充分利用进行深入分析。

Method: 通过网络抓取ESAC数据库中每个合同的详细信息，扩展了ESAC提供的概览表格，并结合定性和定量方法对合同特征和TA格局进行深入分析。

Result: 研究机构似乎“被困”在转型协议中，未能成为通向完全开放获取世界的桥梁，反而陷入了混合系统。

Conclusion: 这种局面赋予了传统（非开放获取）出版商巨大的市场权力，提高了进入壁垒，降低了竞争，并增加了图书馆和大学的成本。

Abstract: Transformative agreements between academic publishers and research institutions are ubiquitous. The 'Efficiency and Standards for Article Charges' (ESAC) Initiative lists more than 1,000 contracts in its database. We make use of this unique dataset by web-scraping the details of every contract to substantially expand the overview spreadsheet provided by the ESAC Initiative. Based on that hitherto unused data source, we combine qualitative and quantitative methods to conduct an in-depth analysis of the contract characteristics and the TA landscape. Our analysis demonstrates that research institutions seem to be 'trapped' in transformative agreements. Instead of being a bridge towards a fully Open Access world, academia is stuck in the hybrid system. This endows the legacy (non-Open Access) publishing houses with substantial market power. It raises entry barriers, lowers competition, and increases costs for libraries and universities.

</details>


### [99] [Ensuring Adherence to Standards in Experiment-Related Metadata Entered Via Spreadsheets](https://arxiv.org/abs/2409.08897)
*Martin J. O'Connor,Josef Hardi,Marcos Martínez-Romero,Sowmya Somasundaram,Brendan Honick,Stephen A. Fisher,Ajay Pillai,Mark A. Musen*

Main category: cs.DL

TL;DR: 本文介绍了一种端到端的方法，支持科学家使用电子表格输入元数据，同时确保严格遵守社区元数据标准并提供质量控制。


<details>
  <summary>Details</summary>
Motivation: 尽管有复杂的工具，但研究人员在提供元数据时倾向于使用电子表格，尽管电子表格在确保元数据一致性和符合正式规范方面存在局限性。

Method: 该方法包括可定制的模板、可直接从电子表格访问的受控术语和本体，以及一个交互式网络工具，用于识别和修复基于电子表格的元数据中的错误。

Result: 该方法正在生物医学联盟 HuBMAP 中部署，以定义和收集各种生物检测的元数据。

Conclusion: 本文提出的方法成功地将电子表格的便利性与元数据标准依从性和质量控制的需求结合起来，并在实际项目中得到了应用。

Abstract: Scientists increasingly recognize the importance of providing rich, standards-adherent metadata to describe their experimental results. Despite the availability of sophisticated tools to assist in the process of data annotation, investigators generally seem to prefer to use spreadsheets when supplying metadata, despite the limitations of spreadsheets in ensuring metadata consistency and compliance with formal specifications. In this paper, we describe an end-to-end approach that supports spreadsheet-based entry of metadata, while ensuring rigorous adherence to community-based metadata standards and providing quality control. Our methods employ several key components, including customizable templates that capture metadata standards and that can inform the spreadsheets that investigators use to author metadata, controlled terminologies and ontologies for defining metadata values that can be accessed directly from a spreadsheet, and an interactive Web-based tool that allows users to rapidly identify and fix errors in their spreadsheet-based metadata. We demonstrate how this approach is being deployed in a biomedical consortium known as HuBMAP to define and collect metadata about a wide range of biological assays.

</details>


### [100] [A Comprehensive Approach to Ensuring Quality in Spreadsheet-Based Metadata](https://arxiv.org/abs/2312.09107)
*Martin J. O'Connor,Marcos Martínez-Romero,Mete Ugur Akdogan,Josef Hardi,Mark A. Musen*

Main category: cs.DL

TL;DR: 本文介绍了一种端到端的方法，支持基于电子表格的元数据录入，并提供严格的合规性和质量控制，该方法使用了可定制模板、受控术语和交互式网络工具。


<details>
  <summary>Details</summary>
Motivation: 尽管电子表格在确保合规性和质量方面存在局限性，但科学家们仍然倾向于使用它们来提供元数据。现有工具存在学习曲线陡峭和定制化受限等缺点。

Method: 该方法采用多种关键策略，包括用于定义元数据的可定制模板、对在定义这些模板时使用受控术语的集成支持，以及一个交互式网络工具，允许用户快速识别和修复其提供的基于电子表格的元数据中的错误。

Result: 本文描述的端到端方法支持基于电子表格的元数据录入，同时提供严格的合规性和质量控制。

Conclusion: 该方法正在一个生物医学联盟中部署，用于定义和收集关于科学实验的元数据。

Abstract: While scientists increasingly recognize the importance of metadata in describing their data, spreadsheets remain the preferred tool for supplying this information despite their limitations in ensuring compliance and quality. Various tools have been developed to address these limitations, but they suffer from their own shortcomings, such as steep learning curves and limited customization. In this paper, we describe an end-to-end approach that supports spreadsheet-based entry of metadata while providing rigorous compliance and quality control. Our approach employs several key strategies, including customizable templates for defining metadata, integral support for the use of controlled terminologies when defining these templates, and an interactive Web-based tool that allows users to rapidly identify and fix errors in the spreadsheet-based metadata they supply. We demonstrate how this approach is being deployed in a biomedical consortium to define and collect metadata about scientific experiments.

</details>


### [101] [Towards Semantic Interoperability in Historical Research: Documenting Research Data and Knowledge with Synthesis](https://arxiv.org/abs/2107.13957)
*Pavlos Fafalios,Konstantina Konsolaki,Lida Charami,Kostas Petrakis,Manos Paterakis,Dimitris Angelakis,Yannis Tzitzikas,Chrysoula Bekiari,Martin Doerr*

Main category: cs.DL

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 503 - [{'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: A vast area of research in historical science concerns the documentation and study of artefacts and related evidence. Current practice mostly uses spreadsheets or simple relational databases to organise the information as rows with multiple columns of related attributes. This form offers itself for data analysis and scholarly interpretation, however it also poses problems including i) the difficulty for collaborative but controlled documentation by a large number of users, ii) the lack of representation of the details from which the documented relations are inferred, iii) the difficulty to extend the underlying data structures as well as to combine and integrate data from multiple and diverse information sources, and iv) the limitation to reuse the data beyond the context of a particular research activity. To support historians to cope with these problems, in this paper we describe the Synthesis documentation system and its use by a large number of historians in the context of an ongoing research project in the field of History of Art. The system is Web-based and collaborative, and makes use of existing standards for information documentation and publication (CIDOC-CRM, RDF), focusing on semantic interoperability and the production of data of high value and long-term validity.

</details>


### [102] [FAST CAT: Collaborative Data Entry and Curation for Semantic Interoperability in Digital Humanities](https://arxiv.org/abs/2105.13733)
*Pavlos Fafalios,Kostas Petrakis,Georgios Samaritakis,Korina Doerr,Athina Kritsotaki,Yannis Tzitzikas,Martin Doerr*

Main category: cs.DL

TL;DR: FAST CAT是一个为数字人文和经验研究设计的协作系统，旨在解决传统数据管理工具（如电子表格和关系数据库）的局限性，例如数据依赖性强、细节表示不足以及数据验证困难等问题。该系统通过支持语义互操作性，在一个海事历史项目中得到了应用。


<details>
  <summary>Details</summary>
Motivation: 描述性实证科学（如历史学）在进行量化分析和数据管理时，主要依赖电子表格软件和关系型数据库管理系统。然而，这种做法存在局限性，包括收集到的数据高度依赖初始研究假设，通常对其他研究无用；缺乏对推断出关系所依据的细节的表示；以及难以重新查阅原始数据源进行验证、纠正或改进。

Method: 本文提出了FAST CAT，一个用于数字人文和类似形式实证研究中的辅助数据录入和整理的协作系统。论文描述了相关挑战，以及支持语义互操作性的整体方法。

Result: 论文讨论了FAST CAT在一个名为SeaLiT的欧洲（ERC）海事历史项目中的应用。该项目旨在研究19世纪50年代至20世纪20年代地中海地区汽船引入对经济、社会和人口的影响。

Conclusion: FAST CAT系统旨在解决数字人文和其他经验研究中现有数据管理工具的局限性，通过提供协作式辅助数据录入和整理来支持语义互操作性。

Abstract: Descriptive and empirical sciences, such as History, are the sciences that collect, observe and describe phenomena in order to explain them and draw interpretative conclusions about influences, driving forces and impacts under given circumstances. Spreadsheet software and relational database management systems are still the dominant tools for quantitative analysis and overall data management in these these sciences, allowing researchers to directly analyse the gathered data and perform scholarly interpretation. However, this current practice has a set of limitations, including the high dependency of the collected data on the initial research hypothesis, usually useless for other research, the lack of representation of the details from which the registered relations are inferred, and the difficulty to revisit the original data sources for verification, corrections or improvements. To cope with these problems, in this paper we present FAST CAT, a collaborative system for assistive data entry and curation in Digital Humanities and similar forms of empirical research. We describe the related challenges, the overall methodology we follow for supporting semantic interoperability, and discuss the use of FAST CAT in the context of a European (ERC) project of Maritime History, called SeaLiT, which examines economic, social and demographic impacts of the introduction of steamboats in the Mediterranean area between the 1850s and the 1920s.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [103] [Pivoting the paradigm: the role of spreadsheets in K-12 data science](https://arxiv.org/abs/2506.03232)
*Oren Tirschwell,Nicholas Jon Horton*

Main category: stat.OT

TL;DR: 本文探讨了电子表格工具在K-12教育中培养数据和计算技能的潜力，提出了数据驱动的学习成果，并讨论了实施的挑战和策略。


<details>
  <summary>Details</summary>
Motivation: 电子表格工具在K-12学生和教师中广泛使用，不仅用于数据收集和组织，还能促进数据可视化和互动，有助于培养数据和计算技能。

Method: 本文回顾了K-12数据工具的现有框架，提出了通过将电子表格融入课程可实现的数据驱动学习成果，并探讨了电子表格如何帮助发展数据洞察力和计算流畅性。文中还提供了课堂活动示例，识别了采用的挑战和障碍，提出了教学方法以降低师生学习曲线，并强调了专业发展的重要性。

Result: 论文指出电子表格工具可以有效地培养K-12学生的数据和计算技能，并提出了具体的学习成果。它还识别了推广和深入使用的挑战，并提供了相应的教学策略和专业发展建议。

Conclusion: 电子表格是K-12教育中发展数据科学和STEM学科相关技能的强大工具，但需要合适的教学方法和专业的教师培训来充分发挥其潜力。

Abstract: Spreadsheet tools are widely accessible to and commonly used by K-12 students and teachers. They have an important role in data collection and organization. Beyond data organization, spreadsheets also make data visible and easy to interact with, facilitating student engagement in data exploration and analysis. Though not suitable for all circumstances, spreadsheets can and do help foster data and computing skills for K-12 students. This paper 1) reviews prior frameworks on K-12 data tools; 2) proposes data-driven learning outcomes that can be accomplished by incorporating spreadsheets into the curriculum; and 3) discusses how spreadsheets can help develop data acumen and computational fluency. We provide example class activities, identify challenges and barriers to adoption, suggest pedagogical approaches to ease the learning curve for instructors and students, and discuss the need for professional development to facilitate deeper use of spreadsheets for data science and STEM disciplines.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [104] [LEI2JSON: Schema-based Validation and Conversion of Livestock Event Information](https://arxiv.org/abs/2310.17414)
*Mahir Habib,Muhammad Ashad Kabir,Lihong Zheng*

Main category: eess.SY

TL;DR: 本文介绍了一个名为LEI2JSON的工具，这是一个Google Sheets插件，旨在帮助畜牧生产者将畜牧事件数据标准化为JSON格式，以符合LEI（畜牧事件信息）模式，从而节省时间和资源。


<details>
  <summary>Details</summary>
Motivation: 畜牧生产者在标准化（即转换和验证）畜牧事件数据方面常常需要帮助。

Method: LEI2JSON通过构建带有适当列标题、注释和验证规则的电子表格模板，将电子表格数据转换为JSON格式，并根据LEI模式验证输出。它支持将畜牧事件信息无缝存储在本地或Google Drive中。此外，我们还进行了广泛的实验评估来评估该工具的有效性。

Result: LEI2JSON为畜牧生产者提供了一种有效的数据标准化机制，从而大幅节省了时间和资源。

Conclusion: LEI2JSON工具能够有效地标准化畜牧事件数据，为生产者节省时间和资源。

Abstract: Livestock producers often need help in standardising (i.e., converting and validating) their livestock event data. This article introduces a novel solution, LEI2JSON (Livestock Event Information To JSON). The tool is an add-on for Google Sheets, adhering to the livestock event information (LEI) schema. The core objective of LEI2JSON is to provide livestock producers with an efficient mechanism to standardise their data, leading to substantial savings in time and resources. This is achieved by building the spreadsheet template with the appropriate column headers, notes, and validation rules, converting the spreadsheet data into JSON format, and validating the output against the schema. LEI2JSON facilitates the seamless storage of livestock event information locally or on Google Drive in JSON. Additionally, we have conducted an extensive experimental evaluation to assess the effectiveness of the tool.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [105] [Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!](https://arxiv.org/abs/2309.02110)
*James P. Dilger*

Main category: math.HO

TL;DR: 对Wordle玩家数据分析显示了作弊行为、玩家对起始词的忠诚度以及外部因素对玩家选择的影响。


<details>
  <summary>Details</summary>
Motivation: 探索Wordle玩家的行为模式，特别是关于作弊、起始词选择和受外部因素影响的情况，以提供量化的证据。

Method: 作者收集了2023年5月至8月Wordle玩家第一猜的数据，并通过分析这些数据来推断玩家行为。

Result: A) 每天约有0.2-0.5%的玩家在一次尝试中解开谜题，这暗示有4,000-10,000名玩家通过作弊获取答案。B) 至少1/3的玩家有固定的起始词或循环使用，即使起始词曾作为目标词出现，他们也保持忠诚。C) 2023年8月15日，约30,000名玩家突然改变了他们的起始词，可能是受填字游戏线索的影响。

Conclusion: 本研究提供了关于Wordle作弊行为、玩家起始词选择习惯以及玩家受外部因素影响的量化证据。

Abstract: Wordle is a popular, online word game offered by the New York Times (nytimes.com). Currently there are some 2 million players of the English version worldwide. Players have 6 attempts to guess the daily word (target word) and after each attempt, the player receives color-coded information about the correctness and position of each letter in the guess. After either a successful completion of the puzzle or the final unsuccessful attempt, software can assess the player's luck and skill using Information Theory and can display data for the first, second, ..., sixth guesses of a random sample of all players. Recently, I discovered that the latter data is presented in a format that can easily be copied and pasted into a spreadsheet. I compiled data on Wordle players' first guesses from May 2023 - August 2023 and inferred some interesting information about Wordle players. A) Every day, about 0.2-0.5% of players solve the puzzle in one attempt. Because the odds of guessing the one of 2,315 possible target words at random is 0.043%, this implies that 4,000 - 10,000 players cheat by obtaining the target word outside of playing the game! B) At least 1/3 of the players have a favorite starting word, or cycle through several. And even though players should be aware that target words are never repeated, most players appear to remain loyal to their starting word even after its appearance as a target word. C) On August 15, 2023, about 30,000 players abruptly changed their starting word, presumably based on a crossword puzzle clue! Wordle players can be influenced! This study goes beyond social media postings, surveys, and Google Trends to provide solid, quantitative evidence about cheating in Wordle.

</details>


### [106] [GeoGebra e situações que envolvem modelação numa abordagem STEAM](https://arxiv.org/abs/1907.02099)
*J. M. D. S. Dos Santos,A. P. Silveira,A. E. S. Trocado*

Main category: math.HO

TL;DR: 本文介绍了一套旨在葡萄牙语区数学课堂中，通过GeoGebra软件实施STEAM教学方法的材料，初期面向教师，后续将适应学生，内容涵盖二维和三维几何建模。


<details>
  <summary>Details</summary>
Motivation: 在数学课堂中实施包含GeoGebra交互式数学软件的STEAM方法。

Method: 开发涉及二维和三维几何问题的建模任务，利用GeoGebra的2D、3D、CAS窗口和电子表格等功能进行分析，研究实体和曲面的切割平面。任务设计考虑用户熟练度，并提供脚本支持。

Result: 这些任务允许使用GeoGebra进行几何分析，将数学与其他科学和艺术联系起来，并促进使用和巩固相关数学内容的项目开发。这些任务是葡萄牙语国家GeoGebra培训师课程的一部分，并得到伊比利亚美洲国家教育、科学及文化组织（OEI）的赞助。

Conclusion: 这些任务对伊比利亚地区用户具有吸引力，并将被翻译成西班牙语和英语，以实现全球推广，体现了其在STEAM教育中的价值和潜力。

Abstract: In order to implement a STEAM approach including the use of technology, namely the use of interactive mathematics software GeoGebra, in mathematics classes, in the lusophone space, the materials presented here were conceived, to be implemented in a first phase among teachers. Later, with the necessary adaptations, these tasks will be applied to the students. The tasks deal with modeling situations, in two- and three-dimensional geometric problems, in order to apply GeoGebra software in its analysis to illustrate its capabilities. The different windows of this software are used, namely the 2D and 3D windows, CAS window, spreadsheet and extra two dimensional windows in order to study cutting planes in solids and some surfaces. The tasks are presented so that any user, regardless of the degree of knowledge they have of the software, can follow them, being supported in scripts with some indications of the tools and commands to use. Designed for the teaching and learning of Mathematics, from a STEAM approach, these tasks allow connections with other Sciences and the Arts, and allow the development of projects using and consolidating relevant mathematical contents. These tasks are part of the proposals of activities of the participants of the Training Courses for Trainers in GeoGebra for Portuguese Speaking Countries, which from 2019 have an impact on the STEAM approach. These courses are carried out with the high sponsorship of the Organization of Ibero-American States for Education, Science and Culture (OEI). Given the interest that the tasks have for the users of the Iberian space, as well as their dissemination at a global level, the materials initially developed in Portuguese language will be adapted for Spanish and English speakers.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [107] [Data-Semantics-Aware Recommendation of Diverse Pivot Tables](https://arxiv.org/abs/2507.06171)
*Whanhee Cho,Anna Fariha*

Main category: cs.DB

TL;DR: 该论文介绍了SAGE系统，它是一个数据语义感知的系统，旨在推荐k个预算内多样化的透视表，以克服现有工作中推荐冗余的缺点。SAGE确保每个透视表都具有洞察力、可解释性，并适应用户行为和偏好，同时保证推荐的透视表集合彼此不同。


<details>
  <summary>Details</summary>
Motivation: 数据汇总对于从大型数据集中发现洞察至关重要。尽管电子表格中的透视表提供了一种方便的数据汇总方式，但识别能产生有用透视表的属性组合仍然是一个挑战，尤其是在高维数据集中。现有工作未能充分解决透视表多样化问题，导致推荐冗余。

Method: 本文提出了SAGE系统，它包含两个关键技术贡献：1) 一个数据语义感知模型，用于衡量单个透视表的实用性和透视表集合的多样性；2) 一个可扩展的贪婪算法，通过利用数据语义显著减少组合搜索空间，从而高效选择一组具有高实用性的多样化透视表。

Result: 在三个真实世界数据集上的广泛实验表明，SAGE优于替代方法，并且可以有效地扩展以适应高维数据集。此外，本文还通过几个案例研究强调了SAGE在定性方面优于商业软件和大型语言模型（LLMs）的有效性。

Conclusion: SAGE系统通过引入数据语义感知模型和可扩展的贪婪算法，解决了高维数据集中透视表推荐的挑战，实现了具有洞察力、可解释性、用户适应性且多样化的透视表推荐，显著优于现有方法和商业工具。

Abstract: Data summarization is essential to discover insights from large datasets. In a spreadsheets, pivot tables offer a convenient way to summarize tabular data by computing aggregates over some attributes, grouped by others. However, identifying attribute combinations that will result in useful pivot tables remains a challenge, especially for high-dimensional datasets. We formalize the problem of automatically recommending insightful and interpretable pivot tables, eliminating the tedious manual process. A crucial aspect of recommending a set of pivot tables is to diversify them. Traditional works inadequately address the table-diversification problem, which leads us to consider the problem of pivot table diversification.
  We present SAGE, a data-semantics-aware system for recommending k-budgeted diverse pivot tables, overcoming the shortcomings of prior work for top-k recommendations that cause redundancy. SAGE ensures that each pivot table is insightful, interpretable, and adaptive to the user's actions and preferences, while also guaranteeing that the set of pivot tables are different from each other, offering a diverse recommendation. We make two key technical contributions: (1) a data-semantics-aware model to measure the utility of a single pivot table and the diversity of a set of pivot tables, and (2) a scalable greedy algorithm that can efficiently select a set of diverse pivot tables of high utility, by leveraging data semantics to significantly reduce the combinatorial search space. Our extensive experiments on three real-world datasets show that SAGE outperforms alternative approaches, and efficiently scales to accommodate high-dimensional datasets. Additionally, we present several case studies to highlight SAGE's qualitative effectiveness over commercial software and Large Language Models (LLMs).

</details>


### [108] [A System and Benchmark for LLM-based Q&A on Heterogeneous Data](https://arxiv.org/abs/2409.05735)
*Achille Fokoue,Srideepika Jayaraman,Elham Khabiri,Jeffrey O. Kephart,Yingjie Li,Dhruv Shah,Youssef Drissi,Fenno F. Heath,Anu Bhamidipaty,Fateh A. Tipu,Robert J. Baseman*

Main category: cs.DB

TL;DR: siwarex平台通过自然语言实现了对数据库和API的无缝访问，解决了工业环境中数据源异构性问题，并在扩展的Spider数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 在工业环境中，用户需要通过自然语言访问结构化数据，但现有Text-to-SQL应用难以处理数据源的异构性。

Method: 引入siwarex平台以实现对数据库和API的自然语言访问，并通过扩展Spider数据集，用API替换部分表格进行基准测试。

Result: siwarex平台能很好地应对数据源异构性。

Conclusion: siwarex平台有效地解决了工业环境中数据源异构性问题，并提供了一个新的基准测试数据集。

Abstract: In many industrial settings, users wish to ask questions whose answers may be found in structured data sources such as a spreadsheets, databases, APIs, or combinations thereof. Often, the user doesn't know how to identify or access the right data source. This problem is compounded even further if multiple (and potentially siloed) data sources must be assembled to derive the answer. Recently, various Text-to-SQL applications that leverage Large Language Models (LLMs) have addressed some of these problems by enabling users to ask questions in natural language. However, these applications remain impractical in realistic industrial settings because they fail to cope with the data source heterogeneity that typifies such environments. In this paper, we address heterogeneity by introducing the siwarex platform, which enables seamless natural language access to both databases and APIs. To demonstrate the effectiveness of siwarex, we extend the popular Spider dataset and benchmark by replacing some of its tables by data retrieval APIs. We find that siwarex does a good job of coping with data source heterogeneity. Our modified Spider benchmark will soon be available to the research community

</details>


### [109] [Auditable and reusable crosswalks for fast, scaled integration of scattered tabular data](https://arxiv.org/abs/2409.01517)
*Gavin Chait*

Main category: cs.DB

TL;DR: 该论文介绍了一个开源的策展工具包，旨在生成结构良好且可互操作的数据。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在解决将复杂且分散的表格数据重构为结构良好且可互操作的数据的问题。

Method: 该方法将数据策展分为离散的组件，以模式为中心，对复杂和分散的表格数据进行可审计的重构，使其符合目标模式。转换被捕获为描述模式到模式映射的高级顺序脚本。该工具包可作为Python包和“无代码”可视化Web应用程序使用。

Result: 数据被成功转换，任何符合模式定义的数据都可以使用交叉引用进行重构。通过一个纵向研究的视觉示例进行了展示，该研究将来自数百个地方议会的零散源数据整合到一个数据库中。

Conclusion: 该工具包提供了一种有效的方法，可以将复杂、分散的表格数据重构为结构良好且可互操作的格式，从而提高数据的可用性和集成度。

Abstract: This paper presents an open-source curatorial toolkit intended to produce well-structured and interoperable data. Curation is divided into discrete components, with a schema-centric focus for auditable restructuring of complex and scattered tabular data to conform to a destination schema. Task separation allows development of software and analysis without source data being present. Transformations are captured as high-level sequential scripts describing schema-to-schema mappings, reducing complexity and resource requirements. Ultimately, data are transformed, but the objective is that any data meeting a schema definition can be restructured using a crosswalk. The toolkit is available both as a Python package, and as a 'no-code' visual web application. A visual example is presented, derived from a longitudinal study where scattered source data from hundreds of local councils are integrated into a single database.

</details>


### [110] [Auto-Formula: Recommend Formulas in Spreadsheets using Contrastive Learning for Table Representations](https://arxiv.org/abs/2404.12608)
*Sibei Chen,Yeye He,Weiwei Cui,Ju Fan,Song Ge,Haidong Zhang,Dongmei Zhang,Surajit Chaudhuri*

Main category: cs.DB

TL;DR: 本文介绍了一个名为Auto-Formula的系统，它利用对比学习技术，从组织内相似的电子表格中学习并预测用户想要在目标单元格中编写的公式，从而解决了非技术用户编写复杂公式的挑战。


<details>
  <summary>Details</summary>
Motivation: 电子表格是流行的终端用户编程工具，但非技术用户编写复杂公式仍具挑战性，需要理解复杂的公式语法。

Method: 开发了一个Auto-Formula系统，该系统通过学习和改编来自组织内相似电子表格中已有的公式来预测用户想要编写的公式，并采用了受计算机视觉中“相似人脸识别”启发的对比学习技术。

Result: 在从真实企业电子表格中提取的2000多个测试公式上进行了广泛评估，结果表明Auto-Formula比其他替代方案更有效。

Conclusion: Auto-Formula系统通过利用相似电子表格中的现有公式和对比学习技术，成功地提高了非技术用户编写复杂电子表格公式的效率和准确性。

Abstract: Spreadsheets are widely recognized as the most popular end-user programming tools, which blend the power of formula-based computation, with an intuitive table-based interface. Today, spreadsheets are used by billions of users to manipulate tables, most of whom are neither database experts nor professional programmers.
  Despite the success of spreadsheets, authoring complex formulas remains challenging, as non-technical users need to look up and understand non-trivial formula syntax. To address this pain point, we leverage the observation that there is often an abundance of similar-looking spreadsheets in the same organization, which not only have similar data, but also share similar computation logic encoded as formulas. We develop an Auto-Formula system that can accurately predict formulas that users want to author in a target spreadsheet cell, by learning and adapting formulas that already exist in similar spreadsheets, using contrastive-learning techniques inspired by "similar-face recognition" from compute vision.
  Extensive evaluations on over 2K test formulas extracted from real enterprise spreadsheets show the effectiveness of Auto-Formula over alternatives. Our benchmark data is available at https://github.com/microsoft/Auto-Formula to facilitate future research.

</details>


### [111] [Facilitating Digital Agriculture with Simple Databases](https://arxiv.org/abs/2312.06517)
*Dennis Buckmaster,Sami Basir,Hanae Sakata*

Main category: cs.DB

TL;DR: 为农业从业者提供开源的数据库模板（基于Airtable），帮助他们整理和管理农业运营数据，即使只有基本的电子表格技能也能使用。


<details>
  <summary>Details</summary>
Motivation: 帮助农业从业者，特别是那些电子表格技能有限的农民，更好地管理数据并融入数字农业原则。

Method: 提供多个结构良好的私有数据库模板（基于Airtable），这些模板使用简单的数据验证表单，并提供了一个解释如何构建活动记录数据库的录制研讨会。

Result: 生成整洁、机器和人类可读、可编辑、可导出的运营数据，这些数据可以促进物流、提供上下文元数据并改进企业分析。

Conclusion: 这些资源有助于通过推广和结构化教育计划，促进数字农业原则的普及和应用。

Abstract: As an on-ramp to databases, we offer several well-structured private database templates as open source resources for agriculturalists, particularly those with modest spreadsheet skills. These farmer-oriented Air table databases use simple data-validated forms, with the look and feel of a customized app, to yield operational data that is tidy, machine- and human-readable, editable, and exportable for analysis in other software. Such data can facilitate logistics, provide contextual metadata, and improve enterprise analysis. A recorded workshop explaining how to build a database for activity records is presented. These resources may facilitate infusion of digital agriculture principles through Extension and structured educational programming.

</details>


### [112] [Streamlining Knowledge Graph Construction with a façade: The SPARQL Anything project](https://arxiv.org/abs/2310.16700)
*Luigi Asprino,Enrico Daga,Justin Dowdy,Paul Mulholland,Aldo Gangemi,Marco Ratta*

Main category: cs.DB

TL;DR: 本文介绍了SPARQL Anything，一个为知识工程师设计的数据集成框架，它通过SPARQL查询异构资源，支持多种文件格式，并具有灵活的API查询和数据转换功能。


<details>
  <summary>Details</summary>
Motivation: 知识图谱构建的最新研究提出了门面设计理念。本文旨在描述SPARQL Anything系统的设计理念和软件架构，并评估其与现有替代方案相比对用户的价值。

Method: 本文描述了SPARQL Anything系统的设计原理和软件架构。通过引用一系列可复用的、真实世界的应用场景，并通过社区调查和行业实地报告，评估了其设计假设对用户的价值，并与替代方案进行了比较。

Result: 本文详细阐述了SPARQL Anything的设计和架构，展示了其将异构资源作为RDF查询、支持多种文件格式、灵活的Web API查询、参数化查询和复杂转换管道等特性。此外，还提供了丰富的真实世界应用场景参考，并通过社区调查和行业报告验证了其对用户的价值。

Conclusion: 本文介绍了一个强大的数据集成框架SPARQL Anything，它利用门面设计理念进行知识图谱构建，为查询各种数据源提供了灵活全面的RDF解决方案。其价值得到了社区反馈和行业报告的支持。

Abstract: What should a data integration framework for knowledge engineers look like? Recent research on Knowledge Graph construction proposes the design of a façade, a notion borrowed from object-oriented software engineering. This idea is applied to SPARQL Anything, a system that allows querying heterogeneous resources as-if they were in RDF, in plain SPARQL 1.1, by overloading the SERVICE clause. SPARQL Anything supports a wide variety of file formats, from popular ones (CSV, JSON, XML, Spreadsheets) to others that are not supported by alternative solutions (Markdown, YAML, DOCx, Bibtex). Features include querying Web APIs with high flexibility, parametrised queries, and chaining multiple transformations into complex pipelines. In this paper, we describe the design rationale and software architecture of the SPARQL Anything system. We provide references to an extensive set of reusable, real-world scenarios from various application domains. We report on the value-to-users of the founding assumptions of its design, compared to alternative solutions through a community survey and a field report from the industry.

</details>


### [113] [DataVinci: Learning Syntactic and Semantic String Repairs](https://arxiv.org/abs/2308.10922)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Gust Verbruggen*

Main category: cs.DB

TL;DR: DataVinci是一个全自动的字符串数据错误检测和修复系统，它通过学习正则表达式模式并结合LLM处理语义部分来修复混合的句法和语义错误，在错误检测和修复方面优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 字符串数据在实际数据集中非常常见（Excel中占67.6%），但现有的字符串数据清洗系统存在局限性：它们通常仅限于错误检测，需要用户提供标注、示例或约束，或者只关注句法或语义错误，忽略了字符串常同时包含这两种子串。

Method: DataVinci是一个完全无监督的系统。它通过学习覆盖大多数值的正则表达式模式来识别不符合模式的数据作为错误，并基于这些模式和从其他列学习到的约束自动推导编辑。为了处理同时包含句法和语义子串的情况，DataVinci使用LLM在学习模式和推导编辑之前对语义部分进行抽象和具体化。对于无法形成多数模式的数据，DataVinci利用现有程序的执行信息来识别和纠正数据修复。

Result: DataVinci在4个现有和新基准上进行评估时，在错误检测和修复方面均优于7个基线系统。

Conclusion: DataVinci在无监督字符串数据错误检测和修复方面取得了显著进展，它能够有效处理包括混合句法和语义组件在内的复杂字符串错误，无需用户交互。

Abstract: String data is common in real-world datasets: 67.6% of values in a sample of 1.8 million real Excel spreadsheets from the web were represented as text. Systems that successfully clean such string data can have a significant impact on real users. While prior work has explored errors in string data, proposed approaches have often been limited to error detection or require that the user provide annotations, examples, or constraints to fix the errors. Furthermore, these systems have focused independently on syntactic errors or semantic errors in strings, but ignore that strings often contain both syntactic and semantic substrings. We introduce DataVinci, a fully unsupervised string data error detection and repair system. DataVinci learns regular-expression-based patterns that cover a majority of values in a column and reports values that do not satisfy such patterns as data errors. DataVinci can automatically derive edits to the data error based on the majority patterns and constraints learned over other columns without the need for further user interaction. To handle strings with both syntactic and semantic substrings, DataVinci uses an LLM to abstract (and re-concretize) portions of strings that are semantic prior to learning majority patterns and deriving edits. Because not all data can result in majority patterns, DataVinci leverages execution information from an existing program (which reads the target data) to identify and correct data repairs that would not otherwise be identified. DataVinci outperforms 7 baselines on both error detection and repair when evaluated on 4 existing and new benchmarks.

</details>


### [114] [Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples](https://arxiv.org/abs/2307.14565)
*Peng Li,Yeye He,Cong Yan,Yue Wang,Surajit Chaudhuri*

Main category: cs.DB

TL;DR: 该论文介绍了一个名为Auto-Tables的系统，它可以自动将非关系型表格转换为标准关系型表格，以便进行SQL查询分析，从而解决了用户在数据准备过程中遇到的一个痛点。


<details>
  <summary>Details</summary>
Motivation: 在现实世界中，超过30%的表格（如电子表格和网络表格）不符合关系型标准。为了使用基于SQL的分析工具查询这些表格，需要进行复杂的表格重构转换，而手动编程这些转换对技术和非技术用户来说都是一个难题。

Method: 开发了一个名为Auto-Tables的系统，该系统能够自动合成多步骤转换流程（使用Python或其他语言），将非关系型表格转换为标准关系型形式，用于下游分析。该系统还收集了244个来自用户电子表格和在线论坛的真实测试案例，构建了一个全面的基准。

Result: Auto-Tables系统能够成功地为超过70%的测试案例合成转换，且速度快，无需用户输入。

Conclusion: Auto-Tables是一个有效的工具，可以帮助技术和非技术用户准备数据进行分析，通过自动化非关系型表格到关系型表格的转换，解决了数据准备的痛点。

Abstract: Relational tables, where each row corresponds to an entity and each column corresponds to an attribute, have been the standard for tables in relational databases. However, such a standard cannot be taken for granted when dealing with tables "in the wild". Our survey of real spreadsheet-tables and web-tables shows that over 30% of such tables do not conform to the relational standard, for which complex table-restructuring transformations are needed before these tables can be queried easily using SQL-based analytics tools. Unfortunately, the required transformations are non-trivial to program, which has become a substantial pain point for technical and non-technical users alike, as evidenced by large numbers of forum questions in places like StackOverflow and Excel/Power-BI/Tableau forums.
  We develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations (in Python or other languages), to transform non-relational tables into standard relational forms for downstream analytics, obviating the need for users to manually program transformations. We compile an extensive benchmark for this new task, by collecting 244 real test cases from user spreadsheets and online forums. Our evaluation suggests that Auto-Tables can successfully synthesize transformations for over 70% of test cases at interactive speeds, without requiring any input from users, making this an effective tool for both technical and non-technical users to prepare data for analytics.

</details>


### [115] [Efficient and Compact Spreadsheet Formula Graphs](https://arxiv.org/abs/2302.05482)
*Dixin Tang,Fanchao Chen,Christopher De Leon,Tana Wattanawaroon,Jeaseok Yun,Srinivasan Seshadri,Aditya G. Parameswaran*

Main category: cs.DB

TL;DR: TACO是一个框架，用于高效压缩电子表格中的公式图，显著减少查询和维护时间，从而提升交互性。它利用了表格局部性原理。


<details>
  <summary>Details</summary>
Motivation: 电子表格中的公式图通常庞大且复杂，导致查询和维护耗时，降低了系统的交互性。

Method: TACO框架利用了表格的“表格局部性”特性（即相邻单元格可能具有相似的公式结构）。它基于四种表格局部性模式开发了算法，用于压缩公式图，可以直接查询压缩后的图而无需解压缩，并支持在更新时增量维护图。

Result: TACO能显著减少公式图的大小。在查询公式图方面，与基线实现和商业电子表格系统相比，TACO的速度提升分别高达34,972倍和632倍。

Conclusion: TACO通过高效压缩和管理公式图，显著提升了电子表格系统的查询和维护效率及交互性。

Abstract: Spreadsheets are one of the most popular data analysis tools, wherein users can express computation as formulae alongside data. The ensuing dependencies are tracked as formula graphs. Efficiently querying and maintaining these formula graphs is critical for interactivity across multiple settings. Unfortunately, formula graphs are often large and complex such that querying and maintaining them is time-consuming, reducing interactivity. We propose TACO, a framework for efficiently compressing formula graphs, thereby reducing the time for querying and maintenance. The efficiency of TACO stems from a key spreadsheet property: tabular locality, which means that cells close to each other are likely to have similar formula structures. We leverage four such tabular locality-based patterns and develop algorithms for compressing formula graphs using these patterns, directly querying the compressed graph without decompression, and incrementally maintaining the graph during updates. We integrate TACO into an open-source spreadsheet system and show that TACO can significantly reduce formula graph sizes. For querying formula graphs, the speedups of TACO over a baseline implemented in our framework and a commercial spreadsheet system are up to 34,972x and 632x, respectively.

</details>


### [116] [Sigma Workbook: A Spreadsheet for Cloud Data Warehouses](https://arxiv.org/abs/2204.03128)
*James Gale,Max Seiden,Deepanshu Utkarsh,Jason Frantz,Rob Woollen,Çağatay Demiralp*

Main category: cs.DB

TL;DR: Sigma Workbook是一个交互式系统，它通过类似电子表格的界面，让业务用户能够在大规模云数据仓库中轻松进行可视化数据分析，并自动生成和执行SQL查询。


<details>
  <summary>Details</summary>
Motivation: 现有的云数据仓库数据分析工具在即席转换方面能力有限，或者对业务用户来说难以使用。

Method: Sigma Workbook提供了一个易于访问的类似电子表格的界面，支持通过直接操作进行分析。它根据用户交互动态构建匹配的SQL查询，并直接在云数据仓库上执行这些查询。

Result: 通过三个真实用例（群组分析、会话化和数据增强）展示了Sigma Workbook。

Conclusion: Sigma Workbook易于使用、可扩展且富有表达力。

Abstract: Cloud data warehouses (CDWs) bring large-scale data and compute power closer to users in enterprises. However, existing tools for analyzing data in CDWs are either limited in ad-hoc transformations or difficult to use for business users. Here we introduce Sigma Workbook, a new interactive system that enables business users to easily perform a visual analysis of data in CDWs at scale. For this, Sigma Workbook provides an accessible spreadsheet-like interface for analysis through direct manipulation. Sigma Workbook dynamically constructs matching SQL queries from user interactions, building on the versatility and expressivity of SQL. Constructed queries are directly executed on CDWs, leveraging the superior characteristics of the new generation CDWs, including scalability. We demonstrate Sigma Workbook through 3 real-life use cases -- cohort analysis, sessionization, and data augmentation -- and underline Workbook's ease of use, scalability, and expressivity.

</details>


### [117] [Efficient Specialized Spreadsheet Parsing for Data Science](https://arxiv.org/abs/2202.13189)
*Felix Henze,Haralampos Gavriilidis,Eleni Tzirita Zacharatou,Volker Markl*

Main category: cs.DB

TL;DR: 在商品系统上进行高级分析的电子表格加载速度慢且占用内存大。本文介绍了一种通过将解压缩与解析紧密结合，优化解析例程并利用并行性来最小化内存使用和运行时间的新型解析器。与现有技术相比，它实现了高达3倍的加载速度提升，并减少了40倍的内存使用。


<details>
  <summary>Details</summary>
Motivation: 当前的电子表格加载方法在商品系统上进行高级分析时，存在运行时间长或内存占用大的问题，阻碍了数据探索。

Method: 提出了一种新型解析器，通过将解压缩和解析紧密耦合来最小化内存使用。通过优化的电子表格特定解析例程和并行化来减少运行时间。实现了一个将Excel电子表格加载到R环境的原型。

Result: 该新方法比现有技术快3倍，内存消耗减少40倍。

Conclusion: 所提出的新型解析器通过显著减少运行时间和内存使用，使得在商品系统上进行电子表格加载变得实用。

Abstract: Spreadsheets are widely used for data exploration. Since spreadsheet systems have limited capabilities, users often need to load spreadsheets to other data science environments to perform advanced analytics. However, current approaches for spreadsheet loading suffer from either high runtime or memory usage, which hinders data exploration on commodity systems. To make spreasheet loading practical on commodity systems, we introduce a novel parser that minimizes memory usage by tightly coupling decompression and parsing. Furthermore, to reduce the runtime, we introduce optimized spreadsheet-specific parsing routines and employ parallelism. To evaluate our approach, we implement a prototype for loading Excel spreadsheets into R environments. Our evaluation shows that our novel approach is up to 3x faster while consuming up to 40x less memory than state-of-the-art approaches.

</details>


### [118] [Spread2RML: Constructing Knowledge Graphs by Predicting RML Mappings on Messy Spreadsheets](https://arxiv.org/abs/2110.12829)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: Spread2RML通过预测RML映射来简化杂乱电子表格的数据到RDF知识图谱的转换。


<details>
  <summary>Details</summary>
Motivation: RDF映射语言（RML）允许将半结构化数据映射到RDF知识图谱，但电子表格由于其复杂和混乱的数据模型，映射创建耗时。

Method: Spread2RML使用一组可扩展的RML对象映射模板，根据启发式方法为每个列应用这些模板。

Result: 在从高度混乱的合成数据到整洁度较低的data.gov电子表格等三个数据集上，取得了初步有希望的结果，特别是该方法具有完全自动化处理相当混乱数据的能力。

Conclusion: Spread2RML在自动化杂乱电子表格的RML映射创建方面显示出前景，有望显著减少人工工作量。

Abstract: The RDF Mapping Language (RML) allows to map semi-structured data to RDF knowledge graphs. Besides CSV, JSON and XML, this also includes the mapping of spreadsheet tables. Since spreadsheets have a complex data model and can become rather messy, their mapping creation tends to be very time consuming. In order to reduce such efforts, this paper presents Spread2RML which predicts RML mappings on messy spreadsheets. This is done with an extensible set of RML object map templates which are applied for each column based on heuristics. In our evaluation, three datasets are used ranging from very messy synthetic data to spreadsheets from data.gov which are less messy. We obtained first promising results especially with regard to our approach being fully automatic and dealing with rather messy data.

</details>


### [119] [Supercomputing Enabled Deployable Analytics for Disaster Response](https://arxiv.org/abs/2108.11525)
*Kaira Samuel,Jeremy Kepner,Michael Jones,Lauren Milechin,Vijay Gadepally,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Michael Houle,Anna Klein,Victor Lopez,Julie Mullen,Andrew Prout,Albert Reuther,Antonio Rosa,Sid Samsi,Charles Yee,Peter Michaleas*

Main category: cs.DB

TL;DR: 为应对疫情，本文提出了一种为急救人员和一线工作人员提供高级分析的方法，通过预先计算好包含地理空间人口普查数据的文件（Google Earth和Microsoft Excel格式），这些文件可以在没有网络和额外软件的情况下运行，从而帮助他们快速分析人口数据以更好地应对紧急情况。


<details>
  <summary>Details</summary>
Motivation: 急救人员和其他一线工作人员可以受益于高级分析，但有限的网络访问和软件安全要求阻碍了使用行业中常见的基于云的微服务分析平台。

Method: 该方法通过预先计算各种分析结果为文件，这些文件可以使用标准预装软件，不需要网络访问或额外软件，并且可以在各种传统硬件上运行。具体而言，使用MIT SuperCloud处理数据，生成数千个代表高级分析的Google Earth和Microsoft Excel文件，其中包含按县分类的每个普查区的人口普查数据（总人口、15岁以下人口、65岁以上人口、中位年龄）。Excel文件包含经纬度坐标单位转换功能，Google Earth文件探索了多种人口密度颜色图。

Result: 利用MIT SuperCloud的数百个核心，可以在几分钟内生成新的分析结果，并快速映射人口普查数据，生成了数千个Google Earth和Microsoft Excel文件。

Conclusion: 通过Google Earth和Microsoft Excel快速映射人口普查数据，有可能为应急响应人员提供一个强大的工具，以提高应急准备能力。

Abstract: First responders and other forward deployed essential workers can benefit from advanced analytics. Limited network access and software security requirements prevent the usage of standard cloud based microservice analytic platforms that are typically used in industry. One solution is to precompute a wide range of analytics as files that can be used with standard preinstalled software that does not require network access or additional software and can run on a wide range of legacy hardware. In response to the COVID-19 pandemic, this approach was tested for providing geo-spatial census data to allow quick analysis of demographic data for better responding to emergencies. These data were processed using the MIT SuperCloud to create several thousand Google Earth and Microsoft Excel files representative of many advanced analytics. The fast mapping of census data using Google Earth and Microsoft Excel has the potential to give emergency responders a powerful tool to improve emergency preparedness. Our approach displays relevant census data (total population, population under 15, population over 65, median age) per census block, sorted by county, through a Microsoft Excel spreadsheet (xlsx file) and Google Earth map (kml file). The spreadsheet interface includes features that allow users to convert between different longitude and latitude coordinate units. For the Google Earth files, a variety of absolute and relative colors maps of population density have been explored to provide an intuitive and meaningful interface. Using several hundred cores on the MIT SuperCloud, new analytics can be generated in a few minutes.

</details>


### [120] [Table2Charts: Recommending Charts by Learning Shared Table Representations](https://arxiv.org/abs/2008.11015)
*Mengyu Zhou,Qingtao Li,Xinyi He,Yuejiang Li,Yibo Liu,Wei Ji,Shi Han,Yining Chen,Daxin Jiang,Dongmei Zhang*

Main category: cs.DB

TL;DR: Table2Charts是一个基于深度Q学习的框架，结合复制机制和启发式搜索，用于从多维数据集中推荐常用图表，其性能优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 人们通常会创建不同类型的图表来探索多维数据集。然而，推荐常用图表面临效率、数据不平衡和表格上下文等挑战。

Method: Table2Charts框架采用深度Q学习、复制机制和启发式搜索，从大量的（表格，图表）对语料库中学习常见模式，执行表格到序列的生成，其中每个序列都遵循一个图表模板。

Result: Table2Charts能够学习表格字段的共享表示，使得不同图表类型的推荐任务可以相互增强。在包含16.5万张表格和26.6万张图表的大型电子表格语料库上，Table2Charts在多类型任务（召回率R@3=0.61，R@1=0.43，召回率翻倍）和人工评估中均优于其他图表推荐系统。

Conclusion: Table2Charts通过学习常见模式有效解决了图表推荐的挑战，并在自动化指标和人工评估中均取得了卓越的性能。

Abstract: It is common for people to create different types of charts to explore a multi-dimensional dataset (table). However, to recommend commonly composed charts in real world, one should take the challenges of efficiency, imbalanced data and table context into consideration. In this paper, we propose Table2Charts framework which learns common patterns from a large corpus of (table, charts) pairs. Based on deep Q-learning with copying mechanism and heuristic searching, Table2Charts does table-to-sequence generation, where each sequence follows a chart template. On a large spreadsheet corpus with 165k tables and 266k charts, we show that Table2Charts could learn a shared representation of table fields so that recommendation tasks on different chart types could mutually enhance each other. Table2Charts outperforms other chart recommendation systems in both multi-type task (with doubled recall numbers R@3=0.61 and R@1=0.43) and human evaluations.

</details>


### [121] [Sigma Worksheet: Interactive Construction of OLAP Queries](https://arxiv.org/abs/2012.00697)
*James Gale,Max Seiden,Gretchen Atwood,Jason Frantz,Rob Woollen,Çağatay Demiralp*

Main category: cs.DB

TL;DR: Sigma Worksheet是一个交互式系统，它通过熟悉的电子表格界面和动态构建SQL查询，帮助企业用户在大规模云数据仓库中进行即席可视化分析，提高决策效率。


<details>
  <summary>Details</summary>
Motivation: 现有云数据仓库（CDWs）数据分析工具在即席转换方面受限或对业务用户而言难以使用，而业务用户是企业中最大的用户群体，因此需要一种更易用的解决方案。

Method: Sigma Worksheet提供了一个电子表格式的交互界面，用户通过直接操作进行数据分析。系统从用户交互中动态构建SQL查询，并直接在云数据仓库上执行。通过两个实际用例（群组分析和会话化）、TPC-H基准测试以及一项针对100人的调查和70人的半结构化访谈来评估其表达能力、查询性能和用户体验。

Result: Sigma Worksheet在表达能力上表现良好，其生成的SQL查询性能与基准参考查询相当。用户反馈表明，Sigma Worksheet更易于使用和学习，显著提高了用户生产力。

Conclusion: Sigma Worksheet有效解决了云数据仓库中数据分析工具的易用性问题，提高了用户生产力。未来的改进方向是为用户在数据分析的各个步骤提供指导，以进一步提升用户体验。

Abstract: The new generation of cloud data warehouses (CDWs) brings large amounts of data and compute power closer to users in enterprises. The ability to directly access the warehouse data, interactively analyze and explore it at scale can empower users to improve their decision making cycles. However, existing tools for analyzing data in CDWs are either limited in ad-hoc transformations or difficult to use for business users, the largest user segment in enterprises. Here we introduce Sigma Worksheet, a new interactive system that enables users to easily perform ad-hoc visual analysis of data in CDWs at scale. For this, Sigma Worksheet provides an accessible spreadsheet-like interface for data analysis through direct manipulation. Sigma Worksheet dynamically constructs matching SQL queries from user interactions on this familiar interface, building on the versatility and expressivity of SQL. Sigma Worksheet executes constructed queries directly on CDWs, leveraging the superior characteristics of the new generation CDWs, including scalability. To evaluate Sigma Worksheet, we first demonstrate its expressivity through two real life use cases, cohort analysis and sessionization. We then measure the performance of the Worksheet generated queries with a set of experiments using the TPC-H benchmark. Results show the performance of our compiled SQL queries is comparable to that of the reference queries of the benchmark. Finally, to assess the usefulness of Sigma Worksheet in deployment, we elicit feedback through a 100-person survey followed by a semi-structured interview study with 70 participants. We find that Sigma Worksheet is easier to use and learn, improving the productivity of users. Our findings also suggest Sigma Worksheet can further improve user experience by providing guidance to users at various steps of data analysis.

</details>


### [122] [Mapping Spreadsheets to RDF: Supporting Excel in RML](https://arxiv.org/abs/2104.13600)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 503 - [{'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: The RDF Mapping Language (RML) enables, among other formats, the mapping of tabular data as Comma-Separated Values (CSV) files to RDF graphs. Unfortunately, the widely used spreadsheet format is currently neglected by its specification and well-known implementations. Therefore, we extended one of the tools which is RML Mapper to support Microsoft Excel spreadsheet files and demonstrate its capabilities in an interactive online demo. Our approach allows to access various meta data of spreadsheet cells in typical RML maps. Some experimental features for more specific use cases are also provided. The implementation code is publicly available in a GitHub fork.

</details>


### [123] [Dataset Generation Patterns for Evaluating Knowledge Graph Construction](https://arxiv.org/abs/2104.13576)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: 为了评估工业场景中的知识图谱构建方法，本文提出了一种生成尽可能真实的合成个人和企业数据的方法。他们从真实的工业电子表格中发现了11种独特的模式，并开发了一个名为Data Sprout的生成器来重现这些模式。


<details>
  <summary>Details</summary>
Motivation: 由于保密性问题，真实、带标签的个人和企业数据集难以公开，但这对于评估工业场景中的知识图谱构建方法非常有用。

Method: 基于知识工作者生产或管理数据时存在的习惯，本文从真实的工业电子表格中推导出了11种独特的生成模式。然后，开发了一个名为Data Sprout的生成器，能够重现这些模式以生成合成电子表格。

Result: 本文初步推导出了在工业真实电子表格中发现的11种独特模式，并展示了一个名为Data Sprout的生成器能够重现这些模式。

Conclusion: 本文介绍了一种通过识别真实数据中的模式来生成具有真实感合成数据集的方法，并以Data Sprout为例进行了演示。

Abstract: Confidentiality hinders the publication of authentic, labeled datasets of personal and enterprise data, although they could be useful for evaluating knowledge graph construction approaches in industrial scenarios. Therefore, our plan is to synthetically generate such data in a way that it appears as authentic as possible. Based on our assumption that knowledge workers have certain habits when they produce or manage data, generation patterns could be discovered which can be utilized by data generators to imitate real datasets. In this paper, we initially derived 11 distinct patterns found in real spreadsheets from industry and demonstrate a suitable generator called Data Sprout that is able to reproduce them. We describe how the generator produces spreadsheets in general and what altering effects the implemented patterns have.

</details>


### [124] [Interactively Constructing Knowledge Graphs from Messy User-Generated Spreadsheets](https://arxiv.org/abs/2103.03537)
*Markus Schröder,Christian Jilek,Michael Schulze,Andreas Dengel*

Main category: cs.DB

TL;DR: 这篇论文提出了一种交互式方法，通过图形用户界面批量标注电子表格单元格，将混乱的电子表格数据转换为知识图谱，并解决了现有方法面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 当知识工作者自由填写电子表格时，内容可能非常非结构化，导致难以被人类和机器正确解读。当数据维护策略缺失且用户生成的数据变得“混乱”时，将这些电子表格转换为知识图谱是一项具有挑战性的任务。

Method: 该方法包括一个图形用户界面，使知识工程师能够批量标注电子表格单元格中的提取信息。基于这些单元格的标注，最终形成一个知识图谱。

Result: 通过使用工业场景中的五个电子表格，在评估中构建了一个包含25,000个三元组的知识图谱。该方法与最先进的RDF映射语言（RML）进行了比较，结果突出了该方法的贡献。

Conclusion: 该论文提出了一种有效的交互式方法，通过GUI解决将混乱电子表格数据转换为知识图谱的挑战，并表现优于现有方法。

Abstract: When spreadsheets are filled freely by knowledge workers, they can contain rather unstructured content. For humans and especially machines it becomes difficult to interpret such data properly. Therefore, spreadsheets are often converted to a more explicit, formal and structured form, for example, to a knowledge graph. However, if a data maintenance strategy has been missing and user-generated data becomes "messy", the construction of knowledge graphs will be a challenging task. In this paper, we catalog several of those challenges and propose an interactive approach to solve them. Our approach includes a graphical user interface which enables knowledge engineers to bulk-annotate spreadsheet cells with extracted information. Based on the cells' annotations a knowledge graph is ultimately formed. Using five spreadsheets from an industrial scenario, we built a 25k-triple graph during our evaluation. We compared our method with the state-of-the-art RDF Mapping Language (RML) attempt. The comparison highlights contributions of our approach.

</details>


### [125] [Leam: An Interactive System for In-situ Visual Text Analysis](https://arxiv.org/abs/2009.03520)
*Sajjadur Rahman,Peter Griggs,Çağatay Demiralp*

Main category: cs.DB

TL;DR: Leam是一个系统，它将文本分析过程视为一个单一的连续体，解决了现有系统在数据异构性、溯源、工作流可重用性、可再现性以及与既定实践兼容性方面的挑战。它结合了计算笔记本、电子表格和可视化工具的优势，并提供了交互式用户界面、新的数据模型和富有表现力的代数。


<details>
  <summary>Details</summary>
Motivation: 随着网络上数字文本规模和可用性的增加，企业利用文本分析来改进其服务和产品。然而，现有的文本分析系统通常只支持部分分析阶段，并且无法解决数据异构性、溯源、工作流可重用性、可再现性以及与既定实践兼容性等挑战。

Method: 作者提出了Leam系统，它基于解决上述挑战的设计考虑，将文本分析过程视为一个单一的连续体。Leam结合了计算笔记本、电子表格和可视化工具的优势。

Result: Leam系统具有交互式用户界面，用于运行文本分析工作流；一个新的数据模型，用于管理多种原子和复合数据类型；以及一种富有表现力的代数，能够捕获代表文本分析各个阶段的多种操作，并实现系统不同组件（包括数据、代码和可视化）之间的协调。

Conclusion: 论文报告了Leam开发的当前进展，并通过使用示例展示了其有效性。最后，作者概述了Leam的几项增强功能，并为开发交互式可视化文本分析系统确定了几个研究方向。

Abstract: With the increase in scale and availability of digital text generated on the web, enterprises such as online retailers and aggregators often use text analytics to mine and analyze the data to improve their services and products alike. Text data analysis is an iterative, non-linear process with diverse workflows spanning multiple stages, from data cleaning to visualization. Existing text analytics systems usually accommodate a subset of these stages and often fail to address challenges related to data heterogeneity, provenance, workflow reusability and reproducibility, and compatibility with established practices. Based on a set of design considerations we derive from these challenges, we propose Leam, a system that treats the text analysis process as a single continuum by combining advantages of computational notebooks, spreadsheets, and visualization tools. Leam features an interactive user interface for running text analysis workflows, a new data model for managing multiple atomic and composite data types, and an expressive algebra that captures diverse sets of operations representing various stages of text analysis and enables coordination among different components of the system, including data, code, and visualizations. We report our current progress in Leam development while demonstrating its usefulness with usage examples. Finally, we outline a number of enhancements to Leam and identify several research directions for developing an interactive visual text analysis system.

</details>


### [126] [ObjTables: structured spreadsheets that promote data quality, reuse, and integration](https://arxiv.org/abs/2005.05227)
*Jonathan R. Karr,Wolfram Liebermeister,Arthur P. Goldberg,John A. P. Sekar,Bilal Shaikh*

Main category: cs.DB

TL;DR: ObjTables是一个工具包，它通过将电子表格与模式和对象关系映射系统结合起来，使电子表格具有人类和机器可读性，从而促进数据重用和元分析。


<details>
  <summary>Details</summary>
Motivation: 理解系统行为如何从复杂网络中产生，需要聚合、重用和整合异构信息。电子表格是关键的数据源，但由于缺乏定义对象、关系和属性的模式，通常难以重新分析。

Method: 开发了ObjTables，一个将电子表格与模式和对象关系映射系统相结合的工具包。它包括一个模式格式；用于指示每个电子表格和列所代表的类和属性的标记；针对科学信息的多种数据类型；以及用于使用模式读取、写入、验证、比较、合并、修订和分析电子表格的高级软件。

Result: 通过使电子表格更易于重用，ObjTables可以实现前所未有的二次元分析。通过使其易于为新型数据构建新格式和相关软件，ObjTables还可以加速新兴的科学领域。

Conclusion: ObjTables通过提高电子表格的可重用性，赋能了前所未有的二次元分析，并加速了新兴科学领域的发展。

Abstract: A central challenge in science is to understand how systems behaviors emerge from complex networks. This often requires aggregating, reusing, and integrating heterogeneous information. Supplementary spreadsheets to articles are a key data source. Spreadsheets are popular because they are easy to read and write. However, spreadsheets are often difficult to reanalyze because they capture data ad hoc without schemas that define the objects, relationships, and attributes that they represent. To help researchers reuse and compose spreadsheets, we developed ObjTables, a toolkit that makes spreadsheets human- and machine-readable by combining spreadsheets with schemas and an object-relational mapping system. ObjTables includes a format for schemas; markup for indicating the class and attribute represented by each spreadsheet and column; numerous data types for scientific information; and high-level software for using schemas to read, write, validate, compare, merge, revision, and analyze spreadsheets. By making spreadsheets easier to reuse, ObjTables could enable unprecedented secondary meta-analyses. By making it easy to build new formats and associated software for new types of data, ObjTables can also accelerate emerging scientific fields.

</details>


### [127] [Needles in the 'Sheet'stack: Augmented Analytics to get Insights from Spreadsheets](https://arxiv.org/abs/2006.08224)
*Medha Atre,Anand Deshpande,Reshma Godse,Pooja Deokar,Sandip Moharir,Dhruva Ray,Akshay Chitlangia,Trupti Phadnis,Yugansh Goyal*

Main category: cs.DB

TL;DR: 该演示提出了一种商业智能解决方案，用于分析周期性电子表格以提供洞察，目标用户是没有数据库架构知识或传统分析资源的人。


<details>
  <summary>Details</summary>
Motivation: 尽管现代商业智能工具已经很先进，但它们通常需要熟悉数据库架构或大量资源。因此，需要一种能够分析周期性电子表格并为缺乏技术专业知识或资源的用户提供洞察的商业智能解决方案。

Method: 该解决方案通过检查来自不同报告（视图）的周期性电子表格来生成洞察。关键在于，它在无需预先了解数据库架构、报告结构或数据信息的情况下运行。

Result: 该摘要未明确提供结果。

Conclusion: 该摘要未明确提供结论。

Abstract: Business intelligence (BI) tools for database analytics have come a long way and nowadays also provide ready insights or visual query explorations, e.g. QuickInsights by Microsoft Power BI, SpotIQ by ThoughtSpot, Zenvisage, etc. In this demo, we focus on providing insights by examining periodic spreadsheets of different reports (aka views), without prior knowledge of the schema of the database or reports, or data information. Such a solution is targeted at users without the familiarity with the database schema or resources to conduct analytics in the contemporary way.

</details>


### [128] [A Billion Updates per Second Using 30,000 Hierarchical In-Memory D4M Databases](https://arxiv.org/abs/1902.00846)
*Jeremy Kepner,Vijay Gadepally,Lauren Milechin,Siddharth Samsi,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Micheal Houle,Micheal Jones,Anne Klein,Peter Michaleas,Julie Mullen,Andrew Prout,Antonio Rosa,Charles Yee,Albert Reuther*

Main category: cs.DB

TL;DR: 该论文介绍了一种分层实现的D4M关联数组，用于大规模网络数据的高性能流式更新，在MIT SuperCloud上实现了每秒19亿次更新的速度。


<details>
  <summary>Details</summary>
Motivation: 分析大规模网络需要对图表示进行高性能的流式更新。关联数组非常适合表示和分析流式网络数据，但对大型关联数组进行流式更新需要分层实现来优化内存层次结构的性能。

Method: 通过实现分层D4M关联数组。

Result: 在MIT SuperCloud上，在1,100个服务器节点上运行34,000个分层D4M关联数组实例，实现了每秒1,900,000,000次更新的持续更新速率。

Conclusion: 这种能力使MIT SuperCloud能够分析极其庞大的流式网络数据集。

Abstract: Analyzing large scale networks requires high performance streaming updates of graph representations of these data. Associative arrays are mathematical objects combining properties of spreadsheets, databases, matrices, and graphs, and are well-suited for representing and analyzing streaming network data. The Dynamic Distributed Dimensional Data Model (D4M) library implements associative arrays in a variety of languages (Python, Julia, and Matlab/Octave) and provides a lightweight in-memory database. Associative arrays are designed for block updates. Streaming updates to a large associative array requires a hierarchical implementation to optimize the performance of the memory hierarchy. Running 34,000 instances of a hierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of 1,900,000,000 updates per second. This capability allows the MIT SuperCloud to analyze extremely large streaming network data sets.

</details>


### [129] [Towards Semantically Enhanced Data Understanding](https://arxiv.org/abs/1806.04952)
*Markus Schröder,Christian Jilek,Jörn Hees,Andreas Dengel*

Main category: cs.DB

TL;DR: 该论文提出了一种语义模型，将数据与其文档相互关联起来，以减少开销，并促进机器学习中更好的数据理解和利用。


<details>
  <summary>Details</summary>
Motivation: 在机器学习中，数据理解需要大量文档，但目前的做法是文档分散、非结构化，导致查找开销大，且其他应用无法利用。

Method: 该论文提出了一种方法，使用单一的语义模型将数据与其文档相互关联。

Result: 该模型使数据科学家能够轻松查找和浏览相关数据信息。它还支持其他应用程序进行数据搜索、比较、集成和可视化。论文展示了一个早期原型。

Conclusion: 将数据和文档互联的语义模型显著改善了数据理解，减少了开销，并通过为数据科学家和支持应用程序提供统一、可消费的结构，增强了数据利用。

Abstract: In the field of machine learning, data understanding is the practice of getting initial insights in unknown datasets. Such knowledge-intensive tasks require a lot of documentation, which is necessary for data scientists to grasp the meaning of the data. Usually, documentation is separate from the data in various external documents, diagrams, spreadsheets and tools which causes considerable look up overhead. Moreover, other supporting applications are not able to consume and utilize such unstructured data. That is why we propose a methodology that uses a single semantic model that interlinks data with its documentation. Hence, data scientists are able to directly look up the connected information about the data by simply following links. Equally, they can browse the documentation which always refers to the data. Furthermore, the model can be used by other approaches providing additional support, like searching, comparing, integrating or visualizing data. To showcase our approach we also demonstrate an early prototype.

</details>


### [130] [WebRelate: Integrating Web Data with Spreadsheets using Examples](https://arxiv.org/abs/1711.05787)
*Jeevana Priya Inala,Rishabh Singh*

Main category: cs.DB

TL;DR: WebRelate是一个系统，它通过输入-输出示例帮助数据科学家和电子表格用户将半结构化网络数据与关系数据集成，将其分解为URL学习和依赖于输入的网络数据提取两个子任务。


<details>
  <summary>Details</summary>
Motivation: 网络源与关系数据之间的数据集成面临挑战，因为大多数网站不提供直接获取表格数据的接口，且用户需要编写复杂的脚本来提取数据，这超出了许多数据科学家和最终用户的编程专业知识。

Method: WebRelate将网络数据集成任务分解为URL学习和依赖输入的网络数据提取。它通过学习字符串转换程序来生成URL，并从示例中学习程序来提取所需数据。该系统设计了表达性领域特定语言，并提出了高效的合成算法。

Result: WebRelate在88个实际网络数据集成任务上进行了评估，结果表明它可以在几秒钟内学习所需的程序，并且对于大多数任务只需一个示例。

Conclusion: WebRelate通过基于示例的学习方法，高效解决了非程序员在URL生成和数据提取方面的网络数据集成挑战。

Abstract: Data integration between web sources and relational data is a key challenge faced by data scientists and spreadsheet users. There are two main challenges in programmatically joining web data with relational data. First, most websites do not expose a direct interface to obtain tabular data, so the user needs to formulate a logic to get to different webpages for each input row in the relational table. Second, after reaching the desired webpage, the user needs to write complex scripts to extract the relevant data, which is often conditioned on the input data. Since many data scientists and end-users come from diverse backgrounds, writing such complex regular-expression based logical scripts to perform data integration tasks is unfortunately often beyond their programming expertise.
  We present WebRelate, a system that allows users to join semi-structured web data with relational data in spreadsheets using input-output examples. WebRelate decomposes the web data integration task into two sub-tasks of i) URL learning and ii) input-dependent web extraction. The first sub-task generates the URLs for the webpages containing the desired data for all rows in the relational table. WebRelate achieves this by learning a string transformation program using a few example URLs. The second sub-task uses examples of desired data to be extracted from the corresponding webpages and learns a program to extract the data for the other rows. We design expressive domain-specific languages for URL generation and web data extraction, and present efficient synthesis algorithms for learning programs in these DSLs from few input-output examples. We evaluate WebRelate on 88 real-world web data integration tasks taken from online help forums and Excel product team, and show that WebRelate can learn the desired programs within few seconds using only 1 example for the majority of the tasks.

</details>


<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [131] [Real-time stock analysis for blending recipes in industrial plants](https://arxiv.org/abs/1909.02960)
*Florin Zamfir,Nicolae Paraschiv,Emil Pricop*

Main category: cs.OH

TL;DR: 该论文提出并实现了一个C#应用程序，旨在解决公司使用Excel电子表格管理库存和过程数据的低效和风险问题。该应用程序通过集成规划算法，实现实时库存分析、识别混合过程所需原料、确定可生产的成品数量以及优化成品数量，从而协助操作员进行决策。


<details>
  <summary>Details</summary>
Motivation: 许多公司使用Excel电子表格管理库存和过程数据，这些表格难以理解和跟踪，且存在公式被随机修改或删除的风险。特别是在已知配方的混合过程中，需要一个能够集中数据并协助操作员决策的应用。

Method: 开发了一个基于C#的实时库存分析应用程序，该应用程序集成了规划算法。这些算法旨在识别混合过程所需的成分、确定现有成分可生产的成品数量以及确定成品的最佳数量。

Result: 该C#应用程序能够进行实时库存分析，易于操作员在控制室使用。它能帮助操作员识别混合过程所需的原料、确定现有原料可生产的成品数量，并优化成品数量。用户可以逐步构建结果。

Conclusion: 该应用程序通过集中数据和提供规划算法支持，有效解决了传统Excel管理库存的痛点，提升了库存管理和混合过程决策的效率和准确性，易于操作员使用。

Abstract: Many companies use Excel spreadsheets to keep stock records and to calculate process-specific data. These spreadsheets are often hard to understand and track. And if the user does not protect them, there is a risk that the user randomly changes or erase formulas. The paper focuses on the stocks of products used in a blending process with a known recipe. Developing an application that can bring this data in a centralized form and that can assist the operator in decide is a necessity. When a programmer implements an application that uses data from plants he needs to consider one fundamental aspect as reading real-time data from the process. The real-time stock analysis application takes into account all the above elements. The application is easy to use by an operator in the command room of installation because of the planning algorithms integrated into it. The algorithms proposed and implemented in this paper have well-defined goals: identifying the ingredients needed to achieve the blending process for required quantities, determine the quantities of the finished product that can be made with the existing ingredients and determine the optimum quantities of the finished product. The application implemented in C# intensively uses these algorithms and gives the user the ability to build the result step by step.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [132] [A CNN toolbox for skin cancer classification](https://arxiv.org/abs/1908.08187)
*Fabrizio Nunnari,Daniel Sonntag*

Main category: eess.IV

TL;DR: 该论文描述了一个用于皮肤癌分类深度神经网络配置的软件工具箱。它允许开发人员快速设置CNN架构和超参数配置，并为非技术用户提供简单的界面来探索不同的配置设置。初步结果量化了图像增强、图像分辨率和重缩放滤波器对黑色素瘤检测性能和训练时间的影响。


<details>
  <summary>Details</summary>
Motivation: 开发一个软件工具箱，使开发人员能够快速设置新的卷积神经网络（CNN）架构和超参数配置，同时允许非技术用户探索不同的配置设置，以适应不同的数据集，从而简化皮肤癌分类中深度神经网络的配置过程。

Method: 该方法描述了一个软件工具箱，其软件架构允许快速设置CNN架构和超参数配置。它还提供了一个类似电子表格的用户界面，供非技术用户探索配置。初步结果是通过两个CNN在皮肤镜图像上的黑色素瘤检测背景下进行的，量化了图像增强、图像分辨率和重缩放滤波器对整体检测性能和训练时间的影响。

Result: 初步结果显示，图像增强、图像分辨率和重缩放滤波器对皮肤镜图像上黑色素瘤检测的整体检测性能和训练时间有量化影响。

Conclusion: 本文介绍了一个用于皮肤癌分类深度神经网络配置的软件工具箱，该工具箱通过允许快速配置和灵活探索设置来简化该过程。初步结果证明了该工具箱的实用性，并为未来集成元学习或AutoML系统奠定了基础。

Abstract: We describe a software toolbox for the configuration of deep neural networks in the domain of skin cancer classification. The implemented software architecture allows developers to quickly set up new convolutional neural network (CNN) architectures and hyper-parameter configurations. At the same time, the user interface, manageable as a simple spreadsheet, allows non-technical users to explore different configuration settings that need to be explored when switching to different data sets. In future versions, meta leaning frameworks can be added, or AutoML systems that continuously improve over time. Preliminary results, conducted with two CNNs in the context melanoma detection on dermoscopic images, quantify the impact of image augmentation, image resolution, and rescaling filter on the overall detection performance and training time.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [133] [Billion-files File Systems (BfFS): A Comparison](https://arxiv.org/abs/2408.01805)
*Sohail Shaikh*

Main category: cs.PF

TL;DR: 本研究通过创建、存储和读取十亿个文件，分析了流行的Linux文件系统（EXT4、XFS、BtrFS、ZFS和F2FS）的性能、资源利用率和性能下降。


<details>
  <summary>Details</summary>
Motivation: 由于数据量呈指数级增长，需要快速处理，因此数据需要尽可能靠近计算设备以减少传输延迟。这促使人们关注本地文件系统，以理解其内部工作原理、性能及局限性。

Method: 本研究通过创建、存储和读取十亿个文件来分析EXT4、XFS、BtrFS、ZFS和F2FS等流行的Linux文件系统。研究捕获并分析了读/写吞吐量、存储块使用情况、磁盘空间利用率和开销以及其他对系统设计人员和集成商有用的指标。此外，研究还探讨了在创建大量文件和文件夹期间及之后文件系统性能下降等副作用。

Result: 研究捕获并分析了读/写吞吐量、存储块使用情况、磁盘空间利用率和开销，以及其他对系统设计人员和集成商有用的指标。此外，研究还探讨了在创建大量文件和文件夹期间及之后文件系统性能下降等副作用。

Conclusion: 本研究对在极端文件数量下主流Linux文件系统的性能特性、资源消耗及潜在性能下降进行了深入分析，为系统设计人员和集成商提供了有价值的参考信息。

Abstract: As the volume of data being produced is increasing at an exponential rate that needs to be processed quickly, it is reasonable that the data needs to be available very close to the compute devices to reduce transfer latency. Due to this need, local filesystems are getting close attention to understand their inner workings, performance, and more importantly their limitations. This study analyzes few popular Linux filesystems: EXT4, XFS, BtrFS, ZFS, and F2FS by creating, storing, and then reading back one billion files from the local filesystem. The study also captured and analyzed read/write throughput, storage blocks usage, disk space utilization and overheads, and other metrics useful for system designers and integrators. Furthermore, the study explored other side effects such as filesystem performance degradation during and after these large numbers of files and folders are created.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [134] [Using the Abstract Computer Architecture Description Language to Model AI Hardware Accelerators](https://arxiv.org/abs/2402.00069)
*Mika Markus Müller,Alexander Richard Manfred Borst,Konstantin Lübeck,Alexander Louis-Ferdinand Jung,Oliver Bringmann*

Main category: cs.AR

TL;DR: AI硬件加速器的选择和配置是一个复杂挑战，本文提出使用ACADL对AI硬件加速器进行建模、映射DNN，并进行时序仿真以评估性能。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络（DNNs）的普及，AI硬件加速器成为必需品。然而，AI产品制造商在选择和配置加速器时面临挑战，现有工具无法提供详细的性能理解。

Method: 本文使用抽象计算机架构描述语言（ACADL）对AI硬件加速器进行建模，利用ACADL描述将DNNs映射到加速器上，并通过时序仿真语义收集性能结果。

Result: 本摘要没有直接给出具体结果，而是描述了使用ACADL进行建模、映射和时序仿真的方法，旨在提供一种评估AI硬件加速器性能的有效途径。

Conclusion: 通过使用ACADL，可以更清晰地理解和比较AI硬件加速器的性能特性，从而帮助制造商做出更明智的选择和配置。

Abstract: Artificial Intelligence (AI) has witnessed remarkable growth, particularly through the proliferation of Deep Neural Networks (DNNs). These powerful models drive technological advancements across various domains. However, to harness their potential in real-world applications, specialized hardware accelerators are essential. This demand has sparked a market for parameterizable AI hardware accelerators offered by different vendors.
  Manufacturers of AI-integrated products face a critical challenge: selecting an accelerator that aligns with their product's performance requirements. The decision involves choosing the right hardware and configuring a suitable set of parameters. However, comparing different accelerator design alternatives remains a complex task. Often, engineers rely on data sheets, spreadsheet calculations, or slow black-box simulators, which only offer a coarse understanding of the performance characteristics.
  The Abstract Computer Architecture Description Language (ACADL) is a concise formalization of computer architecture block diagrams, which helps to communicate computer architecture on different abstraction levels and allows for inferring performance characteristics. In this paper, we demonstrate how to use the ACADL to model AI hardware accelerators, use their ACADL description to map DNNs onto them, and explain the timing simulation semantics to gather performance results.

</details>


### [135] [Online Model Swapping in Architectural Simulation](https://arxiv.org/abs/2012.01571)
*Patrick Lavin,Jeffrey Young,Rich Vuduc,Jonathan Beard*

Main category: cs.AR

TL;DR: 本文提出了一种通过在线监控模拟行为并自动将详细模型替换为更简单的统计近似模型的方法，以弥合简单模拟和详细模拟之间的鸿沟，在实现显著加速的同时保持较低误差。


<details>
  <summary>Details</summary>
Motivation: 随着系统和应用程序变得越来越复杂，详细模拟所需的时间也越来越多。模拟时间增加会导致设计迭代变慢，迫使架构师在需要快速迭代设计时使用更简单的模型，例如电子表格。然而，从简单模拟迁移到更详细的模拟通常需要多次执行才能找到简单模型可能有效的地方，这可能比一开始就运行详细模型更昂贵。此外，架构师通常必须依靠直觉来选择这些更简单的模型，这进一步使问题复杂化。

Method: 本研究提出了一种方法，通过在线监控模拟行为并自动将详细模型替换为更简单的统计近似模型，来弥合简单模拟和详细模拟之间的差距。作者在开源模拟器SVE-Cachesim中实现了这一方法，以替换内存层次结构中的一级数据缓存（L1D）。

Result: 该概念验证表明，该技术不仅可以处理局部时不变统计数据的近似，还可以处理随时间变化的统计数据（例如，L1D是一种时间序列函数）以及下游副作用（例如，L1D过滤二级缓存的访问）。在超过90%的模拟中使用了近似缓存模型，模拟周期计数的误差仅为8%，而作者的简化模型每次“执行”所需的计算量减少了2到8倍。

Conclusion: 本文提出的技术成功地弥合了简单模拟和详细模拟之间的差距，在可接受的误差范围内提供了显著的性能改进，使其成为复杂模拟场景的可行解决方案。

Abstract: As systems and applications grow more complex, detailed simulation takes an ever increasing amount of time. The prospect of increased simulation time resulting in slower design iteration forces architects to use simpler models, such as spreadsheets, when they want to iterate quickly on a design. However, the task of migrating from a simple simulation to one with more detail often requires multiple executions to find where simple models could be effective, which could be more expensive than running the detailed model in the first place. Also, architects must often rely on intuition to choose these simpler models, further complicating the problem.
  In this work, we present a method of bridging the gap between simple and detailed simulation by monitoring simulation behavior online and automatically swapping out detailed models with simpler statistical approximations. We demonstrate the potential of our methodology by implementing it in the open-source simulator SVE-Cachesim to swap out the level one data cache (L1D) within a memory hierarchy. This proof of concept demonstrates that our technique can handle a non-trivial use-case in not just approximation of local time-invariant statistics, but also those that vary with time (e.g., the L1D is a form of a time-series function), and downstream side-effects (e.g., the L1D filters accesses for the level two cache). Our simulation swaps out the built-in cache model with only an 8% error in the simulated cycle count while using the approximated cache models for over 90% of the simulation, and our simpler models require two to eight times less computation per "execution" of the model

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [136] [Is spreadsheet syntax better than numeric indexing for cell selection?](https://arxiv.org/abs/2505.23296)
*Philip Heltweg,Dirk Riehle,Georg-Daniel Schwarz*

Main category: cs.PL

TL;DR: 本文通过一项大规模受控实验，比较了数据工程中单元格选择的两种不同语法（数字索引和电子表格样式）在代码读写速度和正确性方面的表现。结果表明，电子表格样式语法在减少错误和提高速度方面优于数字索引语法，尤其适合非软件工程背景的数据从业者。


<details>
  <summary>Details</summary>
Motivation: 数据工程中选择单元格子集是一项常见任务，存在多种表达选择的方法，如数字索引和电子表格样式。本文旨在比较这两种方法在实际应用中的表现，以找到更有效率和更少出错的方式。

Method: 研究人员进行了一项大规模受控实验，招募学生参与者作为数据从业者的代表。实验中，参与者被要求使用数字索引和电子表格样式两种语法进行代码的读写，并记录其速度和正确性。

Result: 实验结果显示，在阅读代码时，参与者使用电子表格样式语法犯的错误更少。在编写代码时，使用电子表格样式语法时，参与者不仅犯错更少，而且速度更快。

Conclusion: 因此，对于数据工程领域，领域特定语法（例如电子表格样式语法）对于支持没有软件工程背景的从业者来说，是一种有前景的替代方案，值得在未来工具中探索。

Abstract: Selecting a subset of cells is a common task in data engineering, for example, to remove errors or select only specific parts of a table. Multiple approaches to express this selection exist. One option is numeric indexing, commonly found in general programming languages, where a tuple of numbers identifies the cell. Alternatively, the separate dimensions can be referred to using different enumeration schemes like "A1" for the first cell, commonly found in software such as spreadsheet systems.
  In a large-scale controlled experiment with student participants as proxy for data practitioners, we compare the two options with respect to speed and correctness of reading and writing code.
  The results show that, when reading code, participants make less mistakes using spreadsheet-style syntax. Additionally, when writing code, they make fewer mistakes and are faster when using spreadsheet syntax compared to numeric syntax.
  From this, a domain-specific syntax, such as spreadsheet syntax for data engineering, appears to be a promising alternative to explore in future tools to support practitioners without a software engineering background.

</details>


### [137] [Solving Data-centric Tasks using Large Language Models](https://arxiv.org/abs/2402.11734)
*Shraddha Barke,Christian Poelitz,Carina Suzana Negreanu,Benjamin Zorn,José Cambronero,Andrew D. Gordon,Vu Le,Elnaz Nouri,Nadia Polikarpova,Advait Sarkar,Brian Slininger,Neil Toronto,Jack Williams*

Main category: cs.PL

TL;DR: 该论文探讨了在数据中心任务中，如何选择和包含多少数据到大型语言模型（LLM）的提示中，并提出了一种聚类-然后-选择的提示技术以提高性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型正在取代像StackOverflow这样的帮助论坛，尤其对非专业程序员和终端用户的数据中心任务（如电子表格操作和数据整理）很有帮助。然而，仅凭自然语言描述很难解决这些任务，需要包含数据。问题是：应该包含多少数据以及哪些数据？

Method: 首先，作者创建了一个从StackOverflow帖子中挖掘出的真实世界自然语言到代码的表格数据处理任务数据集。其次，他们引入了一种“聚类-然后-选择”的提示技术，将输入数据中最具代表性的行添加到LLM提示中。

Result: 实验表明，LLM的性能确实对提示中传递的数据量敏感。对于输入表格中语法变异很大的任务，他们提出的聚类-然后-选择技术优于随机选择基线。

Conclusion: 在数据中心任务中，LLM提示中数据量和数据选择方式对性能有显著影响，所提出的聚类-然后-选择方法在处理复杂情况时表现出色。

Abstract: Large language models (LLMs) are rapidly replacing help forums like StackOverflow, and are especially helpful for non-professional programmers and end users. These users are often interested in data-centric tasks, such as spreadsheet manipulation and data wrangling, which are hard to solve if the intent is only communicated using a natural-language description, without including the data. But how do we decide how much data and which data to include in the prompt? This paper makes two contributions towards answering this question. First, we create a dataset of real-world NL-to-code tasks manipulating tabular data, mined from StackOverflow posts. Second, we introduce a cluster-then-select prompting technique, which adds the most representative rows from the input data to the LLM prompt. Our experiments show that LLM performance is indeed sensitive to the amount of data passed in the prompt, and that for tasks with a lot of syntactic variation in the input table, our cluster-then-select technique outperforms a random selection baseline.

</details>


### [138] [FLAME: A small language model for spreadsheet formulas](https://arxiv.org/abs/2301.13779)
*Harshit Joshi,Abishai Ebenezer,José Cambronero,Sumit Gulwani,Aditya Kanade,Vu Le,Ivan Radiček,Gust Verbruggen*

Main category: cs.PL

TL;DR: FLAME是一个小型高效的Transformer模型，专门针对Excel公式进行训练，在公式修复、补全和检索方面优于大型模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在电子表格公式编写辅助方面存在训练成本高昂和部署困难的问题，因为它们的规模庞大。

Method: FLAME是一个基于Transformer的模型，专门使用Excel公式进行训练。它利用草图去重来构建训练数据集，引入了Excel专用的公式分词器，并使用领域特定的掩码跨度预测和噪声自编码作为预训练目标。

Result: FLAME（60M参数）在14个公式修复和补全评估设置中的10个方面，性能优于Davinci（175B）、Cushman（12B）和CodeT5（220M）等更大的模型。在公式检索方面，FLAME也优于CodeT5、CodeBERT和GraphCodeBERT。

Conclusion: FLAME通过利用领域洞察力，以更小的模型规模和更少的训练数据实现了有竞争力的性能。

Abstract: Spreadsheets are a vital tool for end-user data management. Using large language models for formula authoring assistance in these environments can be difficult, as these models are expensive to train and challenging to deploy due to their size (up to billions of parameters). We present FLAME, a transformer-based model trained exclusively on Excel formulas that leverages domain insights to achieve competitive performance while being substantially smaller (60M parameters) and training on two orders of magnitude less data. We curate a training dataset using sketch deduplication, introduce an Excel-specific formula tokenizer, and use domain-specific versions of masked span prediction and noisy auto-encoding as pre-training objectives. We evaluate FLAME on formula repair, formula completion, and similarity-based formula retrieval. FLAME can outperform much larger models, such as the Davinci (175B) and Cushman (12B) variants of Codex and CodeT5 (220M), in 10 of 14 evaluation settings for the repair and completion tasks. For formula retrieval, FLAME outperforms CodeT5, CodeBERT, and GraphCodeBERT.

</details>


### [139] [Enhanced Spreadsheet Computing with Finite-Domain Constraint Satisfaction](https://arxiv.org/abs/2203.16346)
*Ezana N. Beyenne,Hai-Feng Guo*

Main category: cs.PL

TL;DR: 本文展示了如何扩展电子表格计算范式，以解决约束满足问题，通过在一个可视化环境中支持有限域约束求解，并构建一种电子表格特定的约束语言，从而显著简化了许多基于约束的应用程序的开发。


<details>
  <summary>Details</summary>
Motivation: 电子表格是广泛使用的计算工具，但由于其单向数据流，主要限于簿记类应用，无法有效解决约束满足问题。

Method: 提出了一种增强型电子表格系统，该系统在一个可视化环境中良好支持有限域约束求解，并构建了一种电子表格特定的约束语言，供普通用户以声明性和可扩展的方式指定数据单元格之间的约束。

Result: 新的电子表格系统使用可视化表格界面，显著简化了许多基于约束的应用程序的开发。

Conclusion: 扩展的电子表格范式在解决约束满足问题方面具有可用性和实用性，克服了传统电子表格的局限性。

Abstract: The spreadsheet application is among the most widely used computing tools in modern society. It provides excellent usability and usefulness, and it easily enables a non-programmer to perform programming-like tasks in a visual tabular "pen and paper" approach. However, spreadsheets are mostly limited to bookkeeping-like applications due to their mono-directional data flow. This paper shows how the spreadsheet computing paradigm is extended to break this limitation for solving constraint satisfaction problems. We present an enhanced spreadsheet system where finite-domain constraint solving is well supported in a visual environment. Furthermore, a spreadsheet-specific constraint language is constructed for general users to specify constraints among data cells in a declarative and scalable way. The new spreadsheet system significantly simplifies the development of many constraint-based applications using a visual tabular interface. Examples are given to illustrate the usability and usefulness of the extended spreadsheet paradigm.
  KEYWORDS: Spreadsheet computing, Finite-domain constraint satisfaction, Constraint logic programming

</details>


### [140] [Typed Image-based Programming with Structure Editing](https://arxiv.org/abs/2110.08993)
*Jonathan Edwards,Tomas Petricek*

Main category: cs.PL

TL;DR: 该论文提出通过类型和结构编辑实现基于镜像编程的协作，以解决其在协作和部署方面的不足，并处理持久化数据的模式变更问题。


<details>
  <summary>Details</summary>
Motivation: 基于镜像的编程系统（如Smalltalk、LISP）虽然简化了开发并鼓励探索性编程，但与传统的基于文件的编程相比，在协作和部署方面支持不足，这限制了其商业成功。

Method: 通过静态类型和结构编辑来解决基于镜像编程中的协作问题，并专注于持久化数据的模式变更。利用静态类型来表达和执行模式变更，并通过结构编辑来捕获类型定义的变化，以自动适应数据。提出了一种实现“结构编辑版本控制”的理论。

Result: 主要技术贡献是提出了实现“结构编辑版本控制”的理论。

Conclusion: 如果该方法能扩展到整个编程体验，将为基于镜像编程中的协作提供一种新方式。

Abstract: Many beloved programming systems are image-based: self-contained worlds that persist both code and data in a single file. Examples include Smalltalk, LISP, HyperCard, Flash, and spreadsheets. Image-based programming avoids much of the complexity of modern programming technology stacks and encourages more casual and exploratory programming. However conventional file-based programming has better support for collaboration and deployment. These problems have been blamed for the limited commercial success of Smalltalk. We propose to enable collaboration in image-based programming via types and structure editing.
  We focus on the problem of schema change on persistent data. We turn to static types, which paradoxically require more schema change but also provide a mechanism to express and execute those changes. To determine those changes we turn to structure editing, so that we can capture changes in type definitions with sufficient fidelity to automatically adapt the data to suit. We conjecture that typical schema changes can be handled through structure editing of static types.
  That positions us to tackle collaboration with what could be called version control for structure editing. We present a theory realizing this idea, which is our main technical contribution. While we focus here on editing types, if we can extend the approach to cover the entire programming experience then it would offer a new way to collaborate in image-based programming.

</details>


### [141] [ExceLint: Automatically Finding Spreadsheet Formula Errors](https://arxiv.org/abs/1901.11100)
*Daniel W. Barowy,Emery D. Berger,Benjamin Zorn*

Main category: cs.PL

TL;DR: 该论文提出ExceLint，一个针对Microsoft Excel的静态分析工具，它利用信息论方法识别电子表格中公式导致的异常，从而发现公式错误。ExceLint快速有效，在70个电子表格中表现优于现有分析。


<details>
  <summary>Details</summary>
Motivation: 电子表格是最广泛使用的编程环境之一，尤其在金融等领域，错误可能导致灾难性后果，因此有必要寻找电子表格公式错误。

Method: 该方法是一种专门用于查找电子表格公式错误的静态分析。它直接利用电子表格的矩形特性，并使用信息论方法识别对附近矩形区域造成特别“令人惊讶的破坏”的公式。其实现称为ExceLint，适用于Microsoft Excel。

Result: ExceLint快速且有效：在70个电子表格的数据集中，ExceLint每个电子表格的分析中位数时间为5秒，并且显著优于现有最先进的分析。

Conclusion: ExceLint是一个快速有效的静态分析工具，用于发现电子表格公式错误，通过利用电子表格的矩形特性和信息论方法，它能显著提升错误检测能力。

Abstract: Spreadsheets are one of the most widely used programming environments, and are widely deployed in domains like finance where errors can have catastrophic consequences. We present a static analysis specifically designed to find spreadsheet formula errors. Our analysis directly leverages the rectangular character of spreadsheets. It uses an information-theoretic approach to identify formulas that are especially surprising disruptions to nearby rectangular regions. We present ExceLint, an implementation of our static analysis for Microsoft Excel. We demonstrate that ExceLint is fast and effective: across a corpus of 70 spreadsheets, ExceLint takes a median of 5 seconds per spreadsheet, and it significantly outperforms the state of the art analysis.

</details>


### [142] [Synthesizing Bijective Lenses](https://arxiv.org/abs/1710.03248)
*Anders Miltner,Kathleen Fisher,Benjamin C. Pierce,David Walker,Steve Zdancewic*

Main category: cs.PL

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\nPlease retry in 50.836433457s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '250'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '50s'}]}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Bidirectional transformations between different data representations occur frequently in modern software systems. They appear as serializers and deserializers, as database views and view updaters, and more. Manually building bidirectional transformations---by writing two separate functions that are intended to be inverses---is tedious and error prone. A better approach is to use a domain-specific language in which both directions can be written as a single expression. However, these domain-specific languages can be difficult to program in, requiring programmers to manage fiddly details while working in a complex type system.
  To solve this, we present Optician, a tool for type-directed synthesis of bijective string transformers. The inputs to Optician are two ordinary regular expressions representing two data formats and a few concrete examples for disambiguation. The output is a well-typed program in Boomerang (a bidirectional language based on the theory of lenses). The main technical challenge involves navigating the vast program search space efficiently enough. Unlike most prior work on type-directed synthesis, our system operates in the context of a language with a rich equivalence relation on types (the theory of regular expressions). We synthesize terms of a equivalent language and convert those generated terms into our lens language. We prove the correctness of our synthesis algorithm. We also demonstrate empirically that our new language changes the synthesis problem from one that admits intractable solutions to one that admits highly efficient solutions. We evaluate Optician on a benchmark suite of 39 examples including both microbenchmarks and realistic examples derived from other data management systems including Flash Fill, a tool for synthesizing string transformations in spreadsheets, and Augeas, a tool for bidirectional processing of Linux system configuration files.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [143] [Excel: Automated Ledger or Analytics IDE?](https://arxiv.org/abs/2409.12976)
*Andrew Kumiega*

Main category: cs.CY

TL;DR: Excel已演变为分析的集成开发环境（IDE），集成了多种强大工具。因此，需要扩展现有的电子表格风险框架，以应对将Excel作为分析IDE所带来的新风险。


<details>
  <summary>Details</summary>
Motivation: 鉴于Excel已从简单的分类账自动化工具发展成为分析IDE，但现有的电子表格风险管理框架未能跟上这一变化，导致风险日益增加。因此，建立一个全面的风险框架来管理这种独特的开发环境变得至关重要。

Method: 本文将解释如何扩展当前的电子表格风险框架，以管理将Excel用作分析IDE所带来的日益增长的风险。

Result: 本文的结果是阐明了Excel作为分析IDE的转变，并指出有必要扩展现有的风险框架来应对随之而来的风险。

Conclusion: 当前的电子表格风险框架需要扩展，以有效管理将Excel作为分析IDE所带来的日益增长的风险。

Abstract: Since the inception of VisiCalc over four decades ago, spreadsheets have undergone a gradual transformation, evolving from simple ledger automation tools to the current state of Excel, which can be described as an Integrated Development Environment (IDE) for analytics. The slow evolution of Excel from an automation tool for ledgers to an IDE for analytics explains why many people have not noticed that Excel includes a fully functional database, an OLAP Engine, multiple statistical programming languages, multiple third-party software libraries, dynamic charts, and real time data connectors. The simplicity of accessing these multiple tools is a low-code framework controlled from the Excel tool that is effectively an IDE. Once we acknowledge Excel's shift from a desk top application to an IDE for analytics, the importance of establishing a comprehensive risk framework for managing this distinctive development environment becomes clear. In this paper we will explain how the current risk framework for spreadsheets needs to be expanded to manage the growing risks of using Excel as an IDE for analytics.

</details>


### [144] [Improving Feedback from Automated Reviews of Student Spreadsheets](https://arxiv.org/abs/2311.10728)
*Sören Aguirre Reid,Frank Kammer,Jonas-Ian Kuche,Pia-Doreen Ritzke,Markus Siepermann,Max Stephan,Armin Wagenknecht*

Main category: cs.CY

TL;DR: 开发了一个智能辅导系统（ITS），用于自动批改Excel作业并提供个性化反馈。该系统通过值匹配、公式分析和质量评估等多种方式分析学生提交的作业，并根据学生的学习水平提供不同详细程度的反馈。结果显示，更高水平的反馈能提高正确提交的比例，并受到学生的积极评价。


<details>
  <summary>Details</summary>
Motivation: 电子表格是广泛使用的工具，但教学环境中评估电子表格作业的数字化解决方案仍然稀缺。

Method: 开发了一个智能辅导系统（ITS），用于自动审查学生的Excel提交并提供个性化反馈。该系统通过值匹配、公式分析和解决方案质量评估等方式分析提交。为了考虑学生的学习水平，系统开发了反馈级别，通过不同的分析方法逐步提供更多关于错误的信息。教师只需提供一个参考解决方案。

Result: 更高水平的反馈被证明可以提高正确提交的百分比，并且学生认为反馈易于理解和有帮助。

Conclusion: 所开发的ITS有效地评估了Excel作业，提供了个性化和适应性的反馈，提高了提交的正确性，并受到了学生的积极评价。

Abstract: Spreadsheets are one of the most widely used tools for end users. As a result, spreadsheets such as Excel are now included in many curricula. However, digital solutions for assessing spreadsheet assignments are still scarce in the teaching context. Therefore, we have developed an Intelligent Tutoring System (ITS) to review students' Excel submissions and provide individualized feedback automatically. Although the lecturer only needs to provide one reference solution, the students' submissions are analyzed automatically in several ways: value matching, detailed analysis of the formulas, and quality assessment of the solution. To take the students' learning level into account, we have developed feedback levels for an ITS that provide gradually more information about the error by using one of the different analyses. Feedback at a higher level has been shown to lead to a higher percentage of correct submissions and was also perceived as well understandable and helpful by the students.

</details>


### [145] [How Beaufort, Neumann and Gates met? Subject integration with spreadsheeting](https://arxiv.org/abs/2309.12353)
*Maria Csernoch,Julia Csernoch*

Main category: cs.CY

TL;DR: 该论文提出了一种新颖的方法，在八年级行动研究中利用蒲福尺度来整合学科并构建数字问题解决图式。研究发现，该方法在培养学生内容知识和数字技能方面比传统方法更有效，并且适用于各种可数字化解决的纸质问题。


<details>
  <summary>Details</summary>
Motivation: 计算思维应成为与阅读、写作和算术并列的第四项基本技能，但培养数字问题解决的图式仍面临挑战。

Method: 该研究提出了一种新颖的方法，通过使用著名的蒲福尺度来支持学科整合和构建数字图式。在一个八年级行动研究中，将传统的纸质问题和数据检索过程转化为数字环境进行展示。

Result: 研究发现，学生的学科知识和数字技能的发展比在传统教材和去语境化的数字环境中更有效率。

Conclusion: 所提出的方法不仅能够有效提高学生的数字问题解决能力和学科知识，而且可以适用于任何将其解决方案转化为数字环境会更有效的纸质问题，从而在学科和信息学两方面为构建图式提供多种形式。

Abstract: Computational thinking should be the fourth fundamental skill, along with reading, writing, and arithmetic (3R). To reach the level where computational thinking skills, especially digital problem solving have their own schemata, there is a long way to go. In the present paper, a novel approach is detailed to support subject integration and building digital schemata, on the well-known Beaufort scale. The conversion of a traditional, paper-based problem and a data retrieval process are presented within the frame of a Grade 8 action research study. It is found that both students content knowledge and their digital skills developed more efficiently than in traditional course book and decontextualized digital environments. Furthermore, the method presented here can be adapted to any paper-based problems whose solutions would be more effective in a digital environment and which offer various forms for building schemata both in the subject matter and informatics.

</details>


### [146] [A Simpler Method for Understanding Emergency Shelter Access Patterns](https://arxiv.org/abs/2210.13619)
*Geoffrey G. Messier*

Main category: cs.CY

TL;DR: 简化访问指标（SAM）是一种新的方法，用于描述紧急庇护所的访问模式，以此衡量庇护所客户的脆弱性。它能由非技术人员使用电子表格操作实现，并产生与传统聚类分析相似的结果，但所需数据更少，且能实时反映外部因素（如“住房优先”计划和COVID-19封锁）的影响。


<details>
  <summary>Details</summary>
Motivation: 为庇护所运营者提供一种直观、易于非技术人员理解的访问模式分析方法，以衡量客户脆弱性。

Method: 提出简化访问指标（SAM），并使用北美大型庇护所的客户数据进行演示，将其结果与传统的过渡性、偶发性和长期性客户聚类分析进行比较。

Result: SAM产生了与传统过渡性、偶发性和长期性客户聚类分析相似的结果，但所需数据更少。它能够实时反映外部因素对庇护所访问模式的影响（例如“住房优先”计划和COVID-19封锁），并允许庇护所工作人员直接将SAM的“软”输出用作衡量脆弱性的指标。

Conclusion: SAM是庇护所工作人员理解客户脆弱性、评估外部因素对庇护所访问模式影响的有效工具，它超越了传统的客户分类标签。

Abstract: The Simplified Access Metric (SAM) is a new approach for characterizing emergency shelter access patterns as a measure of shelter client vulnerability. The goal of SAM is to provide shelter operators with an intuitive way to understand access patterns that can be implemented by non-technical staff using spreadsheet operations. Client data from a large North American shelter will be used to demonstrate that SAM produces similar results to traditional transitional, episodic and chronic client cluster analysis. Since SAM requires less data than cluster analysis, it is also able to generate a real time picture of how shelter access patterns are affected by external factors. Timelines generated from nine years of shelter client data using SAM demonstrate the impact of Housing First programming and the COVID-19 lockdown on how people access shelter. Finally, SAM allows shelter staff to move beyond assigning transitional, episodic and chronic labels and instead use the "soft" output of SAM directly as a measure of vulnerability.

</details>


### [147] [Long-Term Mentoring for Computer Science Researchers](https://arxiv.org/abs/2208.04738)
*Emily Ruppel,Sihang Liu,Elba Garza,Sukyoung Ryu,Alexandra Silva,Talia Ringer*

Main category: cs.CY

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 503 - [{'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Early in the pandemic, we -- leaders in the research areas of programming languages (PL) and computer architecture (CA) -- realized that we had a problem: the only way to form new lasting connections in the community was to already have lasting connections in the community. Both of our academic communities had wonderful short-term mentoring programs to address this problem, but it was clear that we needed long-term mentoring programs.
  Those of us in CA approached this scientifically, making an evidence-backed case for community-wide long-term mentoring. In the meantime, one of us in PL had impulsively launched an unofficial long-term mentoring program, founded on chaos and spreadsheets. In January 2021, the latter grew to an official cross-institutional long-term mentoring program called SIGPLAN-M; in January 2022, the former grew to Computer Architecture Long-term Mentoring (CALM).
  The impacts have been strong: SIGPLAN-M reaches 328 mentees and 234 mentors across 41 countries, and mentees have described it as "life changing" and "a career saver." And while CALM is in its pilot phase -- with 13 mentors and 21 mentees across 7 countries -- it has received very positive feedback. The leaders of SIGPLAN-M and CALM shared our designs, impacts, and challenges along the way. Now, we wish to share those with you. We hope this will kick-start a larger long-term mentoring effort across all of computer science.

</details>


### [148] [Optimization Helps Scheduling Nursing Staff at the Long-Term Care Homes of the City of Toronto](https://arxiv.org/abs/2102.09461)
*Manion Anderson,Merve Bodur,Scott Rathwell,Vahid Sarhangian*

Main category: cs.CY

TL;DR: 该研究开发了一个基于电子表格的排班工具，用于优化多伦多长期护理院的护士排班，以减少缺勤率并满足护士偏好。


<details>
  <summary>Details</summary>
Motivation: 多伦多长期护理院的护士排班任务日益艰巨，兼职护士缺勤率高。

Method: 开发了一个基于电子表格的排班工具，其核心是一个分层优化模型，该模型在满足最高需求的同时，生成具有最高总偏好得分的可行排班，并遵守复杂的资历要求。

Result: 该工具在一个拥有391张床位的护理院中实施，将排班生成时间从数十小时缩短到不到一小时。排班成功考虑了护士偏好，平均超过94%的班次被列为最优先选择。

Conclusion: 该排班工具显著提高了排班效率，并提升了护士对班次的满意度，有效解决了长期护理院的排班难题。

Abstract: The City of Toronto Long Term Care Homes & Services (LTCH&S) division is one of the largest providers of long-term care in the Canadian province of Ontario, providing care to 2,640 residents at 10 homes across Toronto. Our collaboration with LTCH&S was initiated to facilitate the increasingly challenging task of scheduling nursing staff and reduce high absenteeism rate observed among the part-time nurses. We developed a spreadsheet-based scheduling tool to automate the generation of schedules and incorporate nurses' preferences for different shifts into the schedules. At the core of the scheduling tool is a hierarchical optimization model that generates a feasible schedule with the highest total preference score while satisfying the maximum possible demand. Feasible schedules had to abide by a set of complex seniority requirements which prioritized more senior nurses when allocating the available shifts. Our scheduling tool was implemented in a 391-bed home in Toronto. The tool allowed nursing managers to generate feasible schedules within a fraction of an hour, in contrast to the status-quo manual approach which could took up to tens of hours. In addition, the schedules successfully accounted for preferences with on average above 94% of the allocated shifts ranked as most preferred.

</details>


### [149] [Developing Excel Thought Leadership](https://arxiv.org/abs/2006.04793)
*David Lyford-Smith*

Main category: cs.CY

TL;DR: 英国特许会计师协会（ICAEW）在五年内发布了三份关于电子表格使用和工作环境最佳实践的领导力论文。本文将回顾这些论文的历史、主要经验教训以及它们如何帮助ICAEW在该领域发展其地位。


<details>
  <summary>Details</summary>
Motivation: 旨在回顾ICAEW在电子表格使用最佳实践方面的三份思想领导力论文的发展历程、主要经验教训，并探讨这些论文如何帮助ICAEW在该领域确立其地位。

Method: 通过回顾和讨论ICAEW三份关于电子表格最佳实践的论文的历史、关键经验教训及其对ICAEW在该领域地位发展的影响。

Result: 摘要中未明确提及具体研究结果，主要阐述了论文将进行的回顾和讨论内容。

Conclusion: 摘要中未明确提及结论，预计结论将围绕ICAEW在电子表格实践领域的发展和经验教训展开。

Abstract: Over a period of five years, the Institute of Chartered Accountants in England and Wales (ICAEW) has developed a suite of three 'thought leadership' papers surrounding good practice in spreadsheet use and spreadsheet work environments. We will review the history of these three papers, the key lessons which each has to teach, and discuss how the process of making them has helped ICAEW to develop its position in the field.

</details>


### [150] [A Case Study of Spreadsheet Use within the Finance and Academic Registry units within a Higher Education Institution](https://arxiv.org/abs/1909.07462)
*Simon Thorne,Jamie Hancock*

Main category: cs.CY

TL;DR: 一项对英国高等教育机构电子表格使用情况的案例研究发现，电子表格数量庞大，开发者画像典型，并且需要明确的指导方针来确保数据完整性和效率。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在探讨英国高等教育机构中电子表格的使用情况，以及其对向外部资助机构准确报告、内部数据完整性和效率的影响。

Method: 通过对英国一所高等教育机构的学术注册和财务部门进行案例研究，分析了电子表格在重要性、培训、经验、目的、技术、创建的电子表格规模和共享方面的使用情况。

Result: 研究结果表明，该机构创建和使用了大量的电子表格，电子表格开发者的画像与其他研究相似，并且该机构需要制定明确的原则和指导方针，以确保数据完整性、减少重复工作并优化电子表格的使用以实现机构目标。

Conclusion: 机构需要制定明确的电子表格开发原则和指导方针，以确保数据完整性、减少重复工作，并优化电子表格的使用以实现机构目标。

Abstract: This paper presents the findings of a case study of spreadsheet use in a higher education institution in the UK. The paper considers the use of spreadsheets in two units of the organisation, academic registry and finance. Spreadsheet use is explored in terms of importance, training, experience, purpose, techniques deployed, size of spreadsheets created and sharing of spreadsheets. The implications of the results are then considered in terms of accurate reporting to external funding bodies such the funding councils, internal data integrity and internal data efficiencies. The results show a large volume of spreadsheets being created and used, that the profile of spreadsheet developers is typical of other studies of spreadsheet use and the need for the organisation to have clear principles and guidelines for the development of spreadsheet models in the organisation to ensure data integrity, reduce duplication of effort and to optimise the use of spreadsheets to meet the institutions goals.

</details>


### [151] [Computer Simulation of Neural Networks Using Spreadsheets: The Dawn of the Age of Camelot](https://arxiv.org/abs/1807.00018)
*Serhiy O. Semerikov,Illia O. Teplytskyi,Yuliia V. Yechkalo,Arnold E. Kiv*

Main category: cs.CY

TL;DR: 该文章探讨了在电子表格环境中开发神经网络计算机模拟训练方法的必要性。它系统地回顾了现有的方法，并分析了计算神经科学领域的历史发展和关键人物（如Nicolas Rashevsky和Walter Pitts），确定了三种有前景的历史模型（Rashevsky的连续两因素模型、McCulloch和Pitts的离散模型以及Householder和Landahl的离散连续模型），认为掌握这些基于历史和遗传方法论的模型对于在该环境中获取神经网络模拟能力至关重要。


<details>
  <summary>Details</summary>
Motivation: 该文章旨在阐明在电子表格环境中开发神经网络计算机模拟训练方法的必要性。

Method: 研究方法包括对现有将电子表格应用于人工神经网络模拟的方法进行系统回顾，区分了解决该问题的基本途径（如联合应用电子表格与模拟工具、第三方插件、宏、标准优化插件以及不使用插件和宏创建网络）。此外，通过分析1890-1950年的著作，确定了《数学生物物理学公报》、其创始人Nicolas Rashevsky及其科学社区在创建和发展计算神经科学模型和方法中的作用。研究还识别了创建神经网络的心理物理学基础、神经计算的数学基础和神经工程方法，并讨论了Walter Pitts在结合描述性和定量训练理论中的作用。

Result: 研究结果确定了在电子表格环境中进行网络计算机模拟训练的基本方法。它明确了《数学生物物理学公报》、Nicolas Rashevsky和相关科学社区在计算神经科学模型和方法发展中的重要作用。研究识别了神经网络创建的心理物理学基础、数学基础和神经工程方法（特别是图像识别）。文章还指出，要获得在电子表格环境中进行神经模拟的能力，需要掌握基于历史和遗传方法的模型。最终，确定了三组有前景的模型（Rashevsky的连续两因素模型、McCulloch和Pitts的离散模型以及Householder和Landahl的离散连续模型），这些模型在开发相应方法方面具有潜力。

Conclusion: 文章得出结论，为了在电子表格环境中获得神经模拟能力，应该掌握基于历史和遗传方法的模型，特别是Rashevsky的连续两因素模型、McCulloch和Pitts的离散模型以及Householder和Landahl的离散连续模型，因为这些模型在开发相关训练方法方面具有前景。

Abstract: The article substantiates the necessity to develop training methods of computer simulation of neural networks in the spreadsheet environment. The systematic review of their application to simulating artificial neural networks is performed. The authors distinguish basic approaches to solving the problem of network computer simulation training in the spreadsheet environment, joint application of spreadsheets and tools of neural network simulation, application of third-party add-ins to spreadsheets, development of macros using the embedded languages of spreadsheets; use of standard spreadsheet add-ins for non-linear optimization, creation of neural networks in the spreadsheet environment without add-ins and macros. After analyzing a collection of writings of 1890-1950, the research determines the role of the scientific journal "Bulletin of Mathematical Biophysics", its founder Nicolas Rashevsky and the scientific community around the journal in creating and developing models and methods of computational neuroscience. There are identified psychophysical basics of creating neural networks, mathematical foundations of neural computing and methods of neuroengineering (image recognition, in particular). The role of Walter Pitts in combining the descriptive and quantitative theories of training is discussed. It is shown that to acquire neural simulation competences in the spreadsheet environment, one should master the models based on the historical and genetic approach. It is indicated that there are three groups of models, which are promising in terms of developing corresponding methods - the continuous two-factor model of Rashevsky, the discrete model of McCulloch and Pitts, and the discrete-continuous models of Householder and Landahl.

</details>


### [152] [Edu-Edition Spreadsheet Competency Framework](https://arxiv.org/abs/1802.00496)
*Maria Csernoch,Piroska Biró*

Main category: cs.CY

TL;DR: 本文介绍了电子表格能力框架的教育版（E2SCF），旨在通过强调高数学能力、计算机支持的真实世界问题解决和双向知识转移，从教育初期开始培养学生的电子表格能力和可迁移的问题解决技能。


<details>
  <summary>Details</summary>
Motivation: 当前的金融专业人士电子表格能力框架显示，电子表格能力的培养应尽早从教育阶段开始，并由专家教师提供支持，以提高效率。

Method: 提出了电子表格能力框架的教育版（E2SCF），其核心特点是高数学能力的计算机支持真实世界问题解决。该方法基于早期培训中的双向知识转移、数据和错误分析与处理以及电子表格的编程方面。

Result: E2SCF旨在帮助基础和普通用户建立扎实的电子表格知识，并培养可迁移的问题解决技能和能力。

Conclusion: E2SCF通过在教育早期阶段引入结构化方法，强调实践和理论结合，有效提升学生的电子表格运用和问题解决能力。

Abstract: Based on the Spreadsheet Competency Framework for finance professionals, in the present paper we introduce the Edu-Edition of the Spreadsheet Competency Framework (E2SCF). We claim that building spreadsheet competences should start in education, as early as possible, and this process is a lot more effective if support arrives from expert teachers. The main feature of E2SCF is high mathability computer-supported real world problem solving. This approach is based on - from the very beginning of training - a two-directional knowledge transfer, data and error analysis and handling, and the programming aspect of spreadsheets. Based on these features, E2SCF is set up for basic and general users to build up firm spreadsheet knowledge and to develop transferable problem solving skills and competences.

</details>


### [153] [The Future of Spreadsheets in the Big Data Era](https://arxiv.org/abs/1801.10231)
*David Birch,David Lyford-Smith,Yike Guo*

Main category: cs.CY

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\nPlease retry in 49.924631489s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '250'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '49s'}]}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: The humble spreadsheet is the most widely used data storage, manipulation and modelling tool. Its ubiquity over the past 30 years has seen its successful application in every area of life. Surprisingly the spreadsheet has remained fundamentally unchanged over the past three decades. As spreadsheet technology enters its 4th decade a number of drivers of change are beginning to impact upon the spreadsheet. The rise of Big Data, increased end-user computing and mobile computing will undoubtedly increasingly shape the evolution and use of spreadsheet technology.
  To explore the future of spreadsheet technology a workshop was convened with the aim of "bringing together academia and industry to examine the future direction of spreadsheet technology and the consequences for users". This paper records the views of the participants on the reasons for the success of the spreadsheet, the trends driving change and the likely directions of change for the spreadsheet. We then set out key directions for further research in the evolution and use of spreadsheets. Finally we look at the implications of these trends for the end users who after all are the reason for the remarkable success of the spreadsheet.

</details>


### [154] [The Role of Spreadsheets in Clinical Decision Support: A Survey of the Medical Algorithms Company User Community](https://arxiv.org/abs/1801.07782)
*Simon Thorne*

Main category: cs.CY

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\nPlease retry in 24.191423403s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '250'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '24s'}]}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: This paper presents and discusses the results of a small scoping survey of Clinical Decision Support System (CDSS) users from the Medical Algorithms Company website which hosts 24,000 different CDSS. These results are analysed, discussed, and compared with other similar studies and contribute to the wider understanding of how CDSS impact on clinical practice. The results show that CDSS provided by Medal are being used by clinical professionals in a variety of settings, both as an operational tool and as a research and reference tool. Whilst these tools are implemented and executed in a database, the initial logic is worked out on a spreadsheet. The paper describes that process and examines some of the results of the survey.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [155] [Reducing Aleatoric and Epistemic Uncertainty through Multi-modal Data Acquisition](https://arxiv.org/abs/2501.18268)
*Arthur Hoarau,Benjamin Quost,Sébastien Destercke,Willem Waegeman*

Main category: cs.LG

TL;DR: 该论文提出了一种创新的数据采集框架，用于多模态人工智能系统中的不确定性分离，以指导在样本量和数据模态两个方向上的采样。其核心假设是，增加模态数量可以降低任意不确定性，而增加观测数据可以降低认知不确定性。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统需要结合多模态数据进行准确预测，但这挑战了机器学习社区中关于认知不确定性可减少而任意不确定性不可减少的传统假设，尤其是在信息来源多样时。

Method: 论文引入了一个创新的数据采集框架，其中不确定性分离可以产生可操作的决策，允许在样本量和数据模态两个方向上进行采样。该框架结合了主动学习、主动特征获取和不确定性量化。

Result: 主要假设是任意不确定性随模态数量增加而减少，而认知不确定性随观测数量增加而减少。通过在两个多模态数据集上的概念验证实现展示了该数据采集框架。

Conclusion: 该研究提出了一种新的方法来处理多模态AI系统中的不确定性，挑战了传统观念，并建议通过增加模态数量来降低任意不确定性，为数据采集提供了新的策略。

Abstract: To generate accurate and reliable predictions, modern AI systems need to combine data from multiple modalities, such as text, images, audio, spreadsheets, and time series. Multi-modal data introduces new opportunities and challenges for disentangling uncertainty: it is commonly assumed in the machine learning community that epistemic uncertainty can be reduced by collecting more data, while aleatoric uncertainty is irreducible. However, this assumption is challenged in modern AI systems when information is obtained from different modalities. This paper introduces an innovative data acquisition framework where uncertainty disentanglement leads to actionable decisions, allowing sampling in two directions: sample size and data modality. The main hypothesis is that aleatoric uncertainty decreases as the number of modalities increases, while epistemic uncertainty decreases by collecting more observations. We provide proof-of-concept implementations on two multi-modal datasets to showcase our data acquisition framework, which combines ideas from active learning, active feature acquisition and uncertainty quantification.

</details>


### [156] [Large Scale Transfer Learning for Tabular Data via Language Modeling](https://arxiv.org/abs/2406.12031)
*Josh Gardner,Juan C. Perdomo,Ludwig Schmidt*

Main category: cs.LG

TL;DR: TabuLa-8B是一个用于表格预测的语言模型，它通过在大规模表格数据集上微调Llama 3-8B，实现了在零样本和少样本设置下超越现有最先进表格预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在语言建模和计算机视觉等领域推动了迁移学习，但在表格数据领域尚未产生类似影响，本研究旨在弥补这一差距。

Method: 研究人员从TabLib语料库中提取了一个包含超过2.1B行和4M唯一表格的大规模高质量训练数据集，并采用新颖的打包和注意力机制，对Llama 3-8B大语言模型进行了微调，用于表格数据预测（分类和分箱回归）。

Result: TabuLa-8B在329个数据集的测试中，未见表格的零样本准确率比随机猜测高出15个百分点以上，并且在少样本设置（1-32样本）下，无需对目标数据集进行任何微调，其准确率比在相同或多达16倍数据上显式训练的XGBoost和TabPFN模型高出5-15个百分点。

Conclusion: TabuLa-8B通过引入一个针对表格预测的语言模型，显著提升了在表格数据领域进行迁移学习的能力，并在零样本和少样本场景下展现出优于现有模型的卓越性能。

Abstract: Tabular data -- structured, heterogeneous, spreadsheet-style data with rows and columns -- is widely used in practice across many domains. However, while recent foundation models have reduced the need for developing task-specific datasets and predictors in domains such as language modeling and computer vision, this transfer learning paradigm has not had similar impact in the tabular domain. In this work, we seek to narrow this gap and present TabuLa-8B, a language model for tabular prediction. We define a process for extracting a large, high-quality training dataset from the TabLib corpus, proposing methods for tabular data filtering and quality control. Using the resulting dataset, which comprises over 2.1B rows from over 4M unique tables, we fine-tune a Llama 3-8B large language model (LLM) for tabular data prediction (classification and binned regression) using a novel packing and attention scheme for tabular prediction. Through evaluation across a test suite of 329 datasets, we find that TabuLa-8B has zero-shot accuracy on unseen tables that is over 15 percentage points (pp) higher than random guessing, a feat that is not possible with existing state-of-the-art tabular prediction models (e.g. XGBoost, TabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the target datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN models that are explicitly trained on equal, or even up to 16x more data. We release our model, code, and data along with the publication of this paper.

</details>


### [157] [Generating tabular datasets under differential privacy](https://arxiv.org/abs/2308.14784)
*Gianluca Truda*

Main category: cs.LG

TL;DR: 该论文提出TableDiffusion，一个用于表格数据合成的差分隐私扩散模型，在质量-隐私权衡方面优于GAN，避免了模式崩溃，并且数据和隐私效率更高。


<details>
  <summary>Details</summary>
Motivation: 机器学习需要高质量的训练数据，其中敏感的表格数据需要合成以保护隐私。现有的差分隐私生成对抗网络（DP-GANs）在合成敏感表格数据时面临质量-隐私权衡、训练不稳定和模式崩溃等问题。

Method: 本研究实现了利用注意力机制学习可逆表格表示的新型端到端模型，并引入了TableDiffusion，这是首个用于表格数据合成的差分隐私扩散模型。TableDiffusion通过预测添加的噪声来绕过重建混合类型表格数据的挑战。

Result: TableDiffusion生成了更高保真度的合成数据集，避免了模式崩溃问题，并在私有化表格数据合成方面取得了最先进的性能。它在数据和隐私效率方面远远优于对抗式范式。

Conclusion: 扩散范式（TableDiffusion）在差分隐私表格数据合成方面表现出比对抗式范式（GANs）更优越的性能，因为它具有更高的数据和隐私效率以及更平滑的迭代训练过程。

Abstract: Machine Learning (ML) is accelerating progress across fields and industries, but relies on accessible and high-quality training data. Some of the most important datasets are found in biomedical and financial domains in the form of spreadsheets and relational databases. But this tabular data is often sensitive in nature. Synthetic data generation offers the potential to unlock sensitive data, but generative models tend to memorise and regurgitate training data, which undermines the privacy goal. To remedy this, researchers have incorporated the mathematical framework of Differential Privacy (DP) into the training process of deep neural networks. But this creates a trade-off between the quality and privacy of the resulting data. Generative Adversarial Networks (GANs) are the dominant paradigm for synthesising tabular data under DP, but suffer from unstable adversarial training and mode collapse, which are exacerbated by the privacy constraints and challenging tabular data modality. This work optimises the quality-privacy trade-off of generative models, producing higher quality tabular datasets with the same privacy guarantees. We implement novel end-to-end models that leverage attention mechanisms to learn reversible tabular representations. We also introduce TableDiffusion, the first differentially-private diffusion model for tabular data synthesis. Our experiments show that TableDiffusion produces higher-fidelity synthetic datasets, avoids the mode collapse problem, and achieves state-of-the-art performance on privatised tabular data synthesis. By implementing TableDiffusion to predict the added noise, we enabled it to bypass the challenges of reconstructing mixed-type tabular data. Overall, the diffusion paradigm proves vastly more data and privacy efficient than the adversarial paradigm, due to augmented re-use of each data batch and a smoother iterative training process.

</details>


### [158] [Smart Metro: Deep Learning Approaches to Forecasting the MRT Line 3 Ridership](https://arxiv.org/abs/2304.07303)
*Jayrald Empino,Jean Allyson Junsay,Mary Grace Verzon,Mideth Abisado,Shekinah Lor Huyo-a,Gabriel Avelino Sampedro*

Main category: cs.LG

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 503 - [{'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Since its establishment in 1999, the Metro Rail Transit Line 3 (MRT3) has served as a transportation option for numerous passengers in Metro Manila, Philippines. The Philippine government's transportation department records more than a thousand people using the MRT3 daily and forecasting the daily passenger count may be rather challenging. The MRT3's daily ridership fluctuates owing to variables such as holidays, working days, and other unexpected issues. Commuters do not know how many other commuters are on their route on a given day, which may hinder their ability to plan an efficient itinerary. Currently, the DOTr depends on spreadsheets containing historical data, which might be challenging to examine. This study presents a time series prediction of daily traffic to anticipate future attendance at a particular station on specific days.

</details>


### [159] [Adversarial Networks and Machine Learning for File Classification](https://arxiv.org/abs/2301.11964)
*Ken St. Germain,Josh Angichiodo*

Main category: cs.LG

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 503 - [{'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Correctly identifying the type of file under examination is a critical part of a forensic investigation. The file type alone suggests the embedded content, such as a picture, video, manuscript, spreadsheet, etc. In cases where a system owner might desire to keep their files inaccessible or file type concealed, we propose using an adversarially-trained machine learning neural network to determine a file's true type even if the extension or file header is obfuscated to complicate its discovery. Our semi-supervised generative adversarial network (SGAN) achieved 97.6% accuracy in classifying files across 11 different types. We also compared our network against a traditional standalone neural network and three other machine learning algorithms. The adversarially-trained network proved to be the most precise file classifier especially in scenarios with few supervised samples available. Our implementation of a file classifier using an SGAN is implemented on GitHub (https://ksaintg.github.io/SGAN-File-Classier).

</details>


### [160] [TabularNet: A Neural Network Architecture for Understanding Semantic Structures of Tabular Data](https://arxiv.org/abs/2106.03096)
*Lun Du,Fei Gao,Xu Chen,Ran Jia,Junshan Wang,Jiang Zhang,Shi Han,Dongmei Zhang*

Main category: cs.LG

TL;DR: TabularNet是一种新颖的神经网络架构，通过空间编码器（使用池化和Bi-GRU）和关系编码器（使用基于WordNet树的图构建和GCN）同时提取表格数据的空间和关系信息，在表格理解任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有研究在表格数据理解中，通常采用卷积神经网络（CNN）建模空间信息，但忽略了单元格之间更复杂的关系信息（如层级和并列关系）。自动理解表格固有的语义结构是一个关键问题。

Method: 提出了TabularNet神经网络架构。空间编码器利用行/列级别的池化和双向门控循环单元（Bi-GRU）来捕获统计信息和局部位置相关性。关系信息方面，设计了一种基于WordNet树的新图构建方法，并采用基于图卷积网络（GCN）的编码器，专注于单元格之间的层级和并列关系。该架构可作为不同理解任务的统一神经网络骨干，并应用于多任务场景。

Result: 在两个真实世界电子表格数据集上的三个分类任务中进行了广泛实验，结果表明所提出的TabularNet优于现有最先进的基线方法。

Conclusion: TabularNet能够有效地从表格中提取空间和关系信息，在表格数据理解任务中超越了现有方法。

Abstract: Tabular data are ubiquitous for the widespread applications of tables and hence have attracted the attention of researchers to extract underlying information. One of the critical problems in mining tabular data is how to understand their inherent semantic structures automatically. Existing studies typically adopt Convolutional Neural Network (CNN) to model the spatial information of tabular structures yet ignore more diverse relational information between cells, such as the hierarchical and paratactic relationships. To simultaneously extract spatial and relational information from tables, we propose a novel neural network architecture, TabularNet. The spatial encoder of TabularNet utilizes the row/column-level Pooling and the Bidirectional Gated Recurrent Unit (Bi-GRU) to capture statistical information and local positional correlation, respectively. For relational information, we design a new graph construction method based on the WordNet tree and adopt a Graph Convolutional Network (GCN) based encoder that focuses on the hierarchical and paratactic relationships between cells. Our neural network architecture can be a unified neural backbone for different understanding tasks and utilized in a multitask scenario. We conduct extensive experiments on three classification tasks with two real-world spreadsheet data sets, and the results demonstrate the effectiveness of our proposed TabularNet over state-of-the-art baselines.

</details>


### [161] [Visual Backpropagation](https://arxiv.org/abs/1906.04011)
*Roy S. Freedman*

Main category: cs.LG

TL;DR: 该论文展示了一种在电子表格中实现反向传播的声明式函数式编程方法，称为可视化反向传播，它透明且不需要宏或外部代码。


<details>
  <summary>Details</summary>
Motivation: 旨在利用数组工作表公式和手动计算，在电子表格中提供一种可视化和透明的反向传播实现。

Method: 采用“可视化反向传播”方法，通过声明式函数式编程规范在电子表格中实现反向传播，利用数组工作表公式、手动计算和类似于脉动阵列的顺序计算。该方法避免使用宏、用户定义函数、循环、赋值语句或与常规语言编写的程序链接。

Result: 实现了一种在电子表格中可视化且透明的反向传播。论文通过在一个标准回归问题上将可视化反向传播解决方案与Tensorflow（Python）解决方案进行比较进行了说明。

Conclusion: 开发了一种名为“可视化反向传播”的方法，它提供了一种在电子表格中实现反向传播的可视化和透明方式，独立于传统的编程结构。

Abstract: We show how a declarative functional programming specification of backpropagation yields a visual and transparent implementation within spreadsheets. We call our method Visual Backpropagation. This backpropagation implementation exploits array worksheet formulas, manual calculation, and has a sequential order of computation similar to the processing of a systolic array. The implementation uses no hidden macros nor user-defined functions; there are no loops, assignment statements, or links to any procedural programs written in conventional languages. As an illustration, we compare a Visual Backpropagation solution to a Tensorflow (Python) solution on a standard regression problem.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [162] [Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models](https://arxiv.org/abs/2505.23667)
*Lang Cao,Jingxian Xu,Hanbing Liu,Jinyu Wang,Mengyu Zhou,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 大型语言模型在表格数据推理方面存在困难。Formula Tuning (Fortune) 是一个强化学习框架，它训练语言模型生成可执行的电子表格公式，用于对表格数据进行问答，从而提高性能，尤其是在数值和符号推理任务中。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LMs) 在处理表格数据时，尤其是在复杂场景下，在准确的数值或符号推理方面仍然存在困难。电子表格公式提供了一种强大且富有表达力的媒介来表示可执行的符号操作，但其丰富的推理模式在很大程度上尚未得到充分利用。

Method: 本文提出了 Formula Tuning (Fortune)，一个强化学习 (RL) 框架。该框架训练语言模型生成可执行的电子表格公式，用于对通用表格数据进行问答。Formula Tuning 通过使用二元答案正确性作为奖励信号，减少了对监督公式标注的依赖，从而引导模型通过推理学习公式推导。

Result: Formula Tuning 大幅提升了语言模型的性能，特别是在多步数值和符号推理任务上。一个 7B 模型在表格理解方面超越了 OpenAI o1，并在七个表格推理基准测试中展示了其有效性。

Conclusion: 公式驱动的强化学习（特别是 Fortune）在推进语言模型中的符号表格推理方面具有巨大潜力。

Abstract: Tables are a fundamental structure for organizing and analyzing data, making effective table understanding a critical capability for intelligent systems. While large language models (LMs) demonstrate strong general reasoning abilities, they continue to struggle with accurate numerical or symbolic reasoning over tabular data, especially in complex scenarios. Spreadsheet formulas provide a powerful and expressive medium for representing executable symbolic operations, encoding rich reasoning patterns that remain largely underutilized. In this paper, we propose Formula Tuning (Fortune), a reinforcement learning (RL) framework that trains LMs to generate executable spreadsheet formulas for question answering over general tabular data. Formula Tuning reduces the reliance on supervised formula annotations by using binary answer correctness as a reward signal, guiding the model to learn formula derivation through reasoning. We provide a theoretical analysis of its advantages and demonstrate its effectiveness through extensive experiments on seven table reasoning benchmarks. Formula Tuning substantially enhances LM performance, particularly on multi-step numerical and symbolic reasoning tasks, enabling a 7B model to outperform OpenAI o1 on table understanding. This highlights the potential of formula-driven RL to advance symbolic table reasoning in LMs.

</details>


### [163] [The AI Risk Repository: A Comprehensive Meta-Review, Database, and Taxonomy of Risks From Artificial Intelligence](https://arxiv.org/abs/2408.12622)
*Peter Slattery,Alexander K. Saeri,Emily A. C. Grundy,Jess Graham,Michael Noetel,Risto Uuk,James Dao,Soroush Pour,Stephen Casper,Neil Thompson*

Main category: cs.AI

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 503 - [{'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: The risks posed by Artificial Intelligence (AI) are of considerable concern to academics, auditors, policymakers, AI companies, and the public. However, a lack of shared understanding of AI risks can impede our ability to comprehensively discuss, research, and react to them. This paper addresses this gap by creating an AI Risk Repository to serve as a common frame of reference. This comprises a living database of 777 risks extracted from 43 taxonomies, which can be filtered based on two overarching taxonomies and easily accessed, modified, and updated via our website and online spreadsheets. We construct our Repository with a systematic review of taxonomies and other structured classifications of AI risk followed by an expert consultation. We develop our taxonomies of AI risk using a best-fit framework synthesis. Our high-level Causal Taxonomy of AI Risks classifies each risk by its causal factors (1) Entity: Human, AI; (2) Intentionality: Intentional, Unintentional; and (3) Timing: Pre-deployment; Post-deployment. Our mid-level Domain Taxonomy of AI Risks classifies risks into seven AI risk domains: (1) Discrimination & toxicity, (2) Privacy & security, (3) Misinformation, (4) Malicious actors & misuse, (5) Human-computer interaction, (6) Socioeconomic & environmental, and (7) AI system safety, failures, & limitations. These are further divided into 23 subdomains. The AI Risk Repository is, to our knowledge, the first attempt to rigorously curate, analyze, and extract AI risk frameworks into a publicly accessible, comprehensive, extensible, and categorized risk database. This creates a foundation for a more coordinated, coherent, and complete approach to defining, auditing, and managing the risks posed by AI systems.

</details>


### [164] [SpreadsheetLLM: Encoding Spreadsheets for Large Language Models](https://arxiv.org/abs/2407.09025)
*Haoyu Dong,Jianbo Zhao,Yuzhang Tian,Junyu Xiong,Shiyu Xia,Mengyu Zhou,Yun Lin,José Cambronero,Yeye He,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 503 - [{'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Spreadsheets are characterized by their extensive two-dimensional grids, flexible layouts, and varied formatting options, which pose significant challenges for large language models (LLMs). In response, we introduce SpreadsheetLLM, pioneering an efficient encoding method designed to unleash and optimize LLMs' powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs' token constraints, making it impractical for most applications. To tackle this challenge, we develop SheetCompressor, an innovative encoding framework that compresses spreadsheets effectively for LLMs. It comprises three modules: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in the spreadsheet table detection task, outperforming the vanilla approach by 25.6% in GPT4's in-context learning setting. Moreover, fine-tuned LLM with SheetCompressor has an average compression ratio of 25 times, and achieves a state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%. Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet understanding and validate it in a new and demanding spreadsheet QA task. We methodically leverage the inherent layout and structure of spreadsheets, demonstrating that SpreadsheetLLM is highly effective across a variety of spreadsheet tasks.

</details>


### [165] [SheetAgent: Towards A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models](https://arxiv.org/abs/2403.03636)
*Yibin Chen,Yifu Yuan,Zeyu Zhang,Yan Zheng,Jinyi Liu,Fei Ni,Jianye Hao,Hangyu Mao,Fuzheng Zhang*

Main category: cs.AI

TL;DR: 本文介绍了SheetRM，一个用于测试复杂电子表格操作的新基准，并提出了SheetAgent，一个基于LLM的自主代理，通过其规划器、信息器和检索器模块，实现了对电子表格的精确操作和高级推理，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: LLM在自动电子表格操作方面尚未在复杂的实际任务中进行研究，这些任务涉及推理挑战（例如，多步骤推理和模糊需求）。

Method: 提出了一种名为SheetAgent的新型自主代理，它利用LLM的强大功能。SheetAgent由三个协作模块组成：规划器、信息器和检索器，通过迭代任务推理和反思，在无需人工交互的情况下实现电子表格的高级推理和精确操作。

Result: 广泛的实验表明，SheetAgent在多个基准测试上比基线提高了20-40%的通过率，从而提高了电子表格操作的精度并展示了卓越的表格推理能力。

Conclusion: SheetAgent显著提高了电子表格操作的精度，并展示了卓越的表格推理能力，解决了实际挑战。

Abstract: Spreadsheets are ubiquitous across the World Wide Web, playing a critical role in enhancing work efficiency across various domains. Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce SheetRM, a benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges. To mitigate the above challenges, we further propose SheetAgent, a novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of three collaborative modules: Planner, Informer, and Retriever, achieving both advanced reasoning and accurate manipulation over spreadsheets without human interaction through iterative task reasoning and reflection. Extensive experiments demonstrate that SheetAgent delivers 20--40\% pass rate improvements on multiple benchmarks over baselines, achieving enhanced precision in spreadsheet manipulation and demonstrating superior table reasoning abilities. More details and visualizations are available at the project website: https://sheetagent.github.io/. The datasets and source code are available at https://anonymous.4open.science/r/SheetAgent.

</details>


### [166] [Large Language Model for Table Processing: A Survey](https://arxiv.org/abs/2402.05121)
*Weizheng Lu,Jing Zhang,Ju Fan,Zihao Fu,Yueguo Chen,Xiaoyong Du*

Main category: cs.AI

TL;DR: 这篇综述全面概述了使用大型语言模型（LLMs）和视觉语言模型（VLMs）处理表格相关任务的现状，包括用户场景、技术方法、训练技术和提示工程，并指出了未来的挑战。


<details>
  <summary>Details</summary>
Motivation: 表格在日常活动中至关重要，通过LLMs或VLMs自动化表格任务具有巨大的公共利益，吸引了学术界和工业界的兴趣。

Method: 本文提供了一份全面的综述，涵盖了表格相关任务的用户场景和技术方面。它涵盖了传统的表格问答任务以及新兴的电子表格操作和表格数据分析领域。总结了针对表格处理的LLM和VLM训练技术，并讨论了提示工程，特别是使用LLM驱动的代理来完成各种表格相关任务。

Result: 综述涵盖了各种表格任务、LLMs和VLMs的训练技术以及提示工程方法，并提出了多样化用户输入和思维链慢速推理等挑战。

Conclusion: 该综述全面概述了LLM和VLM在表格任务中的应用，并探讨了当前面临的挑战。

Abstract: Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, web table question answering, and image table information extraction. Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides a comprehensive overview of table-related tasks, examining both user scenarios and technical aspects. It covers traditional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processing. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought.

</details>


### [167] [FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language](https://arxiv.org/abs/2310.17306)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Elnaz Nouri,Mohammad Raza,Gust Verbruggen*

Main category: cs.AI

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 503 - [{'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires them to understand and implement the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system. To evaluate FormaT5 on diverse and real scenarios, we create an extensive benchmark of 1053 CF tasks, containing real-world descriptions collected from four different sources. We release our benchmarks to encourage research in this area. Abstention and filling allow FormaT5 to outperform 8 different neural approaches on our benchmarks, both with and without examples. Our results illustrate the value of building domain-specific learning systems.

</details>


### [168] [Don't Treat the Symptom, Find the Cause! Efficient Artificial-Intelligence Methods for (Interactive) Debugging](https://arxiv.org/abs/2306.12850)
*Patrick Rodler*

Main category: cs.AI

TL;DR: 模型化诊断是应对现代复杂系统故障的关键方法，旨在最小化故障造成的损害。


<details>
  <summary>Details</summary>
Motivation: 现代世界对日益复杂的系统高度依赖，导致系统故障的可能性及潜在负面影响增大。因此，最大程度地减少故障损害（系统停机时间和维修成本）成为一个至关重要的需求。

Method: 模型化诊断是一种基于原理、独立于领域的方法，通过整合知识表示、自动化推理、启发式问题解决、智能搜索、优化、随机学、统计学、不确定性决策、机器学习以及微积分、组合学和集合论等多种技术，用于检测、定位和修复异常行为系统中的故障。本论文将介绍该领域、指出主要挑战并讨论相关研究方法。

Result: 摘要中未给出具体研究结果，而是阐述了本论文将介绍模型化诊断主题、指出该领域的主要挑战，并讨论作者研究中解决这些问题的一些方法。

Conclusion: 模型化诊断是检测和修复复杂系统故障的必要且通用的方法。本论文旨在深入探讨其基础、挑战和解决方案。

Abstract: In the modern world, we are permanently using, leveraging, interacting with, and relying upon systems of ever higher sophistication, ranging from our cars, recommender systems in e-commerce, and networks when we go online, to integrated circuits when using our PCs and smartphones, the power grid to ensure our energy supply, security-critical software when accessing our bank accounts, and spreadsheets for financial planning and decision making. The complexity of these systems coupled with our high dependency on them implies both a non-negligible likelihood of system failures, and a high potential that such failures have significant negative effects on our everyday life. For that reason, it is a vital requirement to keep the harm of emerging failures to a minimum, which means minimizing the system downtime as well as the cost of system repair. This is where model-based diagnosis comes into play.
  Model-based diagnosis is a principled, domain-independent approach that can be generally applied to troubleshoot systems of a wide variety of types, including all the ones mentioned above, and many more. It exploits and orchestrates i.a. techniques for knowledge representation, automated reasoning, heuristic problem solving, intelligent search, optimization, stochastics, statistics, decision making under uncertainty, machine learning, as well as calculus, combinatorics and set theory to detect, localize, and fix faults in abnormally behaving systems.
  In this thesis, we will give an introduction to the topic of model-based diagnosis, point out the major challenges in the field, and discuss a selection of approaches from our research addressing these issues.

</details>


### [169] [CORNET: Learning Table Formatting Rules By Example](https://arxiv.org/abs/2208.06032)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Mohammad Raza,Gust Verbruggen*

Main category: cs.AI

TL;DR: CORNET系统通过结合符号规则枚举和神经排序器，从用户示例中自动学习电子表格的条件格式规则，解决了用户编写格式规则困难的问题，并且能够发现比用户手写更短的规则。


<details>
  <summary>Details</summary>
Motivation: 电子表格的样式格式对于呈现和分析都很重要，但编写自动格式化规则对用户来说具有挑战性，因为它需要了解底层规则语言和数据逻辑。

Method: 我们提出了CORNET，一个通过结合符号规则枚举和神经排序器，从用户提供的格式化单元格示例中自动学习条件格式规则的系统。CORNET从归纳式编程的进展中获得启发。

Result: 从超过180万个真实工作表中提取的超过45万条唯一格式规则的表格数据集用于激励和评估。CORNET在各种评估设置中准确地学习规则，并且发现比用户编写的规则更短，甚至能发现用户手动格式化的电子表格中的规则。

Conclusion: CORNET系统能够有效且准确地从用户示例中学习电子表格的条件格式规则，简化了用户操作，并提高了规则的效率。

Abstract: Spreadsheets are widely used for table manipulation and presentation. Stylistic formatting of these tables is an important property for both presentation and analysis. As a result, popular spreadsheet software, such as Excel, supports automatically formatting tables based on rules. Unfortunately, writing such formatting rules can be challenging for users as it requires knowledge of the underlying rule language and data logic. We present CORNET, a system that tackles the novel problem of automatically learning such formatting rules from user examples in the form of formatted cells. CORNET takes inspiration from advances in inductive programming and combines symbolic rule enumeration with a neural ranker to learn conditional formatting rules. To motivate and evaluate our approach, we extracted tables with over 450K unique formatting rules from a corpus of over 1.8M real worksheets. Since we are the first to introduce conditional formatting, we compare CORNET to a wide range of symbolic and neural baselines adapted from related domains. Our results show that CORNET accurately learns rules across varying evaluation setups. Additionally, we show that CORNET finds shorter rules than those that a user has written and discovers rules in spreadsheets that users have manually formatted.

</details>


### [170] [Named Entity Recognition in Industrial Tables using Tabular Language Models](https://arxiv.org/abs/2209.14812)
*Aneta Koleva,Martin Ringsquandl,Mark Buckley,Rakebul Hasan,Volker Tresp*

Main category: cs.AI

TL;DR: 本文研究了如何将专门的基于 Transformer 的表格数据编码模型应用于工业命名实体识别（NER）问题，特别是在电子表格数据上。通过开发基于领域知识图谱的数据增强策略，解决了标注数据缺乏和技术复杂性问题，并证明了表格归纳偏置对于模型性能和收敛至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管表格数据在工业中无处不在，但表格 Transformer 模型的应用仍然缺失。本文旨在将这些模型应用于一个工业命名实体识别（NER）问题，其中实体在表格结构的电子表格中提及。主要挑战包括电子表格的高度技术性以及缺乏标注数据。

Method: 开发了一种基于现有领域特定知识图谱的专用表格数据增强策略。研究了表格结构作为归纳偏置与表格作为线性化序列相比的优势。

Result: 数据增强策略显著提升了低资源场景下的性能。表格 Transformer 模型优于其他基线模型。

Conclusion: 表格归纳偏置对于基于 Transformer 的模型的收敛至关重要。

Abstract: Specialized transformer-based models for encoding tabular data have gained interest in academia. Although tabular data is omnipresent in industry, applications of table transformers are still missing. In this paper, we study how these models can be applied to an industrial Named Entity Recognition (NER) problem where the entities are mentioned in tabular-structured spreadsheets. The highly technical nature of spreadsheets as well as the lack of labeled data present major challenges for fine-tuning transformer-based models. Therefore, we develop a dedicated table data augmentation strategy based on available domain-specific knowledge graphs. We show that this boosts performance in our low-resource scenario considerably. Further, we investigate the benefits of tabular structure as inductive bias compared to tables as linearized sequences. Our experiments confirm that a table transformer outperforms other baselines and that its tabular inductive bias is vital for convergence of transformer-based models.

</details>


### [171] [Spreadsheet computing with Finite Domain Constraint Enhancements](https://arxiv.org/abs/2203.10944)
*Ezana N. Beyenne*

Main category: cs.AI

TL;DR: 该论文扩展了电子表格计算范式，通过整合有限约束求解器来解决约束满足问题。它允许电子表格单元格与有限域或指定单元格间关系的约束相关联，并提供了一套电子表格特定的约束以帮助控制大型电子表格应用程序实现的扩展性。


<details>
  <summary>Details</summary>
Motivation: 电子表格计算因其易用性和实用性而广受欢迎，但受限于单向数据流，只能处理簿记类任务。本文旨在克服这一局限性，使其能够解决更复杂的问题。

Method: 本文提出了一种将有限约束求解器与电子表格计算范式无缝结合的框架。该框架允许单元格附加到有限域或约束，并提供了一个约束求解接口以及一套电子表格特有的约束，以提高大型电子表格应用的扩展性。

Result: 通过该扩展，电子表格范式能够解决约束满足问题。论文通过示例展示了其可用性和实用性。

Conclusion: 该论文成功地将电子表格计算范式扩展到能够处理约束满足问题，从而增强了其功能，超越了传统的簿记任务。

Abstract: Spreadsheet computing is one of the more popular computing methodologies in today's modern society. The spreadsheet application's ease of use and usefulness has enabled non-programmers to perform programming-like tasks in a familiar setting modeled after the tabular "pen and paper" approach. However, spreadsheet applications are limited to bookkeeping-like tasks due to their single-direction data flow. This thesis demonstrates an extension of the spreadsheet computing paradigm in overcoming this limitation to solve constraint satisfaction problems. We present a framework seamlessly incorporating a finite constraint solver with the spreadsheet computing paradigm. This framework allows the individual cells in the spreadsheet to be attached to either a finite domain or a constraint specifying the relationship among the cells. The framework provides an interface for constraint solving and further enhances the spreadsheet computing paradigm by providing a set of spreadsheet-specific constraints that will aid in controlling the scalability of large spreadsheet applications implementations. Finally, we provide examples to demonstrate the usability and usefulness of the extended spreadsheet paradigm.
  Keywords: Spreadsheet computing, Constraint Logic Programming, Constraint satisfaction, Domain-Specific language, Excel, SWI Prolog, C#

</details>


### [172] [Human-Machine Collaboration for Democratizing Data Science](https://arxiv.org/abs/2004.11113)
*Clément Gautrais,Yann Dauxais,Stefano Teso,Samuel Kolb,Gust Verbruggen,Luc De Raedt*

Main category: cs.AI

TL;DR: 本文介绍了一个名为VisualSynth的人机协作框架，旨在通过允许用户在标准电子表格软件中，利用彩色草图和AI技术，执行和自动化各种数据分析任务，从而实现数据科学的普及化。


<details>
  <summary>Details</summary>
Motivation: 大多数人希望分析他们的数据，但很少有人拥有数据科学专业知识。本文的动机是解决这一问题，通过人机协作使数据科学民主化。

Method: VisualSynth通过用户提供彩色草图（即，对电子表格的部分进行着色）来部分指定数据科学任务，然后利用人工智能技术确定并执行这些任务。

Result: VisualSynth使用户能够进行和自动化各种数据分析任务，包括数据整理、数据选择、聚类、约束学习、预测建模和自动完成。

Conclusion: VisualSynth旨在通过直观的人机协作和AI技术，在电子表格环境中普及数据科学，使用户能够轻松进行数据分析。

Abstract: Everybody wants to analyse their data, but only few posses the data science expertise to to this. Motivated by this observation we introduce a novel framework and system \textsc{VisualSynth} for human-machine collaboration in data science.
  It wants to democratize data science by allowing users to interact with standard spreadsheet software in order to perform and automate various data analysis tasks ranging from data wrangling, data selection, clustering, constraint learning, predictive modeling and auto-completion. \textsc{VisualSynth} relies on the user providing colored sketches, i.e., coloring parts of the spreadsheet, to partially specify data science tasks, which are then determined and executed using artificial intelligence techniques.

</details>


### [173] [Automated Discovery of Data Transformations for Robotic Process Automation](https://arxiv.org/abs/2001.01007)
*Volodymyr Leno,Marlon Dumas,Marcello La Rosa,Fabrizio Maria Maggi,Artem Polyvyanyy*

Main category: cs.AI

TL;DR: 该论文提出了一种通过分析用户界面日志来发现机器人流程自动化（RPA）中数据传输例程的方法，并提出了优化方案以提高效率。


<details>
  <summary>Details</summary>
Motivation: 公司需要发现哪些具体的例程可以被自动化，以及如何自动化，以充分利用RPA带来的机会。

Method: 通过分析用户交互（UI）日志来发现用户从一个电子表格或（Web）表单向另一个表单传输数据的例程。将此问题映射到通过示例发现数据转换的问题，并在此基础上提出了两种优化方法，利用UI日志信息和数据传输中字母数字标记分别复制的特点。

Result: 发现直接应用现有数据转换发现技术计算效率低下，并提出了两种利用UI日志信息和数据传输特点的优化方法。

Conclusion: 所提出的方法及其优化方案通过复制真实世界重复数据传输例程的UI日志进行了评估。

Abstract: Robotic Process Automation (RPA) is a technology for automating repetitive routines consisting of sequences of user interactions with one or more applications. In order to fully exploit the opportunities opened by RPA, companies need to discover which specific routines may be automated, and how. In this setting, this paper addresses the problem of analyzing User Interaction (UI) logs in order to discover routines where a user transfers data from one spreadsheet or (Web) form to another. The paper maps this problem to that of discovering data transformations by example - a problem for which several techniques are available. The paper shows that a naive application of a state-of-the-art technique for data transformation discovery is computationally inefficient. Accordingly, the paper proposes two optimizations that take advantage of the information in the UI log and the fact that data transfers across applications typically involve copying alphabetic and numeric tokens separately. The proposed approach and its optimizations are evaluated using UI logs that replicate a real-life repetitive data transfer routine.

</details>


### [174] [Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples](https://arxiv.org/abs/1804.01186)
*Ashwin Kalyan,Abhishek Mohta,Oleksandr Polozov,Dhruv Batra,Prateek Jain,Sumit Gulwani*

Main category: cs.AI

TL;DR: NGDS是一种混合程序合成技术，结合了符号逻辑和统计模型，能从少量输入输出示例中快速准确地合成用户意图的程序，且所需数据量少。


<details>
  <summary>Details</summary>
Motivation: 现有的程序合成系统要么过度依赖手动设计的演绎逻辑，要么需要大量数据用于纯统计模型，并且在具有挑战性的基准测试中无法实现实时合成。

Method: 本文提出了神经引导演绎搜索（NGDS），一种结合了符号逻辑技术和统计模型的混合合成技术。它利用演绎搜索框架将神经组件的学习问题简化为监督学习设置，从而可以在少量真实世界数据上进行训练，并利用强大的循环神经网络编码器。

Result: NGDS在真实世界客户场景中，能够合成准确的程序，与现有最先进系统相比，速度提高了12倍。

Conclusion: NGDS是一种有效的程序合成方法，即使在数据稀疏的情况下，也能产生准确且泛化能力强的程序，并显著提高合成速度。

Abstract: Synthesizing user-intended programs from a small number of input-output examples is a challenging problem with several important applications like spreadsheet manipulation, data wrangling and code refactoring. Existing synthesis systems either completely rely on deductive logic techniques that are extensively hand-engineered or on purely statistical models that need massive amounts of data, and in general fail to provide real-time synthesis on challenging benchmarks. In this work, we propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique that combines the best of both symbolic logic techniques and statistical models. Thus, it produces programs that satisfy the provided specifications by construction and generalize well on unseen examples, similar to data-driven systems. Our technique effectively utilizes the deductive search framework to reduce the learning problem of the neural component to a simple supervised learning setup. Further, this allows us to both train on sparingly available real-world data and still leverage powerful recurrent neural network encoders. We demonstrate the effectiveness of our method by evaluating on real-world customer scenarios by synthesizing accurate programs with up to 12x speed-up compared to state-of-the-art systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [175] [FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining](https://arxiv.org/abs/2109.07323)
*Zhoujun Cheng,Haoyu Dong,Ran Jia,Pengfei Wu,Shi Han,Fan Cheng,Dongmei Zhang*

Main category: cs.IR

TL;DR: FORTAP是第一个利用大量电子表格公式进行数值推理感知的表格预训练方法，在单元格类型分类和公式预测两个下游任务上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 表格存储了丰富的数值数据，但对表格进行数值推理仍然是一个挑战。本研究发现，执行表格数值计算的电子表格公式自然是数值推理的强监督信号。更重要的是，网络上可以轻易获取大量带有专家制作公式的电子表格。

Method: FORTAP是第一个利用大量电子表格公式进行数值推理感知的表格预训练方法。我们设计了两个公式预训练任务，以明确指导FORTAP学习半结构化表格中的数值引用和计算。

Result: FORTAP在单元格类型分类和公式预测这两个代表性下游任务上取得了最先进的结果。

Conclusion: 展示了数值推理感知预训练的巨大潜力。

Abstract: Tables store rich numerical data, but numerical reasoning over tables is still a challenge. In this paper, we find that the spreadsheet formula, which performs calculations on numerical values in tables, is naturally a strong supervision of numerical reasoning. More importantly, large amounts of spreadsheets with expert-made formulae are available on the web and can be obtained easily. FORTAP is the first method for numerical-reasoning-aware table pretraining by leveraging large corpus of spreadsheet formulae. We design two formula pretraining tasks to explicitly guide FORTAP to learn numerical reference and calculation in semi-structured tables. FORTAP achieves state-of-the-art results on two representative downstream tasks, cell type classification and formula prediction, showing great potential of numerical-reasoning-aware pretraining.

</details>


### [176] [Detecting Layout Templates in Complex Multiregion Files](https://arxiv.org/abs/2109.06630)
*Gerardo Vitagliano,Lan Jiang,Felix Naumann*

Main category: cs.IR

TL;DR: Mondrian方法能自动识别多区域电子表格中的布局模板，并有效提取区域，优于现有表格识别算法。


<details>
  <summary>Details</summary>
Motivation: 电子表格因其灵活的结构，在数据管理和分析中虽广泛使用，但其多区域特性使得自动化分析变得困难，需要大量准备工作。

Method: Mondrian方法分三阶段：首先将文件渲染为图像并检查潜在区域元素；其次使用聚类算法将元素分组形成区域；最后将每个文件布局表示为图并进行比较以找到布局模板。

Result: 该方法在检测文件内可靠区域边界和跨文件识别重复布局方面表现最佳。

Conclusion: Mondrian方法提供了一种有效识别和提取多区域电子表格中布局模板的自动化解决方案，显著优于现有技术。

Abstract: Spreadsheets are among the most commonly used file formats for data management, distribution, and analysis. Their widespread employment makes it easy to gather large collections of data, but their flexible canvas-based structure makes automated analysis difficult without heavy preparation. One of the common problems that practitioners face is the presence of multiple, independent regions in a single spreadsheet, possibly separated by repeated empty cells. We define such files as "multiregion" files. In collections of various spreadsheets, we can observe that some share the same layout. We present the Mondrian approach to automatically identify layout templates across multiple files and systematically extract the corresponding regions. Our approach is composed of three phases: first, each file is rendered as an image and inspected for elements that could form regions; then, using a clustering algorithm, the identified elements are grouped to form regions; finally, every file layout is represented as a graph and compared with others to find layout templates. We compare our method to state-of-the-art table recognition algorithms on two corpora of real-world enterprise spreadsheets. Our approach shows the best performances in detecting reliable region boundaries within each file and can correctly identify recurring layouts across files.

</details>


### [177] [TUTA: Tree-based Transformers for Generally Structured Table Pre-training](https://arxiv.org/abs/2010.12537)
*Zhiruo Wang,Haoyu Dong,Ran Jia,Jia Li,Zhiyi Fu,Shi Han,Dongmei Zhang*

Main category: cs.IR

TL;DR: 本文提出了TUTA，一个统一的预训练架构，用于理解通用结构化表格，通过结构感知机制和渐进式预训练目标，在表格理解任务上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的表格理解方法主要关注关系型表格，而忽略了其他常见的表格结构。理解表格需要空间、层次和语义信息。

Method: 提出TUTA，一个用于通用结构化表格的统一预训练架构。通过设计双维度坐标树来描述空间和层次信息，并提出基于树的注意力机制和位置嵌入来捕获这些信息。此外，还设计了三个渐进式预训练目标，以实现令牌、单元格和表格级别上的表示。在未标记的网页和电子表格上进行预训练，并在单元格类型分类和表格类型分类任务上进行微调。

Result: TUTA非常有效，在五个广泛研究的数据集上取得了最先进的性能。

Conclusion: TUTA成功解决了理解通用结构化表格的挑战，并取得了卓越的性能。

Abstract: Tables are widely used with various structures to organize and present data. Recent attempts on table understanding mainly focus on relational tables, yet overlook to other common table structures. In this paper, we propose TUTA, a unified pre-training architecture for understanding generally structured tables. Noticing that understanding a table requires spatial, hierarchical, and semantic information, we enhance transformers with three novel structure-aware mechanisms. First, we devise a unified tree-based structure, called a bi-dimensional coordinate tree, to describe both the spatial and hierarchical information of generally structured tables. Upon this, we propose tree-based attention and position embedding to better capture the spatial and hierarchical information. Moreover, we devise three progressive pre-training objectives to enable representations at the token, cell, and table levels. We pre-train TUTA on a wide range of unlabeled web and spreadsheet tables and fine-tune it on two critical tasks in the field of table structure understanding: cell type classification and table type classification. Experiments show that TUTA is highly effective, achieving state-of-the-art on five widely-studied datasets.

</details>


### [178] [TableSense: Spreadsheet Table Detection with Convolutional Neural Networks](https://arxiv.org/abs/2106.13500)
*Haoyu Dong,Shijie Liu,Shi Han,Zhouyu Fu,Dongmei Zhang*

Main category: cs.IR

TL;DR: TableSense是一个端到端的基于CNN的电子表格表格检测框架，通过有效的单元格特征化、增强的CNN模型和主动学习，实现了91.3%的召回率和86.5%的精确度，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动表格检测是电子表格数据智能的关键技术，但由于表格结构和布局的多样性，检测任务面临挑战。

Method: TableSense框架包括：1. 有效的单元格特征化方案。2. 增强的卷积神经网络模型以实现精确的表格边界检测。3. 提出一种有效的不确定性度量来指导基于主动学习的智能采样算法，从而高效构建了包含22,176个表格和10,220个工作表的训练数据集。

Result: TableSense在EoB-2指标下，召回率为91.3%，精确度为86.5%，显著优于现有商品化电子表格工具中使用的检测算法和计算机视觉领域最先进的卷积神经网络。

Conclusion: TableSense是一种高效的端到端电子表格表格检测框架。

Abstract: Spreadsheet table detection is the task of detecting all tables on a given sheet and locating their respective ranges. Automatic table detection is a key enabling technique and an initial step in spreadsheet data intelligence. However, the detection task is challenged by the diversity of table structures and table layouts on the spreadsheet. Considering the analogy between a cell matrix as spreadsheet and a pixel matrix as image, and encouraged by the successful application of Convolutional Neural Networks (CNN) in computer vision, we have developed TableSense, a novel end-to-end framework for spreadsheet table detection. First, we devise an effective cell featurization scheme to better leverage the rich information in each cell; second, we develop an enhanced convolutional neural network model for table detection to meet the domain-specific requirement on precise table boundary detection; third, we propose an effective uncertainty metric to guide an active learning based smart sampling algorithm, which enables the efficient build-up of a training dataset with 22,176 tables on 10,220 sheets with broad coverage of diverse table structures and layouts. Our evaluation shows that TableSense is highly effective with 91.3\% recall and 86.5\% precision in EoB-2 metric, a significant improvement over both the current detection algorithm that are used in commodity spreadsheet tools and state-of-the-art convolutional neural networks in computer vision.

</details>


### [179] [Recommending Related Tables](https://arxiv.org/abs/1907.03595)
*Shuo Zhang,Krisztian Balog*

Main category: cs.IR

TL;DR: 该论文介绍了一种推荐相关表格的任务，并提出了一种基于多语义空间表示和判别学习模型的方法，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 表格是组织和操作数据的强大工具，而电子表格程序是最受欢迎的计算机应用之一。此任务的动机是为电子表格用户主动推荐网络上相关的结构化内容。

Method: 该方法的核心是将表格元素表示在多个语义空间中，然后使用判别学习模型组合元素级别的相似性来计算表格之间的相似度。

Result: 在专门构建的维基百科表格测试集上，所提出的方法展现了最先进的性能。

Conclusion: 论文介绍并解决了相关表格推荐任务，并开发了一个理论上合理且性能优越的表格匹配框架。

Abstract: Tables are an extremely powerful visual and interactive tool for structuring and manipulating data, making spreadsheet programs one of the most popular computer applications. In this paper we introduce and address the task of recommending related tables: given an input table, identifying and returning a ranked list of relevant tables. One of the many possible application scenarios for this task is to provide users of a spreadsheet program proactively with recommendations for related structured content on the Web. At its core, the related table recommendation task boils down to computing the similarity between a pair of tables. We develop a theoretically sound framework for performing table matching. Our approach hinges on the idea of representing table elements in multiple semantic spaces, and then combining element-level similarities using a discriminative learning model. Using a purpose-built test collection from Wikipedia tables, we demonstrate that the proposed approach delivers state-of-the-art performance.

</details>


### [180] [SmartTable: A Spreadsheet Program with Intelligent Assistance](https://arxiv.org/abs/1805.06353)
*Shuo Zhang,Vugar Abdul Zada,Krisztian Balog*

Main category: cs.IR

TL;DR: SmartTable是一个在线表格应用，提供智能辅助功能，帮助用户添加实体（行）和属性（列）。


<details>
  <summary>Details</summary>
Motivation: 为在线表格应用提供智能辅助能力，特别关注关系型表格的实体和属性的填充与扩展。

Method: 开发了一个在线表格应用SmartTable，通过两种方式提供智能辅助：一是填充额外实体（行），二是扩展额外属性（列）。该实现细节已提供并开源。

Result: 成功开发并发布了SmartTable在线表格应用，实现了智能辅助功能，目前已开源并可在http://smarttable.cc访问。

Conclusion: SmartTable为关系型表格的实体和属性的智能填充与扩展提供了一个有效的在线解决方案。

Abstract: We introduce SmartTable, an online spreadsheet application that is equipped with intelligent assistance capabilities. With a focus on relational tables, describing entities along with their attributes, we offer assistance in two flavors: (i) for populating the table with additional entities (rows) and (ii) for extending it with additional entity attributes (columns). We provide details of our implementation, which is also released as open source. The application is available at http://smarttable.cc.

</details>
