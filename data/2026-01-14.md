<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.CL](#cs.CL) [Total: 14]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.PL](#cs.PL) [Total: 5]
- [eess.SY](#eess.SY) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.DB](#cs.DB) [Total: 20]
- [math.HO](#math.HO) [Total: 2]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]
- [stat.OT](#stat.OT) [Total: 1]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.CY](#cs.CY) [Total: 10]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.OH](#cs.OH) [Total: 1]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.HC](#cs.HC) [Total: 27]
- [cs.SE](#cs.SE) [Total: 36]
- [cs.MS](#cs.MS) [Total: 1]
- [cs.DL](#cs.DL) [Total: 5]
- [cs.SD](#cs.SD) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 6]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Image deidentification in the XNAT ecosystem: use cases and solutions](https://arxiv.org/abs/2504.20657)
*Alex Michie,Simon J Doran*

Main category: cs.CV

TL;DR: XNAT平台通过结合XNAT生态系统中的独立工具，对DICOM数据进行去标识化处理，以应对MIDI-B挑战赛。尽管初期得分较低，但通过后续反馈和改进，最终得分达到99.61%。


<details>
  <summary>Details</summary>
Motivation: 在学术界广泛使用XNAT平台管理DICOM图像数据库进行研究项目，需要详细描述利用XNAT及其生态系统工具进行DICOM数据去标识化的工作流程，并列出不同场景下可能需要的去标识化措施。

Method: 结合XNAT平台和XNAT生态系统中的独立工具，对DICOM数据进行去标识化处理。在MIDI-B挑战赛的验证阶段，对已有的本地方法进行了调整，并在测试阶段取得了97.91%的初步成绩。之后，利用组织者提供的反馈报告和MIDI-B持续基准测试设施，进一步改进了方法，最终得分达到99.61%。

Result: 基于规则的方法能够移除所有与姓名相关的信息，但在处理地址数据方面存在不足。初步的机器学习模型实验在移除地址方面取得部分成功，但可能过于激进，导致整体性能略有下降至99.54%。最终的去标识化失败率估计为0.19%。

Conclusion: 未来的工作将侧重于改进地址识别能力，并更好地移除图像像素中包含的可识别数据。同时，仍在与挑战赛组织者讨论关于“答案密钥”的技术细节。

Abstract: XNAT is a server-based data management platform widely used in academia for curating large databases of DICOM images for research projects. We describe in detail a deidentification workflow for DICOM data using facilities in XNAT, together with independent tools in the XNAT "ecosystem". We list different contexts in which deidentification might be needed, based on our prior experience. The starting point for participation in the Medical Image De-Identification Benchmark (MIDI-B) challenge was a set of pre-existing local methodologies, which were adapted during the validation phase of the challenge. Our result in the test phase was 97.91\%, considerably lower than our peers, due largely to an arcane technical incompatibility of our methodology with the challenge's Synapse platform, which prevented us receiving feedback during the validation phase. Post-submission, additional discrepancy reports from the organisers and via the MIDI-B Continuous Benchmarking facility, enabled us to improve this score significantly to 99.61\%. An entirely rule-based approach was shown to be capable of removing all name-related information in the test corpus, but exhibited failures in dealing fully with address data. Initial experiments using published machine-learning models to remove addresses were partially successful but showed the models to be "over-aggressive" on other types of free-text data, leading to a slight overall degradation in performance to 99.54\%. Future development will therefore focus on improving address-recognition capabilities, but also on better removal of identifiable data burned into the image pixels. Several technical aspects relating to the "answer key" are still under discussion with the challenge organisers, but we estimate that our percentage of genuine deidentification failures on the MIDI-B test corpus currently stands at 0.19\%. (Abridged from original for arXiv submission)

</details>


### [2] [Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language](https://arxiv.org/abs/2204.00598)
*Andy Zeng,Maria Attarian,Brian Ichter,Krzysztof Choromanski,Adrian Wong,Stefan Welker,Federico Tombari,Aveek Purohit,Michael Ryoo,Vikas Sindhwani,Johnny Lee,Vincent Vanhoucke,Pete Florence*

Main category: cs.CV

TL;DR: 大型预训练模型（如“基础”模型）展现出依赖于其训练数据领域的独特能力。由于这些领域通常只有很小的重叠，例如视觉-语言模型（VLMs）在互联网规模的图像标题上进行训练，而大型语言模型（LMs）则在没有图像的互联网规模文本（如电子表格、SAT问题、代码）上进行进一步训练，因此这些模型在不同领域存储了不同形式的常识知识。


<details>
  <summary>Details</summary>
Motivation: 研究不同领域预训练模型的多样性如何能够通过Socratic Models（SMs）框架被利用，以在无需微调的情况下捕捉新的多模态能力。

Method: 提出了一种名为Socratic Models（SMs）的模块化框架，该框架允许通过多模态信息提示（zero-shot）组合多个预训练模型，使它们能够相互交换信息并获得新的多模态能力，而无需进行微调。

Result: SMs在零样本图像字幕和视频到文本检索方面具有竞争力，并能实现新的应用，例如（一）回答关于自主视频的自由形式问题，（二）通过与外部API和数据库（如网络搜索）交互，与人们进行多模态辅助对话（例如，用于烹饪食谱），以及（三）机器人感知和规划。

Conclusion: Socratic Models（SMs）框架能够有效利用不同领域预训练模型的知识多样性，在无需微调的情况下实现先进的多模态任务，并开辟新的应用领域。

Abstract: Large pretrained (e.g., "foundation") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.

</details>


### [3] [TableParser: Automatic Table Parsing with Weak Supervision from Spreadsheets](https://arxiv.org/abs/2201.01654)
*Susie Xi Rao,Johannes Rausch,Peter Egger,Ce Zhang*

Main category: cs.CV

TL;DR: TableParser可以解析PDF和扫描图像中的表格，并利用弱监督来提高精度。


<details>
  <summary>Details</summary>
Motivation: 提取表格结构和内容在许多应用中非常重要。

Method: 开发了TableParser系统，并结合了TableAnnotator和ExcelAnnotator进行弱监督学习。

Result: TableParser在解析表格方面表现出高精度，并且通过实验证明了领域自适应的有效性。

Conclusion: 共享TableParser、TableAnnotator和ExcelAnnotator这些资源，以促进该领域的研究。

Abstract: Tables have been an ever-existing structure to store data. There exist now different approaches to store tabular data physically. PDFs, images, spreadsheets, and CSVs are leading examples. Being able to parse table structures and extract content bounded by these structures is of high importance in many applications. In this paper, we devise TableParser, a system capable of parsing tables in both native PDFs and scanned images with high precision. We have conducted extensive experiments to show the efficacy of domain adaptation in developing such a tool. Moreover, we create TableAnnotator and ExcelAnnotator, which constitute a spreadsheet-based weak supervision mechanism and a pipeline to enable table parsing. We share these resources with the research community to facilitate further research in this interesting direction.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [4] [From Rows to Reasoning: A Retrieval-Augmented Multimodal Framework for Spreadsheet Understanding](https://arxiv.org/abs/2601.08741)
*Anmol Gulati,Sahil Sen,Waqar Sarguroh,Kevin Paul*

Main category: cs.CL

TL;DR: LLMs在处理大型企业电子表格时遇到困难，我们提出了FRTR-Bench基准和FRTR框架，显著提高了多模态电子表格推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的电子表格推理方法在处理包含大量数据、多个链接工作表和嵌入式视觉内容的大型企业电子表格时存在可扩展性和用户交互模拟的局限性。

Method: FRTR框架通过将工作簿分解为行、列和块嵌入，使用混合词汇-密集检索（RRF）和集成多模态嵌入来处理数值和视觉信息。

Result: FRTR在FRTR-Bench基准上实现了74%的准确率（Claude Sonnet 4.5），在SpreadsheetLLM基准上实现了87%的准确率（GPT-5），同时将代币使用量减少了约50%。

Conclusion: FRTR框架在多模态电子表格推理方面取得了显著的进步，克服了现有方法的局限性，并在基准测试中取得了优异的成绩。

Abstract: Large Language Models (LLMs) struggle to reason over large-scale enterprise spreadsheets containing thousands of numeric rows, multiple linked sheets, and embedded visual content such as charts and receipts. Prior state-of-the-art spreadsheet reasoning approaches typically rely on single-sheet compression or full-context encoding, which limits scalability and fails to reflect how real users interact with complex, multimodal workbooks. We introduce FRTR-Bench, the first large-scale benchmark for multimodal spreadsheet reasoning, comprising 30 enterprise-grade Excel workbooks spanning nearly four million cells and more than 50 embedded images. To address these challenges, we present From Rows to Reasoning (FRTR), an advanced, multimodal retrieval-augmented generation framework that decomposes Excel workbooks into granular row, column, and block embeddings, employs hybrid lexical-dense retrieval with Reciprocal Rank Fusion (RRF), and integrates multimodal embeddings to reason over both numerical and visual information. We tested FRTR on six LLMs, achieving 74% answer accuracy on FRTR-Bench with Claude Sonnet 4.5, a substantial improvement over prior state-of-the-art approaches that reached only 24%. On the SpreadsheetLLM benchmark, FRTR achieved 87% accuracy with GPT-5 while reducing token usage by roughly 50% compared to context-compression methods.

</details>


### [5] [SQuARE: Structured Query & Adaptive Retrieval Engine For Tabular Formats](https://arxiv.org/abs/2512.04292)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Main category: cs.CL

TL;DR: SQuARE是一个混合检索框架，通过识别电子表格的复杂性来处理多行标题、合并单元格和单位注释等问题，从而提高问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有技术在处理具有复杂结构（如多行标题、合并单元格、单位注释）的电子表格以及缺乏一致模式的表格时，在准确问答方面存在困难。

Method: SQuARE采用一种混合检索方法，首先计算一个基于标题深度和合并密度的连续分数来评估表格的复杂性，然后根据评估结果，将查询路由到两种策略之一：保留结构的块检索，或在自动构建的关系表示上执行SQL查询。该框架还包含一个轻量级代理，用于在置信度低时监督检索、细化或组合来自两个路径的结果。

Result: 在多标题公司资产负债表、合并单元格较多的世界银行工作簿和多样化的公共数据集上，SQuARE在检索精度和端到端答案准确性方面均优于单一策略基线和ChatGPT-4o，同时保持了可预测的延迟。

Conclusion: SQuARE通过解耦检索与模型选择，可以兼容新兴的表格基础模型，为实现更鲁棒的表格理解提供了实用的桥梁，并且能够保留标题层级、时间标签和单位等信息，确保返回值的准确性和可验证性。

Abstract: Accurate question answering over real spreadsheets remains difficult due to multirow headers, merged cells, and unit annotations that disrupt naive chunking, while rigid SQL views fail on files lacking consistent schemas. We present SQuARE, a hybrid retrieval framework with sheet-level, complexity-aware routing. It computes a continuous score based on header depth and merge density, then routes queries either through structure-preserving chunk retrieval or SQL over an automatically constructed relational representation. A lightweight agent supervises retrieval, refinement, or combination of results across both paths when confidence is low. This design maintains header hierarchies, time labels, and units, ensuring that returned values are faithful to the original cells and straightforward to verify. Evaluated on multi-header corporate balance sheets, a heavily merged World Bank workbook, and diverse public datasets, SQuARE consistently surpasses single-strategy baselines and ChatGPT-4o on both retrieval precision and end-to-end answer accuracy while keeping latency predictable. By decoupling retrieval from model choice, the system is compatible with emerging tabular foundation models and offers a practical bridge toward a more robust table understanding.

</details>


### [6] [An Empirical Study of Validating Synthetic Data for Formula Generation](https://arxiv.org/abs/2407.10657)
*Usneek Singh,José Cambronero,Sumit Gulwani,Aditya Kanade,Anirudh Khatry,Vu Le,Mukul Singh,Gust Verbruggen*

Main category: cs.CL

TL;DR: LLMs可用于辅助电子表格公式编写，但相关资源稀缺。本文提出了一种利用模型生成合成自然语言（NL）描述并进行验证的方法，以提高LLM在公式编写任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 电子表格公式的自然语言描述资源稀缺，阻碍了预训练模型在这一任务上的表现，并限制了微调能力。

Method: 生成合成的自然语言描述，并使用代理目标来验证这些描述的准确性。

Result: 验证可以提高模型在公式编写任务上的性能，并且尽管验证会过滤掉一些有挑战性的例子，但它能提高微调后模型能解决问题的复杂性。

Conclusion: 对LLM生成的合成训练样本进行验证，可以有效提升模型在电子表格公式编写任务上的性能。

Abstract: Large language models (LLMs) can be leveraged to help with writing formulas in spreadsheets, but resources on these formulas are scarce, impacting both the base performance of pre-trained models and limiting the ability to fine-tune them. Given a corpus of formulas, we can use a(nother) model to generate synthetic natural language utterances for fine-tuning. However, it is important to validate whether the NL generated by the LLM is indeed accurate to be beneficial for fine-tuning. In this paper, we provide empirical results on the impact of validating these synthetic training examples with surrogate objectives that evaluate the accuracy of the synthetic annotations. We demonstrate that validation improves performance over raw data across four models (2 open and 2 closed weight). Interestingly, we show that although validation tends to prune more challenging examples, it increases the complexity of problems that models can solve after being fine-tuned on validated data.

</details>


### [7] [TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios](https://arxiv.org/abs/2403.19318)
*Xiaokang Zhang,Sijia Luo,Bohan Zhang,Zeyao Ma,Jing Zhang,Yang Li,Guanlin Li,Zijun Yao,Kangli Xu,Jinchang Zhou,Daniel Zhang-Li,Jifan Yu,Shu Zhao,Juanzi Li,Jie Tang*

Main category: cs.CL

TL;DR: TableLLM是一个拥有80亿参数的大型语言模型，专门用于处理文档和电子表格中的表格数据操作任务，并提出了一种远程监督训练方法，包括推理过程扩展策略和交叉验证策略，以提高模型的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决现实世界办公场景中文档和电子表格中表格数据处理的挑战，我们引入了TableLLM。

Method: 我们提出了一种远程监督训练方法，该方法包括推理过程扩展策略和交叉验证策略，以训练TableLLM理解推理模式和确保自动生成数据的质量。

Result: 通过在为文档和电子表格格式定制的基准上进行评估，TableLLM在处理表格数据方面表现优于现有的通用和专注于表格数据的LLM。

Conclusion: TableLLM在处理表格数据方面表现出了显著的优势，并已公开提供模型检查点、源代码、基准和Web应用程序。

Abstract: We introduce TableLLM, a robust large language model (LLM) with 8 billion parameters, purpose-built for proficiently handling tabular data manipulation tasks, whether they are embedded within documents or spreadsheets, catering to real-world office scenarios. We propose a distant supervision method for training, which comprises a reasoning process extension strategy, aiding in training LLMs to understand reasoning patterns more effectively as well as a cross-way validation strategy, ensuring the quality of the automatically generated data. To evaluate the performance of TableLLM, we have crafted benchmarks tailored to address both document and spreadsheet formats as well as constructed a well-organized evaluation pipeline capable of handling both scenarios. Thorough evaluations underscore the advantages of TableLLM when compared to various existing general-purpose and tabular data-focused LLMs. We have publicly released the model checkpoint, source code, benchmarks, and a web application for user interaction. Our codes and data are publicly available at https://github.com/TableLLM/TableLLM.

</details>


### [8] [GiesKaNe: Bridging Past and Present in Grammatical Theory and Practical Application](https://arxiv.org/abs/2502.05113)
*Volker Emmrich*

Main category: cs.CL

TL;DR: GiesKaNe项目是一个参考历史和句法注释的树库，旨在连接历史和当代语料库。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨GiesKaNe项目（德国吉森和卡塞尔大学“新德语句法基本结构”）语料库的编译要求，该项目具有参考性、历史性和句法深度注释树库的特点。

Method: 通过结合人工专业知识和机器辅助流程来管理方法复杂性，包括分词、规范化、句子定义、标记、解析和注释员间一致性等基础课题，以及语法模型、注释模式和现有事实注释标准之间的比较，并提出一种对文本进行概念口语和书面语连续体分类的新方法。

Result: 该项目证明了即使是像GiesKaNe这样雄心勃勃的项目，也可以利用现有的研究基础设施和简单的电子表格进行有效实施，而无需专门的注释工具。

Conclusion: GiesKaNe项目通过结合现有工具和新方法，成功地构建了一个包含历史和句法信息的参考语料库，并为语料库的编译提供了新的见解。

Abstract: This article explores the requirements for corpus compilation within the GiesKaNe project (University of Giessen and Kassel, Syntactic Basic Structures of New High German). The project is defined by three central characteristics: it is a reference corpus, a historical corpus, and a syntactically deeply annotated treebank. As a historical corpus, GiesKaNe aims to establish connections with both historical and contemporary corpora, ensuring its relevance across temporal and linguistic contexts. The compilation process strikes the balance between innovation and adherence to standards, addressing both internal project goals and the broader interests of the research community. The methodological complexity of such a project is managed through a complementary interplay of human expertise and machine-assisted processes. The article discusses foundational topics such as tokenization, normalization, sentence definition, tagging, parsing, and inter-annotator agreement, alongside advanced considerations. These include comparisons between grammatical models, annotation schemas, and established de facto annotation standards as well as the integration of human and machine collaboration. Notably, a novel method for machine-assisted classification of texts along the continuum of conceptual orality and literacy is proposed, offering new perspectives on text selection. Furthermore, the article introduces an approach to deriving de facto standard annotations from existing ones, mediating between standardization and innovation. In the course of describing the workflow the article demonstrates that even ambitious projects like GiesKaNe can be effectively implemented using existing research infrastructure, requiring no specialized annotation tools. Instead, it is shown that the workflow can be based on the strategic use of a simple spreadsheet and integrates the capabilities of the existing infrastructure.

</details>


### [9] [MiMoTable: A Multi-scale Spreadsheet Benchmark with Meta Operations for Table Reasoning](https://arxiv.org/abs/2412.11711)
*Zheng Li,Yang Du,Mao Zheng,Mingyang Song*

Main category: cs.CL

TL;DR: MiMoTable是一个包含真实世界电子表格的多尺度基准测试，用于评估大语言模型（LLMs）的表格推理能力，并引入了新的元操作难度评估标准。


<details>
  <summary>Details</summary>
Motivation: 填补现有表格推理基准测试与真实世界复杂多样的表格和问题之间的差距。

Method: 提出MiMoTable基准测试，其中包含真实世界的电子表格，并定义了包含六种元操作的新评估标准来衡量问题难度。

Result: Claude-3.5-Sonnet在MiMoTable上达到了77.4%的准确率，表明LLMs仍有提升空间。现有基准测试的难度评估被证明是有效的，因为LLM的性能随基准测试难度的增加而下降。

Conclusion: MiMoTable及其新的难度评估标准能够有效衡量LLMs在真实世界表格推理任务上的能力和局限性，并为未来的研究提供了方向。

Abstract: Extensive research has been conducted to explore the capability of Large Language Models (LLMs) for table reasoning and has significantly improved the performance on existing benchmarks. However, tables and user questions in real-world applications are more complex and diverse, presenting an unignorable gap compared to the existing benchmarks. To fill the gap, we propose a \textbf{M}ult\textbf{i}-scale spreadsheet benchmark with \textbf{M}eta \textbf{o}perations for \textbf{Table} reasoning, named as MiMoTable. Specifically, MiMoTable incorporates two key features. First, the tables in MiMoTable are all spreadsheets used in real-world scenarios, which cover seven domains and contain different types. Second, we define a new criterion with six categories of meta operations for measuring the difficulty of each question in MiMoTable, simultaneously as a new perspective for measuring the difficulty of the existing benchmarks. Experimental results show that Claude-3.5-Sonnet achieves the best performance with 77.4\% accuracy, indicating that there is still significant room to improve for LLMs on MiMoTable. Furthermore, we grade the difficulty of existing benchmarks according to our new criteria. Experiments have shown that the performance of LLMs decreases as the difficulty of benchmarks increases, thereby proving the effectiveness of our proposed new criterion.

</details>


### [10] [SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation](https://arxiv.org/abs/2406.14991)
*Zeyao Ma,Bohan Zhang,Jing Zhang,Jifan Yu,Xiaokang Zhang,Xiaohan Zhang,Sijia Luo,Xi Wang,Jie Tang*

Main category: cs.CL

TL;DR: SpreadsheetBench是一个包含912个真实世界Excel问题的基准测试，用于评估大型语言模型(LLMs)处理电子表格的能力。


<details>
  <summary>Details</summary>
Motivation: 现有电子表格基准测试依赖于合成查询和简化的电子表格文件，未能反映真实用户需求的复杂性。本研究旨在创建一个更贴近实际工作流程的基准，以评估LLMs在处理真实、复杂的电子表格任务方面的能力。

Method: 创建了一个名为SpreadsheetBench的新基准，其中包含912个来自在线Excel论坛的真实用户问题及其相关的电子表格。这些电子表格包含多种表格、非标准关系表和丰富的非文本元素。此外，提出了一种类似于在线评判平台的评估指标，为每个指令创建多个电子表格文件作为测试用例，以评估模型处理不同数值的电子表格的鲁棒性。

Result: 在SpreadsheetBench基准上对各种LLMs进行了评估，发现在单轮和多轮推理设置下，最先进的模型与人类表现之间存在显著差距，表明该基准的挑战性。

Conclusion: SpreadsheetBench基准测试揭示了当前LLMs在处理真实世界电子表格任务方面存在巨大提升空间，并且其评估方法能够有效衡量模型在处理不同数据变体的鲁棒性。

Abstract: We introduce SpreadsheetBench, a challenging spreadsheet manipulation benchmark exclusively derived from real-world scenarios, designed to immerse current large language models (LLMs) in the actual workflow of spreadsheet users. Unlike existing benchmarks that rely on synthesized queries and simplified spreadsheet files, SpreadsheetBench is built from 912 real questions gathered from online Excel forums, which reflect the intricate needs of users. The associated spreadsheets from the forums contain a variety of tabular data such as multiple tables, non-standard relational tables, and abundant non-textual elements. Furthermore, we propose a more reliable evaluation metric akin to online judge platforms, where multiple spreadsheet files are created as test cases for each instruction, ensuring the evaluation of robust solutions capable of handling spreadsheets with varying values. Our comprehensive evaluation of various LLMs under both single-round and multi-round inference settings reveals a substantial gap between the state-of-the-art (SOTA) models and human performance, highlighting the benchmark's difficulty.

</details>


### [11] [NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries](https://arxiv.org/abs/2402.14853)
*Wei Zhao,Zhitao Hou,Siyuan Wu,Yan Gao,Haoyu Dong,Yao Wan,Hongyu Zhang,Yulei Sui,Haidong Zhang*

Main category: cs.CL

TL;DR: 该论文提出 NL2Formula 任务，旨在根据自然语言查询生成可执行的电子表格公式，并发布了一个包含70,799个样本的数据集和名为 fCoder 的基线模型。


<details>
  <summary>Details</summary>
Motivation: 为简化电子表格中公式的编写，该研究提出 NL2Formula 任务，以解决用户在数据分析中面临的公式编写繁琐且易错的问题。

Method: 构建了一个包含70,799个配对的自然语言查询和电子表格公式的数据集，涵盖21,670个表格和37种公式函数，并实现了一个名为 fCoder 的序列到序列基线模型来完成 NL2Formula 任务。

Result: fCoder 模型在实验中表现出色，优于现有基线模型，并与 GPT-3.5 (text-davinci-003) 进行了比较。

Conclusion: NL2Formula 任务的有效性得到了验证，但仍存在挑战，需要进一步的研究。

Abstract: Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets, is a widespread practice among users performing data analysis. However, crafting formulas on spreadsheets remains a tedious and error-prone task for many end-users, particularly when dealing with complex operations. To alleviate the burden associated with writing spreadsheet formulas, this paper introduces a novel benchmark task called NL2Formula, with the aim to generate executable formulas that are grounded on a spreadsheet table, given a Natural Language (NL) query as input. To accomplish this, we construct a comprehensive dataset consisting of 70,799 paired NL queries and corresponding spreadsheet formulas, covering 21,670 tables and 37 types of formula functions. We realize the NL2Formula task by providing a sequence-to-sequence baseline implementation called fCoder. Experimental results validate the effectiveness of fCoder, demonstrating its superior performance compared to the baseline models. Furthermore, we also compare fCoder with an initial GPT-3.5 model (i.e., text-davinci-003). Lastly, through in-depth error analysis, we identify potential challenges in the NL2Formula task and advocate for further investigation.

</details>


### [12] [InstructExcel: A Benchmark for Natural Language Instruction in Excel](https://arxiv.org/abs/2310.14495)
*Justin Payan,Swaroop Mishra,Mukul Singh,Carina Negreanu,Christian Poelitz,Chitta Baral,Subhro Roy,Rasika Chakravarthy,Benjamin Van Durme,Elnaz Nouri*

Main category: cs.CL

TL;DR: LLMs can be used to generate Excel OfficeScripts from natural language instructions. A new benchmark, InstructExcel, with over 10k samples covering 170+ operations, was created using Excel's 'Automate' feature. Experiments show InstructExcel is challenging for SOTA models, and performance improves with GPT-4, more examples, and dynamic prompting.


<details>
  <summary>Details</summary>
Motivation: Investigate if LLMs can generate Excel OfficeScripts from natural language instructions to solve Excel-specific tasks.

Method: Introduced a new large-scale benchmark, InstructExcel, by leveraging Excel's 'Automate' feature to generate OfficeScripts from user actions. The benchmark includes over 10k samples covering 170+ Excel operations across 2,000 publicly available Excel spreadsheets. Conducted experiments in zero-shot and few-shot settings.

Result: InstructExcel is a challenging benchmark for state-of-the-art models like GPT-4. Performance improvements were observed by using GPT-4 over GPT-3.5, providing more in-context examples, and employing dynamic prompting.

Conclusion: LLMs show potential in generating Excel OfficeScripts, but current models face challenges on complex tasks. The InstructExcel benchmark provides a valuable resource for evaluating and advancing LLM capabilities in this domain. Performance can be enhanced through model selection, in-context learning, and advanced prompting techniques.

Abstract: With the evolution of Large Language Models (LLMs) we can solve increasingly more complex NLP tasks across various domains, including spreadsheets. This work investigates whether LLMs can generate code (Excel OfficeScripts, a TypeScript API for executing many tasks in Excel) that solves Excel specific tasks provided via natural language user instructions. To do so we introduce a new large-scale benchmark, InstructExcel, created by leveraging the 'Automate' feature in Excel to automatically generate OfficeScripts from users' actions. Our benchmark includes over 10k samples covering 170+ Excel operations across 2,000 publicly available Excel spreadsheets. Experiments across various zero-shot and few-shot settings show that InstructExcel is a hard benchmark for state of the art models like GPT-4. We observe that (1) using GPT-4 over GPT-3.5, (2) providing more in-context examples, and (3) dynamic prompting can help improve performance on this benchmark.

</details>


### [13] [Active Learning with Tabular Language Models](https://arxiv.org/abs/2211.04128)
*Martin Ringsquandl,Aneta Koleva*

Main category: cs.CL

TL;DR: 现有的表格语言模型研究在实际应用中仍面临挑战，主要是因为标注高技术、领域特定的表格成本高昂。本研究首次将主动学习应用于表格语言模型，特别是在亚单元格命名实体识别任务中。


<details>
  <summary>Details</summary>
Motivation: 在工业界，虽然存在大量表格数据，但获取专家标注的成本很高。主动学习有望降低标注成本，但此前尚无将主动学习与表格语言模型相结合的研究。因此，本研究旨在探索主动学习如何应用于表格语言模型以解决标注成本问题。

Method: 本文研究了在真实工业场景下，针对亚单元格命名实体识别任务，不同的主动学习获取函数对表格语言模型的影响。

Result: 实验结果表明，包含内置多样性的单元格级别获取函数能显著降低标注成本，而强制性的表格级别多样性则会带来负面影响。此外，研究还揭示了在计算效率和人类标注者视角方面存在的根本性问题。

Conclusion: 包含内置多样性的单元格级别获取函数是降低表格语言模型标注成本的有效策略，但强制表格多样性则适得其反。未来的研究需要关注计算效率和人类标注者的体验。

Abstract: Despite recent advancements in tabular language model research, real-world applications are still challenging. In industry, there is an abundance of tables found in spreadsheets, but acquisition of substantial amounts of labels is expensive, since only experts can annotate the often highly technical and domain-specific tables. Active learning could potentially reduce labeling costs, however, so far there are no works related to active learning in conjunction with tabular language models. In this paper we investigate different acquisition functions in a real-world industrial tabular language model use case for sub-cell named entity recognition. Our results show that cell-level acquisition functions with built-in diversity can significantly reduce the labeling effort, while enforced table diversity is detrimental. We further see open fundamental questions concerning computational efficiency and the perspective of human annotators.

</details>


### [14] [Table-To-Text generation and pre-training with TabT5](https://arxiv.org/abs/2210.09162)
*Ewa Andrejczuk,Julian Martin Eisenschlos,Francesco Piccinno,Syrine Krichene,Yasemin Altun*

Main category: cs.CL

TL;DR: TABT5是一个新的编码器-解码器模型，在表格理解任务上取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 现有的编码器-only transformer模型在表格理解任务上存在局限性，主要限于类似分类的任务，例如单元格选择或蕴含识别。

Method: TABT5通过引入解码器组件，并结合表格特定嵌入和预训练来利用输入结构，从而克服了编码器-only模型的局限性。

Result: TABT5在多个领域取得了新的最先进成果，包括电子表格公式预测（序列准确率提高了15%）、问答（序列准确率提高了2.5%）以及数据到文本生成（BLEU分数提高了2.5%）。

Conclusion: TABT5作为一种编码器-解码器模型，通过结合解码器、表格特定嵌入和预训练，在表格理解任务上取得了显著的进展，并在多个任务上实现了最先进的性能。

Abstract: Encoder-only transformer models have been successfully applied to different table understanding tasks, as in TAPAS (Herzig et al., 2020). A major limitation of these architectures is that they are constrained to classification-like tasks such as cell selection or entailment detection. We present TABT5, an encoder-decoder model that generates natural language text based on tables and textual inputs. TABT5 overcomes the encoder-only limitation by incorporating a decoder component and leverages the input structure with table specific embeddings and pre-training. TABT5 achieves new state-of-the-art results on several domains, including spreadsheet formula prediction with a 15% increase in sequence accuracy, QA with a 2.5% increase in sequence accuracy and data-to-text generation with a 2.5% increase in BLEU.

</details>


### [15] [Table Pre-training: A Survey on Model Architectures, Pre-training Objectives, and Downstream Tasks](https://arxiv.org/abs/2201.09745)
*Haoyu Dong,Zhoujun Cheng,Xinyi He,Mengyu Zhou,Anda Zhou,Fan Zhou,Ao Liu,Shi Han,Dongmei Zhang*

Main category: cs.CL

TL;DR: 大量表格可从各种文档类型中收集，表格预训练框架取得了成功，并在表格问答、表格类型识别、列关系分类、表格搜索、公式预测等任务上取得了新的进展。为充分利用无标签表格中的监督信号，已设计并评估了各种预训练目标，例如，对单元格值进行去噪、预测数值关系和隐式执行 SQL。为了最好地利用（半）结构化表格的特点，已经探索了各种表格语言模型，特别是具有特殊设计的注意力机制的模型。由于表格通常与自由格式文本一起出现并进行交互，因此表格预训练通常采用表格-文本联合预训练的形式，这引起了多个领域的研究兴趣。本调查旨在全面回顾表格预训练的不同模型设计、预训练目标和下游任务，并进一步分享我们对现有挑战和未来机遇的看法和愿景。


<details>
  <summary>Details</summary>
Motivation: 表格预训练框架的成功以及在各种下游任务上的进展，促使研究人员探索如何充分利用无标签表格中的监督信号，并结合表格的（半）结构化特性。

Method: 提出各种预训练目标（如单元格值去噪、数值关系预测、隐式 SQL 执行）和表格语言模型（特别是具有特殊注意力机制的模型）。采用表格-文本联合预训练的形式。

Result: 表格预训练在表格问答、表格类型识别、列关系分类、表格搜索、公式预测等任务上取得了新的进展。

Conclusion: 本调查旨在全面回顾表格预训练的不同模型设计、预训练目标和下游任务，并分享对现有挑战和未来机遇的看法和愿景。

Abstract: Since a vast number of tables can be easily collected from web pages, spreadsheets, PDFs, and various other document types, a flurry of table pre-training frameworks have been proposed following the success of text and images, and they have achieved new state-of-the-arts on various tasks such as table question answering, table type recognition, column relation classification, table search, formula prediction, etc. To fully use the supervision signals in unlabeled tables, a variety of pre-training objectives have been designed and evaluated, for example, denoising cell values, predicting numerical relationships, and implicitly executing SQLs. And to best leverage the characteristics of (semi-)structured tables, various tabular language models, particularly with specially-designed attention mechanisms, have been explored. Since tables usually appear and interact with free-form text, table pre-training usually takes the form of table-text joint pre-training, which attracts significant research interests from multiple domains. This survey aims to provide a comprehensive review of different model designs, pre-training objectives, and downstream tasks for table pre-training, and we further share our thoughts and vision on existing challenges and future opportunities.

</details>


### [16] [TableQuery: Querying tabular data with natural language](https://arxiv.org/abs/2202.00454)
*Abhijith Neil Abraham,Fariz Rahman,Damanpreet Kaur*

Main category: cs.CL

TL;DR: TableQuery是一个使用预训练的深度学习模型来查询表格数据的工具，解决了现有方法需要将整个表格输入模型而无法处理大型或实时更新数据库的限制。


<details>
  <summary>Details</summary>
Motivation: 现有的表格数据问答方法存在局限性，无法处理大型表格或实时更新的数据库，因为它们需要将整个表格作为输入，这在实际应用中是不切实际的。

Method: TableQuery使用预训练的用于自由文本问答的深度学习模型，将自然语言查询转换为结构化查询，可以直接在数据库或电子表格上运行，无需将整个数据加载到内存或序列化数据库。

Result: TableQuery无需将整个数据加载到内存或序列化数据库，并且可以轻松替换为性能更好的新型预训练问答模型，而无需重新训练。

Conclusion: TableQuery通过利用预训练的自由文本问答模型，有效地解决了表格数据查询中的实际挑战，提高了效率和可扩展性。

Abstract: This paper presents TableQuery, a novel tool for querying tabular data using deep learning models pre-trained to answer questions on free text. Existing deep learning methods for question answering on tabular data have various limitations, such as having to feed the entire table as input into a neural network model, making them unsuitable for most real-world applications. Since real-world data might contain millions of rows, it may not entirely fit into the memory. Moreover, data could be stored in live databases, which are updated in real-time, and it is impractical to serialize an entire database to a neural network-friendly format each time it is updated. In TableQuery, we use deep learning models pre-trained for question answering on free text to convert natural language queries to structured queries, which can be run against a database or a spreadsheet. This method eliminates the need for fitting the entire data into memory as well as serializing databases. Furthermore, deep learning models pre-trained for question answering on free text are readily available on platforms such as HuggingFace Model Hub (7). TableQuery does not require re-training; when a newly trained model for question answering with better performance is available, it can replace the existing model in TableQuery.

</details>


### [17] [The Impact of Text Presentation on Translator Performance](https://arxiv.org/abs/2011.05978)
*Samuel Läubli,Patrick Simianer,Joern Wuebker,Geza Kovacs,Rico Sennrich,Spence Green*

Main category: cs.CL

TL;DR: CAT工具的设计选择会影响翻译人员的速度和准确性。句子分割和上下排列的视图可以提高翻译速度和错误识别能力，但对于审校任务，未分割的文本效果更好。


<details>
  <summary>Details</summary>
Motivation: 评估计算机辅助翻译（CAT）工具中的常见设计选择（如文档分割成句子、并排或上下视图）对翻译人员效率的影响。

Method: 进行了一项对照实验，测量了翻译人员在三种文本处理任务（文本再现、句内错误识别和审校）中的速度和准确性。

Result: 句子分割比未分割文本能更快地进行文本再现，并且更容易识别句内错误。上下排列的视图比并排视图能更快地进行文本再现。然而，在审校任务中，未分割的文本在准确性和时间效率方面表现最佳。

Conclusion: 研究结果表明，句子分割和上下视图对翻译效率有积极影响，但对于审校任务，未分割文本是更优的选择。这些发现对CAT工具的设计实践具有直接意义。

Abstract: Widely used computer-aided translation (CAT) tools divide documents into segments such as sentences and arrange them in a side-by-side, spreadsheet-like view. We present the first controlled evaluation of these design choices on translator performance, measuring speed and accuracy in three experimental text processing tasks. We find significant evidence that sentence-by-sentence presentation enables faster text reproduction and within-sentence error identification compared to unsegmented text, and that a top-and-bottom arrangement of source and target sentences enables faster text reproduction compared to a side-by-side arrangement. For revision, on the other hand, our results suggest that presenting unsegmented text results in the highest accuracy and time efficiency. Our findings have direct implications for best practices in designing CAT tools.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [18] [Data Encryption Battlefield: A Deep Dive into the Dynamic Confrontations in Ransomware Attacks](https://arxiv.org/abs/2504.20681)
*Arash Mahboubi,Hamed Aboutorab,Seyit Camtepe,Hang Thanh Bui,Khanh Luong,Keyvan Ansari,Shenlu Wang,Bazara Barry*

Main category: cs.CR

TL;DR: 本研究提出了一种使用在线增量机器学习算法来检测不断变化的加密技术，特别是针对降低熵的AES-Base64编码和间歇性加密的策略。


<details>
  <summary>Details</summary>
Motivation: 为了应对日益复杂的网络安全威胁，特别是攻击者不断改进加密技术以逃避检测，需要开发先进的检测方法。

Method: 研究利用包含32.6GB数据、11928个文件和75种不同勒索软件家族的大型数据集，评估了Hoeffding树和具有暖启动功能的随机森林等在线增量机器学习算法在检测不同加密策略方面的有效性。

Result: 结果显示，Hoeffding树算法在检测传统的AES-Base64加密方法方面表现出色，而随机森林算法（带暖启动）则更有效地识别间歇性加密。

Conclusion: 本研究强调了采用针对性的机器学习解决方案来应对复杂的勒索软件加密策略的必要性，其中Hoeffding树和随机森林在不同场景下显示出各自的优势。

Abstract: In the rapidly evolving landscape of cybersecurity threats, ransomware represents a significant challenge. Attackers increasingly employ sophisticated encryption methods, such as entropy reduction through Base64 encoding, and partial or intermittent encryption to evade traditional detection methods. This study explores the dynamic battle between adversaries who continuously refine encryption strategies and defenders developing advanced countermeasures to protect vulnerable data. We investigate the application of online incremental machine learning algorithms designed to predict file encryption activities despite adversaries evolving obfuscation techniques. Our analysis utilizes an extensive dataset of 32.6 GB, comprising 11,928 files across multiple formats, including Microsoft Word documents (doc), PowerPoint presentations (ppt), Excel spreadsheets (xlsx), image formats (jpg, jpeg, png, tif, gif), PDFs (pdf), audio (mp3), and video (mp4) files. These files were encrypted by 75 distinct ransomware families, facilitating a robust empirical evaluation of machine learning classifiers effectiveness against diverse encryption tactics. Results highlight the Hoeffding Tree algorithms superior incremental learning capability, particularly effective in detecting traditional and AES-Base64 encryption methods employed to lower entropy. Conversely, the Random Forest classifier with warm-start functionality excels at identifying intermittent encryption methods, demonstrating the necessity of tailored machine learning solutions to counter sophisticated ransomware strategies.

</details>


### [19] [JUBILEE: Secure Debt Relief and Forgiveness](https://arxiv.org/abs/2109.07267)
*David Cerezo Sánchez*

Main category: cs.CR

TL;DR: JUBILEE是一种安全、免信任的债务减免机制，通过激励机制和安全计算提高了债务结算的效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在实现一个安全、免信任的债务减免和债务清偿机制，以促进更和谐的债务结算，并通过激励机制鼓励各方真实披露其私人信息。

Method: JUBILEE机制结合了安全计算技术和激励机制，以实现个人理性、激励相容、策略证明、事后有效和最优的债务减免。研究中还包含了 "The Secure Spreadsheet" 的简易实现，以及在区块链上使用 Raziel 智能合约和 Pravuil 共识的实现。

Result: JUBILEE 机制在个人理性、激励相容、策略证明、事后有效和最优性方面优于现有方法。通过引入安全计算技术，JUBILEE 首次实现了 "债务人的祝福"，使得债务结算的预期利润和成功率均高于未使用安全计算的方案。

Conclusion: JUBILEE 是一种创新的、安全的、免信任的债务减免机制，通过结合安全计算和激励机制，显著提高了债务结算的效率和成功率，并为债务人带来了 "祝福"。

Abstract: JUBILEE is a securely computed mechanism for debt relief and forgiveness in a frictionless manner without involving trusted third parties, leading to more harmonious debt settlements by incentivising the parties to truthfully reveal their private information. JUBILEE improves over all previous methods:
  - individually rational, incentive-compatible, truthful/strategy-proof, ex-post efficient, optimal mechanism for debt relief and forgiveness with private information
  - by the novel introduction of secure computation techniques to debt relief, the "blessing of the debtor" is hereby granted for the first time: debt settlements with higher expected profits and a higher probability of success than without using secure computation
  A simple and practical implementation is included for "The Secure Spreadsheet". Another implementation is realised using Raziel smart contracts on a blockchain with Pravuil consensus.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [20] [Is spreadsheet syntax better than numeric indexing for cell selection?](https://arxiv.org/abs/2505.23296)
*Philip Heltweg,Dirk Riehle,Georg-Daniel Schwarz*

Main category: cs.PL

TL;DR: 单元格选择的两种方法：数字索引和电子表格样式。


<details>
  <summary>Details</summary>
Motivation: 比较数字索引和电子表格样式在代码读取和写入方面的速度和正确性。

Method: 通过包含学生参与者的大规模对照实验来比较这两种方法。

Result: 电子表格样式在代码读取和写入方面均优于数字索引，参与者犯的错误更少，速度更快。

Conclusion: 领域特定语法（如电子表格样式）可能是一种有前景的工具，可用于支持非软件工程背景的数据从业者。

Abstract: Selecting a subset of cells is a common task in data engineering, for example, to remove errors or select only specific parts of a table. Multiple approaches to express this selection exist. One option is numeric indexing, commonly found in general programming languages, where a tuple of numbers identifies the cell. Alternatively, the separate dimensions can be referred to using different enumeration schemes like "A1" for the first cell, commonly found in software such as spreadsheet systems.
  In a large-scale controlled experiment with student participants as proxy for data practitioners, we compare the two options with respect to speed and correctness of reading and writing code.
  The results show that, when reading code, participants make less mistakes using spreadsheet-style syntax. Additionally, when writing code, they make fewer mistakes and are faster when using spreadsheet syntax compared to numeric syntax.
  From this, a domain-specific syntax, such as spreadsheet syntax for data engineering, appears to be a promising alternative to explore in future tools to support practitioners without a software engineering background.

</details>


### [21] [FLAME: A small language model for spreadsheet formulas](https://arxiv.org/abs/2301.13779)
*Harshit Joshi,Abishai Ebenezer,José Cambronero,Sumit Gulwani,Aditya Kanade,Vu Le,Ivan Radiček,Gust Verbruggen*

Main category: cs.PL

TL;DR: FLAME是一个比现有大型语言模型小得多（6000万参数）但性能具有竞争力的模型，专门用于Excel公式创作辅助，它在公式修复、补全和检索任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在Excel公式创作辅助方面存在训练成本高、部署难度大（参数量高达数十亿）的问题。FLAME旨在解决这些问题，提供一个更小、更高效的解决方案。

Method: FLAME是一个基于Transformer的模型，仅使用Excel公式进行训练。它采用了领域特定的技术，包括：使用草图去重来策划训练数据集；引入Excel特定的公式分词器；以及使用掩码跨度预测和噪声自动编码的领域特定版本作为预训练目标。

Result: 在公式修复和补全任务中，FLAME在14个评估设置中的10个优于更大的模型（如Davinci (175B) 和 Cushman (12B) 版本的Codex以及CodeT5 (220M)）。在公式检索任务中，FLAME的表现优于CodeT5、CodeBERT和GraphCodeBERT。

Conclusion: FLAME通过利用领域见解，成功地在Excel公式创作辅助方面实现了具有竞争力的性能，同时显著减小了模型规模和训练数据量，克服了现有大型语言模型的局限性。

Abstract: Spreadsheets are a vital tool for end-user data management. Using large language models for formula authoring assistance in these environments can be difficult, as these models are expensive to train and challenging to deploy due to their size (up to billions of parameters). We present FLAME, a transformer-based model trained exclusively on Excel formulas that leverages domain insights to achieve competitive performance while being substantially smaller (60M parameters) and training on two orders of magnitude less data. We curate a training dataset using sketch deduplication, introduce an Excel-specific formula tokenizer, and use domain-specific versions of masked span prediction and noisy auto-encoding as pre-training objectives. We evaluate FLAME on formula repair, formula completion, and similarity-based formula retrieval. FLAME can outperform much larger models, such as the Davinci (175B) and Cushman (12B) variants of Codex and CodeT5 (220M), in 10 of 14 evaluation settings for the repair and completion tasks. For formula retrieval, FLAME outperforms CodeT5, CodeBERT, and GraphCodeBERT.

</details>


### [22] [Enhanced Spreadsheet Computing with Finite-Domain Constraint Satisfaction](https://arxiv.org/abs/2203.16346)
*Ezana N. Beyenne,Hai-Feng Guo*

Main category: cs.PL

TL;DR: 电子表格的计算范式被扩展到支持有限域约束满足问题，提供了一个可视化界面和一种特定的声明式约束语言。


<details>
  <summary>Details</summary>
Motivation: 电子表格主要局限于类似簿记的应用，因为它们是单向数据流；该研究旨在扩展电子表格的计算范式，以克服这一限制，用于解决约束满足问题。

Method: 提出了一个增强的电子表格系统，在可视化环境中良好地支持有限域约束求解，并构建了一种电子表格特定的约束语言，供普通用户声明式、可扩展地指定数据单元格之间的约束。

Result: 新的电子表格系统通过可视化表格界面显著简化了许多基于约束的应用的开发，并通过示例说明了其可用性和实用性。

Conclusion: 该研究成功地将电子表格计算范式扩展到解决约束满足问题，为用户提供了一个更强大、更通用的工具。

Abstract: The spreadsheet application is among the most widely used computing tools in modern society. It provides excellent usability and usefulness, and it easily enables a non-programmer to perform programming-like tasks in a visual tabular "pen and paper" approach. However, spreadsheets are mostly limited to bookkeeping-like applications due to their mono-directional data flow. This paper shows how the spreadsheet computing paradigm is extended to break this limitation for solving constraint satisfaction problems. We present an enhanced spreadsheet system where finite-domain constraint solving is well supported in a visual environment. Furthermore, a spreadsheet-specific constraint language is constructed for general users to specify constraints among data cells in a declarative and scalable way. The new spreadsheet system significantly simplifies the development of many constraint-based applications using a visual tabular interface. Examples are given to illustrate the usability and usefulness of the extended spreadsheet paradigm.
  KEYWORDS: Spreadsheet computing, Finite-domain constraint satisfaction, Constraint logic programming

</details>


### [23] [Typed Image-based Programming with Structure Editing](https://arxiv.org/abs/2110.08993)
*Jonathan Edwards,Tomas Petricek*

Main category: cs.PL

TL;DR: 图像式编程系统通过类型和结构编辑实现协作，解决了模式变更问题。


<details>
  <summary>Details</summary>
Motivation: 图像式编程系统（如Smalltalk）鼓励探索性编程，但缺乏协作和部署支持。

Method: 提出使用静态类型和结构编辑来处理模式变更，并提供一种用于结构编辑的版本控制理论。

Result: 通过静态类型和结构编辑，可以自动适应数据以处理模式变更，并为协作提供基础。

Conclusion: 提出的版本控制理论是主要的技能贡献，有潜力扩展到整个编程体验，为图像式编程带来新的协作方式。

Abstract: Many beloved programming systems are image-based: self-contained worlds that persist both code and data in a single file. Examples include Smalltalk, LISP, HyperCard, Flash, and spreadsheets. Image-based programming avoids much of the complexity of modern programming technology stacks and encourages more casual and exploratory programming. However conventional file-based programming has better support for collaboration and deployment. These problems have been blamed for the limited commercial success of Smalltalk. We propose to enable collaboration in image-based programming via types and structure editing.
  We focus on the problem of schema change on persistent data. We turn to static types, which paradoxically require more schema change but also provide a mechanism to express and execute those changes. To determine those changes we turn to structure editing, so that we can capture changes in type definitions with sufficient fidelity to automatically adapt the data to suit. We conjecture that typical schema changes can be handled through structure editing of static types.
  That positions us to tackle collaboration with what could be called version control for structure editing. We present a theory realizing this idea, which is our main technical contribution. While we focus here on editing types, if we can extend the approach to cover the entire programming experience then it would offer a new way to collaborate in image-based programming.

</details>


### [24] [ExceLint: Automatically Finding Spreadsheet Formula Errors](https://arxiv.org/abs/1901.11100)
*Daniel W. Barowy,Emery D. Berger,Benjamin Zorn*

Main category: cs.PL

TL;DR: ExceLint是一个用于Microsoft Excel的静态分析工具，可以快速有效地检测电子表格中的公式错误，其表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 鉴于电子表格在金融等关键领域的广泛应用以及其中错误可能带来的严重后果，需要一种专门用于查找电子表格公式错误的静态分析方法。

Method: 提出了一种利用电子表格的矩形特性，并结合信息论方法来识别可能破坏附近矩形区域的意外公式的静态分析技术，并实现了ExceLint工具。

Result: ExceLint在70个电子表格的语料库上进行了测试，中位数分析时间为每个电子表格5秒，并且在检测错误方面显著优于现有的分析方法。

Conclusion: ExceLint是一种快速有效的工具，能够发现电子表格中的公式错误，并且性能优于现有技术。

Abstract: Spreadsheets are one of the most widely used programming environments, and are widely deployed in domains like finance where errors can have catastrophic consequences. We present a static analysis specifically designed to find spreadsheet formula errors. Our analysis directly leverages the rectangular character of spreadsheets. It uses an information-theoretic approach to identify formulas that are especially surprising disruptions to nearby rectangular regions. We present ExceLint, an implementation of our static analysis for Microsoft Excel. We demonstrate that ExceLint is fast and effective: across a corpus of 70 spreadsheets, ExceLint takes a median of 5 seconds per spreadsheet, and it significantly outperforms the state of the art analysis.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [25] [LEI2JSON: Schema-based Validation and Conversion of Livestock Event Information](https://arxiv.org/abs/2310.17414)
*Mahir Habib,Muhammad Ashad Kabir,Lihong Zheng*

Main category: eess.SY

TL;DR: LEI2JSON是一个用于Google表格的插件，可以将畜牧业事件数据标准化并转换为JSON格式，以节省时间和资源。


<details>
  <summary>Details</summary>
Motivation: 帮助畜牧业生产者标准化（转换和验证）他们的畜牧业事件数据。

Method: 构建包含适当列标题、注释和验证规则的电子表格模板，将电子表格数据转换为JSON格式，并根据模式验证输出。

Result: LEI2JSON可以实现畜牧业事件信息的无缝本地存储或在Google云端硬盘中以JSON格式存储。已进行广泛的实验评估。

Conclusion: LEI2JSON为畜牧业生产者提供了一种有效的标准化其数据的方法，从而节省了大量时间和资源。

Abstract: Livestock producers often need help in standardising (i.e., converting and validating) their livestock event data. This article introduces a novel solution, LEI2JSON (Livestock Event Information To JSON). The tool is an add-on for Google Sheets, adhering to the livestock event information (LEI) schema. The core objective of LEI2JSON is to provide livestock producers with an efficient mechanism to standardise their data, leading to substantial savings in time and resources. This is achieved by building the spreadsheet template with the appropriate column headers, notes, and validation rules, converting the spreadsheet data into JSON format, and validating the output against the schema. LEI2JSON facilitates the seamless storage of livestock event information locally or on Google Drive in JSON. Additionally, we have conducted an extensive experimental evaluation to assess the effectiveness of the tool.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [26] [A CNN toolbox for skin cancer classification](https://arxiv.org/abs/1908.08187)
*Fabrizio Nunnari,Daniel Sonntag*

Main category: eess.IV

TL;DR: 一个用于皮肤癌分类的深度学习模型配置软件工具箱。


<details>
  <summary>Details</summary>
Motivation: 为了快速配置深度学习模型用于皮肤癌分类，同时允许非技术用户探索不同的配置设置。

Method: 实现了一个软件架构，允许开发人员快速设置新的卷积神经网络（CNN）架构和超参数配置。用户界面可被视为一个简单的电子表格。

Result: 初步结果表明，该工具箱在皮肤镜图像黑色素瘤检测方面，量化了图像增强、图像分辨率和重缩放滤波器对整体检测性能和训练时间的影响。

Conclusion: 该软件工具箱能够简化皮肤癌分类深度学习模型的配置过程，并为未来的改进（如元学习和 AutoML）奠定了基础。

Abstract: We describe a software toolbox for the configuration of deep neural networks in the domain of skin cancer classification. The implemented software architecture allows developers to quickly set up new convolutional neural network (CNN) architectures and hyper-parameter configurations. At the same time, the user interface, manageable as a simple spreadsheet, allows non-technical users to explore different configuration settings that need to be explored when switching to different data sets. In future versions, meta leaning frameworks can be added, or AutoML systems that continuously improve over time. Preliminary results, conducted with two CNNs in the context melanoma detection on dermoscopic images, quantify the impact of image augmentation, image resolution, and rescaling filter on the overall detection performance and training time.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [27] [Data-Semantics-Aware Recommendation of Diverse Pivot Tables](https://arxiv.org/abs/2507.06171)
*Whanhee Cho,Anna Fariha*

Main category: cs.DB

TL;DR: SAGE是一个数据语义感知系统，用于推荐k个预算内多样化的透视表，解决了现有方法在处理高维数据集时的冗余问题。它通过数据语义模型和可扩展的贪心算法，确保推荐的透视表既有洞察力、可解释性，又彼此不同。


<details>
  <summary>Details</summary>
Motivation: 现有的数据汇总方法，特别是手动创建透视表，在高维数据集下效率低下且难以发现有用的组合。因此，需要一种自动化的方法来推荐有洞察力且多样化的透视表。

Method: SAGE提出了一种数据语义感知模型来评估单个透视表的效用和一组透视表的多样性，并结合一个可扩展的贪心算法来高效地选择一组高效用且多样化的透视表。

Result: 在三个真实世界数据集上的大量实验表明，SAGE的性能优于其他方法，并且能够有效地扩展到高维数据集。此外，SAGE在定性效果上优于商业软件和大型语言模型。

Conclusion: SAGE成功地解决了自动推荐多样化透视表的挑战，通过利用数据语义显著减少了搜索空间，并在效率和推荐质量方面超越了现有方法和工具。

Abstract: Data summarization is essential to discover insights from large datasets. In a spreadsheets, pivot tables offer a convenient way to summarize tabular data by computing aggregates over some attributes, grouped by others. However, identifying attribute combinations that will result in useful pivot tables remains a challenge, especially for high-dimensional datasets. We formalize the problem of automatically recommending insightful and interpretable pivot tables, eliminating the tedious manual process. A crucial aspect of recommending a set of pivot tables is to diversify them. Traditional works inadequately address the table-diversification problem, which leads us to consider the problem of pivot table diversification.
  We present SAGE, a data-semantics-aware system for recommending k-budgeted diverse pivot tables, overcoming the shortcomings of prior work for top-k recommendations that cause redundancy. SAGE ensures that each pivot table is insightful, interpretable, and adaptive to the user's actions and preferences, while also guaranteeing that the set of pivot tables are different from each other, offering a diverse recommendation. We make two key technical contributions: (1) a data-semantics-aware model to measure the utility of a single pivot table and the diversity of a set of pivot tables, and (2) a scalable greedy algorithm that can efficiently select a set of diverse pivot tables of high utility, by leveraging data semantics to significantly reduce the combinatorial search space. Our extensive experiments on three real-world datasets show that SAGE outperforms alternative approaches, and efficiently scales to accommodate high-dimensional datasets. Additionally, we present several case studies to highlight SAGE's qualitative effectiveness over commercial software and Large Language Models (LLMs).

</details>


### [28] [Facilitating Digital Agriculture with Simple Databases](https://arxiv.org/abs/2312.06517)
*Dennis Buckmaster,Sami Basir,Hanae Sakata*

Main category: cs.DB

TL;DR: 该论文提供了一个开源的、面向农民的Airtable数据库模板，用于收集农业运营数据，即使是那些电子表格技能有限的农民也可以使用。


<details>
  <summary>Details</summary>
Motivation: 为了方便农业数据管理，特别是为电子表格技能有限的农民提供易于使用的数据库解决方案。

Method: 提供结构化的私有数据库模板，使用简单的数据验证表单，并提供一个解释如何构建数据库的研讨会录像。

Result: 生成整洁、机器和人类可读、可编辑且可导出的运营数据，以支持物流、提供元数据和改进企业分析。

Conclusion: 这些资源可以通过推广和结构化教育项目促进数字农业原则的应用。

Abstract: As an on-ramp to databases, we offer several well-structured private database templates as open source resources for agriculturalists, particularly those with modest spreadsheet skills. These farmer-oriented Air table databases use simple data-validated forms, with the look and feel of a customized app, to yield operational data that is tidy, machine- and human-readable, editable, and exportable for analysis in other software. Such data can facilitate logistics, provide contextual metadata, and improve enterprise analysis. A recorded workshop explaining how to build a database for activity records is presented. These resources may facilitate infusion of digital agriculture principles through Extension and structured educational programming.

</details>


### [29] [Streamlining Knowledge Graph Construction with a façade: The SPARQL Anything project](https://arxiv.org/abs/2310.16700)
*Luigi Asprino,Enrico Daga,Justin Dowdy,Paul Mulholland,Aldo Gangemi,Marco Ratta*

Main category: cs.DB

TL;DR: SPARQL Anything 是一个用于查询异构资源的系统，它通过重载 SERVICE 子句，允许使用 SPARQL 1.1 查询多种文件格式（包括 CSV、JSON、XML、Markdown、YAML、DOCx、Bibtex 等）。该系统支持灵活的 Web API 查询、参数化查询和复杂的数据转换管道，并提供了广泛的实际应用场景。本文详细介绍了 SPARQL Anything 的设计理念和软件架构，并通过社区调查和行业报告评估了其相对于替代解决方案的优势。


<details>
  <summary>Details</summary>
Motivation: 设计一个面向知识工程师的数据集成框架，借鉴面向对象软件工程中的外观（façade）概念，以简化知识图谱的构建。

Method: 开发了 SPARQL Anything 系统，该系统通过重载 SERVICE 子句，允许使用 SPARQL 1.1 查询多种格式的异构资源。系统支持多种文件格式、灵活的 Web API 查询、参数化查询和复杂的数据转换管道。

Result: SPARQL Anything 支持广泛的文件格式，包括 CSV、JSON、XML、Markdown、YAML、DOCx、Bibtex 等，并能灵活地查询 Web API。提供了大量的可重用、实际应用场景。通过社区调查和行业报告，证明了其设计假设的价值，并与替代方案进行了比较。

Conclusion: SPARQL Anything 系统在设计理念、软件架构、功能支持和实际应用价值方面都得到了验证，为知识工程师提供了一个强大的数据集成解决方案。

Abstract: What should a data integration framework for knowledge engineers look like? Recent research on Knowledge Graph construction proposes the design of a façade, a notion borrowed from object-oriented software engineering. This idea is applied to SPARQL Anything, a system that allows querying heterogeneous resources as-if they were in RDF, in plain SPARQL 1.1, by overloading the SERVICE clause. SPARQL Anything supports a wide variety of file formats, from popular ones (CSV, JSON, XML, Spreadsheets) to others that are not supported by alternative solutions (Markdown, YAML, DOCx, Bibtex). Features include querying Web APIs with high flexibility, parametrised queries, and chaining multiple transformations into complex pipelines. In this paper, we describe the design rationale and software architecture of the SPARQL Anything system. We provide references to an extensive set of reusable, real-world scenarios from various application domains. We report on the value-to-users of the founding assumptions of its design, compared to alternative solutions through a community survey and a field report from the industry.

</details>


### [30] [DataVinci: Learning Syntactic and Semantic String Repairs](https://arxiv.org/abs/2308.10922)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Gust Verbruggen*

Main category: cs.DB

TL;DR: DataVinci是一个全自动的字符串数据错误检测和修复系统，能学习基于正则表达式的模式，并自动修复不符合模式的数据，同时利用LLM处理包含语义信息的字符串。


<details>
  <summary>Details</summary>
Motivation: 现有字符串数据清洗系统在处理混合句法和语义错误、以及缺乏用户干预方面存在局限性，而DataVinci旨在解决这些问题。

Method: DataVinci通过学习正则表达式模式来检测和修复字符串错误。它利用LLM来抽象和具体化字符串中的语义部分，以便更好地学习模式和推导修复。对于无法形成多数模式的数据，它利用现有程序的执行信息来识别和纠正修复。

Result: DataVinci在错误检测和修复方面优于7个基线系统，并在4个现有和新的基准测试中表现出色。

Conclusion: DataVinci是一个创新的、全自动的无监督字符串数据错误检测和修复系统，能够有效处理复杂的字符串数据，并在实验中取得了优于基线方法的成果。

Abstract: String data is common in real-world datasets: 67.6% of values in a sample of 1.8 million real Excel spreadsheets from the web were represented as text. Systems that successfully clean such string data can have a significant impact on real users. While prior work has explored errors in string data, proposed approaches have often been limited to error detection or require that the user provide annotations, examples, or constraints to fix the errors. Furthermore, these systems have focused independently on syntactic errors or semantic errors in strings, but ignore that strings often contain both syntactic and semantic substrings. We introduce DataVinci, a fully unsupervised string data error detection and repair system. DataVinci learns regular-expression-based patterns that cover a majority of values in a column and reports values that do not satisfy such patterns as data errors. DataVinci can automatically derive edits to the data error based on the majority patterns and constraints learned over other columns without the need for further user interaction. To handle strings with both syntactic and semantic substrings, DataVinci uses an LLM to abstract (and re-concretize) portions of strings that are semantic prior to learning majority patterns and deriving edits. Because not all data can result in majority patterns, DataVinci leverages execution information from an existing program (which reads the target data) to identify and correct data repairs that would not otherwise be identified. DataVinci outperforms 7 baselines on both error detection and repair when evaluated on 4 existing and new benchmarks.

</details>


### [31] [Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples](https://arxiv.org/abs/2307.14565)
*Peng Li,Yeye He,Cong Yan,Yue Wang,Surajit Chaudhuri*

Main category: cs.DB

TL;DR: 许多真实世界的表格不符合关系标准，需要复杂的转换才能进行分析。本研究提出了Auto-Tables系统，可以自动合成多步转换管道，将非关系表转换为关系表，成功率超过70%。


<details>
  <summary>Details</summary>
Motivation: 关系表是数据库的标准，但在实际应用中，许多表格（如电子表格和网页表格）不符合此标准，需要复杂的转换才能方便地使用SQL进行查询，这给用户带来了困扰。

Method: 开发了Auto-Tables系统，可以自动合成包含多步转换（Python或其他语言）的管道，将非关系表转换为标准关系表，供下游分析使用。

Result: Auto-Tables系统在包含244个真实案例的基准测试中，在交互式速度下成功地为超过70%的案例合成了转换，无需用户干预。

Conclusion: Auto-Tables系统能够有效地为技术和非技术用户准备数据以进行分析，解决了手动编程转换的难题。

Abstract: Relational tables, where each row corresponds to an entity and each column corresponds to an attribute, have been the standard for tables in relational databases. However, such a standard cannot be taken for granted when dealing with tables "in the wild". Our survey of real spreadsheet-tables and web-tables shows that over 30% of such tables do not conform to the relational standard, for which complex table-restructuring transformations are needed before these tables can be queried easily using SQL-based analytics tools. Unfortunately, the required transformations are non-trivial to program, which has become a substantial pain point for technical and non-technical users alike, as evidenced by large numbers of forum questions in places like StackOverflow and Excel/Power-BI/Tableau forums.
  We develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations (in Python or other languages), to transform non-relational tables into standard relational forms for downstream analytics, obviating the need for users to manually program transformations. We compile an extensive benchmark for this new task, by collecting 244 real test cases from user spreadsheets and online forums. Our evaluation suggests that Auto-Tables can successfully synthesize transformations for over 70% of test cases at interactive speeds, without requiring any input from users, making this an effective tool for both technical and non-technical users to prepare data for analytics.

</details>


### [32] [Efficient and Compact Spreadsheet Formula Graphs](https://arxiv.org/abs/2302.05482)
*Dixin Tang,Fanchao Chen,Christopher De Leon,Tana Wattanawaroon,Jeaseok Yun,Srinivasan Seshadri,Aditya G. Parameswaran*

Main category: cs.DB

TL;DR: TACO通过利用表格局部性来压缩公式图，从而显著提高查询和维护效率。


<details>
  <summary>Details</summary>
Motivation: 在电子表格中，公式图对于数据分析至关重要，但其庞大和复杂性导致查询和维护耗时，降低了交互性。

Method: TACO利用表格局部性（相邻单元格具有相似的公式结构）的特性，设计了四种基于此的压缩算法，实现了在不解压的情况下直接查询压缩图，并能增量维护。

Result: TACO在集成到开源电子表格系统后，显著减小了公式图的大小。与基线相比，查询速度最多提高了34,972倍；与商业电子表格系统相比，查询速度最多提高了632倍。

Conclusion: TACO通过压缩公式图，有效解决了电子表格查询和维护的效率问题，显著提升了交互性。

Abstract: Spreadsheets are one of the most popular data analysis tools, wherein users can express computation as formulae alongside data. The ensuing dependencies are tracked as formula graphs. Efficiently querying and maintaining these formula graphs is critical for interactivity across multiple settings. Unfortunately, formula graphs are often large and complex such that querying and maintaining them is time-consuming, reducing interactivity. We propose TACO, a framework for efficiently compressing formula graphs, thereby reducing the time for querying and maintenance. The efficiency of TACO stems from a key spreadsheet property: tabular locality, which means that cells close to each other are likely to have similar formula structures. We leverage four such tabular locality-based patterns and develop algorithms for compressing formula graphs using these patterns, directly querying the compressed graph without decompression, and incrementally maintaining the graph during updates. We integrate TACO into an open-source spreadsheet system and show that TACO can significantly reduce formula graph sizes. For querying formula graphs, the speedups of TACO over a baseline implemented in our framework and a commercial spreadsheet system are up to 34,972x and 632x, respectively.

</details>


### [33] [Sigma Workbook: A Spreadsheet for Cloud Data Warehouses](https://arxiv.org/abs/2204.03128)
*James Gale,Max Seiden,Deepanshu Utkarsh,Jason Frantz,Rob Woollen,Çağatay Demiralp*

Main category: cs.DB

TL;DR: Sigma Workbook是一个新的交互式系统，让业务用户能够轻松地对大规模云数据仓库（CDW）中的数据进行可视化分析。


<details>
  <summary>Details</summary>
Motivation: 现有的CDW数据分析工具要么在即席转换方面功能有限，要么对业务用户来说难以使用。

Method: Sigma Workbook提供了一个类似电子表格的易于使用的界面，通过直接操作进行分析，并能从用户交互中动态构建SQL查询，直接在CDW上执行，从而利用了新一代CDW的可扩展性。

Result: 通过三个实际用例（队列分析、会话化和数据增强）展示了Sigma Workbook的易用性、可扩展性和表达能力。

Conclusion: Sigma Workbook使用户能够轻松地对大规模云数据仓库中的数据进行可视化分析。

Abstract: Cloud data warehouses (CDWs) bring large-scale data and compute power closer to users in enterprises. However, existing tools for analyzing data in CDWs are either limited in ad-hoc transformations or difficult to use for business users. Here we introduce Sigma Workbook, a new interactive system that enables business users to easily perform a visual analysis of data in CDWs at scale. For this, Sigma Workbook provides an accessible spreadsheet-like interface for analysis through direct manipulation. Sigma Workbook dynamically constructs matching SQL queries from user interactions, building on the versatility and expressivity of SQL. Constructed queries are directly executed on CDWs, leveraging the superior characteristics of the new generation CDWs, including scalability. We demonstrate Sigma Workbook through 3 real-life use cases -- cohort analysis, sessionization, and data augmentation -- and underline Workbook's ease of use, scalability, and expressivity.

</details>


### [34] [Efficient Specialized Spreadsheet Parsing for Data Science](https://arxiv.org/abs/2202.13189)
*Felix Henze,Haralampos Gavriilidis,Eleni Tzirita Zacharatou,Volker Markl*

Main category: cs.DB

TL;DR: 当前电子表格加载方法存在运行时长或内存占用过高的问题，影响了在通用设备上的数据探索。我们提出了一种新的解析器，通过紧密结合解压和解析来最小化内存使用，并通过优化解析例程和利用并行性来减少运行时间。


<details>
  <summary>Details</summary>
Motivation: 当前电子表格加载方法在通用设备上存在性能瓶颈，阻碍了数据探索。

Method: 提出了一种新的解析器，它将解压和解析紧密耦合以最小化内存使用，并通过优化特定于电子表格的解析例程和利用并行性来减少运行时间。

Result: 在将Excel电子表格加载到R环境的实验中，新方法比现有方法快3倍，内存占用减少40倍。

Conclusion: 我们提出的方法能够高效地在通用设备上加载电子表格，显著提高了性能并减少了内存占用。

Abstract: Spreadsheets are widely used for data exploration. Since spreadsheet systems have limited capabilities, users often need to load spreadsheets to other data science environments to perform advanced analytics. However, current approaches for spreadsheet loading suffer from either high runtime or memory usage, which hinders data exploration on commodity systems. To make spreasheet loading practical on commodity systems, we introduce a novel parser that minimizes memory usage by tightly coupling decompression and parsing. Furthermore, to reduce the runtime, we introduce optimized spreadsheet-specific parsing routines and employ parallelism. To evaluate our approach, we implement a prototype for loading Excel spreadsheets into R environments. Our evaluation shows that our novel approach is up to 3x faster while consuming up to 40x less memory than state-of-the-art approaches.

</details>


### [35] [Spread2RML: Constructing Knowledge Graphs by Predicting RML Mappings on Messy Spreadsheets](https://arxiv.org/abs/2110.12829)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: Spread2RML can automatically predict RML mappings for messy spreadsheets, saving time and effort.


<details>
  <summary>Details</summary>
Motivation: Mapping semi-structured data to RDF knowledge graphs using RML can be time-consuming, especially with complex and messy spreadsheet data. This paper aims to reduce these efforts by predicting RML mappings for such data.

Method: Spread2RML uses an extensible set of RML object map templates applied to each column based on heuristics to predict RML mappings for messy spreadsheets.

Result: The evaluation using three datasets (messy synthetic data and less messy data.gov spreadsheets) showed promising results, particularly in the approach's full automation and ability to handle messy data.

Conclusion: Spread2RML offers a promising, fully automatic solution for generating RML mappings from messy spreadsheets, reducing the effort required for data mapping.

Abstract: The RDF Mapping Language (RML) allows to map semi-structured data to RDF knowledge graphs. Besides CSV, JSON and XML, this also includes the mapping of spreadsheet tables. Since spreadsheets have a complex data model and can become rather messy, their mapping creation tends to be very time consuming. In order to reduce such efforts, this paper presents Spread2RML which predicts RML mappings on messy spreadsheets. This is done with an extensible set of RML object map templates which are applied for each column based on heuristics. In our evaluation, three datasets are used ranging from very messy synthetic data to spreadsheets from data.gov which are less messy. We obtained first promising results especially with regard to our approach being fully automatic and dealing with rather messy data.

</details>


### [36] [Supercomputing Enabled Deployable Analytics for Disaster Response](https://arxiv.org/abs/2108.11525)
*Kaira Samuel,Jeremy Kepner,Michael Jones,Lauren Milechin,Vijay Gadepally,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Michael Houle,Anna Klein,Victor Lopez,Julie Mullen,Andrew Prout,Albert Reuther,Antonio Rosa,Sid Samsi,Charles Yee,Peter Michaleas*

Main category: cs.DB

TL;DR: 在云访问受限的情况下，预先计算分析结果并将其打包为可在本地使用的文件，以支持一线工作人员。


<details>
  <summary>Details</summary>
Motivation: 标准云分析平台受网络和安全限制，无法满足前线工作人员的需求。

Method: 将地理空间人口普查数据预先计算为Google Earth（kml）和Excel（xlsx）文件，以便在无网络或额外软件的情况下使用，并能在多种设备上运行。

Result: 生成了数千个Google Earth和Excel文件，包含了总人口、15岁以下人口、65岁以上人口和人口中位数等关键人口统计数据，可以按郡分类，并在几分钟内生成。

Conclusion: 通过预先计算分析并提供本地可用的文件，可以为应急响应人员提供强大的工具，以改善应急准备工作。

Abstract: First responders and other forward deployed essential workers can benefit from advanced analytics. Limited network access and software security requirements prevent the usage of standard cloud based microservice analytic platforms that are typically used in industry. One solution is to precompute a wide range of analytics as files that can be used with standard preinstalled software that does not require network access or additional software and can run on a wide range of legacy hardware. In response to the COVID-19 pandemic, this approach was tested for providing geo-spatial census data to allow quick analysis of demographic data for better responding to emergencies. These data were processed using the MIT SuperCloud to create several thousand Google Earth and Microsoft Excel files representative of many advanced analytics. The fast mapping of census data using Google Earth and Microsoft Excel has the potential to give emergency responders a powerful tool to improve emergency preparedness. Our approach displays relevant census data (total population, population under 15, population over 65, median age) per census block, sorted by county, through a Microsoft Excel spreadsheet (xlsx file) and Google Earth map (kml file). The spreadsheet interface includes features that allow users to convert between different longitude and latitude coordinate units. For the Google Earth files, a variety of absolute and relative colors maps of population density have been explored to provide an intuitive and meaningful interface. Using several hundred cores on the MIT SuperCloud, new analytics can be generated in a few minutes.

</details>


### [37] [Table2Charts: Recommending Charts by Learning Shared Table Representations](https://arxiv.org/abs/2008.11015)
*Mengyu Zhou,Qingtao Li,Xinyi He,Yuejiang Li,Yibo Liu,Wei Ji,Shi Han,Yining Chen,Daxin Jiang,Dongmei Zhang*

Main category: cs.DB

TL;DR: Table2Charts框架通过深度Q学习和启发式搜索，从大规模(表格,图表)数据集中学习图表推荐模式，解决了效率、数据不平衡和上下文问题，并在多图表推荐和人工评估中表现优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 为解决在多维数据集（表格）可视化中推荐常用图表时面临的效率、数据不平衡和表格上下文等挑战。

Method: Table2Charts框架，基于深度Q学习与复制机制及启发式搜索，进行表格到序列的生成，每个序列遵循一个图表模板。

Result: 在包含165k表格和266k图表的大型电子表格语料库上，Table2Charts学习到了表格字段的共享表示，使得不同图表类型的推荐任务可以相互促进。Table2Charts在多图表推荐任务（R@3=0.61, R@1=0.43，召回率翻倍）和人工评估中均优于其他图表推荐系统。

Conclusion: Table2Charts框架能够有效解决图表推荐中的挑战，并在多图表推荐和人工评估方面取得了优于现有系统的性能。

Abstract: It is common for people to create different types of charts to explore a multi-dimensional dataset (table). However, to recommend commonly composed charts in real world, one should take the challenges of efficiency, imbalanced data and table context into consideration. In this paper, we propose Table2Charts framework which learns common patterns from a large corpus of (table, charts) pairs. Based on deep Q-learning with copying mechanism and heuristic searching, Table2Charts does table-to-sequence generation, where each sequence follows a chart template. On a large spreadsheet corpus with 165k tables and 266k charts, we show that Table2Charts could learn a shared representation of table fields so that recommendation tasks on different chart types could mutually enhance each other. Table2Charts outperforms other chart recommendation systems in both multi-type task (with doubled recall numbers R@3=0.61 and R@1=0.43) and human evaluations.

</details>


### [38] [Sigma Worksheet: Interactive Construction of OLAP Queries](https://arxiv.org/abs/2012.00697)
*James Gale,Max Seiden,Gretchen Atwood,Jason Frantz,Rob Woollen,Çağatay Demiralp*

Main category: cs.DB

TL;DR: Sigma Worksheet 是一款用户友好的交互式系统，可通过熟悉的类似电子表格的界面，在云数据仓库 (CDW) 中对大规模数据进行即席可视化分析。


<details>
  <summary>Details</summary>
Motivation: 现有工具难以满足业务用户的需求，他们是企业中最大的用户群体，需要进行即席转换和易用性。

Method: Sigma Worksheet 通过电子表格式界面和直接操作，动态构建 SQL 查询，直接在 CDW 上执行，利用 CDW 的可扩展性。

Result: Sigma Worksheet 在真实场景（队列分析、会话化）中具有表现力，其生成的 SQL 查询在 TPC-H 基准测试中性能与参考查询相当，并且在用户调查和访谈中被证明更易于使用，提高了用户生产力。

Conclusion: Sigma Worksheet 是一款易于使用的工具，可以提高用户生产力，并建议通过提供指导来进一步改善用户体验。

Abstract: The new generation of cloud data warehouses (CDWs) brings large amounts of data and compute power closer to users in enterprises. The ability to directly access the warehouse data, interactively analyze and explore it at scale can empower users to improve their decision making cycles. However, existing tools for analyzing data in CDWs are either limited in ad-hoc transformations or difficult to use for business users, the largest user segment in enterprises. Here we introduce Sigma Worksheet, a new interactive system that enables users to easily perform ad-hoc visual analysis of data in CDWs at scale. For this, Sigma Worksheet provides an accessible spreadsheet-like interface for data analysis through direct manipulation. Sigma Worksheet dynamically constructs matching SQL queries from user interactions on this familiar interface, building on the versatility and expressivity of SQL. Sigma Worksheet executes constructed queries directly on CDWs, leveraging the superior characteristics of the new generation CDWs, including scalability. To evaluate Sigma Worksheet, we first demonstrate its expressivity through two real life use cases, cohort analysis and sessionization. We then measure the performance of the Worksheet generated queries with a set of experiments using the TPC-H benchmark. Results show the performance of our compiled SQL queries is comparable to that of the reference queries of the benchmark. Finally, to assess the usefulness of Sigma Worksheet in deployment, we elicit feedback through a 100-person survey followed by a semi-structured interview study with 70 participants. We find that Sigma Worksheet is easier to use and learn, improving the productivity of users. Our findings also suggest Sigma Worksheet can further improve user experience by providing guidance to users at various steps of data analysis.

</details>


### [39] [Mapping Spreadsheets to RDF: Supporting Excel in RML](https://arxiv.org/abs/2104.13600)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: RML Mapper 已扩展以支持 Excel 文件，允许将电子表格数据映射到 RDF 图，并提供在线演示。


<details>
  <summary>Details</summary>
Motivation: RML 规范和现有工具忽略了电子表格格式，需要扩展以支持 Excel 文件，从而能够将电子表格数据映射到 RDF 图。

Method: 扩展了 RML Mapper 工具以支持 Microsoft Excel 电子表格文件，允许访问电子表格单元格的元数据，并提供了一些针对特定用例的实验性功能。

Result: 成功扩展了 RML Mapper 以支持 Excel 文件，并提供了一个在线演示来展示其功能。

Conclusion: 该扩展使 RML 能够处理电子表格数据，并为用户提供了更广泛的数据映射选项。

Abstract: The RDF Mapping Language (RML) enables, among other formats, the mapping of tabular data as Comma-Separated Values (CSV) files to RDF graphs. Unfortunately, the widely used spreadsheet format is currently neglected by its specification and well-known implementations. Therefore, we extended one of the tools which is RML Mapper to support Microsoft Excel spreadsheet files and demonstrate its capabilities in an interactive online demo. Our approach allows to access various meta data of spreadsheet cells in typical RML maps. Some experimental features for more specific use cases are also provided. The implementation code is publicly available in a GitHub fork.

</details>


### [40] [Dataset Generation Patterns for Evaluating Knowledge Graph Construction](https://arxiv.org/abs/2104.13576)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: Confidentiality hinders the publication of authentic datasets for knowledge graph construction. This paper generates synthetic data that mimics real-world data by discovering and reproducing 11 distinct patterns found in industry spreadsheets using a generator called Data Sprout.


<details>
  <summary>Details</summary>
Motivation: Authentic, labeled datasets of personal and enterprise data are crucial for evaluating knowledge graph construction approaches in industrial scenarios, but confidentiality concerns prevent their publication. Therefore, there is a need to generate synthetic data that is as authentic as possible.

Method: The paper identifies 11 distinct patterns in real spreadsheets from industry. It then develops a generator named Data Sprout that can reproduce these patterns to create synthetic datasets. The paper describes the general working of the generator and the effects of the implemented patterns.

Result: The paper demonstrates that Data Sprout can reproduce 11 distinct patterns found in real-world industry spreadsheets, thus generating synthetic data that appears authentic.

Conclusion: By identifying and replicating patterns in real-world data, synthetic datasets can be generated to overcome confidentiality issues and support the evaluation of knowledge graph construction methods in industrial settings.

Abstract: Confidentiality hinders the publication of authentic, labeled datasets of personal and enterprise data, although they could be useful for evaluating knowledge graph construction approaches in industrial scenarios. Therefore, our plan is to synthetically generate such data in a way that it appears as authentic as possible. Based on our assumption that knowledge workers have certain habits when they produce or manage data, generation patterns could be discovered which can be utilized by data generators to imitate real datasets. In this paper, we initially derived 11 distinct patterns found in real spreadsheets from industry and demonstrate a suitable generator called Data Sprout that is able to reproduce them. We describe how the generator produces spreadsheets in general and what altering effects the implemented patterns have.

</details>


### [41] [Interactively Constructing Knowledge Graphs from Messy User-Generated Spreadsheets](https://arxiv.org/abs/2103.03537)
*Markus Schröder,Christian Jilek,Michael Schulze,Andreas Dengel*

Main category: cs.DB

TL;DR: 知识工作者随意填写的电子表格可能包含相当非结构化的内容，给机器和人类的理解带来困难。本文提出了一种交互式方法来解决从“混乱”的电子表格构建知识图谱的挑战，通过图形用户界面允许知识工程师批量注释单元格，并基于注释构建知识图谱。


<details>
  <summary>Details</summary>
Motivation: 解决知识工作者随意填写导致的电子表格内容非结构化问题，以及由此产生的构建知识图谱的挑战。

Method: 提出一种交互式方法，包括一个图形用户界面，允许知识工程师批量注释电子表格单元格，并基于注释构建知识图谱。

Result: 使用五个工业场景的电子表格构建了一个包含25000个三元组的知识图谱，并通过与现有RDF映射语言（RML）方法的比较，突显了该方法的贡献。

Conclusion: 所提出的交互式方法能够有效地解决从非结构化电子表格构建知识图谱的挑战，并在实际应用中展现出优于现有方法的优势。

Abstract: When spreadsheets are filled freely by knowledge workers, they can contain rather unstructured content. For humans and especially machines it becomes difficult to interpret such data properly. Therefore, spreadsheets are often converted to a more explicit, formal and structured form, for example, to a knowledge graph. However, if a data maintenance strategy has been missing and user-generated data becomes "messy", the construction of knowledge graphs will be a challenging task. In this paper, we catalog several of those challenges and propose an interactive approach to solve them. Our approach includes a graphical user interface which enables knowledge engineers to bulk-annotate spreadsheet cells with extracted information. Based on the cells' annotations a knowledge graph is ultimately formed. Using five spreadsheets from an industrial scenario, we built a 25k-triple graph during our evaluation. We compared our method with the state-of-the-art RDF Mapping Language (RML) attempt. The comparison highlights contributions of our approach.

</details>


### [42] [Leam: An Interactive System for In-situ Visual Text Analysis](https://arxiv.org/abs/2009.03520)
*Sajjadur Rahman,Peter Griggs,Çağatay Demiralp*

Main category: cs.DB

TL;DR: Leam 是一个结合了计算笔记本、电子表格和可视化工具优点的系统，用于处理文本分析的全部流程，解决了数据异构性、数据溯源、工作流可重用性和可复现性以及兼容性等挑战。


<details>
  <summary>Details</summary>
Motivation: 随着网络上数字文本的规模和可用性的增加，企业（如在线零售商和聚合商）通常使用文本分析来挖掘和分析数据，以改进他们的服务和产品。现有的文本分析系统通常只支持这些阶段的一部分，并且未能解决数据异构性、数据溯源、工作流可重用性和可复现性以及与既有实践的兼容性等挑战。

Method: 提出 Leam 系统，该系统将文本分析过程视为一个单一的连续体，结合了计算笔记本、电子表格和可视化工具的优点。Leam 的特点是：一个用于运行文本分析工作流的交互式用户界面，一个用于管理多个原子和复合数据类型的新数据模型，以及一个能够捕获代表文本分析不同阶段的各种操作并协调系统不同组件（包括数据、代码和可视化）的表达性代数。

Result: 报告了 Leam 的开发进展，并通过使用示例展示了其有用性。

Conclusion: 概述了 Leam 的一些增强功能，并指出了开发交互式可视化文本分析系统的若干研究方向。

Abstract: With the increase in scale and availability of digital text generated on the web, enterprises such as online retailers and aggregators often use text analytics to mine and analyze the data to improve their services and products alike. Text data analysis is an iterative, non-linear process with diverse workflows spanning multiple stages, from data cleaning to visualization. Existing text analytics systems usually accommodate a subset of these stages and often fail to address challenges related to data heterogeneity, provenance, workflow reusability and reproducibility, and compatibility with established practices. Based on a set of design considerations we derive from these challenges, we propose Leam, a system that treats the text analysis process as a single continuum by combining advantages of computational notebooks, spreadsheets, and visualization tools. Leam features an interactive user interface for running text analysis workflows, a new data model for managing multiple atomic and composite data types, and an expressive algebra that captures diverse sets of operations representing various stages of text analysis and enables coordination among different components of the system, including data, code, and visualizations. We report our current progress in Leam development while demonstrating its usefulness with usage examples. Finally, we outline a number of enhancements to Leam and identify several research directions for developing an interactive visual text analysis system.

</details>


### [43] [ObjTables: structured spreadsheets that promote data quality, reuse, and integration](https://arxiv.org/abs/2005.05227)
*Jonathan R. Karr,Wolfram Liebermeister,Arthur P. Goldberg,John A. P. Sekar,Bilal Shaikh*

Main category: cs.DB

TL;DR: ObjTables是一个工具包，通过将电子表格与模式（schemas）和对象关系映射系统相结合，使电子表格既能被人类阅读，也能被机器阅读，从而解决了科学研究中数据重用和整合的挑战。


<details>
  <summary>Details</summary>
Motivation: 科学研究中的一个核心挑战是从复杂的网络中理解系统行为的出现。这通常需要聚合、重用和整合异构信息。文章的补充电子表格是一个关键的数据源。但由于电子表格的创建方式随意，缺乏定义其所代表的对象、关系和属性的模式，因此难以进行再分析。

Method: 开发了ObjTables工具包，它包含：用于模式的格式；用于指示每个电子表格和列所代表的类和属性的标记；用于科学信息的多种数据类型；以及用于使用模式读取、写入、验证、比较、合并、修订和分析电子表格的高级软件。

Result: ObjTables使电子表格更易于重用，有望实现前所未有的二次元分析。通过轻松构建新格式和相关软件来处理新类型的数据，ObjTables还可以加速新兴的科学领域。

Conclusion: ObjTables通过将电子表格与模式和对象关系映射系统相结合，解决了科学研究中电子表格数据难以重用和整合的问题，提高了数据的可读性和可分析性，有望促进科学研究的二次元分析和新兴科学领域的发展。

Abstract: A central challenge in science is to understand how systems behaviors emerge from complex networks. This often requires aggregating, reusing, and integrating heterogeneous information. Supplementary spreadsheets to articles are a key data source. Spreadsheets are popular because they are easy to read and write. However, spreadsheets are often difficult to reanalyze because they capture data ad hoc without schemas that define the objects, relationships, and attributes that they represent. To help researchers reuse and compose spreadsheets, we developed ObjTables, a toolkit that makes spreadsheets human- and machine-readable by combining spreadsheets with schemas and an object-relational mapping system. ObjTables includes a format for schemas; markup for indicating the class and attribute represented by each spreadsheet and column; numerous data types for scientific information; and high-level software for using schemas to read, write, validate, compare, merge, revision, and analyze spreadsheets. By making spreadsheets easier to reuse, ObjTables could enable unprecedented secondary meta-analyses. By making it easy to build new formats and associated software for new types of data, ObjTables can also accelerate emerging scientific fields.

</details>


### [44] [Needles in the 'Sheet'stack: Augmented Analytics to get Insights from Spreadsheets](https://arxiv.org/abs/2006.08224)
*Medha Atre,Anand Deshpande,Reshma Godse,Pooja Deokar,Sandip Moharir,Dhruva Ray,Akshay Chitlangia,Trupti Phadnis,Yugansh Goyal*

Main category: cs.DB

TL;DR: 本論文提出了一種無需事先了解資料庫結構或資料即可分析定期報表（檢視）的見解工具，目標是讓不熟悉資料庫結構的用戶也能進行分析。


<details>
  <summary>Details</summary>
Motivation: 現有的商業智慧（BI）工具雖然進步，但仍需要用戶具備資料庫結構知識。本研究旨在為不熟悉資料庫結構或缺乏資源的用戶提供一種新的分析方法。

Method: 針對不同報表的定期試算表，在無需了解資料庫結構、報表或資料資訊的先決條件下，提供見解。

Result: 提出了一種見解工具，能夠處理定期報表（檢視）並提供有用的資訊。

Conclusion: 該解決方案能讓不熟悉資料庫結構的用戶也能進行數據分析，擴展了商業智慧工具的應用範圍。

Abstract: Business intelligence (BI) tools for database analytics have come a long way and nowadays also provide ready insights or visual query explorations, e.g. QuickInsights by Microsoft Power BI, SpotIQ by ThoughtSpot, Zenvisage, etc. In this demo, we focus on providing insights by examining periodic spreadsheets of different reports (aka views), without prior knowledge of the schema of the database or reports, or data information. Such a solution is targeted at users without the familiarity with the database schema or resources to conduct analytics in the contemporary way.

</details>


### [45] [A Billion Updates per Second Using 30,000 Hierarchical In-Memory D4M Databases](https://arxiv.org/abs/1902.00846)
*Jeremy Kepner,Vijay Gadepally,Lauren Milechin,Siddharth Samsi,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Micheal Houle,Micheal Jones,Anne Klein,Peter Michaleas,Julie Mullen,Andrew Prout,Antonio Rosa,Charles Yee,Albert Reuther*

Main category: cs.DB

TL;DR: 该论文提出了一种名为D4M的库，该库使用分层关联数组来优化流式网络数据的内存层次结构更新性能，并在MIT SuperCloud上实现了每秒19亿次的更新速率。


<details>
  <summary>Details</summary>
Motivation: 分析大规模网络需要高效的流式更新图表示，而关联数组非常适合表示和分析流式网络数据。

Method: 在Python、Julia和Matlab/Octave等多种语言中实现关联数组，并提供轻量级内存数据库。设计了分层关联数组以优化内存层次结构性能，并在MIT SuperCloud上运行了34,000个实例。

Result: 在MIT SuperCloud上运行34,000个分层D4M关联数组实例，实现了每秒1,900,000,000次的持续更新速率。

Conclusion: D4M库及其分层实现能够高效处理超大规模流式网络数据，为MIT SuperCloud分析海量数据提供了能力。

Abstract: Analyzing large scale networks requires high performance streaming updates of graph representations of these data. Associative arrays are mathematical objects combining properties of spreadsheets, databases, matrices, and graphs, and are well-suited for representing and analyzing streaming network data. The Dynamic Distributed Dimensional Data Model (D4M) library implements associative arrays in a variety of languages (Python, Julia, and Matlab/Octave) and provides a lightweight in-memory database. Associative arrays are designed for block updates. Streaming updates to a large associative array requires a hierarchical implementation to optimize the performance of the memory hierarchy. Running 34,000 instances of a hierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of 1,900,000,000 updates per second. This capability allows the MIT SuperCloud to analyze extremely large streaming network data sets.

</details>


### [46] [Towards Semantically Enhanced Data Understanding](https://arxiv.org/abs/1806.04952)
*Markus Schröder,Christian Jilek,Jörn Hees,Andreas Dengel*

Main category: cs.DB

TL;DR: 数据科学中的数据理解可以通过将数据与其文档链接的单一语义模型来改进，从而减少查找开销并支持其他应用程序。


<details>
  <summary>Details</summary>
Motivation: 当前数据理解方法依赖于与数据分离的文档，导致查找开销大且不利于其他应用程序使用。

Method: 提出一种使用单一语义模型链接数据及其文档的方法。

Result: 该模型允许数据科学家直接查找相关信息，浏览文档，并为搜索、比较、集成或可视化等其他应用程序提供支持。已演示早期原型。

Conclusion: 所提出的方法通过将数据与其文档链接的单一语义模型，简化了数据理解过程，提高了效率，并为其他应用程序提供了支持。

Abstract: In the field of machine learning, data understanding is the practice of getting initial insights in unknown datasets. Such knowledge-intensive tasks require a lot of documentation, which is necessary for data scientists to grasp the meaning of the data. Usually, documentation is separate from the data in various external documents, diagrams, spreadsheets and tools which causes considerable look up overhead. Moreover, other supporting applications are not able to consume and utilize such unstructured data. That is why we propose a methodology that uses a single semantic model that interlinks data with its documentation. Hence, data scientists are able to directly look up the connected information about the data by simply following links. Equally, they can browse the documentation which always refers to the data. Furthermore, the model can be used by other approaches providing additional support, like searching, comparing, integrating or visualizing data. To showcase our approach we also demonstrate an early prototype.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [47] [Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!](https://arxiv.org/abs/2309.02110)
*James P. Dilger*

Main category: math.HO

TL;DR: Wordle玩家的行为分析：通过分析玩家的猜测数据，揭示了欺骗行为、玩家对起始词的偏好以及玩家容易受到外部信息影响的现象。


<details>
  <summary>Details</summary>
Motivation: 分析Wordle玩家的行为，特别是利用信息论来评估玩家的运气和技巧，并通过对玩家第一猜测数据的分析来揭示一些有趣的现象，如欺骗行为、玩家对起始词的偏好以及玩家容易受到外部信息影响。

Method: 收集并分析2023年5月至8月Wordle玩家的第一猜测数据，运用信息论评估玩家的运气和技巧，并通过数据分析推断玩家行为模式。

Result: A) 每天约有0.2%-0.5%的玩家一次性猜对单词，远高于随机猜测的概率（0.043%），表明存在玩家作弊行为（每天约4000-10000人）。B) 至少有1/3的玩家有固定的起始词，并且即使目标单词重复出现，玩家仍倾向于继续使用他们喜欢的起始词。C) 在2023年8月15日，约有30000名玩家突然改变了起始词，可能受到了外部信息（如填字游戏线索）的影响。

Conclusion: 该研究通过量化分析Wordle玩家数据，提供了关于Wordle作弊行为、玩家习惯以及玩家易受外部影响的有力证据，超越了传统的社交媒体分析方法。

Abstract: Wordle is a popular, online word game offered by the New York Times (nytimes.com). Currently there are some 2 million players of the English version worldwide. Players have 6 attempts to guess the daily word (target word) and after each attempt, the player receives color-coded information about the correctness and position of each letter in the guess. After either a successful completion of the puzzle or the final unsuccessful attempt, software can assess the player's luck and skill using Information Theory and can display data for the first, second, ..., sixth guesses of a random sample of all players. Recently, I discovered that the latter data is presented in a format that can easily be copied and pasted into a spreadsheet. I compiled data on Wordle players' first guesses from May 2023 - August 2023 and inferred some interesting information about Wordle players. A) Every day, about 0.2-0.5% of players solve the puzzle in one attempt. Because the odds of guessing the one of 2,315 possible target words at random is 0.043%, this implies that 4,000 - 10,000 players cheat by obtaining the target word outside of playing the game! B) At least 1/3 of the players have a favorite starting word, or cycle through several. And even though players should be aware that target words are never repeated, most players appear to remain loyal to their starting word even after its appearance as a target word. C) On August 15, 2023, about 30,000 players abruptly changed their starting word, presumably based on a crossword puzzle clue! Wordle players can be influenced! This study goes beyond social media postings, surveys, and Google Trends to provide solid, quantitative evidence about cheating in Wordle.

</details>


### [48] [GeoGebra e situações que envolvem modelação numa abordagem STEAM](https://arxiv.org/abs/1907.02099)
*J. M. D. S. Dos Santos,A. P. Silveira,A. E. S. Trocado*

Main category: math.HO

TL;DR: 该论文提出了一种在卢萨语区数学课程中实施 STEAM 方法（特别是使用 GeoGebra 交互式数学软件）的教学材料。


<details>
  <summary>Details</summary>
Motivation: 为了在数学课程中实施包含技术（如 GeoGebra 软件）的 STEAM 方法，并培养教师和学生在几何建模方面的能力。

Method: 通过设计一系列可供教师和学生使用的任务，引导用户利用 GeoGebra 软件的二维和三维窗口、CAS 窗口、电子表格等功能，来解决几何建模问题，并提供操作脚本以降低使用门槛。

Result: 开发了一套适用于教师培训和学生学习的 GeoGebra 教学任务，能够促进跨学科联系，并支持数学项目的开展。

Conclusion: 该套教学材料旨在通过 GeoGebra 软件促进数学教学的 STEAM 融合，并计划将其翻译成西班牙语和英语，以扩大其在全球范围内的影响力。

Abstract: In order to implement a STEAM approach including the use of technology, namely the use of interactive mathematics software GeoGebra, in mathematics classes, in the lusophone space, the materials presented here were conceived, to be implemented in a first phase among teachers. Later, with the necessary adaptations, these tasks will be applied to the students. The tasks deal with modeling situations, in two- and three-dimensional geometric problems, in order to apply GeoGebra software in its analysis to illustrate its capabilities. The different windows of this software are used, namely the 2D and 3D windows, CAS window, spreadsheet and extra two dimensional windows in order to study cutting planes in solids and some surfaces. The tasks are presented so that any user, regardless of the degree of knowledge they have of the software, can follow them, being supported in scripts with some indications of the tools and commands to use. Designed for the teaching and learning of Mathematics, from a STEAM approach, these tasks allow connections with other Sciences and the Arts, and allow the development of projects using and consolidating relevant mathematical contents. These tasks are part of the proposals of activities of the participants of the Training Courses for Trainers in GeoGebra for Portuguese Speaking Countries, which from 2019 have an impact on the STEAM approach. These courses are carried out with the high sponsorship of the Organization of Ibero-American States for Education, Science and Culture (OEI). Given the interest that the tasks have for the users of the Iberian space, as well as their dissemination at a global level, the materials initially developed in Portuguese language will be adapted for Spanish and English speakers.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [49] [Hillview: A trillion-cell spreadsheet for big data](https://arxiv.org/abs/1907.04827)
*Mihai Budiu,Parikshit Gopalan,Lalith Suresh,Udi Wieder,Han Kruiger,Marcos K. Aguilera*

Main category: cs.DC

TL;DR: Hillview是一个为处理无法单机处理的大型数据集而设计的分布式电子表格，通过使用名为vizket（可视化草图）的技术，实现了高效的交互式数据探索和可视化，能够处理数十亿行和数万亿单元格的数据。


<details>
  <summary>Details</summary>
Motivation: 提供一个能处理单机无法处理的大型数据集的分布式电子表格，并保持高度交互性，以便数据分析师能够快速探索信息。

Method: 引入可视化草图（vizket），结合数据摘要的算法技术和计算机图形学的渲染原则。Vizket通过并行计算、减少通信、提供渐进式可视化和精确的准确性保证来实现扩展。

Result: 在八台服务器上运行的Hillview能够导航和可视化包含数十亿行和数万亿单元格的数据集，超越了现有系统的性能。

Conclusion: Vizket是一种简单但有效的技术，能够扩展电子表格的功能，处理海量数据，并提供高度的交互性和可视化能力。

Abstract: Hillview is a distributed spreadsheet for browsing very large datasets that cannot be handled by a single machine. As a spreadsheet, Hillview provides a high degree of interactivity that permits data analysts to explore information quickly along many dimensions while switching visualizations on a whim. To provide the required responsiveness, Hillview introduces visualization sketches, or vizketches, as a simple idea to produce compact data visualizations. Vizketches combine algorithmic techniques for data summarization with computer graphics principles for efficient rendering. While simple, vizketches are effective at scaling the spreadsheet by parallelizing computation, reducing communication, providing progressive visualizations, and offering precise accuracy guarantees. Using Hillview running on eight servers, we can navigate and visualize datasets of tens of billions of rows and trillions of cells, much beyond the published capabilities of competing systems.

</details>


### [50] [Streaming 1.9 Billion Hypersparse Network Updates per Second with D4M](https://arxiv.org/abs/1907.04217)
*Jeremy Kepner,Vijay Gadepally,Lauren Milechin,Siddharth Samsi,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Michael Houle,Michael Jones,Anne Klein,Peter Michaleas,Julie Mullen,Andrew Prout,Antonio Rosa,Charles Yee,Albert Reuther*

Main category: cs.DC

TL;DR: D4M库实现了支持类数组、稀疏矩阵、图和网络等数据结构的多维动态分布式数据模型。


<details>
  <summary>Details</summary>
Motivation: D4M的流式更新对内存层级造成巨大压力，需要设计一种能够降低内存压力并显著提高更新速率的层级化关联数组实现。

Method: 通过实现层级化关联数组，并在每个层级中控制条目数量，从而减少内存压力并提高更新速率。该层级化关联数组易于调整参数以实现最佳性能。

Result: 在单实例中，层级化数组实现了超过40,000次/秒的更新速率。在MIT超级计算集群上，扩展到34,000个层级化D4M关联数组实例，实现了1,900,000,000次/秒的持续更新速率。

Conclusion: 层级化关联数组的实现能够有效处理大规模流式网络数据分析的内存和性能挑战。

Abstract: The Dynamic Distributed Dimensional Data Model (D4M) library implements associative arrays in a variety of languages (Python, Julia, and Matlab/Octave) and provides a lightweight in-memory database implementation of hypersparse arrays that are ideal for analyzing many types of network data. D4M relies on associative arrays which combine properties of spreadsheets, databases, matrices, graphs, and networks, while providing rigorous mathematical guarantees, such as linearity. Streaming updates of D4M associative arrays put enormous pressure on the memory hierarchy. This work describes the design and performance optimization of an implementation of hierarchical associative arrays that reduces memory pressure and dramatically increases the update rate into an associative array. The parameters of hierarchical associative arrays rely on controlling the number of entries in each level in the hierarchy before an update is cascaded. The parameters are easily tunable to achieve optimal performance for a variety of applications. Hierarchical arrays achieve over 40,000 updates per second in a single instance. Scaling to 34,000 instances of hierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of 1,900,000,000 updates per second. This capability allows the MIT SuperCloud to analyze extremely large streaming network data sets.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [51] [Using the Abstract Computer Architecture Description Language to Model AI Hardware Accelerators](https://arxiv.org/abs/2402.00069)
*Mika Markus Müller,Alexander Richard Manfred Borst,Konstantin Lübeck,Alexander Louis-Ferdinand Jung,Oliver Bringmann*

Main category: cs.AR

TL;DR: AI硬件加速器需要参数配置，ACADL可用于建模和性能评估。


<details>
  <summary>Details</summary>
Motivation: AI硬件加速器需要参数配置，但目前比较各方案的性能具有挑战性。

Method: 使用ACADL建模AI硬件加速器，并进行模型和性能仿真的映射。

Result: ACADL可以用于AI硬件加速器的建模，并能获得性能结果。

Conclusion: ACADL可用于AI硬件加速器的建模和性能评估。

Abstract: Artificial Intelligence (AI) has witnessed remarkable growth, particularly through the proliferation of Deep Neural Networks (DNNs). These powerful models drive technological advancements across various domains. However, to harness their potential in real-world applications, specialized hardware accelerators are essential. This demand has sparked a market for parameterizable AI hardware accelerators offered by different vendors.
  Manufacturers of AI-integrated products face a critical challenge: selecting an accelerator that aligns with their product's performance requirements. The decision involves choosing the right hardware and configuring a suitable set of parameters. However, comparing different accelerator design alternatives remains a complex task. Often, engineers rely on data sheets, spreadsheet calculations, or slow black-box simulators, which only offer a coarse understanding of the performance characteristics.
  The Abstract Computer Architecture Description Language (ACADL) is a concise formalization of computer architecture block diagrams, which helps to communicate computer architecture on different abstraction levels and allows for inferring performance characteristics. In this paper, we demonstrate how to use the ACADL to model AI hardware accelerators, use their ACADL description to map DNNs onto them, and explain the timing simulation semantics to gather performance results.

</details>


### [52] [Online Model Swapping in Architectural Simulation](https://arxiv.org/abs/2012.01571)
*Patrick Lavin,Jeffrey Young,Rich Vuduc,Jonathan Beard*

Main category: cs.AR

TL;DR: 当系统和应用程序日益复杂时，详细的模拟所需时间会越来越长。为了快速迭代设计，架构师必须依赖更简单的模型，例如电子表格。然而，从简单的模拟迁移到更详细的模拟通常需要多次执行来确定简单模型有效的区域，这可能比直接运行详细模型更昂贵。此外，架构师常常必须依靠直觉来选择这些简单的模型，这使得问题更加复杂。本研究提出了一种连接简单和详细模拟之间差距的方法，通过在线监控模拟行为并自动用更简单的统计近似替换详细模型。我们在开源模拟器 SVE-Cachesim 中实现了我们的方法，用于替换内存层次结构中的一级数据缓存 (L1D)。这个概念验证表明，我们的技术不仅可以处理局部时间不变统计数据的近似，还可以处理随时间变化的统计数据（例如，L1D 是一种时间序列函数），以及下游的副作用（例如，L1D 会过滤掉 L2 缓存的访问）。我们的模拟用近似缓存模型替换了内置缓存模型，在模拟周期计数方面仅产生 8% 的误差，并且在超过 90% 的模拟中使用了近似缓存模型，而我们更简单的模型每次“执行”模型所需的计算量减少了 2 到 8 倍。


<details>
  <summary>Details</summary>
Motivation: 连接简单和详细模拟之间的差距，解决因模拟时间过长而导致的迭代效率低下问题。

Method: 通过在线监控模拟行为，自动用更简单的统计近似替换详细模型。在 SVE-Cachesim 模拟器中实现了该方法，用于替换内存层次结构中的一级数据缓存 (L1D)。

Result: 在模拟周期计数方面仅产生 8% 的误差，并且在超过 90% 的模拟中使用了近似缓存模型，而更简单的模型每次“执行”模型所需的计算量减少了 2 到 8 倍。

Conclusion: 提出的技术可以有效地近似局部时间不变和随时间变化的统计数据，以及处理下游的副作用，从而在保持较高精度的同时显著提高模拟效率。

Abstract: As systems and applications grow more complex, detailed simulation takes an ever increasing amount of time. The prospect of increased simulation time resulting in slower design iteration forces architects to use simpler models, such as spreadsheets, when they want to iterate quickly on a design. However, the task of migrating from a simple simulation to one with more detail often requires multiple executions to find where simple models could be effective, which could be more expensive than running the detailed model in the first place. Also, architects must often rely on intuition to choose these simpler models, further complicating the problem.
  In this work, we present a method of bridging the gap between simple and detailed simulation by monitoring simulation behavior online and automatically swapping out detailed models with simpler statistical approximations. We demonstrate the potential of our methodology by implementing it in the open-source simulator SVE-Cachesim to swap out the level one data cache (L1D) within a memory hierarchy. This proof of concept demonstrates that our technique can handle a non-trivial use-case in not just approximation of local time-invariant statistics, but also those that vary with time (e.g., the L1D is a form of a time-series function), and downstream side-effects (e.g., the L1D filters accesses for the level two cache). Our simulation swaps out the built-in cache model with only an 8% error in the simulated cycle count while using the approximated cache models for over 90% of the simulation, and our simpler models require two to eight times less computation per "execution" of the model

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [53] [Pivoting the paradigm: the role of spreadsheets in K-12 data science](https://arxiv.org/abs/2506.03232)
*Oren Tirschwell,Nicholas Jon Horton*

Main category: stat.OT

TL;DR: 电子表格有助于培养K-12学生的数据和计算能力，本文回顾了相关框架，提出了数据驱动的学习成果，并讨论了其在课程中的应用。


<details>
  <summary>Details</summary>
Motivation: 电子表格工具易于K-12学生和教师使用，在数据收集、组织、探索和分析方面发挥着重要作用，有助于培养数据和计算技能。

Method: 1.回顾了关于K-12数据工具的现有框架；2.提出了通过将电子表格纳入课程可以实现的数据驱动的学习成果；3.讨论了电子表格如何帮助培养数据敏感性和计算流畅性。此外，还提供了课堂活动示例，明确了推广过程中存在的挑战和障碍，提出了用于降低师生学习曲线的教学方法，并讨论了促进电子表格在数据科学和STEM学科中更深入应用所需的专业发展。

Result: 本文提供了课堂活动示例，指出了电子表格在K-12教育中推广的挑战与障碍，并提出了相应的教学方法与专业发展建议。

Conclusion: 电子表格在K-12教育中具有潜力，可以促进数据科学和STEM学科的发展，但需要克服一些挑战，并加强教师培训。

Abstract: Spreadsheet tools are widely accessible to and commonly used by K-12 students and teachers. They have an important role in data collection and organization. Beyond data organization, spreadsheets also make data visible and easy to interact with, facilitating student engagement in data exploration and analysis. Though not suitable for all circumstances, spreadsheets can and do help foster data and computing skills for K-12 students. This paper 1) reviews prior frameworks on K-12 data tools; 2) proposes data-driven learning outcomes that can be accomplished by incorporating spreadsheets into the curriculum; and 3) discusses how spreadsheets can help develop data acumen and computational fluency. We provide example class activities, identify challenges and barriers to adoption, suggest pedagogical approaches to ease the learning curve for instructors and students, and discuss the need for professional development to facilitate deeper use of spreadsheets for data science and STEM disciplines.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [54] [Reducing Aleatoric and Epistemic Uncertainty through Multi-modal Data Acquisition](https://arxiv.org/abs/2501.18268)
*Arthur Hoarau,Benjamin Quost,Sébastien Destercke,Willem Waegeman*

Main category: cs.LG

TL;DR: 多模态数据中，通过增加数据模态数量可减少不确定性，通过增加观测数量可减少认知不确定性。


<details>
  <summary>Details</summary>
Motivation: 多模态数据为分离不确定性带来了新的机遇和挑战，传统的机器学习假设（数据越多，认知不确定性越少；固有不确定性无法减少）在多模态场景下受到质疑。

Method: 提出了一种新颖的数据采集框架，该框架允许从样本数量和数据模态两个方向进行采样，以实现不确定性的分离和可操作的决策。该框架结合了主动学习、主动特征采集和不确定性量化的思想。

Result: 通过在两个多模态数据集上的概念验证实现，展示了所提出的数据采集框架。

Conclusion: 不确定性分离可以通过增加数据模态数量来减少固有的不确定性，通过收集更多的观测来减少认知不确定性。所提出的框架能够实现可操作的决策。

Abstract: To generate accurate and reliable predictions, modern AI systems need to combine data from multiple modalities, such as text, images, audio, spreadsheets, and time series. Multi-modal data introduces new opportunities and challenges for disentangling uncertainty: it is commonly assumed in the machine learning community that epistemic uncertainty can be reduced by collecting more data, while aleatoric uncertainty is irreducible. However, this assumption is challenged in modern AI systems when information is obtained from different modalities. This paper introduces an innovative data acquisition framework where uncertainty disentanglement leads to actionable decisions, allowing sampling in two directions: sample size and data modality. The main hypothesis is that aleatoric uncertainty decreases as the number of modalities increases, while epistemic uncertainty decreases by collecting more observations. We provide proof-of-concept implementations on two multi-modal datasets to showcase our data acquisition framework, which combines ideas from active learning, active feature acquisition and uncertainty quantification.

</details>


### [55] [Large Scale Transfer Learning for Tabular Data via Language Modeling](https://arxiv.org/abs/2406.12031)
*Josh Gardner,Juan C. Perdomo,Ludwig Schmidt*

Main category: cs.LG

TL;DR: TabuLa-8B是一个用于表格预测的语言模型，在零样本和少样本场景下均优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在其他领域取得了成功，但其在表格数据领域的应用仍有待提高。

Method: 使用提取的2.1B行表格数据，结合新颖的打包和注意力机制，对Llama 3-8B模型进行微调。

Result: 在329个数据集的测试中，TabuLa-8B在零样本场景下的准确率比随机猜测高出15个百分点，在少样本场景下比XGBoost和TabPFN高出5-15个百分点。

Conclusion: TabuLa-8B在表格预测任务上展现出显著优势，为表格数据领域的基础模型应用开辟了新途径。

Abstract: Tabular data -- structured, heterogeneous, spreadsheet-style data with rows and columns -- is widely used in practice across many domains. However, while recent foundation models have reduced the need for developing task-specific datasets and predictors in domains such as language modeling and computer vision, this transfer learning paradigm has not had similar impact in the tabular domain. In this work, we seek to narrow this gap and present TabuLa-8B, a language model for tabular prediction. We define a process for extracting a large, high-quality training dataset from the TabLib corpus, proposing methods for tabular data filtering and quality control. Using the resulting dataset, which comprises over 2.1B rows from over 4M unique tables, we fine-tune a Llama 3-8B large language model (LLM) for tabular data prediction (classification and binned regression) using a novel packing and attention scheme for tabular prediction. Through evaluation across a test suite of 329 datasets, we find that TabuLa-8B has zero-shot accuracy on unseen tables that is over 15 percentage points (pp) higher than random guessing, a feat that is not possible with existing state-of-the-art tabular prediction models (e.g. XGBoost, TabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the target datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN models that are explicitly trained on equal, or even up to 16x more data. We release our model, code, and data along with the publication of this paper.

</details>


### [56] [Generating tabular datasets under differential privacy](https://arxiv.org/abs/2308.14784)
*Gianluca Truda*

Main category: cs.LG

TL;DR: 生成模型会记住并复述训练数据，这会破坏隐私目标。为了解决这个问题，研究人员将差分隐私（DP）的数学框架纳入深度神经网络的训练过程。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习（ML）在各个领域和行业中都在加速进步，但它依赖于可访问的高质量训练数据。然而，这些表格数据通常是敏感的。

Method: 本研究优化了生成模型的质量-隐私权衡，在相同的隐私保证下生成更高质量的表格数据集。我们实现了利用注意机制学习可逆表格表示的新型端到端模型。我们还引入了 TableDiffusion，这是第一个用于表格数据合成的差分私有扩散模型。

Result: 实验表明，TableDiffusion 能够生成更高保真度的合成数据集，避免了模型崩溃问题，并在私有表格数据合成方面取得了最先进的性能。通过实现 TableDiffusion 来预测添加的噪声，我们使其能够绕过重建混合类型表格数据的挑战。

Conclusion: 与对抗性范例相比，扩散范例被证明在数据和隐私方面效率更高，这得益于每个数据批次的增强重用和更平滑的迭代训练过程。

Abstract: Machine Learning (ML) is accelerating progress across fields and industries, but relies on accessible and high-quality training data. Some of the most important datasets are found in biomedical and financial domains in the form of spreadsheets and relational databases. But this tabular data is often sensitive in nature. Synthetic data generation offers the potential to unlock sensitive data, but generative models tend to memorise and regurgitate training data, which undermines the privacy goal. To remedy this, researchers have incorporated the mathematical framework of Differential Privacy (DP) into the training process of deep neural networks. But this creates a trade-off between the quality and privacy of the resulting data. Generative Adversarial Networks (GANs) are the dominant paradigm for synthesising tabular data under DP, but suffer from unstable adversarial training and mode collapse, which are exacerbated by the privacy constraints and challenging tabular data modality. This work optimises the quality-privacy trade-off of generative models, producing higher quality tabular datasets with the same privacy guarantees. We implement novel end-to-end models that leverage attention mechanisms to learn reversible tabular representations. We also introduce TableDiffusion, the first differentially-private diffusion model for tabular data synthesis. Our experiments show that TableDiffusion produces higher-fidelity synthetic datasets, avoids the mode collapse problem, and achieves state-of-the-art performance on privatised tabular data synthesis. By implementing TableDiffusion to predict the added noise, we enabled it to bypass the challenges of reconstructing mixed-type tabular data. Overall, the diffusion paradigm proves vastly more data and privacy efficient than the adversarial paradigm, due to augmented re-use of each data batch and a smoother iterative training process.

</details>


### [57] [Smart Metro: Deep Learning Approaches to Forecasting the MRT Line 3 Ridership](https://arxiv.org/abs/2304.07303)
*Jayrald Empino,Jean Allyson Junsay,Mary Grace Verzon,Mideth Abisado,Shekinah Lor Huyo-a,Gabriel Avelino Sampedro*

Main category: cs.LG

TL;DR: 该研究提出了一种时间序列预测方法，用于预测菲律宾马尼拉地铁3号线（MRT3）的每日客流量，以帮助乘客规划行程和交通部门进行数据分析。


<details>
  <summary>Details</summary>
Motivation: 由于MRT3每日客流量波动大且难以预测，给乘客规划行程和交通部门的数据分析带来挑战。

Method: 该研究提出了一种时间序列预测模型，用于预测MRT3特定车站特定日期的每日客流量。

Result: 该研究展示了如何使用时间序列预测来预测MRT3的每日客流量。

Conclusion: 时间序列预测可用于预测MRT3的每日客流量，从而帮助乘客规划行程和交通部门进行数据分析。

Abstract: Since its establishment in 1999, the Metro Rail Transit Line 3 (MRT3) has served as a transportation option for numerous passengers in Metro Manila, Philippines. The Philippine government's transportation department records more than a thousand people using the MRT3 daily and forecasting the daily passenger count may be rather challenging. The MRT3's daily ridership fluctuates owing to variables such as holidays, working days, and other unexpected issues. Commuters do not know how many other commuters are on their route on a given day, which may hinder their ability to plan an efficient itinerary. Currently, the DOTr depends on spreadsheets containing historical data, which might be challenging to examine. This study presents a time series prediction of daily traffic to anticipate future attendance at a particular station on specific days.

</details>


### [58] [Adversarial Networks and Machine Learning for File Classification](https://arxiv.org/abs/2301.11964)
*Ken St. Germain,Josh Angichiodo*

Main category: cs.LG

TL;DR: 使用对抗训练的机器学习神经网络来识别文件真实类型，即使文件扩展名或文件头被混淆。


<details>
  <summary>Details</summary>
Motivation: 在文件类型被隐藏的情况下，准确识别文件类型对于取证调查至关重要。

Method: 使用半监督生成对抗网络（SGAN）进行文件类型分类。

Result: SGAN 在 11 种不同类型的文件上实现了 97.6% 的准确率，并且在标记样本很少的情况下比其他机器学习算法更精确。

Conclusion: 使用 SGAN 实现的文件分类器在文件类型识别方面非常精确，尤其是在标记样本有限的情况下。

Abstract: Correctly identifying the type of file under examination is a critical part of a forensic investigation. The file type alone suggests the embedded content, such as a picture, video, manuscript, spreadsheet, etc. In cases where a system owner might desire to keep their files inaccessible or file type concealed, we propose using an adversarially-trained machine learning neural network to determine a file's true type even if the extension or file header is obfuscated to complicate its discovery. Our semi-supervised generative adversarial network (SGAN) achieved 97.6% accuracy in classifying files across 11 different types. We also compared our network against a traditional standalone neural network and three other machine learning algorithms. The adversarially-trained network proved to be the most precise file classifier especially in scenarios with few supervised samples available. Our implementation of a file classifier using an SGAN is implemented on GitHub (https://ksaintg.github.io/SGAN-File-Classier).

</details>


### [59] [TabularNet: A Neural Network Architecture for Understanding Semantic Structures of Tabular Data](https://arxiv.org/abs/2106.03096)
*Lun Du,Fei Gao,Xu Chen,Ran Jia,Junshan Wang,Jiang Zhang,Shi Han,Dongmei Zhang*

Main category: cs.LG

TL;DR: This paper proposes TabularNet, a novel neural network architecture that extracts both spatial and relational information from tables to understand their semantic structures, outperforming existing methods on classification tasks.


<details>
  <summary>Details</summary>
Motivation: Existing studies on mining tabular data typically use CNNs, which capture spatial information but overlook diverse relational information (hierarchical, paratactic) between cells. This paper aims to address this gap by simultaneously extracting both spatial and relational information.

Method: The proposed TabularNet architecture uses a spatial encoder (row/column-level Pooling and Bi-GRU) to capture statistical and local positional information, and a graph convolutional network (GCN) based encoder with a novel graph construction method using WordNet tree to capture hierarchical and paratactic relationships between cells.

Result: Extensive experiments on three classification tasks using two real-world spreadsheet datasets show that TabularNet is more effective than state-of-the-art baselines.

Conclusion: The proposed TabularNet, a unified neural backbone for different table understanding tasks, effectively extracts both spatial and relational information, demonstrating superior performance on classification tasks.

Abstract: Tabular data are ubiquitous for the widespread applications of tables and hence have attracted the attention of researchers to extract underlying information. One of the critical problems in mining tabular data is how to understand their inherent semantic structures automatically. Existing studies typically adopt Convolutional Neural Network (CNN) to model the spatial information of tabular structures yet ignore more diverse relational information between cells, such as the hierarchical and paratactic relationships. To simultaneously extract spatial and relational information from tables, we propose a novel neural network architecture, TabularNet. The spatial encoder of TabularNet utilizes the row/column-level Pooling and the Bidirectional Gated Recurrent Unit (Bi-GRU) to capture statistical information and local positional correlation, respectively. For relational information, we design a new graph construction method based on the WordNet tree and adopt a Graph Convolutional Network (GCN) based encoder that focuses on the hierarchical and paratactic relationships between cells. Our neural network architecture can be a unified neural backbone for different understanding tasks and utilized in a multitask scenario. We conduct extensive experiments on three classification tasks with two real-world spreadsheet data sets, and the results demonstrate the effectiveness of our proposed TabularNet over state-of-the-art baselines.

</details>


### [60] [Visual Backpropagation](https://arxiv.org/abs/1906.04011)
*Roy S. Freedman*

Main category: cs.LG

TL;DR: 使用电子表格实现了一个声明式函数式编程的 autodiff，称为 Visual Backpropagation。


<details>
  <summary>Details</summary>
Motivation: 将反向传播的实现可视化并使其透明化。

Method: 通过使用数组工作表函数、手动计算和类似 systolic array 的计算顺序来实现。

Result: 提供了一个 Visual Backpropagation 的电子表格实现，并将其与 Tensorflow 的实现进行比较。

Conclusion: Visual Backpropagation 是一种无需传统编程语言即可在电子表格中实现反向传播的方法。

Abstract: We show how a declarative functional programming specification of backpropagation yields a visual and transparent implementation within spreadsheets. We call our method Visual Backpropagation. This backpropagation implementation exploits array worksheet formulas, manual calculation, and has a sequential order of computation similar to the processing of a systolic array. The implementation uses no hidden macros nor user-defined functions; there are no loops, assignment statements, or links to any procedural programs written in conventional languages. As an illustration, we compare a Visual Backpropagation solution to a Tensorflow (Python) solution on a standard regression problem.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [61] [Improving Feedback from Automated Reviews of Student Spreadsheets](https://arxiv.org/abs/2311.10728)
*Sören Aguirre Reid,Frank Kammer,Jonas-Ian Kuche,Pia-Doreen Ritzke,Markus Siepermann,Max Stephan,Armin Wagenknecht*

Main category: cs.CY

TL;DR: 本研究开发了一个智能辅导系统（ITS），用于自动评估学生提交的Excel作业并提供个性化反馈，以解决当前教学环境中缺乏此类数字评估工具的问题。


<details>
  <summary>Details</summary>
Motivation: 现有教学环境中缺乏针对学生Excel作业的数字评估工具，因此需要开发一个能够自动审查学生Excel提交并提供个性化反馈的系统。

Method: 该系统通过值匹配、公式详细分析和解决方案质量评估等多种方式自动分析学生提交的Excel文件。为适应不同学习水平的学生，系统内置了不同的反馈级别，能够根据错误的严重程度逐步提供更多信息。

Result: 研究表明，更高水平的反馈能够显著提高学生作业的正确率，并且学生认为这种反馈清晰易懂且有帮助。

Conclusion: 本研究开发的智能辅导系统（ITS）能够有效评估学生的Excel作业，并通过分层反馈机制为学生提供个性化、可理解且有帮助的指导，从而提高学习效果。

Abstract: Spreadsheets are one of the most widely used tools for end users. As a result, spreadsheets such as Excel are now included in many curricula. However, digital solutions for assessing spreadsheet assignments are still scarce in the teaching context. Therefore, we have developed an Intelligent Tutoring System (ITS) to review students' Excel submissions and provide individualized feedback automatically. Although the lecturer only needs to provide one reference solution, the students' submissions are analyzed automatically in several ways: value matching, detailed analysis of the formulas, and quality assessment of the solution. To take the students' learning level into account, we have developed feedback levels for an ITS that provide gradually more information about the error by using one of the different analyses. Feedback at a higher level has been shown to lead to a higher percentage of correct submissions and was also perceived as well understandable and helpful by the students.

</details>


### [62] [How Beaufort, Neumann and Gates met? Subject integration with spreadsheeting](https://arxiv.org/abs/2309.12353)
*Maria Csernoch,Julia Csernoch*

Main category: cs.CY

TL;DR: 计算思维应作为继读写算之后的第四项基本技能。本研究提出一种新方法，在 Beaufort 温标框架内，通过将传统纸质问题及其数据检索过程数字化，来支持学科整合和数字图式构建。


<details>
  <summary>Details</summary>
Motivation: 计算思维，尤其是数字问题解决，需要更深入的理解和实践，以达到其应有的图式水平。

Method: 将 Beaufort 温标的传统纸质问题及其数据检索过程进行数字化，并在八年级行动研究中进行实践。

Result: 研究发现，与传统的教科书和非情境化的数字环境相比，学生们的内容知识和数字技能都得到了更有效的提升。

Conclusion: 本研究提出的数字化方法可以应用于任何可以从数字化环境中获益的纸质问题，并有助于构建学科内容和信息学方面的图式。

Abstract: Computational thinking should be the fourth fundamental skill, along with reading, writing, and arithmetic (3R). To reach the level where computational thinking skills, especially digital problem solving have their own schemata, there is a long way to go. In the present paper, a novel approach is detailed to support subject integration and building digital schemata, on the well-known Beaufort scale. The conversion of a traditional, paper-based problem and a data retrieval process are presented within the frame of a Grade 8 action research study. It is found that both students content knowledge and their digital skills developed more efficiently than in traditional course book and decontextualized digital environments. Furthermore, the method presented here can be adapted to any paper-based problems whose solutions would be more effective in a digital environment and which offer various forms for building schemata both in the subject matter and informatics.

</details>


### [63] [A Simpler Method for Understanding Emergency Shelter Access Patterns](https://arxiv.org/abs/2210.13619)
*Geoffrey G. Messier*

Main category: cs.CY

TL;DR: SAM是一种新的方法，用于表征应急避难所的可及性模式，作为衡量避难所客户脆弱性的指标。SAM的目标是为避难所运营商提供一种直观的方式来理解可及性模式，并且可以由非技术人员使用电子表格操作来实现。


<details>
  <summary>Details</summary>
Motivation: SAM的目标是为避难所运营商提供一种直观的方式来理解可及性模式，并且可以由非技术人员使用电子表格操作来实现。

Method: 使用来自北美大型避难所的客户数据来演示SAM的产生结果与传统的过渡性、偶发性和慢性客户聚类分析相似。

Result: SAM产生了与传统聚类分析相似的结果，并且由于所需数据较少，因此能够生成关于外部因素如何影响避难所可及性模式的实时图景。使用SAM对九年避难所客户数据生成的时间线证明了“住房优先”计划和COVID-19封锁对人们如何进入避难所的影响。

Conclusion: SAM允许避难所工作人员超越分配过渡性、偶发性和慢性标签，而是直接使用SAM的“软”输出来衡量脆弱性。

Abstract: The Simplified Access Metric (SAM) is a new approach for characterizing emergency shelter access patterns as a measure of shelter client vulnerability. The goal of SAM is to provide shelter operators with an intuitive way to understand access patterns that can be implemented by non-technical staff using spreadsheet operations. Client data from a large North American shelter will be used to demonstrate that SAM produces similar results to traditional transitional, episodic and chronic client cluster analysis. Since SAM requires less data than cluster analysis, it is also able to generate a real time picture of how shelter access patterns are affected by external factors. Timelines generated from nine years of shelter client data using SAM demonstrate the impact of Housing First programming and the COVID-19 lockdown on how people access shelter. Finally, SAM allows shelter staff to move beyond assigning transitional, episodic and chronic labels and instead use the "soft" output of SAM directly as a measure of vulnerability.

</details>


### [64] [Long-Term Mentoring for Computer Science Researchers](https://arxiv.org/abs/2208.04738)
*Emily Ruppel,Sihang Liu,Elba Garza,Sukyoung Ryu,Alexandra Silva,Talia Ringer*

Main category: cs.CY

TL;DR: PL和CA社区领导者发起了长期指导计划，以解决社区内建立人脉的障碍。SIGPLAN-M（PL）和CALM（CA）计划在扩大社区参与度和提供有益指导方面取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 解决在PL和CA学术社区中，新人建立长期人脉联系的障碍，即需要先有人脉才能建立人脉。

Method: CA社区的领导者们进行了循证论证，以支持全社区范围的长期指导。PL社区的领导者们非官方地启动了一个基于电子表格的长期指导计划，后来发展成为SIGPLAN-M。CALM计划是CA社区的对应计划。

Result: SIGPLAN-M覆盖了328名学员和234名导师，遍布41个国家，学员反馈积极，称其“改变人生”和“挽救职业生涯”。CALM计划目前处于试点阶段，有13名导师和21名学员，遍布7个国家，反馈良好。

Conclusion: SIGPLAN-M和CALM计划的领导者们分享了他们的经验，希望能促进整个计算机科学领域更广泛的长期指导计划。

Abstract: Early in the pandemic, we -- leaders in the research areas of programming languages (PL) and computer architecture (CA) -- realized that we had a problem: the only way to form new lasting connections in the community was to already have lasting connections in the community. Both of our academic communities had wonderful short-term mentoring programs to address this problem, but it was clear that we needed long-term mentoring programs.
  Those of us in CA approached this scientifically, making an evidence-backed case for community-wide long-term mentoring. In the meantime, one of us in PL had impulsively launched an unofficial long-term mentoring program, founded on chaos and spreadsheets. In January 2021, the latter grew to an official cross-institutional long-term mentoring program called SIGPLAN-M; in January 2022, the former grew to Computer Architecture Long-term Mentoring (CALM).
  The impacts have been strong: SIGPLAN-M reaches 328 mentees and 234 mentors across 41 countries, and mentees have described it as "life changing" and "a career saver." And while CALM is in its pilot phase -- with 13 mentors and 21 mentees across 7 countries -- it has received very positive feedback. The leaders of SIGPLAN-M and CALM shared our designs, impacts, and challenges along the way. Now, we wish to share those with you. We hope this will kick-start a larger long-term mentoring effort across all of computer science.

</details>


### [65] [Optimization Helps Scheduling Nursing Staff at the Long-Term Care Homes of the City of Toronto](https://arxiv.org/abs/2102.09461)
*Manion Anderson,Merve Bodur,Scott Rathwell,Vahid Sarhangian*

Main category: cs.CY

TL;DR: 多伦多长期护理机构使用基于层次优化模型的排班工具，实现了自动化排班，满足了员工偏好和资历要求，显著提高了排班效率。


<details>
  <summary>Details</summary>
Motivation: 为了应对多伦多长期护理机构日益严峻的护士排班挑战，特别是降低兼职护士的高缺勤率，并整合护士的轮班偏好。

Method: 开发了一个基于层次优化模型的电子表格排班工具，该模型能在满足复杂资历要求的前提下，生成满足最大可能需求且偏好得分最高的可用排班表。

Result: 该工具在多伦多一家391张床位的养老院实施后，将生成可用排班表的时间从手动方式的数小时缩短至数小时以内，并且平均有94%的排班轮次被评为最偏好轮次。

Conclusion: 该排班工具通过自动化和考虑员工偏好及资历要求，有效解决了长期护理机构的排班难题，提高了效率和员工满意度。

Abstract: The City of Toronto Long Term Care Homes & Services (LTCH&S) division is one of the largest providers of long-term care in the Canadian province of Ontario, providing care to 2,640 residents at 10 homes across Toronto. Our collaboration with LTCH&S was initiated to facilitate the increasingly challenging task of scheduling nursing staff and reduce high absenteeism rate observed among the part-time nurses. We developed a spreadsheet-based scheduling tool to automate the generation of schedules and incorporate nurses' preferences for different shifts into the schedules. At the core of the scheduling tool is a hierarchical optimization model that generates a feasible schedule with the highest total preference score while satisfying the maximum possible demand. Feasible schedules had to abide by a set of complex seniority requirements which prioritized more senior nurses when allocating the available shifts. Our scheduling tool was implemented in a 391-bed home in Toronto. The tool allowed nursing managers to generate feasible schedules within a fraction of an hour, in contrast to the status-quo manual approach which could took up to tens of hours. In addition, the schedules successfully accounted for preferences with on average above 94% of the allocated shifts ranked as most preferred.

</details>


### [66] [Developing Excel Thought Leadership](https://arxiv.org/abs/2006.04793)
*David Lyford-Smith*

Main category: cs.CY

TL;DR: ICAEW发布了三篇关于电子表格良好实践的“思想领导力”论文，回顾了这些论文的历史、关键教训以及它们如何帮助ICAEW在该领域建立其地位。


<details>
  <summary>Details</summary>
Motivation: ICAEW在五年内开发了一系列三篇关于电子表格良好实践和电子表格工作环境的“思想领导力”论文。

Method: 回顾了这三篇论文的历史，总结了每篇论文的关键教训，并讨论了论文的创作过程如何帮助ICAEW在该领域建立其地位。

Result: ICAEW在该领域建立了其地位。

Conclusion: 对ICAEW发布的关于电子表格良好实践的“思想领导力”论文的全面回顾和分析。

Abstract: Over a period of five years, the Institute of Chartered Accountants in England and Wales (ICAEW) has developed a suite of three 'thought leadership' papers surrounding good practice in spreadsheet use and spreadsheet work environments. We will review the history of these three papers, the key lessons which each has to teach, and discuss how the process of making them has helped ICAEW to develop its position in the field.

</details>


### [67] [A Case Study of Spreadsheet Use within the Finance and Academic Registry units within a Higher Education Institution](https://arxiv.org/abs/1909.07462)
*Simon Thorne,Jamie Hancock*

Main category: cs.CY

TL;DR: 本篇论文研究了英国一所高等教育机构的电子表格使用情况，重点关注了学术注册和财务部门。研究内容包括电子表格的重要性、培训、经验、用途、所用技术、创建的电子表格大小以及共享情况。研究结果表明，该机构创建和使用了大量的电子表格，开发者的画像与其他研究中的情况相似。为了确保数据完整性、减少重复工作并优化电子表格的使用以实现机构目标，该机构需要制定明确的电子表格模型开发原则和指南。


<details>
  <summary>Details</summary>
Motivation: 探讨了电子表格在英国高等教育机构（学术注册和财务部门）中的使用情况，分析了其重要性、培训、经验、用途、技术、大小和共享等方面。

Method: 进行了案例研究，收集和分析了学术注册和财务部门的电子表格使用数据。

Result: 该机构创建和使用了大量的电子表格，开发者的画像与其他研究中的情况相似。

Conclusion: 为了确保数据完整性、减少重复工作并优化电子表格的使用以实现机构目标，该机构需要制定明确的电子表格模型开发原则和指南。

Abstract: This paper presents the findings of a case study of spreadsheet use in a higher education institution in the UK. The paper considers the use of spreadsheets in two units of the organisation, academic registry and finance. Spreadsheet use is explored in terms of importance, training, experience, purpose, techniques deployed, size of spreadsheets created and sharing of spreadsheets. The implications of the results are then considered in terms of accurate reporting to external funding bodies such the funding councils, internal data integrity and internal data efficiencies. The results show a large volume of spreadsheets being created and used, that the profile of spreadsheet developers is typical of other studies of spreadsheet use and the need for the organisation to have clear principles and guidelines for the development of spreadsheet models in the organisation to ensure data integrity, reduce duplication of effort and to optimise the use of spreadsheets to meet the institutions goals.

</details>


### [68] [Computer Simulation of Neural Networks Using Spreadsheets: The Dawn of the Age of Camelot](https://arxiv.org/abs/1807.00018)
*Serhiy O. Semerikov,Illia O. Teplytskyi,Yuliia V. Yechkalo,Arnold E. Kiv*

Main category: cs.CY

TL;DR: 本文研究了在电子表格环境中模拟神经网络的训练方法，并对其进行了系统回顾。研究区分了几种基本方法，包括单独使用电子表格和神经网络模拟工具、使用第三方插件、开发宏、使用标准插件进行非线性优化、以及在不使用插件和宏的情况下创建神经网络。


<details>
  <summary>Details</summary>
Motivation: 论证了在电子表格环境中开发训练人工神经网络的计算机模拟方法的必要性。

Method: 对现有文献进行了系统性回顾，区分了多种解决神经网络计算机模拟训练问题的方法，并分析了McCulloch-Pitts、Householder和Landahl等模型。

Result: 确定了Rashevsky、McCulloch-Pitts、Householder和Landahl等模型在开发相关方法方面的潜力，并指出了掌握基于历史和遗传方法的模型对于获得电子表格环境中的神经网络模拟能力的重要性。

Conclusion: 在电子表格环境中掌握神经网络模拟能力的必要模型是连续的、离散的和连续-离散的模型。

Abstract: The article substantiates the necessity to develop training methods of computer simulation of neural networks in the spreadsheet environment. The systematic review of their application to simulating artificial neural networks is performed. The authors distinguish basic approaches to solving the problem of network computer simulation training in the spreadsheet environment, joint application of spreadsheets and tools of neural network simulation, application of third-party add-ins to spreadsheets, development of macros using the embedded languages of spreadsheets; use of standard spreadsheet add-ins for non-linear optimization, creation of neural networks in the spreadsheet environment without add-ins and macros. After analyzing a collection of writings of 1890-1950, the research determines the role of the scientific journal "Bulletin of Mathematical Biophysics", its founder Nicolas Rashevsky and the scientific community around the journal in creating and developing models and methods of computational neuroscience. There are identified psychophysical basics of creating neural networks, mathematical foundations of neural computing and methods of neuroengineering (image recognition, in particular). The role of Walter Pitts in combining the descriptive and quantitative theories of training is discussed. It is shown that to acquire neural simulation competences in the spreadsheet environment, one should master the models based on the historical and genetic approach. It is indicated that there are three groups of models, which are promising in terms of developing corresponding methods - the continuous two-factor model of Rashevsky, the discrete model of McCulloch and Pitts, and the discrete-continuous models of Householder and Landahl.

</details>


### [69] [Edu-Edition Spreadsheet Competency Framework](https://arxiv.org/abs/1802.00496)
*Maria Csernoch,Piroska Biró*

Main category: cs.CY

TL;DR: 该论文提出了电子表格能力框架的教育版（E2SCF），强调在教育早期通过专家教师的指导，利用高数学能力的计算机辅助现实世界问题解决来培养学生的电子表格能力，注重双向知识转移、数据和错误分析以及编程方面，旨在为基础用户打下坚实的电子表格知识基础，并培养可转移的问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 应在教育早期开始培养电子表格能力，并在专家教师的支持下进行，以提高效率。

Method: 提出电子表格能力框架的教育版（E2SCF），其主要特点是高数学能力的计算机辅助现实世界问题解决，从一开始就进行双向知识转移、数据和错误分析，以及电子表格的编程方面。

Result: E2SCF 旨在为基础用户和普通用户打下坚实的电子表格知识基础，并培养可转移的问题解决技能和能力。

Conclusion: E2SCF 是一种有效的方法，可以在教育早期通过专家教师的指导，利用高数学能力的计算机辅助现实世界问题解决来培养学生的电子表格能力。

Abstract: Based on the Spreadsheet Competency Framework for finance professionals, in the present paper we introduce the Edu-Edition of the Spreadsheet Competency Framework (E2SCF). We claim that building spreadsheet competences should start in education, as early as possible, and this process is a lot more effective if support arrives from expert teachers. The main feature of E2SCF is high mathability computer-supported real world problem solving. This approach is based on - from the very beginning of training - a two-directional knowledge transfer, data and error analysis and handling, and the programming aspect of spreadsheets. Based on these features, E2SCF is set up for basic and general users to build up firm spreadsheet knowledge and to develop transferable problem solving skills and competences.

</details>


### [70] [The Future of Spreadsheets in the Big Data Era](https://arxiv.org/abs/1801.10231)
*David Birch,David Lyford-Smith,Yike Guo*

Main category: cs.CY

TL;DR: 电子表格是一种广泛使用的数据工具，尽管已存在三十年但基本保持不变。大数据、终端用户计算和移动计算的兴起将推动其发展。


<details>
  <summary>Details</summary>
Motivation: 探讨电子表格技术的未来方向，以及它对用户的潜在影响。

Method: 召集学术界和产业界的专家举行研讨会，探讨电子表格的成功原因、驱动变革的趋势以及未来的发展方向，并为后续研究提出方向。

Result: 与会者就电子表格的成功原因、驱动变革的趋势以及未来可能的发展方向发表了看法。

Conclusion: 分析了大数据、终端用户计算和移动计算对电子表格技术的影响，并指出了对终端用户的潜在影响，这些用户是电子表格取得巨大成功的根本原因。

Abstract: The humble spreadsheet is the most widely used data storage, manipulation and modelling tool. Its ubiquity over the past 30 years has seen its successful application in every area of life. Surprisingly the spreadsheet has remained fundamentally unchanged over the past three decades. As spreadsheet technology enters its 4th decade a number of drivers of change are beginning to impact upon the spreadsheet. The rise of Big Data, increased end-user computing and mobile computing will undoubtedly increasingly shape the evolution and use of spreadsheet technology.
  To explore the future of spreadsheet technology a workshop was convened with the aim of "bringing together academia and industry to examine the future direction of spreadsheet technology and the consequences for users". This paper records the views of the participants on the reasons for the success of the spreadsheet, the trends driving change and the likely directions of change for the spreadsheet. We then set out key directions for further research in the evolution and use of spreadsheets. Finally we look at the implications of these trends for the end users who after all are the reason for the remarkable success of the spreadsheet.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [71] [Using a Catenary Trajectory to Reduce Wellbore Friction in Horizontal Extended Reach Drilling](https://arxiv.org/abs/2309.12317)
*Vu Nguyen*

Main category: cs.RO

TL;DR: 钻井时井筒摩擦是主要成本因素，本文引入了猫眼线概念以减少摩擦，并通过案例研究比较了猫眼线轨迹设计和传统的二维圆弧设计。


<details>
  <summary>Details</summary>
Motivation: 钻井中的井筒摩擦是导致成本增加的主要因素之一，而猫眼线概念可以有效缓解此问题，但需要详细的分析来支持其应用。

Method: 通过案例研究，比较了猫眼线轨迹设计和传统的二维圆弧设计在减少井筒摩擦方面的效果。计算方法已在Excel表格中提供。

Result: 案例研究结果表明，与传统的二维圆弧设计相比，猫眼线轨迹设计在减少钻井中的扭矩和阻力方面具有潜力。

Conclusion: 猫眼线轨迹设计是一种有前景的方法，可以减少钻井中的井筒摩擦，但需要进一步的分析和验证。所提供的Excel表格可用于设计延伸井轨迹。

Abstract: Wellbore friction is one of the biggest concerns when drilling due to its relation to the total cost. The catenary concept was introduced to reduce wellbore friction, but it requires detailed analyses. This project would fill this gap. A catenary shape is simply the natural shape of a rope, chain, or drill string. The drill string will then hang freely inside the wellbore. Perfectly, there should be no contact between the hole and the string, and thus no friction. Torque and drag should be minimized this way. A case study is introduced to examine the outcome between Catenary Trajectory Design and traditional 2D Arc design. The calculation procedure of Catenary Trajectory and 2D Arc Design can be found in an MS Excel spreadsheet which is easy to use and reliable for designing catenary well trajectories for extended-reach wells.

</details>


### [72] [Hazard Analysis of Collaborative Automation Systems: A Two-layer Approach based on Supervisory Control and Simulation](https://arxiv.org/abs/2209.12560)
*Tom P. Huck,Yuvaraj Selvaraj,Constantin Cronrath,Christoph Ledermann,Martin Fabian,Bengt Lennartson,Torsten Kröger*

Main category: cs.RO

TL;DR: 本论文提出了一种结合形式化方法和仿真的两层方法，用于对安全关键系统进行危害分析，以克服当前基于人类推理和简单工具方法的局限性，并解决了测试方法的高成本或危险性问题。


<details>
  <summary>Details</summary>
Motivation: 当前的安全关键系统危害分析方法（如人类推理、经验、检查表、电子表格）因系统复杂性增加而逐渐不适用，而基于测试的方法成本高昂或存在危险。因此，需要一种更有效的危害分析方法，模型驱动的危害分析（包括形式化模型和仿真模型）是一个有前景的方向。

Method: 提出一种两层危害分析方法：第一层使用形式化方法（基于监控器控制理论）合成导致不安全状态的不安全行为；第二层将第一层的结果输入仿真，并使用特定领域的风险度量进行详细分析。

Result: 该方法在工业人机协作系统上进行了验证，展示了其结合形式化方法（详尽分析）和仿真（详细分析）的优势。

Conclusion: 所提出的两层危害分析方法能够结合形式化方法和仿真的优点，对安全关键系统进行有效的危害分析，并在工业人机协作系统上得到了验证。

Abstract: Safety critical systems are typically subjected to hazard analysis before commissioning to identify and analyse potentially hazardous system states that may arise during operation. Currently, hazard analysis is mainly based on human reasoning, past experiences, and simple tools such as checklists and spreadsheets. Increasing system complexity makes such approaches decreasingly suitable. Furthermore, testing-based hazard analysis is often not suitable due to high costs or dangers of physical faults. A remedy for this are model-based hazard analysis methods, which either rely on formal models or on simulation models, each with their own benefits and drawbacks. This paper proposes a two-layer approach that combines the benefits of exhaustive analysis using formal methods with detailed analysis using simulation. Unsafe behaviours that lead to unsafe states are first synthesised from a formal model of the system using Supervisory Control Theory. The result is then input to the simulation where detailed analyses using domain-specific risk metrics are performed. Though the presented approach is generally applicable, this paper demonstrates the benefits of the approach on an industrial human-robot collaboration system.

</details>


<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [73] [Real-time stock analysis for blending recipes in industrial plants](https://arxiv.org/abs/1909.02960)
*Florin Zamfir,Nicolae Paraschiv,Emil Pricop*

Main category: cs.OH

TL;DR: 该论文介绍了一个实时库存分析应用程序，用于管理和优化混合生产过程中的产品库存。


<details>
  <summary>Details</summary>
Motivation: Excel表格在管理库存和计算过程数据时存在难于理解、追踪困难以及易被意外修改或删除公式的风险。因此，开发一个集中的应用程序来辅助操作员决策是必要的。

Method: 开发了一个使用C#编写的应用程序，该应用程序集成了规划算法，能够实时读取生产过程数据。算法目标包括：确定混合过程所需的原料、根据现有原料确定可生产的成品量、以及确定最佳成品产量。

Result: 该应用程序能够以集中化的方式管理库存数据，并通过集成算法向操作员提供决策支持，易于指挥室操作员使用，并允许用户逐步构建结果。

Conclusion: 所提出的应用程序和算法能够有效解决传统Excel表格管理库存的问题，提高混合生产过程的效率和准确性。

Abstract: Many companies use Excel spreadsheets to keep stock records and to calculate process-specific data. These spreadsheets are often hard to understand and track. And if the user does not protect them, there is a risk that the user randomly changes or erase formulas. The paper focuses on the stocks of products used in a blending process with a known recipe. Developing an application that can bring this data in a centralized form and that can assist the operator in decide is a necessity. When a programmer implements an application that uses data from plants he needs to consider one fundamental aspect as reading real-time data from the process. The real-time stock analysis application takes into account all the above elements. The application is easy to use by an operator in the command room of installation because of the planning algorithms integrated into it. The algorithms proposed and implemented in this paper have well-defined goals: identifying the ingredients needed to achieve the blending process for required quantities, determine the quantities of the finished product that can be made with the existing ingredients and determine the optimum quantities of the finished product. The application implemented in C# intensively uses these algorithms and gives the user the ability to build the result step by step.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [74] [Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models](https://arxiv.org/abs/2505.23667)
*Lang Cao,Jingxian Xu,Hanbing Liu,Jinyu Wang,Mengyu Zhou,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 通过使用可执行的表格公式进行微调，增强大型语言模型在表格数据上的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的表格理解方法在处理复杂数值或符号推理时存在困难，而表格公式蕴含的丰富推理模式未被充分利用。

Method: 提出Formula Tuning（Fortune）框架，这是一种强化学习方法，利用二进制答案的正确性作为奖励信号，训练大型语言模型生成可执行的表格公式来回答表格数据问题。

Result: Formula Tuning显著提升了大型语言模型在表格理解任务上的表现，特别是在多步数值和符号推理方面，使得一个7B模型在表格理解能力上超越了OpenAI o1。

Conclusion: 公式驱动的强化学习有潜力推动大型语言模型在符号表格推理方面的发展。

Abstract: Tables are a fundamental structure for organizing and analyzing data, making effective table understanding a critical capability for intelligent systems. While large language models (LMs) demonstrate strong general reasoning abilities, they continue to struggle with accurate numerical or symbolic reasoning over tabular data, especially in complex scenarios. Spreadsheet formulas provide a powerful and expressive medium for representing executable symbolic operations, encoding rich reasoning patterns that remain largely underutilized. In this paper, we propose Formula Tuning (Fortune), a reinforcement learning (RL) framework that trains LMs to generate executable spreadsheet formulas for question answering over general tabular data. Formula Tuning reduces the reliance on supervised formula annotations by using binary answer correctness as a reward signal, guiding the model to learn formula derivation through reasoning. We provide a theoretical analysis of its advantages and demonstrate its effectiveness through extensive experiments on seven table reasoning benchmarks. Formula Tuning substantially enhances LM performance, particularly on multi-step numerical and symbolic reasoning tasks, enabling a 7B model to outperform OpenAI o1 on table understanding. This highlights the potential of formula-driven RL to advance symbolic table reasoning in LMs.

</details>


### [75] [The AI Risk Repository: A Comprehensive Meta-Review, Database, and Taxonomy of Risks From Artificial Intelligence](https://arxiv.org/abs/2408.12622)
*Peter Slattery,Alexander K. Saeri,Emily A. C. Grundy,Jess Graham,Michael Noetel,Risto Uuk,James Dao,Soroush Pour,Stephen Casper,Neil Thompson*

Main category: cs.AI

TL;DR: 本篇论文建立了一个包含777个AI风险的数据库，旨在统一对AI风险的理解，以促进相关讨论、研究和应对。该数据库通过系统性文献综述和专家咨询，结合因果关系和AI风险领域进行分类，并提供在线访问和更新功能。


<details>
  <summary>Details</summary>
Motivation: 当前学术界、审计界、政策制定者、AI公司和公众对AI风险表示担忧，但缺乏统一的理解阻碍了对其进行全面讨论、研究和应对。本文旨在解决这一差距。

Method: 通过系统性地回顾现有的AI风险分类方法，并结合专家咨询，构建了一个AI风险存储库。该存储库提取了43个分类中的777个风险，并使用“最佳契合框架综合”方法开发了两个高级分类：因果分类（实体、意图、时间）和领域分类（涵盖歧视、隐私、错误信息、恶意使用、人机交互、社会经济环境、系统安全等七个领域）。

Result: 创建了一个名为“AI风险存储库”的在线数据库，收录了777个AI风险，这些风险依据因果关系和七个风险领域进行了分类。该存储库可通过网站和在线电子表格访问、修改和更新。

Conclusion: 该AI风险存储库是首个旨在全面梳理、分析和提取AI风险框架的公共数据库，为更协调、连贯和完整的AI风险定义、审计和管理奠定了基础。

Abstract: The risks posed by Artificial Intelligence (AI) are of considerable concern to academics, auditors, policymakers, AI companies, and the public. However, a lack of shared understanding of AI risks can impede our ability to comprehensively discuss, research, and react to them. This paper addresses this gap by creating an AI Risk Repository to serve as a common frame of reference. This comprises a living database of 777 risks extracted from 43 taxonomies, which can be filtered based on two overarching taxonomies and easily accessed, modified, and updated via our website and online spreadsheets. We construct our Repository with a systematic review of taxonomies and other structured classifications of AI risk followed by an expert consultation. We develop our taxonomies of AI risk using a best-fit framework synthesis. Our high-level Causal Taxonomy of AI Risks classifies each risk by its causal factors (1) Entity: Human, AI; (2) Intentionality: Intentional, Unintentional; and (3) Timing: Pre-deployment; Post-deployment. Our mid-level Domain Taxonomy of AI Risks classifies risks into seven AI risk domains: (1) Discrimination & toxicity, (2) Privacy & security, (3) Misinformation, (4) Malicious actors & misuse, (5) Human-computer interaction, (6) Socioeconomic & environmental, and (7) AI system safety, failures, & limitations. These are further divided into 23 subdomains. The AI Risk Repository is, to our knowledge, the first attempt to rigorously curate, analyze, and extract AI risk frameworks into a publicly accessible, comprehensive, extensible, and categorized risk database. This creates a foundation for a more coordinated, coherent, and complete approach to defining, auditing, and managing the risks posed by AI systems.

</details>


### [76] [SpreadsheetLLM: Encoding Spreadsheets for Large Language Models](https://arxiv.org/abs/2407.09025)
*Haoyu Dong,Jianbo Zhao,Yuzhang Tian,Junyu Xiong,Shiyu Xia,Mengyu Zhou,Yun Lin,José Cambronero,Yeye He,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: SpreadsheetLLM是一种用于处理电子表格的LLM，它使用SheetCompressor进行压缩，并在表格检测、QA等任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 电子表格的复杂性给LLM带来了挑战，需要专门的方法来处理。

Method: 提出了一种名为SheetCompressor的编码框架，包含结构锚定压缩、逆索引翻译和数据格式感知聚合三个模块。

Result: SheetCompressor在电子表格表格检测任务上比原始方法提高了25.6%。经过微调的LLM实现了25倍的平均压缩率和78.9%的F1分数，优于现有模型12.3%。

Conclusion: SpreadsheetLLM通过其创新的编码方法和下游任务的验证，证明了其在各种电子表格任务上的有效性。

Abstract: Spreadsheets are characterized by their extensive two-dimensional grids, flexible layouts, and varied formatting options, which pose significant challenges for large language models (LLMs). In response, we introduce SpreadsheetLLM, pioneering an efficient encoding method designed to unleash and optimize LLMs' powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs' token constraints, making it impractical for most applications. To tackle this challenge, we develop SheetCompressor, an innovative encoding framework that compresses spreadsheets effectively for LLMs. It comprises three modules: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in the spreadsheet table detection task, outperforming the vanilla approach by 25.6% in GPT4's in-context learning setting. Moreover, fine-tuned LLM with SheetCompressor has an average compression ratio of 25 times, and achieves a state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%. Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet understanding and validate it in a new and demanding spreadsheet QA task. We methodically leverage the inherent layout and structure of spreadsheets, demonstrating that SpreadsheetLLM is highly effective across a variety of spreadsheet tasks.

</details>


### [77] [SheetAgent: Towards A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models](https://arxiv.org/abs/2403.03636)
*Yibin Chen,Yifu Yuan,Zeyu Zhang,Yan Zheng,Jinyi Liu,Fei Ni,Jianye Hao,Hangyu Mao,Fuzheng Zhang*

Main category: cs.AI

TL;DR: SheetRM是一个包含长时限、多类别、需要推理的电子表格操作任务的基准测试，SheetAgent是一个利用LLM的新型自主代理，通过迭代任务推理和反思来解决这些问题，并在实验中展示了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）在处理复杂的、现实的、需要多步推理和处理模糊需求的任务时，在自动电子表格操作方面存在不足。

Method: 提出SheetRM基准测试，包含长时限、多类别、需要推理的电子表格操作任务；提出SheetAgent自主代理，包含规划、信息提供和检索三个协作模块，通过迭代任务推理和反思实现自动化操作。

Result: SheetAgent在多个基准测试上将通过率提高了20%--40%，提高了电子表格操作的精度，并展示了更强的表格推理能力。

Conclusion: SheetAgent在处理复杂的、现实的电子表格操作任务方面，比现有方法有显著优势，能够进行高级推理和精确操作。

Abstract: Spreadsheets are ubiquitous across the World Wide Web, playing a critical role in enhancing work efficiency across various domains. Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce SheetRM, a benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges. To mitigate the above challenges, we further propose SheetAgent, a novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of three collaborative modules: Planner, Informer, and Retriever, achieving both advanced reasoning and accurate manipulation over spreadsheets without human interaction through iterative task reasoning and reflection. Extensive experiments demonstrate that SheetAgent delivers 20--40\% pass rate improvements on multiple benchmarks over baselines, achieving enhanced precision in spreadsheet manipulation and demonstrating superior table reasoning abilities. More details and visualizations are available at the project website: https://sheetagent.github.io/. The datasets and source code are available at https://anonymous.4open.science/r/SheetAgent.

</details>


### [78] [Large Language Model for Table Processing: A Survey](https://arxiv.org/abs/2402.05121)
*Weizheng Lu,Jing Zhang,Ju Fan,Zihao Fu,Yueguo Chen,Xiaoyong Du*

Main category: cs.AI

TL;DR: 本文全面概述了与表格相关的任务，涵盖了用户场景、技术方面、训练技术和挑战，重点关注大型语言模型（LLMs）和视觉语言模型（VLMs）在自动化表格处理方面的应用。


<details>
  <summary>Details</summary>
Motivation: 自动化处理表格数据（如数据库查询、电子表格操作、网页表格问答和图像表格信息提取）具有显著的公共利益，引起了学术界和工业界的广泛兴趣。

Method: 本研究对表格相关任务进行了全面的概述，涵盖了传统任务（如表格问答）和新兴领域（如电子表格操作和表格数据分析）。此外，还总结了用于表格处理的LLMs和VLMs的训练技术，并讨论了针对各种表格相关任务的提示工程（特别是LLM驱动的代理）。

Result: 对表格问答、电子表格操作、表格数据分析等任务进行了总结，并讨论了LLMs和VLMs的训练技术以及提示工程的应用。

Conclusion: 尽管在表格处理方面取得了进展，但仍存在一些挑战，包括服务多样化的用户输入以及使用链式思考（chain-of-thought）导致的推理速度缓慢。

Abstract: Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, web table question answering, and image table information extraction. Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides a comprehensive overview of table-related tasks, examining both user scenarios and technical aspects. It covers traditional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processing. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought.

</details>


### [79] [FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language](https://arxiv.org/abs/2310.17306)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Elnaz Nouri,Mohammad Raza,Gust Verbruggen*

Main category: cs.AI

TL;DR: FormaT5是一个基于Transformer的模型，可以通过自然语言描述自动生成表格的条件格式化规则，解决了用户编写规则的困难。它通过预测占位符来处理模糊或不完整的用户描述，并能通过其他模型或编程示例来填充这些占位符，从而提高了规则生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 为用户提供一种自动生成数据驱动的条件格式化规则的方法，解决用户在电子表格软件中编写这些规则时遇到的理解和实现逻辑的困难。

Method: 开发了一个名为FormaT5的基于Transformer的模型，该模型能够根据目标表格和自然语言描述生成条件格式化规则。为了解决用户描述中存在的不足或歧义问题，FormaT5学习预测占位符，并通过弃权目标来最小化参数错误。这些占位符随后可以由第二个模型或编程示例系统填充。

Result: FormaT5通过弃权和填充机制，在包含1053个条件格式化任务的基准测试中，优于8种不同的神经方法，无论是否提供示例。这表明了构建特定领域学习系统的价值。

Conclusion: FormaT5通过引入占位符预测和弃权目标，有效解决了在生成条件格式化规则时用户描述不足或模糊的问题，并在基准测试中取得了优于现有方法的性能，证明了其在特定领域学习系统中的价值。

Abstract: Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires them to understand and implement the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system. To evaluate FormaT5 on diverse and real scenarios, we create an extensive benchmark of 1053 CF tasks, containing real-world descriptions collected from four different sources. We release our benchmarks to encourage research in this area. Abstention and filling allow FormaT5 to outperform 8 different neural approaches on our benchmarks, both with and without examples. Our results illustrate the value of building domain-specific learning systems.

</details>


### [80] [Don't Treat the Symptom, Find the Cause! Efficient Artificial-Intelligence Methods for (Interactive) Debugging](https://arxiv.org/abs/2306.12850)
*Patrick Rodler*

Main category: cs.AI

TL;DR: 模型驱动诊断是一种通用的故障排除方法，用于解决复杂系统中的故障。


<details>
  <summary>Details</summary>
Motivation: 现代社会高度依赖日益复杂的系统，系统故障可能导致严重后果，因此最小化故障影响至关重要。

Method: 模型驱动诊断利用知识表示、自动推理、启发式搜索、机器学习等多种技术来检测、定位和修复故障。

Result: 论文将介绍模型驱动诊断，指出该领域的挑战，并讨论相关的研究方法。

Conclusion: 模型驱动诊断是应对复杂系统故障的关键技术。

Abstract: In the modern world, we are permanently using, leveraging, interacting with, and relying upon systems of ever higher sophistication, ranging from our cars, recommender systems in e-commerce, and networks when we go online, to integrated circuits when using our PCs and smartphones, the power grid to ensure our energy supply, security-critical software when accessing our bank accounts, and spreadsheets for financial planning and decision making. The complexity of these systems coupled with our high dependency on them implies both a non-negligible likelihood of system failures, and a high potential that such failures have significant negative effects on our everyday life. For that reason, it is a vital requirement to keep the harm of emerging failures to a minimum, which means minimizing the system downtime as well as the cost of system repair. This is where model-based diagnosis comes into play.
  Model-based diagnosis is a principled, domain-independent approach that can be generally applied to troubleshoot systems of a wide variety of types, including all the ones mentioned above, and many more. It exploits and orchestrates i.a. techniques for knowledge representation, automated reasoning, heuristic problem solving, intelligent search, optimization, stochastics, statistics, decision making under uncertainty, machine learning, as well as calculus, combinatorics and set theory to detect, localize, and fix faults in abnormally behaving systems.
  In this thesis, we will give an introduction to the topic of model-based diagnosis, point out the major challenges in the field, and discuss a selection of approaches from our research addressing these issues.

</details>


### [81] [CORNET: Learning Table Formatting Rules By Example](https://arxiv.org/abs/2208.06032)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Mohammad Raza,Gust Verbruggen*

Main category: cs.AI

TL;DR: CORNET是一个从用户示例中自动学习表格格式化规则的系统。


<details>
  <summary>Details</summary>
Motivation: 用户编写电子表格格式化规则存在挑战，需要了解底层规则语言和数据逻辑。

Method: CORNET结合了归纳编程、符号规则枚举和神经网络排序器来学习条件格式化规则。

Result: CORNET在各种评估设置中都能准确地学习规则，并且找到的比用户编写的更简洁的规则，还能发现用户手动格式化的电子表格中的规则。

Conclusion: CORNET成功地解决了自动学习条件格式化规则的问题，并且在效率和准确性上优于现有方法。

Abstract: Spreadsheets are widely used for table manipulation and presentation. Stylistic formatting of these tables is an important property for both presentation and analysis. As a result, popular spreadsheet software, such as Excel, supports automatically formatting tables based on rules. Unfortunately, writing such formatting rules can be challenging for users as it requires knowledge of the underlying rule language and data logic. We present CORNET, a system that tackles the novel problem of automatically learning such formatting rules from user examples in the form of formatted cells. CORNET takes inspiration from advances in inductive programming and combines symbolic rule enumeration with a neural ranker to learn conditional formatting rules. To motivate and evaluate our approach, we extracted tables with over 450K unique formatting rules from a corpus of over 1.8M real worksheets. Since we are the first to introduce conditional formatting, we compare CORNET to a wide range of symbolic and neural baselines adapted from related domains. Our results show that CORNET accurately learns rules across varying evaluation setups. Additionally, we show that CORNET finds shorter rules than those that a user has written and discovers rules in spreadsheets that users have manually formatted.

</details>


### [82] [Named Entity Recognition in Industrial Tables using Tabular Language Models](https://arxiv.org/abs/2209.14812)
*Aneta Koleva,Martin Ringsquandl,Mark Buckley,Rakebul Hasan,Volker Tresp*

Main category: cs.AI

TL;DR: 面向工业界表格NER任务的表结构增强方法


<details>
  <summary>Details</summary>
Motivation: 现有面向表格数据的Transformer模型主要应用于学术研究，在工业界应用尚有缺失，本篇论文旨在探索Transformer模型在处理工业界表格结构化电子表格中的命名实体识别（NER）任务中的应用。

Method: 提出了一种基于知识图谱的表结构数据增强策略，以应对电子表格的复杂性和标注数据稀疏的挑战。同时，对比了将表格视为二维结构与线性序列两种方式对模型性能的影响。

Result: 所提出的表结构数据增强策略显著提升了模型在低资源场景下的性能。实验证明，相比其他基线模型，表格Transformer模型效果更优，且其表结构归纳偏置对于Transformer模型收敛至关重要。

Conclusion: Transformer模型在处理工业界表格NER任务中具有潜力，通过结合表结构增强策略和利用表格的归纳偏置，可以有效克服数据稀疏和模型收敛的挑战。

Abstract: Specialized transformer-based models for encoding tabular data have gained interest in academia. Although tabular data is omnipresent in industry, applications of table transformers are still missing. In this paper, we study how these models can be applied to an industrial Named Entity Recognition (NER) problem where the entities are mentioned in tabular-structured spreadsheets. The highly technical nature of spreadsheets as well as the lack of labeled data present major challenges for fine-tuning transformer-based models. Therefore, we develop a dedicated table data augmentation strategy based on available domain-specific knowledge graphs. We show that this boosts performance in our low-resource scenario considerably. Further, we investigate the benefits of tabular structure as inductive bias compared to tables as linearized sequences. Our experiments confirm that a table transformer outperforms other baselines and that its tabular inductive bias is vital for convergence of transformer-based models.

</details>


### [83] [Spreadsheet computing with Finite Domain Constraint Enhancements](https://arxiv.org/abs/2203.10944)
*Ezana N. Beyenne*

Main category: cs.AI

TL;DR: 该论文提出了一种将有限约束求解器与电子表格计算范式相结合的框架，以解决约束满足问题，克服了电子表格在数据流方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的电子表格应用程序由于其单向数据流的限制，通常只能处理类似簿记的任务。然而，在当今社会，电子表格因其易用性和实用性而广受欢迎，使得非程序员也能在熟悉的表格环境中执行类似编程的任务。因此，有必要扩展电子表格的计算范式，使其能够处理更复杂的计算问题，例如约束满足问题。

Method: 本论文提出了一种将有限约束求解器无缝集成到电子表格计算范式中的框架。在该框架中，电子表格的单个单元格可以关联到一个有限域或一个指定单元格之间关系的约束。该框架提供了一个用于约束求解的接口，并通过提供一组特定于电子表格的约束来增强电子表格计算范式，这些约束有助于控制大型电子表格应用程序实现的规模。

Result: 该框架允许将有限域或指定单元格之间关系的约束附加到电子表格的单元格中。它还提供了一个约束求解接口，并包含一组特定于电子表格的约束，以帮助管理大型电子表格实现的规模。论文提供了示例来说明这种扩展的电子表格范式的可用性和实用性。

Conclusion: 通过将有限约束求解器集成到电子表格中，该框架成功地扩展了电子表格的计算能力，使其能够解决约束满足问题，从而克服了传统电子表格在数据流方面的限制。这为非程序员提供了一个更强大、更通用的工具，用于解决复杂的计算任务。

Abstract: Spreadsheet computing is one of the more popular computing methodologies in today's modern society. The spreadsheet application's ease of use and usefulness has enabled non-programmers to perform programming-like tasks in a familiar setting modeled after the tabular "pen and paper" approach. However, spreadsheet applications are limited to bookkeeping-like tasks due to their single-direction data flow. This thesis demonstrates an extension of the spreadsheet computing paradigm in overcoming this limitation to solve constraint satisfaction problems. We present a framework seamlessly incorporating a finite constraint solver with the spreadsheet computing paradigm. This framework allows the individual cells in the spreadsheet to be attached to either a finite domain or a constraint specifying the relationship among the cells. The framework provides an interface for constraint solving and further enhances the spreadsheet computing paradigm by providing a set of spreadsheet-specific constraints that will aid in controlling the scalability of large spreadsheet applications implementations. Finally, we provide examples to demonstrate the usability and usefulness of the extended spreadsheet paradigm.
  Keywords: Spreadsheet computing, Constraint Logic Programming, Constraint satisfaction, Domain-Specific language, Excel, SWI Prolog, C#

</details>


### [84] [Human-Machine Collaboration for Democratizing Data Science](https://arxiv.org/abs/2004.11113)
*Clément Gautrais,Yann Dauxais,Stefano Teso,Samuel Kolb,Gust Verbruggen,Luc De Raedt*

Main category: cs.AI

TL;DR: VisualSynth是一个新框架，用于通过电子表格和用户提供的颜色草图进行数据科学分析，旨在普及数据科学。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏数据科学专业知识，许多人无法分析他们的数据。

Method: VisualSynth允许用户通过在电子表格中着色来指定数据科学任务，然后利用人工智能技术来确定和执行这些任务。

Result: 该系统支持从数据整理、数据选择到预测建模等各种数据分析任务。

Conclusion: VisualSynth通过提供一个直观的界面，让用户能够与电子表格软件进行交互，从而实现数据科学的民主化。

Abstract: Everybody wants to analyse their data, but only few posses the data science expertise to to this. Motivated by this observation we introduce a novel framework and system \textsc{VisualSynth} for human-machine collaboration in data science.
  It wants to democratize data science by allowing users to interact with standard spreadsheet software in order to perform and automate various data analysis tasks ranging from data wrangling, data selection, clustering, constraint learning, predictive modeling and auto-completion. \textsc{VisualSynth} relies on the user providing colored sketches, i.e., coloring parts of the spreadsheet, to partially specify data science tasks, which are then determined and executed using artificial intelligence techniques.

</details>


### [85] [Automated Discovery of Data Transformations for Robotic Process Automation](https://arxiv.org/abs/2001.01007)
*Volodymyr Leno,Marlon Dumas,Marcello La Rosa,Fabrizio Maria Maggi,Artem Polyvyanyy*

Main category: cs.AI

TL;DR: RPA技术可以通过分析用户交互(UI)日志来发现可自动化的数据传输例程，并提出优化方法来提高效率。


<details>
  <summary>Details</summary>
Motivation: 为了充分利用RPA的机遇，公司需要发现哪些例程可以被自动化以及如何自动化。

Method: 将用户界面(UI)日志分析以发现数据传输例程的问题映射到通过示例发现数据转换的问题，并提出两种优化方法。

Result: 通过UI日志对所提出的方法和优化进行了评估，这些日志复制了真实世界中可重复的数据传输例程。

Conclusion: 提出了一种通过分析UI日志来发现可自动化的数据传输例程的方法，并包含优化措施以提高计算效率。

Abstract: Robotic Process Automation (RPA) is a technology for automating repetitive routines consisting of sequences of user interactions with one or more applications. In order to fully exploit the opportunities opened by RPA, companies need to discover which specific routines may be automated, and how. In this setting, this paper addresses the problem of analyzing User Interaction (UI) logs in order to discover routines where a user transfers data from one spreadsheet or (Web) form to another. The paper maps this problem to that of discovering data transformations by example - a problem for which several techniques are available. The paper shows that a naive application of a state-of-the-art technique for data transformation discovery is computationally inefficient. Accordingly, the paper proposes two optimizations that take advantage of the information in the UI log and the fact that data transfers across applications typically involve copying alphabetic and numeric tokens separately. The proposed approach and its optimizations are evaluated using UI logs that replicate a real-life repetitive data transfer routine.

</details>


### [86] [Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples](https://arxiv.org/abs/1804.01186)
*Ashwin Kalyan,Abhishek Mohta,Oleksandr Polozov,Dhruv Batra,Prateek Jain,Sumit Gulwani*

Main category: cs.AI

TL;DR: 这是一个结合了逻辑推理和统计模型的混合程序合成技术，能够从少量示例中生成用户意图的程序，并在真实场景中实现了12倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 从少量输入输出示例合成用户意图程序，在电子表格处理、数据整理和代码重构等领域有重要应用，但现有系统要么依赖手工设计的逻辑技术，要么需要海量数据，难以实时合成。

Method: 提出了一种名为神经引导演绎搜索（NGDS）的混合合成技术，结合了符号逻辑技术和统计模型的优点。该技术利用演绎搜索框架来简化神经网络的学习问题，使其能够利用少量真实数据和循环神经网络编码器进行训练。

Result: 在真实客户场景的评估中，该方法能够合成准确的程序，与现有最先进的系统相比，速度提升高达12倍。

Conclusion: NGDS是一种有效的混合合成技术，结合了逻辑推理和神经网络的优势，能够从少量示例中合成准确且泛化能力强的程序，并在效率上超越了现有系统。

Abstract: Synthesizing user-intended programs from a small number of input-output examples is a challenging problem with several important applications like spreadsheet manipulation, data wrangling and code refactoring. Existing synthesis systems either completely rely on deductive logic techniques that are extensively hand-engineered or on purely statistical models that need massive amounts of data, and in general fail to provide real-time synthesis on challenging benchmarks. In this work, we propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique that combines the best of both symbolic logic techniques and statistical models. Thus, it produces programs that satisfy the provided specifications by construction and generalize well on unseen examples, similar to data-driven systems. Our technique effectively utilizes the deductive search framework to reduce the learning problem of the neural component to a simple supervised learning setup. Further, this allows us to both train on sparingly available real-world data and still leverage powerful recurrent neural network encoders. We demonstrate the effectiveness of our method by evaluating on real-world customer scenarios by synthesizing accurate programs with up to 12x speed-up compared to state-of-the-art systems.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [87] [Buckaroo: A Direct Manipulation Visual Data Wrangler](https://arxiv.org/abs/2507.16073)
*Annabelle Warner,Andrew McNutt,Paul Rosen,El Kindi Rezig*

Main category: cs.HC

TL;DR: 数据整理是数据科学项目中耗时最长的部分，通常需要手动编码，容易出错。Buckaroo 是一个可视化系统，可以自动查找和修复数据中的异常，并允许用户通过直接操作视觉对象来纠正数据。


<details>
  <summary>Details</summary>
Motivation: 手动数据整理耗时且容易出错，需要一种更有效的方法来处理数据清理和转换任务。

Method: Buckaroo 使用可视化来突出显示数据中的差异，并允许用户直接操作视觉对象进行即时修复。它会自动查找异常数据组，建议进行整理操作，并允许用户直观地查看整理操作的效果，支持撤销和重做。

Result: Buckaroo 能够自动识别异常数据组，并提供修复建议，同时允许用户通过可视化界面进行数据操作，从而提高数据整理的效率和准确性。

Conclusion: Buckaroo 是一个创新的可视化系统，通过自动化和直观的操作，显著改善了数据整理的流程，解决了手动编码的低效和易错性问题。

Abstract: Preparing datasets -- a critical phase known as data wrangling -- constitutes the dominant phase of data science development, consuming upwards of 80% of the total project time. This phase encompasses a myriad of tasks: parsing data, restructuring it for analysis, repairing inaccuracies, merging sources, eliminating duplicates, and ensuring overall data integrity. Traditional approaches, typically through manual coding in languages such as Python or using spreadsheets, are not only laborious but also error-prone. These issues range from missing entries and formatting inconsistencies to data type inaccuracies, all of which can affect the quality of downstream tasks if not properly corrected. To address these challenges, we present Buckaroo, a visualization system to highlight discrepancies in data and enable on-the-spot corrections through direct manipulations of visual objects. Buckaroo (1) automatically finds "interesting" data groups that exhibit anomalies compared to the rest of the groups and recommends them for inspection; (2) suggests wrangling actions that the user can choose to repair the anomalies; and (3) allows users to visually manipulate their data by displaying the effects of their wrangling actions and offering the ability to undo or redo these actions, which supports the iterative nature of data wrangling. A video companion is available at https://youtu.be/iXdCYbvpQVE

</details>


### [88] [SheetMind: An End-to-End LLM-Powered Multi-Agent Framework for Spreadsheet Automation](https://arxiv.org/abs/2506.12339)
*Ruiyan Zhu,Xi Cheng,Ke Liu,Brian Zhu,Daniel Jin,Neeraj Parihar,Zhoutian Xu,Oliver Gao*

Main category: cs.HC

TL;DR: SheetMind是一个由LLM驱动的多智能体框架，通过自然语言指令实现电子表格自动化。


<details>
  <summary>Details</summary>
Motivation: 将自然语言指令与电子表格功能相结合，实现电子表格自动化。

Method: 该系统由一个管理智能体（将指令分解为子任务）、一个动作智能体（将子任务转换为结构化命令）和一个反思智能体（验证生成的操作与用户意图的一致性）组成。

Result: 在基准数据集上的实验表明，SheetMind在单步任务上的成功率为80%，在多步指令上的成功率约为70%，优于简化版和基线版本。

Conclusion: 多智能体分解和基于语法的执行对于连接自然语言和电子表格功能非常有效。

Abstract: We present SheetMind, a modular multi-agent framework powered by large language models (LLMs) for spreadsheet automation via natural language instructions. The system comprises three specialized agents: a Manager Agent that decomposes complex user instructions into subtasks; an Action Agent that translates these into structured commands using a Backus Naur Form (BNF) grammar; and a Reflection Agent that validates alignment between generated actions and the user's original intent. Integrated into Google Sheets via a Workspace extension, SheetMind supports real-time interaction without requiring scripting or formula knowledge. Experiments on benchmark datasets demonstrate an 80 percent success rate on single step tasks and approximately 70 percent on multi step instructions, outperforming ablated and baseline variants. Our results highlight the effectiveness of multi agent decomposition and grammar based execution for bridging natural language and spreadsheet functionalities.

</details>


### [89] ["How do you even know that stuff?": Barriers to expertise sharing among spreadsheet users](https://arxiv.org/abs/2506.09216)
*Qing,Xia,Advait Sarkar,Duncan Brumby,Anna Cox*

Main category: cs.HC

TL;DR: 电子表格协作在组织内促进知识共享，但专家知识传播面临挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨社会规范和信念如何影响电子表格用户知识分享行为。

Method: 对31位专业电子表格用户进行半结构化访谈。

Result: 电子表格知识分享面临个性化策略适应困难、社会时机把握不当、自我评价冲突、规范信念忽视以及协作干扰担忧等挑战。

Conclusion: 技术设计与社会动态的复杂相互作用影响了功能丰富的软件中的协作学习行为，并为软件设计提供了启示。

Abstract: Spreadsheet collaboration provides valuable opportunities for learning and expertise sharing between colleagues. Sharing expertise is essential for the retention of important technical skillsets within organisations, but previous studies suggest that spreadsheet experts often fail to disseminate their knowledge to others. We suggest that social norms and beliefs surrounding the value of spreadsheet use significantly influence user engagement in sharing behaviours. To explore this, we conducted 31 semi-structured interviews with professional spreadsheet users from two separate samples. We found that spreadsheet providers face challenges in adapting highly personalised strategies to often subjective standards and evaluating the appropriate social timing of sharing. In addition, conflicted self-evaluations of one's spreadsheet expertise, dismissive normative beliefs about the value of this knowledge, and concerns about the potential disruptions associated with collaboration can further deter sharing. We suggest these observations reflect the challenges of long-term learning in feature-rich software designed primarily with initial learnability in mind. We therefore provide implications for design to navigate this tension. Overall, our findings demonstrate how the complex interaction between technology design and social dynamics can shape collaborative learning behaviours in the context of feature-rich software.

</details>


### [90] [Prompting in the Dark: Assessing Human Performance in Prompt Engineering for Data Labeling When Gold Labels Are Absent](https://arxiv.org/abs/2502.11267)
*Zeyu He,Saniya Naphade,Ting-Hao 'Kenneth' Huang*

Main category: cs.HC

TL;DR: 用户在没有人工标注基准的情况下，通过迭代提示LLM进行数据标注（“暗中提示”）的可靠性很低，只有不到一半的参与者在多次迭代后提高了标注准确率，而像DSPy这样的自动提示优化工具在金标稀疏时也面临挑战。


<details>
  <summary>Details</summary>
Motivation: 研究在缺乏人工标注数据的情况下，用户迭代优化提示（prompt engineering）以驱动大型语言模型（LLM）进行数据标注的有效性，以及自动提示优化工具的适用性。

Method: 开发了一个名为PromptingSheet的Google Sheets插件，允许用户在电子表格环境中进行提示的编写、修改和迭代标注。通过招募20名参与者进行实验，分析他们在“暗中提示”场景下的标注准确率变化。

Result: 在20名参与者中，只有9名在进行四次或更多次迭代后，标注准确率有所提高。同时，在金标稀疏的情况下，DSPy等自动化提示优化工具的优化效果也不理想。

Conclusion: 在LLM数据标注场景中，“暗中提示”（无金标）的方法可靠性较低，用户迭代优化提示的效果不稳定。这凸显了金标数据的重要性，并指出了在设计自动化提示工程辅助工具时，需要考虑用户的需求和潜在风险。

Abstract: Millions of users prompt large language models (LLMs) for various tasks, but how good are people at prompt engineering? Do users actually get closer to their desired outcome over multiple iterations of their prompts? These questions are crucial when no gold-standard labels are available to measure progress. This paper investigates a scenario in LLM-powered data labeling, "prompting in the dark," where users iteratively prompt LLMs to label data without using manually-labeled benchmarks. We developed PromptingSheet, a Google Sheets add-on that enables users to compose, revise, and iteratively label data through spreadsheets. Through a study with 20 participants, we found that prompting in the dark was highly unreliable -- only 9 participants improved labeling accuracy after four or more iterations. Automated prompt optimization tools like DSPy also struggled when few gold labels were available. Our findings highlight the importance of gold labels and the needs, as well as the risks, of automated support in human prompt engineering, providing insights for future tool design.

</details>


### [91] [When Copilot Becomes Autopilot: Generative AI's Critical Risk to Knowledge Work and a Critical Solution](https://arxiv.org/abs/2412.15030)
*Advait Sarkar,Xiaotong,Xu,Neil Toronto,Ian Drosos,Christian Poelitz*

Main category: cs.HC

TL;DR: 生成式AI在知识工作中可能带来风险（如幻觉）和机遇（如赋能非专家），尤其是在电子表格领域。最大的风险并非幻觉，而是人类批判性思维的退化。解决方案是设计能促进批判性思维的生成式AI界面，并以教育领域研究为基础。我们提出了一个原型系统，用于电子表格中的批判性筛选，该系统能建议筛选标准、应用标准，并生成“挑衅”文本来批判AI生成的标准，从而开启了AI辅助知识工作批判性思维工具的新研究领域。


<details>
  <summary>Details</summary>
Motivation: 生成式AI可能引入错误，但也为用户（特别是新手）提供了学习和应用复杂软件功能的机会，从而扩大了他们能完成的任务的范围和复杂性。电子表格工作流程是受生成式AI风险和机遇影响的复杂知识工作示例。生成式AI对电子表格工作流程的最大风险并非其“幻觉”能力，而是随着更多工作可以安全地委托给AI，人类批判性思维能力可能退化。

Method: 我们提出了一个原型系统，用于电子表格中的批判性筛选活动。该系统利用生成式AI建议筛选标准，并将这些标准应用于对电子表格中的行进行排序。此外，它还生成“挑衅”文本片段，用于批判AI生成的标准，强调其风险、不足之处和替代方案。

Result: 我们提出了一个用于电子表格批判性筛选的原型系统，该系统能生成筛选标准、应用这些标准，并提供批判性评论（“挑衅”），以促进用户对AI建议进行批判性评估。

Conclusion: 生成式AI在知识工作（如电子表格）中既带来了风险也带来了机遇。最大的风险是人类批判性思维的退化，而非AI的“幻觉”。解决此问题的方法是设计能够促进批判性思维的AI界面。我们提出的原型系统开启了AI作为批评者或挑衅者的研究新领域，并勾勒了相关的研究议程。

Abstract: Generative AI, with its tendency to "hallucinate" incorrect results, may pose a risk to knowledge work by introducing errors. On the other hand, it may also provide unprecedented opportunities for users, particularly non-experts, to learn and apply advanced software features and greatly increase the scope and complexity of tasks they can successfully achieve.
  As an example of a complex knowledge workflow that is subject to risks and opportunities from generative AI, we consider the spreadsheet. AI hallucinations are an important challenge, but they are not the greatest risk posed by generative AI to spreadsheet workflows. Rather, as more work can be safely delegated to AI, the risk is that human critical thinking -- the ability to holistically and rigorously evaluate a problem and its solutions -- is degraded in the process. The solution is to design the interfaces of generative AI systems deliberately to foster and encourage critical thinking in knowledge work, building primarily on a long history of research on critical thinking tools for education.
  We discuss a prototype system for the activity of critical shortlisting in spreadsheets. The system uses generative AI to suggest shortlisting criteria and applies these criteria to sort rows in a spreadsheet. It also generates "provocations": short text snippets that critique the AI-generated criteria, highlighting risks, shortcomings, and alternatives. Our prototype opens up a rich and completely unexplored design space of critical thinking tools for modern AI-assisted knowledge work. We outline a research agenda for AI as a critic or provocateur, including questions about where and when provocations should appear, their form and content, and potential design trade-offs.

</details>


### [92] [Understanding and Evaluating Trust in Generative AI and Large Language Models for Spreadsheets](https://arxiv.org/abs/2412.14062)
*Simon Thorne*

Main category: cs.HC

TL;DR: 生成式AI和LLM在自动化电子表格公式创建方面有潜力，但存在幻觉、偏差和用户技能差异等问题。本文提出了一个基于透明度和可靠性评估的信任框架，并探讨了影响这些指标的因素以及不信任技术的后果。


<details>
  <summary>Details</summary>
Motivation: 由于生成式AI的幻觉、偏差和用户技能差异，其生成的公式可能不准确或不可信。因此，需要一个框架来评估和确保这些公式的信任度。

Method: 提出一个信任框架，从可解释性、可见性、可靠性和道德考量等方面评估公式的透明度和可靠性。同时，分析了幻觉、训练数据偏差和不当提示词对这些指标的影响。

Result: 评估了生成公式的透明度（可解释性和可见性）和可靠性（准确性和公平性），并考虑了影响这些指标的因素（幻觉、数据偏差、提示词）。

Conclusion: 生成式AI在电子表格公式创建方面存在信任度挑战，但通过评估透明度和可靠性，并关注其驱动因素，可以提高其可用性。

Abstract: Generative AI and Large Language Models (LLMs) hold promise for automating spreadsheet formula creation. However, due to hallucinations, bias and variable user skill, outputs obtained from generative AI cannot be assumed to be accurate or trustworthy. To address these challenges, a trustworthiness framework is proposed based on evaluating the transparency and dependability of the formula. The transparency of the formula is explored through explainability (understanding the formula's reasoning) and visibility (inspecting the underlying algorithms). The dependability of the generated formula is evaluated in terms of reliability (consistency and accuracy) and ethical considerations (bias and fairness). The paper also examines the drivers to these metrics in the form of hallucinations, training data bias and poorly constructed prompts. Finally, examples of mistrust in technology are considered and the consequences explored.

</details>


### [93] [Dynamic Prompt Middleware: Contextual Prompt Refinement Controls for Comprehension Tasks](https://arxiv.org/abs/2412.02357)
*Ian Drosos,Jack Williams,Advait Sarkar,Nicholas Wilson*

Main category: cs.HC

TL;DR: 用户在使用生成式AI进行理解任务时，尤其是在电子表格公式、Python代码和文本片段解释等场景下，难以有效地进行提示，以充分表达所需的上下文。提示中间件旨在通过辅助提示构建来解决这一障碍，但用户在表达充分控制以获得符合其偏好的AI响应方面仍存在困难。本研究通过一项包含38名参与者的形成性调查，探讨用户对AI生成解释控制的需求，发现了标准化但可预测的支持与量身定制但不可预测的支持之间的权衡。为了探索这种权衡，我们实现了两种提示中间件方法：动态提示优化控制（Dynamic PRC）和静态提示优化控制（Static PRC）。动态PRC方法生成特定于上下文的UI元素，提供基于用户提示和用户对AI需求的提示优化；而静态PRC方法提供预设的通用优化列表。我们通过一项包含16名参与者的受控用户研究评估了这两种方法，以评估它们对用户控制AI响应以 crafting 更好解释的影响。结果显示，用户偏好动态PRC方法，因为它提供了更多的控制，降低了提供上下文的门槛，并鼓励对任务的探索和反思，但对不同生成控制对最终输出效果的推理仍然具有挑战性。基于参与者的反馈，我们讨论了未来增强用户对AI响应控制的动态PRC系统的设计启示。我们的研究结果表明，动态提示中间件可以通过提供更大的控制来改善生成式AI工作流的用户体验，并引导用户获得更好的AI响应。


<details>
  <summary>Details</summary>
Motivation: 用户在使用生成式AI进行理解任务时，尤其是在电子表格公式、Python代码和文本片段解释等场景下，难以有效地进行提示，以充分表达所需的上下文。提示中间件旨在通过辅助提示构建来解决这一障碍，但用户在表达充分控制以获得符合其偏好的AI响应方面仍存在困难。

Method: 我们进行了一项包含38名参与者的形成性调查，探讨用户对AI生成解释控制的需求，发现了标准化但可预测的支持与量身定制但不可预测的支持之间的权衡。为了探索这种权衡，我们实现了两种提示中间件方法：动态提示优化控制（Dynamic PRC）和静态提示优化控制（Static PRC）。动态PRC方法生成特定于上下文的UI元素，提供基于用户提示和用户对AI需求的提示优化；而静态PRC方法提供预设的通用优化列表。我们通过一项包含16名参与者的受控用户研究评估了这两种方法。

Result: 结果显示，用户偏好动态PRC方法，因为它提供了更多的控制，降低了提供上下文的门槛，并鼓励对任务的探索和反思，但对不同生成控制对最终输出效果的推理仍然具有挑战性。

Conclusion: 动态提示中间件可以通过提供更大的控制来改善生成式AI工作流的用户体验，并引导用户获得更好的AI响应。

Abstract: Effective prompting of generative AI is challenging for many users, particularly in expressing context for comprehension tasks such as explaining spreadsheet formulas, Python code, and text passages. Prompt middleware aims to address this barrier by assisting in prompt construction, but barriers remain for users in expressing adequate control so that they can receive AI-responses that match their preferences.
  We conduct a formative survey (n=38) investigating user needs for control over AI-generated explanations in comprehension tasks, which uncovers a trade-off between standardized but predictable support for prompting, and adaptive but unpredictable support tailored to the user and task. To explore this trade-off, we implement two prompt middleware approaches: Dynamic Prompt Refinement Control (Dynamic PRC) and Static Prompt Refinement Control (Static PRC). The Dynamic PRC approach generates context-specific UI elements that provide prompt refinements based on the user's prompt and user needs from the AI, while the Static PRC approach offers a preset list of generally applicable refinements.
  We evaluate these two approaches with a controlled user study (n=16) to assess the impact of these approaches on user control of AI responses for crafting better explanations. Results show a preference for the Dynamic PRC approach as it afforded more control, lowered barriers to providing context, and encouraged exploration and reflection of the tasks, but that reasoning about the effects of different generated controls on the final output remains challenging. Drawing on participant feedback, we discuss design implications for future Dynamic PRC systems that enhance user control of AI responses. Our findings suggest that dynamic prompt middleware can improve the user experience of generative AI workflows by affording greater control and guide users to a better AI response.

</details>


### [94] [Exploring Higher Education Competencies through Spreadsheet Self-Assessment and Time](https://arxiv.org/abs/2409.12974)
*Maria Csernoch,Judit T. Kiss,Viktor Takács,Domicián Máté*

Main category: cs.HC

TL;DR: 学生在电子表格能力和可靠性方面倾向于进行不准确的自我评估，并且在数字环境中花费的时间比在纸上长，这挑战了“数字原住民”假设。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨大学生在电子表格能力和可靠性方面的自我评估和实际解决问题的能力，并检验“数字原住民”假设。

Method: 通过自我评估和实际解决问题的实践来探索大学生在电子表格能力和可靠性方面的表现，并在纸质和Excel环境中进行比较。

Result: 研究结果表明，学生倾向于不准确地评估自己的电子表格能力，并且在数字环境中完成任务所需的时间大约是在纸上完成任务的两倍，这与‘数字原住民’假设相悖。

Conclusion: 研究强调了在高等教育（尤其是在技术驱动的学科）中，准确的自我评估和时间管理在数字技能发展中的重要性，并对‘数字原住民’能够轻松掌握数字技能的普遍假设提出了质疑。

Abstract: The present paper aims to explore higher education students' spreadsheet competencies and reliability through self-assessment and real-world problem-solving practices. Digital natives alleged skills and competences allowed us to hypothesize that students perform better in Excel than on paper, but the findings cannot confirm this hypothesis. However, our results indicate that students tend to inaccurately assess their spreadsheet competencies compared to their actual performance in both paper-based and Excel tasks. It has also be found that students need at least twice as much time to achieve the same high scores in the digital environment as they do on paper. The results violated the widely accepted assumption that digital native students do not need computer science education, since they are born with it. This study highlights the importance of accurate self-assessment in digital skill development and time management within higher education contexts, particularly in technology-driven disciplines.

</details>


### [95] [Supporting Annotators with Affordances for Efficiently Labeling Conversational Data](https://arxiv.org/abs/2403.07762)
*Austin Z. Henley,David Piorkowski*

Main category: cs.HC

TL;DR: CAL是一个新颖的数据标注界面，通过防止不当选择、提供引导、整合文档和方便地查看历史标注，降低了用户认知负荷，提高了易用性和用户偏好，且不增加任务时间。


<details>
  <summary>Details</summary>
Motivation: 现有的众包标注耗时且昂贵，为了解决人力和单调的问题，需要一个更好的数据标注辅助工具。

Method: 设计并实现了一个名为CAL的新颖界面，其特点包括：1. 防止不当标签被选中；2. 在需要时引导用户选择合适标签；3. 将标注文档整合到界面中；4. 提供查看先前标注的有效途径。并通过用户研究将CAL与标准电子表格进行比较。

Result: 用户研究表明，使用CAL的用户报告的认知负荷较低，任务时间没有增加，用户认为CAL更易于使用，并且用户更喜欢CAL而不是电子表格。

Conclusion: CAL是一个有效的数据标注界面，可以降低认知负荷，提高易用性，并受到用户欢迎，同时保持了标注效率。

Abstract: Without well-labeled ground truth data, machine learning-based systems would not be as ubiquitous as they are today, but these systems rely on substantial amounts of correctly labeled data. Unfortunately, crowdsourced labeling is time consuming and expensive. To address the concerns of effort and tedium, we designed CAL, a novel interface to aid in data labeling. We made several key design decisions for CAL, which include preventing inapt labels from being selected, guiding users in selecting an appropriate label when they need assistance, incorporating labeling documentation into the interface, and providing an efficient means to view previous labels. We implemented a production-quality implementation of CAL and report a user-study evaluation that compares CAL to a standard spreadsheet. Key findings of our study include users using CAL reported lower cognitive load, did not increase task time, users rated CAL to be easier to use, and users preferred CAL over the spreadsheet.

</details>


### [96] [Prompting for Discovery: Flexible Sense-Making for AI Art-Making with Dreamsheets](https://arxiv.org/abs/2310.09985)
*Shm Garanganao Almeda,J. D. Zamfirescu-Pereira,Kyu Won Kim,Pradeep Mani Rathnam,Bjoern Hartmann*

Main category: cs.HC

TL;DR: DreamSheets是一个电子表格界面，通过基于LLM的功能辅助用户进行提示词构建和结果展示，来支持文本到图像（TTI）模型的探索。它使艺术家能够通过定义自己的工作流程来探索TTI模型的巨大设计空间，并发现了用户用来应对TTI设计空间探索挑战的策略以及支持这些策略的界面功能。


<details>
  <summary>Details</summary>
Motivation: 如何设计用户界面来帮助用户在提示词的巨大输入空间中可靠地引导探索，以获得有趣的文本到图像（TTI）模型生成结果。

Method: 通过设计一个名为DreamSheets的探针，在一个电子表格界面中，结合基于LLM的提示词构建辅助功能和同时生成的结果展示，来支持用户探索TTI模型。然后，通过一项初步的实验室研究和一项包含五位专业艺术家的纵向研究，来收集用户策略和界面需求。

Result: 研究揭示了用户在TTI设计空间探索中使用的策略，以及支持这些策略的界面功能，例如使用文本生成来定义探索的局部“轴”。

Conclusion: DreamSheets为TTI模型的设计空间探索提供了一个有效的界面，并为未来的TTI界面设计提供了指导。

Abstract: Design space exploration (DSE) for Text-to-Image (TTI) models entails navigating a vast, opaque space of possible image outputs, through a commensurately vast input space of hyperparameters and prompt text. Minor adjustments to prompt input can surface unexpectedly disparate images. How can interfaces support end-users in reliably steering prompt-space explorations towards interesting results? Our design probe, DreamSheets, supports exploration strategies with LLM-based functions for assisted prompt construction and simultaneous display of generated results, hosted in a spreadsheet interface. The flexible layout and novel generative functions enable experimentation with user-defined workflows. Two studies, a preliminary lab study and a longitudinal study with five expert artists, revealed a set of strategies participants use to tackle the challenges of TTI design space exploration, and the interface features required to support them - like using text-generation to define local "axes" of exploration. We distill these insights into a UI mockup to guide future interfaces.

</details>


### [97] [Does Using ChatGPT Result in Human Cognitive Augmentation?](https://arxiv.org/abs/2401.11042)
*Ron Fulbright,Miranda Morrison*

Main category: cs.HC

TL;DR: ChatGPT can augment human cognitive performance, but doesn't always improve it and can even mislead users, indicating human judgment is still crucial.


<details>
  <summary>Details</summary>
Motivation: Investigate human cognitive augmentation by using ChatGPT and compare its performance against human-only responses.

Method: Conducted two experiments comparing responses generated with and without ChatGPT, evaluating cognitive augmentation. 

Result: Using ChatGPT does not guarantee cognitive augmentation and does not replace human judgment in all tasks. In some cases, it led to negative cognitive augmentation by misleading users.

Conclusion: While ChatGPT can augment human cognition, it does not universally enhance performance and can be misleading, underscoring the continued importance of human judgment and evaluation.

Abstract: Human cognitive performance is enhanced by the use of tools. For example, a human can produce a much greater, and more accurate, volume of mathematical calculation in a unit of time using a calculator or a spreadsheet application on a computer. Such tools have taken over the burden of lower level cognitive grunt work but the human still serves the role of the expert performing higher level thinking and reasoning. Recently, however, unsupervised, deep, machine learning has produced cognitive systems able to outperform humans in several domains. When humans use these tools in a human cog ensemble, the cognitive ability of the human is augmented. In some cases, even non experts can achieve, and even exceed, the performance of experts in a particular domain, synthetic expertise. A new cognitive system, ChatGPT, has burst onto the scene during the past year. This paper investigates human cognitive augmentation due to using ChatGPT by presenting the results of two experiments comparing responses created using ChatGPT with results created not using ChatGPT. We find using ChatGPT does not always result in cognitive augmentation and does not yet replace human judgement, discernment, and evaluation in certain types of tasks. In fact, ChatGPT was observed to result in misleading users resulting in negative cognitive augmentation.

</details>


### [98] [Co-audit: tools to help humans double-check AI-generated content](https://arxiv.org/abs/2310.01297)
*Andrew D. Gordon,Carina Negreanu,José Cambronero,Rasika Chakravarthy,Ian Drosos,Hao Fang,Bhaskar Mitra,Hannah Richardson,Advait Sarkar,Stephanie Simmons,Jack Williams,Ben Zorn*

Main category: cs.HC

TL;DR: AI生成的复杂内容（如摘要、表格、代码）难以由用户审计，因此出现了“协同审计”工具来辅助用户检查AI生成内容的正确性。本文介绍了用于电子表格计算的协同审计工具研究，阐述了其必要性，并提出了协同审计的原则和研究挑战。


<details>
  <summary>Details</summary>
Motivation: 用户越来越需要检查AI生成内容的正确性，但随着AI生成内容的复杂性增加，用户审计变得更加困难。因此，需要协同审计工具来辅助用户检查AI生成内容的质量和正确性，特别是在电子表格计算等错误后果严重的场景中。

Method: 本文介绍了用于电子表格计算的协同审计工具的最新研究，并提出了协同审计的原则。

Result: 本文讨论了协同审计工具在电子表格计算中的应用，强调了其在提高AI生成内容质量和准确性方面的重要性。

Conclusion: 协同审计工具对于需要高质量和高准确性输出的AI应用至关重要，尤其是在电子表格计算等领域。文章提出了协同审计的原则，并指出了未来研究的方向和挑战。

Abstract: Users are increasingly being warned to check AI-generated content for correctness. Still, as LLMs (and other generative models) generate more complex output, such as summaries, tables, or code, it becomes harder for the user to audit or evaluate the output for quality or correctness. Hence, we are seeing the emergence of tool-assisted experiences to help the user double-check a piece of AI-generated content. We refer to these as co-audit tools. Co-audit tools complement prompt engineering techniques: one helps the user construct the input prompt, while the other helps them check the output response. As a specific example, this paper describes recent research on co-audit tools for spreadsheet computations powered by generative models. We explain why co-audit experiences are essential for any application of generative AI where quality is important and errors are consequential (as is common in spreadsheet computations). We propose a preliminary list of principles for co-audit, and outline research challenges.

</details>


### [99] ["What It Wants Me To Say": Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models](https://arxiv.org/abs/2304.06597)
*Michael Xieyang Liu,Advait Sarkar,Carina Negreanu,Ben Zorn,Jack Williams,Neil Toronto,Andrew D. Gordon*

Main category: cs.HC

TL;DR: 大型语言模型可生成代码，但用户需要学习如何有效地提出指令。本文提出一种“接地气”的方法，将代码转换回自然语言，帮助用户理解和使用代码生成模型，尤其是在数据分析场景中。


<details>
  <summary>Details</summary>
Motivation: 非专业用户的挑战在于理解如何有效地引导代码生成，即“抽象匹配”。

Method: 提出“接地气抽象匹配”方法，将代码翻译回系统化、可预测的自然语言，并与不接地气的基线方法进行比较。实验通过24名用户进行，采用“出声思考”方式。

Result: 接地气的方法能提升用户对代码生成模型范围、能力以及有效使用所需语言的理解。

Conclusion: 接地气抽象匹配有助于弥合抽象差距，提升非专业用户使用代码生成工具的体验。

Abstract: Code-generating large language models translate natural language into code. However, only a small portion of the infinite space of naturalistic utterances is effective at guiding code generation. For non-expert end-user programmers, learning this is the challenge of abstraction matching. We examine this challenge in the specific context of data analysis in spreadsheets, in a system that maps the users natural language query to Python code using the Codex generator, executes the code, and shows the result. We propose grounded abstraction matching, which bridges the abstraction gap by translating the code back into a systematic and predictable naturalistic utterance. In a between-subjects, think-aloud study (n=24), we compare grounded abstraction matching to an ungrounded alternative based on previously established query framing principles. We find that the grounded approach improves end-users' understanding of the scope and capabilities of the code-generating model, and the kind of language needed to use it effectively.

</details>


### [100] [Team OS's System for Dialogue Robot Competition 2022](https://arxiv.org/abs/2210.09928)
*Yuki Kubo,Ryo Yanagimoto,Hayato Futase,Mikio Nakano,Zhaojie Luo,Kazunori Komatani*

Main category: cs.HC

TL;DR: OSbot是一个基于状态转换的对话机器人系统，通过关键词提取和情感分析来驱动对话流程，并在2022年对话机器人竞赛预赛中获得第三名。


<details>
  <summary>Details</summary>
Motivation: 开发用于对话机器人竞赛2022的OSbot系统。

Method: 系统基于手动描述的状态转换，并使用关键词提取（基于命名实体提取和预定义的关键词集）和情感分析（基于SVM和Hazumi多模态对话语料库训练）的结果作为转换条件。对话流程可通过电子表格管理，并提供日志功能以快速检查和编辑。

Result: 在竞赛预赛中，该系统取得了第三名的成绩。

Conclusion: OSbot系统能够有效地处理对话流程，并通过结合关键词提取和情感分析在竞赛中取得良好成绩。

Abstract: This paper describes our dialogue robot system, OSbot, developed for Dialogue Robot Competition 2022. The dialogue flow is based on state transitions described manually and the transition conditions use the results of keyword extraction and sentiment analysis. The transitions can be easily viewed and edited by managing them on a spreadsheet. The keyword extraction is based on named entity extraction and our predefined keyword set. The sentiment analysis is text-based and uses SVM, which was trained with the multimodal dialogue corpus Hazumi. We quickly checked and edited a dialogue flow by using a logging function. In the competition's preliminary round, our system ended up in third place.

</details>


### [101] [What is it like to program with artificial intelligence?](https://arxiv.org/abs/2208.06213)
*Advait Sarkar,Andrew D. Gordon,Carina Negreanu,Christian Poelitz,Sruti Srinivasa Ragavan,Ben Zorn*

Main category: cs.HC

TL;DR: LLM-assisted programming is a new paradigm with distinct properties and challenges, differing from prior programmer assistance methods. It presents unique issues and research challenges, especially for non-expert end-user programmers.


<details>
  <summary>Details</summary>
Motivation: To explore how LLM-assisted programming compares to and differs from previous conceptualizations of programmer assistance, and to identify issues and research challenges in applying LLMs to end-user programming.

Method: Draw upon publicly available experience reports of LLM-assisted programming and prior usability/design studies. Conduct a user study where non-expert end-user programmers use LLM-assisted tools for data tasks in spreadsheets.

Result: LLM-assisted programming shares some properties with compilation, pair programming, and programming via search/reuse, but has fundamental technical and practical differences. Preliminary observations from a user study indicate potential issues and open research challenges for end-user programming with LLMs.

Conclusion: LLM-assisted programming should be viewed as a new way of programming with its own distinct properties and challenges. Applying LLMs to end-user programming, especially for non-experts, presents specific issues and requires further research.

Abstract: Large language models, such as OpenAI's codex and Deepmind's AlphaCode, can generate code to solve a variety of problems expressed in natural language. This technology has already been commercialised in at least one widely-used programming editor extension: GitHub Copilot.
  In this paper, we explore how programming with large language models (LLM-assisted programming) is similar to, and differs from, prior conceptualisations of programmer assistance. We draw upon publicly available experience reports of LLM-assisted programming, as well as prior usability and design studies. We find that while LLM-assisted programming shares some properties of compilation, pair programming, and programming via search and reuse, there are fundamental differences both in the technical possibilities as well as the practical experience. Thus, LLM-assisted programming ought to be viewed as a new way of programming with its own distinct properties and challenges.
  Finally, we draw upon observations from a user study in which non-expert end user programmers use LLM-assisted tools for solving data tasks in spreadsheets. We discuss the issues that might arise, and open research challenges, in applying large language models to end-user programming, particularly with users who have little or no programming expertise.

</details>


### [102] [MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization](https://arxiv.org/abs/2209.05739)
*Lu Ying,Xinhuan Shu,Dazhen Deng,Yuchen Yang,Tan Tang,Lingyun Yu,Yingcai Wu*

Main category: cs.HC

TL;DR: MetaGlyph是一个自动生成隐喻图形可视化（MGV）的系统，它通过在线资源选择隐喻图像，并利用蒙特卡洛树搜索算法来设计图形，同时允许用户自定义。


<details>
  <summary>Details</summary>
Motivation: 图形化可视化需要专业的设计技能，本研究旨在提出一个自动生成隐喻图形可视化（MGV）的系统，以降低其创建难度。

Method: 1. 对现有MGV进行定性分析（隐喻体现和图形设计）。2. 提出新的MGV生成框架（通过隐喻图像选择和MGV构建）。3. 自动选择具有相关图像的隐喻（基于数据语义）。4. 整合蒙特卡洛树搜索算法探索图形设计（考虑数据重要性、语义相关性和图形不重叠）。5. 提供编辑反馈以供用户自定义。

Result: 展示了MetaGlyph的示例、使用场景，并通过专家访谈验证了其有效性。

Conclusion: MetaGlyph能够自动生成隐喻图形可视化，降低了设计门槛，并通过用户反馈和专家验证证明了其有效性。

Abstract: Glyph-based visualization achieves an impressive graphic design when associated with comprehensive visual metaphors, which help audiences effectively grasp the conveyed information through revealing data semantics. However, creating such metaphoric glyph-based visualization (MGV) is not an easy task, as it requires not only a deep understanding of data but also professional design skills. This paper proposes MetaGlyph, an automatic system for generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct a qualitative analysis to understand the design of current MGVs from the perspectives of metaphor embodiment and glyph design. Based on the results, we introduce a novel framework for generating MGVs by metaphoric image selection and an MGV construction. Specifically, MetaGlyph automatically selects metaphors with corresponding images from online resources based on the input data semantics. We then integrate a Monte Carlo tree search algorithm that explores the design of an MGV by associating visual elements with data dimensions given the data importance, semantic relevance, and glyph non-overlap. The system also provides editing feedback that allows users to customize the MGVs according to their design preferences. We demonstrate the use of MetaGlyph through a set of examples, one usage scenario, and validate its effectiveness through a series of expert interviews.

</details>


### [103] [PoVRPoint: Authoring Presentations in Mobile Virtual Reality](https://arxiv.org/abs/2201.06337)
*Verena Biener,Travis Gesslein,Daniel Schneider,Felix Kawala,Alexander Otte,Per Ola Kristensson,Michel Pahud,Eyal Ofek,Cuauhtli Campos,Matjaž Kljun,Klen Čopič Pucihar,Jens Grubert*

Main category: cs.HC

TL;DR: 该论文提出了一种名为PoVRPoint的VR工具集，用于在移动设备上进行演示文稿创作，并结合了VR的空间交互能力。


<details>
  <summary>Details</summary>
Motivation: 为了探索VR在移动办公场景下对知识工作者的支持潜力，特别是在演示文稿创作领域。

Method: 结合了移动设备（如平板电脑）上的笔和触摸输入，以及VR提供的三维空间输出和空间输入能力，设计并实现了一套名为PoVRPoint的工具集。通过用户研究，评估了VR的扩展显示空间在识别目标幻灯片、空间对象操作、动画创建和多重遮挡对象排列等方面的效用。

Result: 研究结果表明，VR的宽视场显著提高了目标幻灯片的识别速度，而三维视图则在处理遮挡情况下的对象重新排序时比基线界面更快。用户研究证实了该交互技术的可用性和趣味性。

Conclusion: PoVRPoint工具集利用VR的优势，有效提升了移动场景下演示文稿创作的效率和用户体验。

Abstract: Virtual Reality (VR) has the potential to support mobile knowledge workers by complementing traditional input devices with a large three-dimensional output space and spatial input. Previous research on supporting VR knowledge work explored domains such as text entry using physical keyboards and spreadsheet interaction using combined pen and touch input. Inspired by such work, this paper probes the VR design space for authoring presentations in mobile settings. We propose PoVRPoint -- a set of tools coupling pen- and touch-based editing of presentations on mobile devices, such as tablets, with the interaction capabilities afforded by VR. We study the utility of extended display space to, for example, assist users in identifying target slides, supporting spatial manipulation of objects on a slide, creating animations, and facilitating arrangements of multiple, possibly occluded, shapes. Among other things, our results indicate that 1) the wide field of view afforded by VR results in significantly faster target slide identification times compared to a tablet-only interface for visually salient targets; and 2) the three-dimensional view in VR enables significantly faster object reordering in the presence of occlusion compared to two baseline interfaces. A user study further confirmed that the interaction techniques were found to be usable and enjoyable.

</details>


### [104] [Untidy Data: The Unreasonable Effectiveness of Tables](https://arxiv.org/abs/2106.15005)
*Lyn Bartram,Michael Correll,Melanie Tory*

Main category: cs.HC

TL;DR: 数据表不仅是数据清理的工具，更是数据工作者在整个数据分析过程中理解和操作数据的关键媒介。


<details>
  <summary>Details</summary>
Motivation: 探讨数据工作者如何与表格数据交互和推理，以及表格数据在信息生态系统中的重要性。

Method: 通过定性研究，观察数据工作者如何与表格数据进行交互和推理。

Result: 数据工作者在数据分析的各个阶段都需要直接操作表格数据，通过重组、标注、添加细节和生成不同版本来辅助理解和决策。

Conclusion: 交互式数据表是一种重要的可视化形式，为视觉分析提供了丰富的交互设计空间，并能通过更灵活的人机数据交互来增强数据分析能力。

Abstract: Working with data in table form is usually considered a preparatory and tedious step in the sensemaking pipeline; a way of getting the data ready for more sophisticated visualization and analytical tools. But for many people, spreadsheets -- the quintessential table tool -- remain a critical part of their information ecosystem, allowing them to interact with their data in ways that are hidden or abstracted in more complex tools. This is particularly true for data workers: people who work with data as part of their job but do not identify as professional analysts or data scientists. We report on a qualitative study of how these workers interact with and reason about their data. Our findings show that data tables serve a broader purpose beyond data cleanup at the initial stage of a linear analytic flow: users want to see and "get their hands on" the underlying data throughout the analytics process, reshaping and augmenting it to support sensemaking. They reorganize, mark up, layer on levels of detail, and spawn alternatives within the context of the base data. These direct interactions and human-readable table representations form a rich and cognitively important part of building understanding of what the data mean and what they can do with it. We argue that interactive tables are an important visualization idiom in their own right; that the direct data interaction they afford offers a fertile design space for visual analytics; and that sense making can be enriched by more flexible human-data interaction than is currently supported in visual analytics tools.

</details>


### [105] [Defining and Adopting an End User Computing Policy: A Case Study](https://arxiv.org/abs/1909.00855)
*Roger Turner*

Main category: cs.HC

TL;DR: End User Computing (EUC) policy updates at Wesleyan Assurance Society were implemented using a risk-based approach, involving a custom risk assessment application to identify and mitigate high-risk areas.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the significant risks associated with End User Computing (EUC) if not properly controlled.

Method: The paper details the implementation of an updated EUC policy at Wesleyan Assurance Society, including the development of an End User Computing Risk Assessment Application. This application assesses risk based on complexity, materiality, and control, assigning a risk rating band. The policy employs a risk-based strategy to prioritize mitigation efforts.

Result: The introduction of the updated EUC policy and the risk assessment application provided a structured way to manage EUC risks.

Conclusion: A risk-based approach, supported by a dedicated risk assessment tool, is effective in managing End User Computing risks and achieving timely benefits.

Abstract: End User Computing carries significant risks if not well controlled. This paper is a case study of the introduction of an updated End User Computing policy at the Wesleyan Assurance Society. The paper outlines the plan and identifies various challenges. The paper explains how these challenges were overcome. We wrote an End User Computing Risk Assessment Application which calculates a risk rating band based on the Complexity, Materiality and Control (or lack of it) pertaining to any given application and the basis of assessment is given in this paper. The policy uses a risk based approach for assessing and mitigating against the highest risks first and obtaining the quickest benefit.

</details>


### [106] [Calliope: Automatic Visual Data Story Generation from a Spreadsheet](https://arxiv.org/abs/2010.09975)
*Danqing Shi,Xinyue Xu,Fuling Sun,Yang Shi,Nan Cao*

Main category: cs.HC

TL;DR: Calliope是一个自动化系统，可以将电子表格转换为叙事可视化（如海报或数据视频），并通过在线编辑器进行编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的数据可视化故事创作工具需要用户具备数据分析、可视化和脚本编写等方面的专业技能，效率低下且难以使用。

Method: Calliope 采用了一种新的面向逻辑的蒙特卡洛树搜索算法来探索数据，生成故事片段（数据事实），并根据信息论对其重要性进行排序。每个数据事实都配有自动生成的图表和描述。

Result: 通过三个示例故事、两次对照实验和 10 位领域专家的访谈评估表明，Calliope 可有效提高数据可视化故事的生成效率。

Conclusion: Calliope 能够通过自动化流程根据输入的电子表格创建可视化数据故事，并支持用户通过在线故事编辑器轻松修改，从而有效解决现有工具的不足。

Abstract: Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.

</details>


### [107] [Pen-based Interaction with Spreadsheets in Mobile Virtual Reality](https://arxiv.org/abs/2008.04543)
*Travis Gesslein,Verena Biener,Philipp Gagel,Daniel Schneider,Per Ola Kristensson,Eyal Ofek,Michel Pahud,Jens Grubert*

Main category: cs.HC

TL;DR: VR技术结合平板电脑和笔式输入，能有效提升移动知识工作者在电子表格应用中的交互体验和生产力。


<details>
  <summary>Details</summary>
Motivation: 尽管电子表格被广泛使用，但在移动设备上的交互却充满挑战，而VR在这一领域的应用尚未得到深入探索。特别是在VR提供的沉浸式大空间与移动办公有限的交互空间之间存在矛盾，需要弥合这一差距。

Method: 提出了一套结合沉浸式VR头显和笔式输入的工具集，用于增强平板电脑上的电子表格交互。该工具集利用平板电脑周围和前方的空间进行增强可视化，例如扩展表格显示范围、通过可视化隐藏的单元格依赖性来方便调试。结合精确的笔式输入和空间感知能力，提出了用于高效创建和编辑电子表格的工具，如屏幕外分层菜单、表格依赖性可视化、以及基于注视和触摸的切换电子表格标签等功能。

Result: 通过基于视频的在线调查和专家评估来研究该工具集的可行性，并评估了指示性的人类绩效潜力。

Conclusion: VR技术与平板电脑和笔式输入的结合，为增强电子表格交互的可能性开辟了道路，有望提高移动知识工作者的生产力。

Abstract: Virtual Reality (VR) can enhance the display and interaction of mobile knowledge work and in particular, spreadsheet applications. While spreadsheets are widely used yet are challenging to interact with, especially on mobile devices, using them in VR has not been explored in depth. A special uniqueness of the domain is the contrast between the immersive and large display space afforded by VR, contrasted by the very limited interaction space that may be afforded for the information worker on the go, such as an airplane seat or a small work-space. To close this gap, we present a tool-set for enhancing spreadsheet interaction on tablets using immersive VR headsets and pen-based input. This combination opens up many possibilities for enhancing the productivity for spreadsheet interaction. We propose to use the space around and in front of the tablet for enhanced visualization of spreadsheet data and meta-data. For example, extending sheet display beyond the bounds of the physical screen, or easier debugging by uncovering hidden dependencies between sheet's cells. Combining the precise on-screen input of a pen with spatial sensing around the tablet, we propose tools for the efficient creation and editing of spreadsheets functions such as off-the-screen layered menus, visualization of sheets dependencies, and gaze-and-touch-based switching between spreadsheet tabs. We study the feasibility of the proposed tool-set using a video-based online survey and an expert-based assessment of indicative human performance potential.

</details>


### [108] [EQUS -- helping to see formulae](https://arxiv.org/abs/2007.00003)
*Chris Roast*

Main category: cs.HC

TL;DR: EQUS是一款用于电子表格公式的可视化工具，旨在帮助用户理解复杂的公式。


<details>
  <summary>Details</summary>
Motivation: 电子表格广泛用于数值处理和建模，但其公式易于误解，因此需要一种可视化工具来提高理解度。

Method: 通过与目标用户（中学生）的迭代式设计和形成性评估来开发EQUS，并不断进行重新设计和评估。

Result: EQUS的可视化技术被证明对更广泛的电子表格用户具有相关性，并已作为MS Excel的插件进行开发。

Conclusion: EQUS作为一款可视化工具有助于提高电子表格用户的公式理解能力。

Abstract: Visualisation is often presented as a means of simplifying information and helping people understand complex data. In this paper we describe the design, development and evaluation of an interactive visualisation for spreadsheet formulae (EQUS). The work is justified on the grounds that these are widely used tools for significant numerical processing and modeling, yet the formula developed can be easily misunderstood. The development process was one of iterative refinement engaging an initial target audience of mid-teen learners, involving re-design and formative evaluation. The resulting visualisation techniques have been found to be broadly relevant to spreadsheet users beyond the initial target audience. EQUS has since been developed as fully integrated plug-in for MS Excel.

</details>


### [109] [Taggle: Combining Overview and Details in Tabular Data Visualizations](https://arxiv.org/abs/1712.05944)
*Katarina Furmanova,Samuel Gratzl,Holger Stitz,Thomas Zichner,Miroslava Jaresova,Alexander Lex,Marc Streit*

Main category: cs.HC

TL;DR: Taggle是一种表格可视化技术，通过可视化每一行数据并结合数据驱动的聚合和交互方法，来探索和呈现大型复杂表格，适用于药物发现等数据分析任务。


<details>
  <summary>Details</summary>
Motivation: 目前大多数表格数据可视化技术侧重于概览，而实际分析任务常需关注个体项，并将其与整个数据集关联，因此需要一种关注个体项的可视化技术。

Method: Taggle提供一种以个体项为中心、类似电子表格的可视化方法，为单元格使用视觉编码来单独可视化每一行。同时，它引入了数据驱动的数据子集聚合，并辅以专门用于回答特定分析问题的交互方法，如多列排序、丰富的数据选择和过滤。

Result: 通过药物发现领域专家对复杂基因组数据分析的案例研究，展示了Taggle在处理大型复杂表格方面的有效性。

Conclusion: Taggle通过其以个体项为中心的可视化、数据驱动的聚合以及定制的交互方法，能够有效地支持用户对大型复杂表格进行探索性分析。

Abstract: Most tabular data visualization techniques focus on overviews, yet many practical analysis tasks are concerned with investigating individual items of interest. At the same time, relating an item to the rest of a potentially large table is important. In this work we present Taggle, a tabular visualization technique for exploring and presenting large and complex tables. Taggle takes an item-centric, spreadsheet-like approach, visualizing each row in the source data individually using visual encodings for the cells. At the same time, Taggle introduces data-driven aggregation of data subsets. The aggregation strategy is complemented by interaction methods tailored to answer specific analysis questions, such as sorting based on multiple columns and rich data selection and filtering capabilities. We demonstrate Taggle using a case study conducted by a domain expert on complex genomics data analysis for the purpose of drug discovery.

</details>


### [110] [Are digital natives spreadsheet natives?](https://arxiv.org/abs/1909.00865)
*Maria Csernoch,Piroska Biró*

Main category: cs.HC

TL;DR: 学生在电子表格问题解决方面存在困难，需要基于算法的培训。


<details>
  <summary>Details</summary>
Motivation: 评估信息学一年级学生在电子表格环境中的算法技能和知识转移能力。

Method: 测试学生解决电子表格问题的能力，并分析他们使用的公式和函数。

Result: 学生在解决电子表格问题时遇到困难，使用基于算法的多层数组公式的学生表现更好。

Conclusion: 所有学生，无论其数字原生代身份如何，都需要正式的、高数学能力的、基于算法的培训。

Abstract: The present paper reports the results of testing first year students of Informatics on their algorithmic skills and knowledge transfer abilities in spreadsheet environments. The selection of students plays a crucial role in the project. On the one hand, they have officially finished their spreadsheet training - they know everything - while on the other hand, they do not need any training, since they are digital natives, to whom digital skills are assigned by birth. However, we found that the students had serious difficulties in solving the spreadsheet problems presented: so low were their results that it allowed us to form broad tendencies. Considering computational thinking, algorithmic skills, and knowledge transfer abilities, it is clear that those students performed better who used algorithm-based, multilevel array formulas instead of problem specific, unconnected built-in functions. Furthermore, we can conclude that students, regardless of their birth date and digital generation assigned to them, are in great need of official, high-mathability, algorithm-based training with expert teachers.

</details>


### [111] [Somewhere Around That Number: An Interview Study of How Spreadsheet Users Manage Uncertainty](https://arxiv.org/abs/1905.13072)
*Judith Borghouts,Andrew D. Gordon,Advait Sarkar,Kenton P. O'Hara,Neil Toronto*

Main category: cs.HC

TL;DR: 人们在电子表格中处理不确定性时，会受到电子表格的角色和用户的目标的影响，而目前的电子表格工具对这些需求的支持有限。


<details>
  <summary>Details</summary>
Motivation: 了解人们在电子表格中如何处理不确定性。

Method: 通过对11位来自不同领域的电子表格用户进行访谈。

Result: 用户的目标包括计算和比较不同情景、理解不确定性的性质以及将数据不确定性的复杂性简化为易于理解的演示。用户使用电子表格作为数据库、模板、计算工具、记事本和探索工具。然而，目前的电子表格工具支持这些目标的能力有限，用户需要采用各种变通方法。

Conclusion: 用户处理不确定性的方式受到电子表格的角色和用户目标的影响。目前的电子表格工具对这些需求的支持有限，需要进一步开发。

Abstract: Spreadsheet users regularly deal with uncertainty in their data, for example due to errors and estimates. While an insight into data uncertainty can help in making better informed decisions, prior research suggests that people often use informal heuristics to reason with probabilities, which leads to incorrect conclusions. Moreover, people often ignore or simplify uncertainty. To understand how people currently encounter and deal with uncertainty in spreadsheets, we conducted an interview study with 11 spreadsheet users from a range of domains. We found that how people deal with uncertainty is influenced by the role the spreadsheet plays in people's work and the user's aims. Spreadsheets are used as a database, template, calculation tool, notepad and exploration tool. In doing so, participants' aims were to compute and compare different scenarios, understand something about the nature of the uncertainty in their situation, and translate the complexity of data uncertainty into simplified presentations to other people, usually decision-makers. Spreadsheets currently provide limited tools to support these aims, and participants had various workarounds.

</details>


### [112] [Characterizing Scalability Issues in Spreadsheet Software using Online Forums](https://arxiv.org/abs/1801.03829)
*Kelly Mack,John Lee,Kevin Chang,Karrie Karahalios,Aditya Parameswaran*

Main category: cs.HC

TL;DR: 本研究通过抓取Reddit上的帖子，分析了用户在使用Excel时遇到的问题，特别是与处理大数据量相关的挑战，并为下一代电子表格软件的设计提供了启示。


<details>
  <summary>Details</summary>
Motivation: 鉴于传统用户研究成本高昂，本研究旨在探索一种利用在线论坛数据（如Reddit）的新型工具研究方法，以更具成本效益和广度的方式了解用户需求和挑战，特别是针对Excel电子表格的用户痛点。

Method: 本研究通过抓取Reddit网站上与Excel相关的帖子，收集用户的问题和抱怨，并对其中描述的用户在处理大量数据时遇到的挑战进行分类和分析。

Result: 研究发现了用户在使用Excel时遇到的具体问题，尤其是在处理大型数据集时，并对这些问题进行了归纳和表述。

Conclusion: 研究结果对设计下一代电子表格软件具有指导意义，有助于开发出更有效的解决方案来解决用户面临的挑战。

Abstract: In traditional usability studies, researchers talk to users of tools to understand their needs and challenges. Insights gained via such interviews offer context, detail, and background. Due to costs in time and money, we are beginning to see a new form of tool interrogation that prioritizes scale, cost, and breadth by utilizing existing data from online forums. In this case study, we set out to apply this method of using online forum data to a specific issue---challenges that users face with Excel spreadsheets. Spreadsheets are a versatile and powerful processing tool if used properly. However, with versatility and power come errors, from both users and the software, which make using spreadsheets less effective. By scraping posts from the website Reddit, we collected a dataset of questions and complaints about Excel. Specifically, we explored and characterized the issues users were facing with spreadsheet software in general, and in particular, as resulting from a large amount of data in their spreadsheets. We discuss the implications of our findings on the design of next-generation spreadsheet software.

</details>


### [113] [The Reification of an Incorrect and Inappropriate Spreadsheet Model](https://arxiv.org/abs/1801.10249)
*Grenville J. Croll*

Main category: cs.HC

TL;DR: 电子表格中的信息会被“实体化”，获得其本不应有的属性，如可信度、正确性等。我们通过一个案例研究，近距离观察了一个非营利组织中明显不正确和不恰当的电子表格模型是如何被实体化的。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨信息在电子表格中被“实体化”的现象，即信息一旦被加载到电子表格中，就会获得其本不应有的属性，如可信度、正确性、恰当性、具体性、完整性、实在性、客观性和权威性。

Method: 通过一个案例研究，近距离观察了一个非营利组织中明显不正确和不恰当的电子表格模型是如何被实体化的。

Result: 在案例研究中，我们观察到一个非营利组织使用了明显不正确和不恰当的电子表格模型，并且该模型的信息被实体化了。

Conclusion: 电子表格的实体化现象会导致信息获得不应有的属性，即使信息本身是错误的或不恰当的。

Abstract: Once information is loaded into a spreadsheet, it acquires properties that it may not deserve. These properties include believability, correctness, appropriateness, concreteness, integrity, tangibility, objectivity and authority. The information becomes reified. We describe a case study through which we were able to observe at close hand the reification of a demonstrably incorrect and inappropriate spreadsheet model within a small non profit organisation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [114] [Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE](https://arxiv.org/abs/2506.17330)
*Simon Thorne*

Main category: cs.SE

TL;DR: LLMs在电子表格任务方面表现不佳，需要集成符号推理能力。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在电子表格相关任务中的有效性。

Method: 提出一个全面的基准框架，涵盖公式创建、生成和数据操作任务。

Result: LLMs在简单任务上表现良好，但在复杂的多步操作上表现不佳，输出看似合理但错误。

Conclusion: 目前的LLMs在需要精确逻辑推理的电子表格任务方面存在局限性，需要将符号推理能力集成到LLM架构中。引入FLARE基准来评估LLM在真实世界电子表格逻辑、审计和推理任务上的表现。

Abstract: Large Language Models (LLMs) have demonstrated some significant capabilities across various domains; however, their effectiveness in spreadsheet related tasks remains underexplored. This study introduces a foundation for a comprehensive benchmark framework to evaluate the performance of leading LLMs in executing spreadsheet functions, formula generation and data manipulation tasks. The benchmark encompasses tasks ranging from basic formula creation to complex, real world spreadsheet scenarios. Our findings reveal that while LLMs exhibit proficiency in straightforward tasks, they often falter in complex, multi step operations, frequently producing plausible yet incorrect outputs. These results underscore the limitations of current LLMs in handling spreadsheet tasks that require precise logical reasoning and highlight the need for integrating symbolic reasoning capabilities into LLM architectures. To support this, we introduce FLARE (Formula Logic, Auditing, Reasoning and Evaluation) a new benchmark for evaluating LLM performance on real-world spreadsheet logic, auditing, and reasoning tasks.

</details>


### [115] [Overcoming Vision Language Model Challenges in Diagram Understanding: A Proof-of-Concept with XML-Driven Large Language Models Solutions](https://arxiv.org/abs/2502.04389)
*Shue Shiinoki,Ryo Koshihara,Hayato Motegi,Masumi Morishige*

Main category: cs.SE

TL;DR: 该研究提出了一种利用可编辑源文件（如 xlsx, pptx, docx）的文本元数据来解析图表信息的方法，绕过了对视觉语言模型（VLM）的依赖，并通过大语言模型（LLM）进行分析，提高了图表结构理解的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管目前的视觉语言模型在图像理解方面取得了进展，但准确识别和提取图表中的结构和关系仍然存在挑战。

Method: 提出了一种文本驱动的方法，利用可编辑源文件（xlsx, pptx, docx）中以文本元数据形式存在的图表元素，将提取的形状数据转换为大语言模型（LLM）的文本输入，以分析关系并回答问题。

Result: 通过将提取的形状数据转换为文本输入，LLM 能够分析关系并回答业务问题，实验结果表明，与基于 VLM 的方法相比，该文本驱动的框架在需要详细理解图表结构的问答任务上，能够提供更准确的答案。

Conclusion: 该研究证明了通过直接从原始源文件提取文本信息来规避 VLM 限制的可行性，并为通过 LLM 实现强大的图表理解提供了有前景的途径，有望提高实际业务场景中的工作流程效率和信息分析能力。

Abstract: Diagrams play a crucial role in visually conveying complex relationships and processes within business documentation. Despite recent advances in Vision-Language Models (VLMs) for various image understanding tasks, accurately identifying and extracting the structures and relationships depicted in diagrams continues to pose significant challenges. This study addresses these challenges by proposing a text-driven approach that bypasses reliance on VLMs' visual recognition capabilities. Instead, it utilizes the editable source files--such as xlsx, pptx or docx--where diagram elements (e.g., shapes, lines, annotations) are preserved as textual metadata. In our proof-of-concept, we extracted diagram information from xlsx-based system design documents and transformed the extracted shape data into textual input for Large Language Models (LLMs). This approach allowed the LLM to analyze relationships and generate responses to business-oriented questions without the bottleneck of image-based processing. Experimental comparisons with a VLM-based method demonstrated that the proposed text-driven framework yielded more accurate answers for questions requiring detailed comprehension of diagram structures.The results obtained in this study are not limited to the tested .xlsx files but can also be extended to diagrams in other documents with source files, such as Office pptx and docx formats. These findings highlight the feasibility of circumventing VLM constraints through direct textual extraction from original source files. By enabling robust diagram understanding through LLMs, our method offers a promising path toward enhanced workflow efficiency and information analysis in real-world business scenarios.

</details>


### [116] [On the Workflows and Smells of Leaderboard Operations (LBOps): An Exploratory Study of Foundation Model Leaderboards](https://arxiv.org/abs/2407.04065)
*Zhimin Zhao,Abdul Ali Bangash,Filipe Roseiro Côgo,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: FM榜单在软件工程（SE）领域对于比较和选择第三方FM至关重要，但缺乏标准化评估指南威胁到透明度和有效选择。


<details>
  <summary>Details</summary>
Motivation: 现有FM榜单缺乏标准化评估指南，影响了透明度和有效选择，需要研究榜单运作和识别改进点。

Method: 收集了来自GitHub、Hugging Face Spaces、Papers With Code、电子表格和独立平台等五个来源的1,045个FM榜单，通过卡片分类和协商一致，识别了五种工作流程模式，并开发了领域模型，同时识别了八种榜单异味。

Result: 识别了五种工作流程模式和八种榜单异味，为改善FM榜单的透明度、问责制和协作提供了基础。

Conclusion: 通过减轻已识别的榜单异味，可以提高当前榜单运作（LBOps）的透明度、问责制和协作，从而促进更强大、更负责任的FM比较和选择生态系统。

Abstract: Foundation models (FM), such as large language models (LLMs), which are large-scale machine learning (ML) models, have demonstrated remarkable adaptability in various downstream software engineering (SE) tasks, such as code completion, code understanding, and software development. As a result, FM leaderboards have become essential tools for SE teams to compare and select the best third-party FMs for their specific products and purposes. However, the lack of standardized guidelines for FM evaluation and comparison threatens the transparency of FM leaderboards and limits stakeholders' ability to perform effective FM selection. As a first step towards addressing this challenge, our research focuses on understanding how these FM leaderboards operate in real-world scenarios ("leaderboard operations") and identifying potential pitfalls and areas for improvement ("leaderboard smells"). In this regard, we collect up to 1,045 FM leaderboards from five different sources: GitHub, Hugging Face Spaces, Papers With Code, spreadsheet and independent platform, to examine their documentation and engage in direct communication with leaderboard operators to understand their workflows. Through card sorting and negotiated agreement, we identify five distinct workflow patterns and develop a domain model that captures the key components and their interactions within these workflows. We then identify eight unique types of leaderboard smells in LBOps. By mitigating these smells, SE teams can improve transparency, accountability, and collaboration in current LBOps practices, fostering a more robust and responsible ecosystem for FM comparison and selection.

</details>


### [117] [Will Dynamic Arrays finally change the way Models are built?](https://arxiv.org/abs/2006.14706)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: 电子表格虽然直观且易于使用，但容易出错。动态数组的引入可能通过减少手动更新来提高解决方案的完整性。


<details>
  <summary>Details</summary>
Motivation: 评估电子表格在严肃分析和建模任务中的适用性，特别是动态数组相对于传统技术在提高解决方案完整性方面的潜力。

Method: 探讨动态数组在更专业的开发环境中的应用，以取代传统技术，并强调它们如何减少手动干预和潜在错误。

Result: 动态数组模型需要较少的手动更新，因此有可能减少相关错误和风险，从而提高解决方案的完整性。

Conclusion: 动态数组的采用可以为需要解决方案完整性的专业开发环境带来显著改进，有望取代传统技术。

Abstract: Spreadsheets offer a supremely successful and intuitive means of processing and exchanging numerical content. Its intuitive ad-hoc nature makes it hugely popular for use in diverse areas including business and engineering, yet these very same characteristics make it extraordinarily error-prone; many would question whether it is suitable for serious analysis or modelling tasks. A previous EuSpRIG paper examined the role of Names in increasing solution transparency and providing a readable notation to forge links with the problem domain. Extensive use was made of CSE array formulas, but it is acknowledged that their use makes spreadsheet development a distinctly cumbersome task. Since that time, the new dynamic arrays have been introduced and array calculation is now the default mode of operation for Excel. This paper examines the thesis that their adoption within a more professional development environment could replace traditional techniques where solution integrity is important. A major advantage of fully dynamic models is that they require less manual intervention to keep them updated and so have the potential to reduce the attendant errors and risk.

</details>


### [118] [A Structured Approach to the development of Solutions in Excel](https://arxiv.org/abs/1704.01142)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: Excel 缺乏超越单元格公式的结构化能力，本文提出使用非常规技术来构建可扩展的 Excel 解决方案。


<details>
  <summary>Details</summary>
Motivation: Excel 普及了数字处理能力，但缺乏超越单单元格公式的结构化能力，导致难以扩展和出错。需要引入结构化方法来解决这个问题。

Method: 本文提出使用有争议或较少使用的技术，将 Excel 解决方案构建为一系列类似于编程语言步骤的公式。

Result: 通过一系列公式的顺序执行，解决了复杂问题，实现了类似编程语言的结构化。

Conclusion: 使用非常规的 Excel 技术可以为复杂问题提供一种结构化的解决方案策略，克服其固有的结构限制。

Abstract: Spreadsheets offer a supremely successful democratisation platform, placing the manipulation and presentation of numbers within the grasp of users that have little or no mathematical expertise or IT experience. What appears to be almost completely lacking within a "normal" solution built using Excel default settings is the deployment of any structure that extends beyond a single-cell formula. The structural elements that allow conventional code to scale without escalating errors appear to be absent. This paper considers the use of controversial or lesser-used techniques to create a coherent solution strategy in which the problem is solved by a sequence of formulas resembling the steps of a programmed language.

</details>


### [119] [Spreadsheet-based Configuration of Families of Real-Time Specifications](https://arxiv.org/abs/2310.20395)
*José Proença,David Pereira,Giann Spilere Nandi,Sina Borrami,Jonas Melchert*

Main category: cs.SE

TL;DR: 该论文提出了一种利用形式化模型和需求中的可变性来简化实时系统模型检测的方法，通过 Excel 电子表格配置模型，并自动生成和运行模型检测实例。


<details>
  <summary>Details</summary>
Motivation: 实时系统模型检测的复杂性以及在模型详细程度和避免状态爆炸之间的权衡，促使研究者探索更有效的方法。

Method: 利用形式化模型和需求中的可变性，通过结构化的 Excel 电子表格来配置模型变体，并开发原型工具自动处理电子表格、生成模型实例并运行模型检测。

Result: 开发了一个原型工具，能够处理 Excel 电子表格配置的模型变体，并自动执行模型检测，简化了模型检测流程，并在此基础上提出进一步利用有效特征组合进行分析的扩展。

Conclusion: 通过利用模型和需求中的可变性，并结合易于使用的电子表格界面，可以有效地简化实时系统的模型检测过程，同时保持了与模型检测器的简单交互。

Abstract: Model checking real-time systems is complex, and requires a careful trade-off between including enough detail to be useful and not too much detail to avoid state explosion. This work exploits variability of the formal model being analysed and the requirements being checked, to facilitate the model-checking of variations of real-time specifications.  This work results from the collaboration between academics and Alstom, a railway company with a concrete use-case, in the context of the VALU3S European project. The configuration of the variability of the formal specifications is described in MS Excel spreadsheets with a particular structure, making it easy to use also by developers. These spreadsheets are processed automatically by our prototype tool that generates instances and runs the model checker.  We propose the extension of our previous work by exploiting analysis over valid combination of features, while preserving the simplicity of a spreadsheet-based interface with the model checker.

</details>


### [120] [SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models](https://arxiv.org/abs/2305.19308)
*Hongxin Li,Jingran Su,Yuntao Chen,Qing Li,Zhaoxiang Zhang*

Main category: cs.SE

TL;DR: SheetCopilot 是一个使用自然语言控制电子表格的代理，可以自动化日常任务。


<details>
  <summary>Details</summary>
Motivation: 大多数终端用户缺乏自动化重复且易错的电子表格任务的技能，而大型语言模型（LLM）的出现使得用自然语言指挥软件成为可能。

Method: SheetCopilot 代理通过自然语言任务和控制电子表格来完成需求。它提出了一组原子动作来抽象电子表格功能，并设计了一个基于状态机的任务规划框架，用于 LLM 与电子表格的交互。此外，还构建了一个包含 221 个电子表格控制任务的数据集，并建立了全自动评估流程。

Result: SheetCopilot 在单个生成中能正确完成 44.3% 的任务，大大优于代码生成基线。

Conclusion: SheetCopilot 能够有效地自动化电子表格任务，并优于现有方法。

Abstract: Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill to automate these burdensome works. With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal. In this work, we propose a SheetCopilot agent that takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for LLMs to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of LLMs in software control tasks. Our SheetCopilot correctly completes 44.3\% of tasks for a single generation, outperforming the strong code generation baseline by a wide margin. Our project page:https://sheetcopilot.github.io/.

</details>


### [121] [Excel as a Turing-complete Functional Programming Environment](https://arxiv.org/abs/2309.00115)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: Excel 的动态数组功能带来了变革，催生了更接近于形式化编程的新型电子表格构建方法。


<details>
  <summary>Details</summary>
Motivation: 2018 年 Excel 引入动态数组功能后，电子表格的构建方式发生了重大变化，促使人们探索新的构建方法。

Method: 探讨了 Excel 社区中出现的、将传统电子表格的即席实践转变为更接近形式化编程的新方法，并讨论了这些新方法的潜在影响。

Result: 虽然尚不确定这些新功能在商业和工程领域的普及程度及其对风险的影响，但已出现一些新兴趋势。

Conclusion: Excel 的动态数组功能正在推动电子表格构建方法的革新，使其更趋向于形式化编程，未来发展值得关注。

Abstract: Since the calculation engine of Excel was the subject of a major upgrade to accommodate Dynamic Arrays in 2018 there has been a series of seismic changes to the art of building spreadsheet solutions. This paper will show the ad-hoc end user practices of traditional spreadsheets can be replaced by radically different approaches that have far more in common with formal programming. It is too early to guess the extent to which the new functionality will be adopted by the business and engineering communities and the impact that may have upon risk. Nevertheless, some trends are emerging from pioneering work within the Excel community which we will discuss here.

</details>


### [122] [A Use Case-Engineering Resources Taxonomy for Analytical Spreadsheet Models](https://arxiv.org/abs/2309.00104)
*Thomas A. Grossman,Vijay Mehrotra*

Main category: cs.SE

TL;DR: 本论文提出了一个分析型电子表格模型的分类法，扩展了现有的三类型分类法，识别出九种模型类型，并探讨了它们的性质、定义、与文献的联系以及可能产生的假设。该分类法有助于识别电子表格开发指南的适用性，并为理解电子表格错误、风险和演变提供了一个框架。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在为分析型电子表格模型提供一个全面的分类法，考虑到其用例和开发过程中投入的工程资源，以解决现有分类法的局限性。

Method: 通过扩展现有的三类型分类法，识别出九种分析型电子表格模型类型，并探讨它们的性质、定义、与文献的联系以及可能产生的假设。

Result: 识别出九种分析型电子表格模型类型，并阐述了它们各自的性质、定义、与文献的联系以及可能的产生途径。

Conclusion: 该分类法有助于识别不同电子表格开发指南的适用性，为审视电子表格错误和风险提供了一个视角，并为理解电子表格的长期演变提供了一个结构。此外，该分类法还为未来的研究开辟了道路。

Abstract: This paper presents a taxonomy for analytical spreadsheet models. It considers both the use case that a spreadsheet is meant to serve, and the engineering resources devoted to its development. We extend a previous three-type taxonomy, to identify nine types of spreadsheet models, that encompass the many analytical spreadsheet models seen in the literature. We connect disparate research literature to distinguish between an "analytical solution" and an "industrial-quality analytical spreadsheet model". We explore the nature of each of the nine types, propose definitions for some, relate them to the literature, and hypothesize on how they might arise. The taxonomy aids in identifying where various spreadsheet development guidelines are most useful, provides a lens for viewing spreadsheet errors and risk, and offers a structure for understanding how spreadsheets change over time. This taxonomy opens the door to many interesting research questions, including refinements to itself.

</details>


### [123] [Experimenting with ChatGPT for Spreadsheet Formula Generation: Evidence of Risk in AI Generated Spreadsheets](https://arxiv.org/abs/2309.00095)
*Simon Thorne*

Main category: cs.SE

TL;DR: ChatGPT在处理简单问题时能生成正确的电子表格公式，但在信息不确定或问题复杂时准确性会下降，并可能产生“幻觉”。


<details>
  <summary>Details</summary>
Motivation: 探索ChatGPT在无背景知识的情况下生成电子表格公式的能力。

Method: 通过一系列实验，测试ChatGPT在不同信息复杂度下的电子表格公式生成能力。

Result: 在信息不确定或问题复杂的情况下，ChatGPT生成电子表格公式的准确性下降，并出现推理、推断和演绎能力下降，甚至产生“幻觉”现象。

Conclusion: ChatGPT在生成电子表格公式方面有一定潜力，但在信息限制、不确定性或复杂问题面前，其准确性和推理能力会受到限制。

Abstract: Large Language Models (LLM) have become sophisticated enough that complex computer programs can be created through interpretation of plain English sentences and implemented in a variety of modern languages such as Python, Java Script, C++ and Spreadsheets. These tools are powerful and relatively accurate and therefore provide broad access to computer programming regardless of the background or knowledge of the individual using them. This paper presents a series of experiments with ChatGPT to explore the tool's ability to produce valid spreadsheet formulae and related computational outputs in situations where ChatGPT has to deduce, infer and problem solve the answer. The results show that in certain circumstances, ChatGPT can produce correct spreadsheet formulae with correct reasoning, deduction and inference. However, when information is limited, uncertain or the problem is too complex, the accuracy of ChatGPT breaks down as does its ability to reason, infer and deduce. This can also result in false statements and "hallucinations" that all subvert the process of creating spreadsheet formulae.

</details>


### [124] [Demonstration of CORNET: A System For Learning Spreadsheet Formatting Rules By Example](https://arxiv.org/abs/2308.07357)
*Mukul Singh,Jose Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Gust Verbruggen*

Main category: cs.SE

TL;DR: CORNET是一个系统，可以根据用户提供的示例自动学习条件格式规则。


<details>
  <summary>Details</summary>
Motivation: 电子表格软件的条件格式功能需要用户手动编写规则，存在不便。CORNET旨在自动化这一过程。

Method: CORNET结合了归纳程序合成、基于半监督聚类和迭代决策树学习的符号规则枚举，以及用于生成准确条件格式规则的神经网络排序器。

Result: CORNET可以根据用户提供的少量示例，为Microsoft Excel生成条件格式规则建议。

Conclusion: CORNET作为Microsoft Excel的插件，能够通过学习用户示例来自动生成条件格式规则，简化用户操作。

Abstract: Data management and analysis tasks are often carried out using spreadsheet software. A popular feature in most spreadsheet platforms is the ability to define data-dependent formatting rules. These rules can express actions such as "color red all entries in a column that are negative" or "bold all rows not containing error or failure." Unfortunately, users who want to exercise this functionality need to manually write these conditional formatting (CF) rules. We introduce CORNET, a system that automatically learns such conditional formatting rules from user examples. CORNET takes inspiration from inductive program synthesis and combines symbolic rule enumeration, based on semi-supervised clustering and iterative decision tree learning, with a neural ranker to produce accurate conditional formatting rules. In this demonstration, we show CORNET in action as a simple add-in to Microsoft Excel. After the user provides one or two formatted cells as examples, CORNET generates formatting rule suggestions for the user to apply to the spreadsheet.

</details>


### [125] [Excel Spreadsheet Analyzer](https://arxiv.org/abs/2211.06333)
*Amir Nassereldine,Patrick Chen,Jinjun Xiong*

Main category: cs.SE

TL;DR: 该工具旨在通过创建电子表格的抽象中间表示（AIR）来弥补电子表格和Python之间的数据分析鸿沟，从而在迁移到Python进行分析时保留单元格之间的依赖关系。


<details>
  <summary>Details</summary>
Motivation: 许多公司依赖电子表格进行数值分析，但其缺乏数据科学界所需的丰富库和社区支持。从电子表格迁移到Python等科学编程语言会丢失公式和单元格依赖关系等关键信息。

Method: 提出了一种创建电子表格抽象中间表示（AIR）的工具，该表示能保留数据间的相互依赖关系。在此基础上构建了一个Python库，以便在Python中进行数据分析。

Result: 该工具能够生成电子表格的AIR，保留了单元格间的依赖关系。基于此AIR的Python库允许用户在Python环境中进行数据分析。

Conclusion: 该工具及其Python库能够有效地将电子表格数据迁移到Python环境，同时保留重要的公式和依赖关系信息，从而在数据分析中实现无缝衔接。

Abstract: Spreadsheets are widely used in various fields to do large numerical analysis. While several companies have relied on spreadsheets for decades, data scientists are going in the direction of using scientific programming languages such as python to do their data analysis due to the support, community, and vast amount of libraries. While using python to analyze a company's spreadsheets, some information such as the formulas and dependencies of a cell are lost. We propose a tool that creates an abstract intermediate representation (AIR) of a spreadsheet. This representation facilitates the transfer from spreadsheets into scientific programming languages while preserving inter-dependency information about data. In addition to that, we build a python library on top of our tool to perform some data analysis in python.

</details>


### [126] [Exploring Spreadsheet Use and Practices in a Technologically Constrained Setting](https://arxiv.org/abs/2201.07696)
*Khwima Mckinley Mkamanga,Simon Thorne*

Main category: cs.SE

TL;DR: This paper examines how spreadsheets impact business operations in a Malawian water utility, highlighting both automation benefits and significant risks due to management, technological, and human factors. It concludes that comprehensive policies are needed to govern spreadsheet use and mitigate risks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the impacts of spreadsheets on business operations within a water utility parastatal in Malawi, a context representative of semi-governmental bodies in technologically underdeveloped countries. It seeks to understand the scope and lifecycle of spreadsheet usage, as well as related organizational policy and governance, to inform future practices and risk management strategies.

Method: The study focused on analyzing the scope of use and life cycle of spreadsheets, alongside organizational policy and governance within the water utility. While the specific methodologies are not detailed, the focus indicates a qualitative or mixed-methods approach to understanding spreadsheet usage and its implications.

Result: Findings indicate that the widespread use of spreadsheets has facilitated business automation within the organization. However, the research also identified significant risks stemming from management, technological, and human factors associated with this pervasive usage.

Conclusion: The research confirms that substantial improvements are necessary, particularly in establishing comprehensive policies and regulations to govern the development and adoption of spreadsheets within the organization, in order to manage the associated risks effectively.

Abstract: This paper explores the impacts of spreadsheets on business operations in a water utility parastatal in Malawi, Sub-Saharan Africa. The organisation is a typical example of a semi-government body operating in a technologically underdeveloped country. The study focused on spreadsheet scope of use and life cycle as well as organisational policy and governance. The results will help define future spreadsheet usage by influencing new approaches for managing potential risks associated with spreadsheets in the organization. Generally, findings indicate that the proliferation of spreadsheets in the organization has provided an enabling environment for business automation. The paper also highlights management, technological and human factor issues contributing to high risks associated with the pervasive spreadsheet use. The conclusions drawn from the research confirms that there is ample room for improvement in many areas such as implementation of comprehensive policies and regulations governing spreadsheet development processes and adoption.

</details>


### [127] [Methodology for Assessing the State of the Practice for Domain X](https://arxiv.org/abs/2110.11575)
*Spencer Smith,Jacques Carette,Peter Michalski,Ao Dong,Olu Owojaiye*

Main category: cs.SE

TL;DR: 本文提出了一种评估研究软件开发实践状况的方法。


<details>
  <summary>Details</summary>
Motivation: 为了改进研究软件的开发方法和工具，需要了解其目前的实践状况。

Method: 该方法包括识别领域、筛选软件包、收集代码和文档、收集仓库数据、填写测量模板、访谈开发者、使用 AHP 进行排名以及分析数据。

Result: 通过该方法，可以回答关于研究软件开发实践的特定问题，例如存在的工件、使用的工具、采用的原则和流程、开发者的痛点以及用于提高可维护性和可复现性等质量的措施。

Conclusion: 该方法为评估研究软件开发实践提供了一个结构化的框架，并估计完成一项评估需要 173 人时。

Abstract: To improve software development methods and tools for research software, we first need to understand the current state of the practice. Therefore, we have developed a methodology for assessing the state of the software development practices for a given research software domain. For each domain we wish to answer questions such as: i) What artifacts (documents, code, test cases, etc.) are present? ii) What tools are used? iii) What principles, process and methodologies are used? iv) What are the pain points for developers? v) What actions are used to improve qualities like maintainability and reproducibility? To answer these questions, our methodology prescribes the following steps: i) Identify the domain; ii) Identify a list of candidate software packages; iii) Filter the list to a length of about 30 packages; iv) Gather source code and documentation for each package; v) Collect repository related data on each software package, like number of stars, number of open issues, number of lines of code; vi) Fill in the measurement template (the template consists of 108 questions to assess 9 qualities (including the qualities of installability, usability and visibility)); vii) Interview developers (the interview consists of 20 questions and takes about an hour); viii) Rank the software using the Analytic Hierarchy Process (AHP); and, ix) Analyze the data to answer the questions posed above. A domain expert should be engaged throughout the process, to ensure that implicit information about the domain is properly represented and to assist with conducting an analysis of the commonalities and variabilities between the 30 selected packages. Using our methodology, spreadsheet templates and AHP tool, we estimate (based on our experience with using the process) the time to complete an assessment for a given domain at 173 person hours.

</details>


### [128] [Agile Elicitation of Scalability Requirements for Open Systems: A Case Study](https://arxiv.org/abs/2108.00567)
*Gunnar Brataas,Antonio Martini,Geir Kjetil Hanssen,Georg Ræder*

Main category: cs.SE

TL;DR: ScrumScale模型是一个用于敏捷软件开发的可伸缩性需求获取的工具。


<details>
  <summary>Details</summary>
Motivation: 软件开发中获取可伸缩性需求的方法研究不足。

Method: 提出ScrumScale模型，一个基于协调理论的电子表格工具，并通过开放银行案例研究进行验证。

Result: 在开放银行案例研究中，ScrumScale模型被用于需求获取，历时55小时，被认为提供了一种系统化的方法，并促进了与其他利益相关者的沟通。

Conclusion: ScrumScale模型为敏捷软件开发中的可伸缩性需求获取提供了一个轻量级且有效的解决方案。

Abstract: Eliciting scalability requirements during agile software development is complicated and poorly described in previous research. This article presents a lightweight artifact for eliciting scalability requirements during agile software development: the ScrumScale model. The ScrumScale model is a simple spreadsheet. The scalability concepts underlying the ScrumScale model are clarified in this design science research, which also utilizes coordination theory. This paper describes the open banking case study, where a legacy banking system becomes open. This challenges the scalability of this legacy system. The first step in understanding this challenge is to elicit the new scalability requirements. In the open banking case study, key stakeholders from TietoEVRY spent 55 hours eliciting TietoEVRY's open banking project's scalability requirements. According to TietoEVRY, the ScrumScale model provided a systematic way of producing scalability requirements. For TietoEVRY, the scalability concepts behind the ScrumScale model also offered significant advantages in dialogues with other stakeholders.

</details>


### [129] [SpreadsheetCoder: Formula Prediction from Semi-structured Context](https://arxiv.org/abs/2106.15339)
*Xinyun Chen,Petros Maniatis,Rishabh Singh,Charles Sutton,Hanjun Dai,Max Lin,Denny Zhou*

Main category: cs.SE

TL;DR: SpreadsheetCoder是一个基于BERT的模型，可以从表格数据（包括表头和半结构化数据）中合成电子表格公式。


<details>
  <summary>Details</summary>
Motivation: 以往的电子表格公式合成方法仅使用输入输出示例，忽略了表格数据中的丰富上下文（如行列依赖性和表头信息）。

Method: 提出了一种名为SpreadsheetCoder的新型模型架构，该架构基于BERT，能够同时表示行和列的表格上下文，并使用包含表头和半结构化数据的电子表格大型数据集进行训练。

Result: SpreadsheetCoder在表格上下文公式预测方面达到了42.51%的准确率，显著优于不使用丰富上下文的基线方法。

Conclusion: SpreadsheetCoder在实际应用中表现出色，与基于规则的系统相比，在Google表格上辅助了82%更多的用户编写公式。

Abstract: Spreadsheet formula prediction has been an important program synthesis problem with many real-world applications. Previous works typically utilize input-output examples as the specification for spreadsheet formula synthesis, where each input-output pair simulates a separate row in the spreadsheet. However, this formulation does not fully capture the rich context in real-world spreadsheets. First, spreadsheet data entries are organized as tables, thus rows and columns are not necessarily independent from each other. In addition, many spreadsheet tables include headers, which provide high-level descriptions of the cell data. However, previous synthesis approaches do not consider headers as part of the specification. In this work, we present the first approach for synthesizing spreadsheet formulas from tabular context, which includes both headers and semi-structured tabular data. In particular, we propose SpreadsheetCoder, a BERT-based model architecture to represent the tabular context in both row-based and column-based formats. We train our model on a large dataset of spreadsheets, and demonstrate that SpreadsheetCoder achieves top-1 prediction accuracy of 42.51%, which is a considerable improvement over baselines that do not employ rich tabular context. Compared to the rule-based system, SpreadsheetCoder assists 82% more users in composing formulas on Google Sheets.

</details>


### [130] [From webtables to datatables](https://arxiv.org/abs/2006.14694)
*Mária Csernoch*

Main category: cs.SE

TL;DR: 该论文详细介绍了如何将网页表格（Webtables）转换为可编辑的表格格式，并提供了两种实现方案：一种使用文字处理器，另一种完全使用电子表格应用程序。


<details>
  <summary>Details</summary>
Motivation: 网页表格是教授电子表格技能、在商业和专业组织中应用和发展知识转移项目、展示和处理各种现实世界问题及解决方案、进行讨论和调试，以及发展和利用计算思维技能的极佳资源。

Method: 提出了一种转换算法，并提供了两种具体的实现方案：一种使用文字处理器，另一种完全使用电子表格应用程序。

Result: 提供了两种转换网页表格的解决方案，并为进一步的讨论、创新和其他解决方案的结合留下了空间。

Conclusion: 网页表格是培养和应用各种与电子表格相关的技能的宝贵资源，通过具体的转换案例，可以促进计算思维和解决实际问题的能力。

Abstract: Webtables -- tables and table-like structures on webpages -- are excellent sources for teaching spreadsheeting, in commercial and professional organisations by utilizing and developing knowledge-transfer items, presenting and handling various real-world problems and solutions, discussing and debugging, and in general, developing and utilizing computational thinking skills. In the present paper the conversion process of one of the LOL Boards (League of Legends, Riot Games Inc. 2019) is detailed. After presenting the algorithm of the conversion, two solutions are offered -- one in a word processor, the other purely in a spreadsheet application -- leaving space for discussions, inventing other solutions and combining them.

</details>


### [131] [Implementation Strategies for Multidimensional Spreadsheets](https://arxiv.org/abs/2006.05814)
*Paul Mireault*

Main category: cs.SE

TL;DR: Excel 开发者在实施多维变量时，主要采用两种策略：一是将多维变量投影到二维平面，二是采用数据库方法，将多维变量表示为数据集表，并使用主键，这种方法能简化公式。


<details>
  <summary>Details</summary>
Motivation: 探究 Excel 开发者在处理多维变量时的实施策略。

Method: 分析参与挑战的 Excel 开发者实施多维变量的电子表格，识别其策略。

Result: 大部分参与者采用投影策略，少数参与者采用数据库方法，后者能简化公式。

Conclusion: 对于在 Excel 中处理多维变量，数据库方法是一种更优的策略，可以简化公式的编写。

Abstract: Seasoned Excel developers were invited to participate in a challenge to implement a spreadsheet with multi-dimensional variables. We analyzed their spreadsheet to see the different implement strategies employed. We identified two strategies: most participants used a projection of three or four-dimensional variables on the two-dimensional plane used by Excel. A few participants used a database approach where the multi-dimensional variables are presented in the form of a dataset table with the appropriate primary key. This approach leads to simpler formulas.

</details>


### [132] [Abstracting spreadsheet data flow through hypergraph redrawing](https://arxiv.org/abs/2006.04794)
*David Birch,Nicolai Stawinoga,Jack Binks,Bruno Nicoletti,Paul Kelly*

Main category: cs.SE

TL;DR: 传统的电子表格容易出错是因为其抽象层次低。本文提出通过“重新划定单元格边界”来提高电子表格的抽象层次，将电子表格转换为细粒度图，并将单元格表示为围绕一组操作符/数据节点的超图边。


<details>
  <summary>Details</summary>
Motivation: 传统的电子表格抽象层次低，用户被迫从低层单元格构建数据模型，而单元格只能包含标量值，并且单元格之间的链接是隐藏的。

Method: 将电子表格转换为细粒度图，操作符和值作为节点，单元格表示为超图边。提出重新划定单元格边界的技术，以创建更高层次的单元格。

Result: 通过通用子表达式识别和子树同构在检测向量（数组）操作方面的应用，说明了该方法。

Conclusion: 提高电子表格的抽象层次，使之更忠实地表示最终用户的现实世界/心智模型。

Abstract: We believe the error prone nature of traditional spreadsheets is due to their low level of abstraction. End user programmers are forced to construct their data models from low level cells which we define as "a data container or manipulator linked by user-intent to model their world and positioned to reflect its structure". Spreadsheet cells are limited in what they may contain (scalar values) and the links between them are inherently hidden. This paper proposes a method of raising the level of abstraction of spreadsheets by "redrawing the boundary" of the cell. To expose the hidden linkage structure we transform spreadsheets into fine-grained graphs with operators and values as nodes. "cells" are then represented as hypergraph edges by drawing a boundary "wall" around a set of operator/data nodes. To extend what cells may contain and to create a higher level model of the spreadsheet we propose that researchers should seek techniques to redraw these boundaries to create higher level "cells" which will more faithfully represent the end-user's real world/mental model. We illustrate this approach via common sub-expression identification and the application of sub-tree isomorphisms for the detection of vector (array) operations.

</details>


### [133] [Comprehensive review for common types of errors using spreadsheets](https://arxiv.org/abs/1912.09209)
*Ali Aburas*

Main category: cs.SE

TL;DR: 本研究全面回顾和分类了电子表格错误查找和修复的方法，并讨论了研究方法、错误类型以及用户常犯的错误。


<details>
  <summary>Details</summary>
Motivation: 鉴于电子表格的广泛应用及其普遍存在的错误问题，本研究旨在全面梳理和分类现有的错误查找和修复方法。

Method: 对关于电子表格错误查找和修复的最新研究方法进行描述和分类，涵盖其定义、工作原理和能发现的错误类型，并探讨用户常犯的错误。

Result: 提供了对电子表格错误查找和修复方法的全面概述，包括它们的定义、工作原理、适用错误类型以及用户常见错误。

Conclusion: 对电子表格错误查找和修复的研究进行了系统性梳理，为理解和改进电子表格的准确性提供了参考。

Abstract: Thanks to their flexibility and capability to perform different tasks and organize data in the best form and format, spreadsheets are widely used in different organizations and by different end users. Many business organizations rely on spreadsheets to fulfill their various tasks. On the other hand, the number of spreadsheets that contain errors are very high, thus researchers have developed different tools aimed at the prevention, detection, and correction of errors in spreadsheets. This research work is a comprehensive review that describes and classifies approaches on finding and fixing errors in spreadsheets. The paper discusses up-to-date research work approaches in terms of definition, how they work, and kinds of errors they can find in spreadsheets. The paper looks also for the kinds of errors that end users commonly make in spreadsheets.

</details>


### [134] [A Coding-free Software Framework of Developing Web Data Management Systems](https://arxiv.org/abs/1910.05685)
*Can Yang,Shiying Pan,Runmin Li,Yu Liu,Lizhang Peng*

Main category: cs.SE

TL;DR: 本论文提出了一种无需编码即可构建数据管理系统的理论和方法，并提供了一个实际的应用平台、一套构建方法和一套数据交换接口。


<details>
  <summary>Details</summary>
Motivation: 由于软件开发的专业性，即使是小型系统，对于非程序员来说开发数据管理系统仍然很困难。SaaS的兴起为无代码开发带来了可行性。

Method: 基于SaaS架构，通过抽象数据管理系统的共性功能，设计了一个通用的Web平台，用于快速生成和发布定制化的系统实例。提出了一种使用电子表格中的特定需求表来开发数据管理系统的方法。该平台通过解析表模型并在运行阶段实现目标系统来映射需求表。

Result: 实现并部署了所提出的框架。实验结果证明了在Web数据管理系统开发中无需编码的方法的可行性和可用性。

Conclusion: 本研究提出的无需编码的方法在开发Web数据管理系统方面是可行且可用的。

Abstract: More and more enterprises recently intend to deploy data management systems in the cloud. Due to the professionalism of software development, it has still been difficult for non-programmers to develop this kind of systems, even a small one. However, the development of SaaS brings forth the more feasibility of coding-free software development than before. Based on the SaaS architecture, this paper presents a set of theory and method for coding-free construction of a data management system, on which our contributions involve in a practical application platform, a set of construction method and a set of interface on data exchange. By abstracting the common features of data management systems, we design a universal web platform to quickly generate and publish customized system instances. Moreover, we propose a kind of method to develop a data management system using a specific requirements table in spreadsheet. The corresponding platform maps the requirements table into a system instance through parsing the table model and implementing the objective system in the running stage. Finally, we implement the proposed framework and deploy it on web. The empirical result demonstrates the feasibility and availability of the coding-free method in developing web data management systems.

</details>


### [135] [Structured Spreadsheet Modelling and Implementation with Multiple Dimensions -- Part 2: Implementation](https://arxiv.org/abs/1909.00891)
*Paul Mireault*

Main category: cs.SE

TL;DR: 本文介绍了如何实现一个多维问题，以生成易于维护的电子表格。


<details>
  <summary>Details</summary>
Motivation: 实现多维问题，并生成易于维护的电子表格。

Method: 提供具体的实现步骤。

Result: 生成易于维护的电子表格。

Conclusion: 本文介绍了实现多维问题的具体步骤，以生成易于维护的电子表格。

Abstract: In Part 1, we showed how to develop a conceptual model of a problem involving variables of multiple dimensions, like Products, Regions, Sectors and Months. The conceptual model is presented as a Formula Diagram, giving a global view of the interaction between all the variables, and a Formula List, giving a precise view of the interaction between the variables. In this paper, we present precise steps to implement a multi-dimensional problem in a way that will produce a spreadsheet that is easy to maintain

</details>


### [136] [On the Refinement of Spreadsheet Smells by means of Structure Information](https://arxiv.org/abs/1810.04542)
*Patrick Koch,Birgit Hofer,Franz Wotawa*

Main category: cs.SE

TL;DR: 现有的电子表格异味检测技术存在缺陷，导致报告不准确或冗余。本文提出通过推理结构信息来优化异味检测，包括改进现有异味检测方法并引入新的检测技术，以减少误报并发现新的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的电子表格异味检测技术存在缺陷，会导致不准确或冗余的报告，用户难以理解和维护，并且容易出错。

Method: 提出一种静态分析方法来推断相关单元格的簇和块，并利用这些结构信息来改进现有的异味检测技术（例如，通过考虑单元格组和计算块）并引入三种新的异味检测技术。

Result: 通过实证评估，改进后的技术成功减少了不正确和冗余的异味报告，并且新引入的异味检测技术发现了新的问题。

Conclusion: 通过利用推断的电子表格结构信息，可以改进电子表格的异味检测，从而提高报告的准确性和有效性。

Abstract: Spreadsheet users are often unaware of the risks imposed by poorly designed spreadsheets. One way to assess spreadsheet quality is to detect smells which attempt to identify parts of spreadsheets that are hard to comprehend or maintain and which are more likely to be the root source of bugs. Unfortunately, current spreadsheet smell detection techniques suffer from a number of drawbacks that lead to incorrect or redundant smell reports. For example, the same quality issue is often reported for every copy of a cell, which may overwhelm users. To deal with these issues, we propose to refine spreadsheet smells by exploiting inferred structural information for smell detection. We therefore first provide a detailed description of our static analysis approach to infer clusters and blocks of related cells. We then elaborate on how to improve existing smells by providing three example refinements of existing smells that incorporate information about cell groups and computation blocks. Furthermore, we propose three novel smell detection techniques that make use of the inferred spreadsheet structures. Empirical evaluation of the proposed techniques suggests that the refinements successfully reduce the number of incorrectly and redundantly reported smells, and novel deficits are revealed by the newly introduced smells.

</details>


### [137] [Now You're Thinking With Structures: A Concept for Structure-based Interactions with Spreadsheets](https://arxiv.org/abs/1809.03435)
*Patrick Koch*

Main category: cs.SE

TL;DR: Spreadsheets are hard to understand when complex. This paper presents a concept for structure-aware understanding and interaction with spreadsheets, using structural information to enhance visualizations and user actions. An implemented tool for structure inference and visualization is the first step, with plans to introduce proactive and reactive interaction mechanics and provide structure-aware functionality as an add-in. The goal is to improve user productivity and spreadsheet quality.


<details>
  <summary>Details</summary>
Motivation: Complex spreadsheets are difficult to comprehend and adapt. A higher-order mental model can facilitate the cognition of complex systems.

Method: The paper proposes a concept for structure-aware understanding and interaction with spreadsheets by extending previous work on structure inference. Structural information is used to enrich visualizations, enhance traditional user actions reactively, and provide tools to proactively alter the spreadsheet makeup. An initial tool for structure inference and visualization has been implemented.

Result: An initial tool for structure inference and visualization has been implemented, adhering to the common spreadsheet layout. This serves as a framework for future development.

Conclusion: Providing tools for thinking about and interacting with spreadsheets in a structure-aware manner is believed to benefit users by improving productivity and overall spreadsheet quality. The work aims to add a layer of functionality alongside the established spreadsheet interface.

Abstract: Spreadsheets are the go-to tool for computerized calculation and modelling, but are hard to comprehend and adapt after reaching a certain complexity. In general, cognition of complex systems is facilitated by having a higher order mental model of the system in question to work with. We therefore present a concept for structure-aware understanding of and interaction with spreadsheets that extends previous work on structure inference in the domain. Following this concept, structural information is used to enrich visualizations, reactively enhance traditional user actions, and provide tools to proactively alter the overall spreadsheet makeup instead of individual cells The intended systems should, in first approximation, not replace common spreadsheet tools, but provide an additional layer of functionality alongside the established interface. In ongoing work, we therefore implemented a tool for structure inference and visualization along the common spreadsheet layout. Based on this framework, we plan to introduce the envisioned proactive and reactive interaction mechanics, and finally provide structure-aware unctionality as an add-in for common spreadsheet processors. We believe that providing the tools for thinking about and interacting with spreadsheets in this manner will benefit users both in terms of productivity and overall spreadsheet quality.

</details>


### [138] [Typed Table Transformations](https://arxiv.org/abs/1809.02746)
*Martin Erwig*

Main category: cs.SE

TL;DR: Spreadsheet tables can be transformed by manipulating row and column types, forming a basis for future research.


<details>
  <summary>Details</summary>
Motivation: The paper aims to present a new approach to transforming spreadsheet tables based on the manipulation of row and column types, viewing spreadsheet tables as built from typed data.

Method: The paper introduces a new approach to transforming spreadsheet tables by focusing on the transformations of row and column types, illustrating the basic idea of type-based table construction and transformation.

Result: The paper illustrates the basic idea of type-based table construction and transformation.

Conclusion: The paper lays out a series of research questions that need to be addressed in future work regarding type-based table transformation.

Abstract: Spreadsheet tables are often labeled, and these labels effectively constitute types for the data in the table. In such cases tables can be considered to be built from typed data where the placement of values within the table is controlled by the types used for rows and columns. We present a new approach to the transformations of spreadsheet tables that is based on transformations of row and column types. We illustrate the basic idea of type-based table construction and transformation and lay out a series of research questions that should be addressed in future work.

</details>


### [139] [Implementing WHERE and ORDER BY as spreadsheet formulas](https://arxiv.org/abs/1809.00025)
*Paul Mireault*

Main category: cs.SE

TL;DR: SQL的WHERE和ORDER BY子句在电子表格中实现


<details>
  <summary>Details</summary>
Motivation: SQL的WHERE和ORDER BY子句在电子表格中实现，以解决其不能自动响应计算值变化的问题。

Method: 开发电子表格公式来模拟SQL的WHERE和ORDER BY子句。

Result: 在电子表格中实现了SQL的WHERE和ORDER BY子句的功能。

Conclusion: 所开发的电子表格公式能够有效地在电子表格中实现SQL的WHERE和ORDER BY子句的功能，并自动响应计算值变化。

Abstract: The WHERE and ORDER BY clauses of the SQL SELECT statement select a subset of rows in the result of a database query and present the result in the specified order. In a spreadsheet program like Microsoft Excel, one could use the filter and sort buttons, or use its Query or its Pivot Table tools to achieve a similar effect. The disadvantage of using those tools is that they don't react automatically to changes in the calculated values of the spreadsheet. In this paper, we develop spreadsheet formulas that implement SQL's WHERE and ORDER BY clauses.

</details>


### [140] [The use of Charts, Pivot Tables, and Array Formulas in two Popular Spreadsheet Corpora](https://arxiv.org/abs/1808.10642)
*Bas Jansen,Felienne Hermans*

Main category: cs.SE

TL;DR: 公司在决策时广泛使用电子表格，但电子表格极易出错，可能导致决策失误和经济损失。虽然现有研究大多集中在公式的错误检查，但对图表、数据透视表和数组公式等其他电子表格功能的研究却很少。为了提高电子表格的质量，必须理解这些功能的使用情况，并将其纳入相关研究。本文旨在分析 Enron 和 EUSES 这两个流行的电子表格语料库中这些功能的使用情况。


<details>
  <summary>Details</summary>
Motivation: 电子表格在工业界得到广泛应用，然而其固有的易错性可能导致公司基于错误信息做出决策，造成经济损失。尽管现有研究主要关注公式的易错性，但对图表、数据透视表和数组公式等其他电子表格功能的研究却十分有限。因此，为了提升电子表格的质量，有必要深入理解这些功能的使用方式，并将其纳入研究范畴。

Method: 通过分析 Enron 和 EUSES 这两个广泛使用的电子表格语料库，研究图表、数据透视表和数组公式等电子表格功能的使用情况。

Result: 对 Enron 和 EUSES 两个电子表格语料库的分析结果。

Conclusion: 为了全面提升电子表格质量，研究者需要将图表、数据透视表和数组公式等在内的电子表格功能的使用情况纳入研究范围，因为这些功能在支持决策方面发挥着重要作用。

Abstract: The use of spreadsheets in industry is widespread. Companies base decisions on information coming from spreadsheets. Unfortunately, spreadsheets are error-prone and this increases the risk that companies base their decisions on inaccurate information, which can lead to incorrect decisions and loss of money. In general, spreadsheet research is aimed to reduce the error-proneness of spreadsheets. Most research is concentrated on the use of formulas. However, there are other constructions in spreadsheets, like charts, pivot tables, and array formulas, that are also used to present decision support information to the user. There is almost no research about how these constructions are used. To improve spreadsheet quality it is important to understand how spreadsheets are used and to obtain a complete understanding, the use of charts, pivot tables, and array formulas should be included in research. In this paper, we analyze two popular spreadsheet corpora: Enron and EUSES on the use of the aforementioned constructions.

</details>


### [141] [Asheetoxy: A Taxonomy for Classifying Negative Spreadsheet-related Phenomena](https://arxiv.org/abs/1808.10231)
*Daniel Kulesz,Stefan Wagner*

Main category: cs.SE

TL;DR: Asheetoxy 是一种新的电子表格错误分类法，它不使用“错误”一词，并且易于分类。


<details>
  <summary>Details</summary>
Motivation: 许多组织的关键业务依赖于电子表格，但电子表格中的错误导致了许多糟糕的决策。然而，研究人员和专业人士在研究电子表格错误时，缺乏一个普遍接受的分类法，并且现有的分类法存在一些限制。

Method: 提出了一种名为 Asheetoxy 的新分类法，该分类法侧重于现象，避免使用“错误”一词，并且不需要有关底层流程或用户“心智状态”的详细信息。

Result: 一项有 7 名参与者参与的初步研究表明，即使是非电子表格研究人员，也可以使用 Asheetoxy 对现实世界中的电子表格现象进行分类。

Conclusion: Asheetoxy 是一种简单且面向现象的分类法，可以解决现有电子表格错误分类法的局限性，并有望在研究人员和专业人士中得到更广泛的应用。

Abstract: Spreadsheets (sometimes also called Excel programs) are powerful tools which play a business-critical role in many organizations. However, due to faulty spreadsheets many bad decisions have been taken in recent years. Since then, a number of researchers have been studying spreadsheet errors. However, one issue that hinders discussion among researchers and professionals is the lack of a commonly accepted taxonomy.
  Albeit a number of taxonomies for spreadsheet errors have been proposed in previous work, a major issue is that they use the term error that itself is already ambiguous. Furthermore, to apply most existing taxonomies, detailed knowledge about the underlying process and knowledge about the "brain state" of the acting spreadsheet users is required. Due to these limitations, known error-like phenomena in freely available spreadsheet corpora cannot be classified with these taxonomies.
  We propose Asheetoxy, a simple and phenomenon-oriented taxonomy that avoids the problematic term error altogether. An initial study with 7 participants indicates that even non-spreadsheet researchers similarly classify real-world spreadsheet phenomena using Asheetoxy.

</details>


### [142] [Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (SEMS'18)](https://arxiv.org/abs/1808.09174)
*Birgit Hofer,Jorge Mendes*

Main category: cs.SE

TL;DR: 该论文集是关于电子表格软件工程方法的第五届国际研讨会的会议记录。


<details>
  <summary>Details</summary>
Motivation: 提供关于电子表格软件工程的最新研究和实践。

Method: 收集和发表了与电子表格软件工程相关的论文。

Result: 展示了在电子表格软件工程领域的研究成果。

Conclusion: 汇集了该领域的专家，促进了交流和合作。

Abstract: Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (SEMS'18), held on October 1st, 2018, in Lisbon, Portugal, and co-located with the 2018 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC).

</details>


### [143] [Combining Spreadsheet Smells for Improved Fault Prediction](https://arxiv.org/abs/1805.10493)
*Patrick Koch,Konstantin Schekotihin,Dietmar Jannach,Birgit Hofer,Franz Wotawa*

Main category: cs.SE

TL;DR: 电子表格中的臭虫可以通过 AdaBoost 分类器进行预测。


<details>
  <summary>Details</summary>
Motivation: 由于电子表格中的错误可能对业务产生严重影响，因此有必要提高电子表格的故障预测能力。

Method: 提出了一种基于机器学习的方法，利用 AdaBoost 集成分类器结合了各个臭虫的预测结果，以提高故障预测的准确性。

Result: 在包含真实世界电子表格错误的两个公共数据集上进行实验，结果显示故障预测的准确性有了显著提高。

Conclusion: 所提出的基于 AdaBoost 的方法在预测电子表格故障方面比单独使用各个臭虫更有效。

Abstract: Spreadsheets are commonly used in organizations as a programming tool for business-related calculations and decision making. Since faults in spreadsheets can have severe business impacts, a number of approaches from general software engineering have been applied to spreadsheets in recent years, among them the concept of code smells. Smells can in particular be used for the task of fault prediction. An analysis of existing spreadsheet smells, however, revealed that the predictive power of individual smells can be limited. In this work we therefore propose a machine learning based approach which combines the predictions of individual smells by using an AdaBoost ensemble classifier. Experiments on two public datasets containing real-world spreadsheet faults show significant improvements in terms of fault prediction accuracy.

</details>


### [144] [An Easy & Collaborative RDF Data Entry Method using the Spreadsheet Metaphor](https://arxiv.org/abs/1804.04175)
*Markus Schröder,Christian Jilek,Jörn Hees,Sven Hertling,Andreas Dengel*

Main category: cs.SE

TL;DR: 提出了一种易于使用的、零配置的、基于Web的电子表格编辑器，可将电子表格条目同时转换为RDF语句，以简化知识工作者（尤其是工业领域的知识工作者）创建语义数据的过程。


<details>
  <summary>Details</summary>
Motivation: 现有电子表格方法易于数据输入，但将其转换为RDF语句对普通知识工作者来说很复杂。本研究旨在为各种用户（包括RDF专家和新手）提供一种简化的方法来创建语义数据。

Method: 开发了一种基于Web的电子表格编辑器，该编辑器可以实时将电子表格条目转换为RDF语句，并且易于使用且无需配置。

Result: 在用户研究中，使用该编辑器，参与者能够更快地创建更多的RDF语句，并且在质量上相似甚至显著优于其他方法。

Conclusion: 所提出的电子表格编辑器能够有效简化语义数据的创建过程，并提高用户的生产力和结果质量。

Abstract: Spreadsheets are widely used by knowledge workers, especially in the industrial sector. Their methodology enables a well understood, easy and fast possibility to enter data. As filling out a spreadsheet is more accessible to common knowledge workers than defining RDF statements, in this paper, we propose an easy-to-use, zero-configuration, web-based spreadsheet editor that simultaneously transfers spreadsheet entries into RDF statements. It enables various kinds of users to easily create semantic data whether they are RDF experts or novices. The typical scenario we address focuses on creating instance data starting with an empty knowledge base that is filled incrementally. In a user study, participants were able to create more statements in shorter time, having similar or even significantly outperforming quality, compared to other approaches.

</details>


### [145] [Structured Spreadsheet Modelling and Implementation with Multiple Dimensions - Part 1: Modelling](https://arxiv.org/abs/1801.09777)
*Paul Mireault*

Main category: cs.SE

TL;DR: The paper discusses the use of dimensions in models, particularly the time dimension, and how a second dimension is often handled by repeating formulas or creating multiple worksheets.


<details>
  <summary>Details</summary>
Motivation: To address the common practice of using dimensions in models, especially the time dimension, and the inefficiencies in handling a second dimension.

Method: The paper implicitly describes existing methods for handling dimensions, focusing on the repetition of formulas or worksheets for a second dimension.

Result: The paper identifies that repeating blocs of formulas or creating multiple worksheets are common but potentially inefficient ways to represent a second dimension.

Conclusion: The abstract suggests that there are existing, albeit potentially cumbersome, methods for incorporating multiple dimensions into models, particularly highlighting the time dimension and the challenges of adding a second one.

Abstract: Dimensions are an integral part of many models we use every day. Without thinking about it, we frequently use the time dimension: many financial and accounting spreadsheets have columns representing months or years. Representing a second dimension is often done by repeating blocs of formulas in a worksheet or creating multiple worksheets with the same structure.

</details>


### [146] [Mitigating Spreadsheet Risk in Complex Multi-Dimensional Models in Excel](https://arxiv.org/abs/1802.01640)
*Steve Litt*

Main category: cs.SE

TL;DR: Excel 易于使用但手动操作风险高。PivotModel 旨在通过利用 Excel 的强大功能来降低复杂多维模型的风险。


<details>
  <summary>Details</summary>
Motivation: Excel 被广泛用于数据分析，但其手动操作的特性带来了显著的风险，尤其是在处理复杂的“复杂多维模型”时。这些模型涉及复杂的算法、层级结构和数据库写回，如规划和预测。因此，需要一个解决方案来降低这种与 Excel 相关的风险。

Method: 提出了一种名为“PivotModel”的解决方案，该方案类似于 Excel 中的 PivotTable，但它利用了 Microsoft Excel 平台的强大功能来处理“复杂多维模型”。

Result: PivotModel 解决方案旨在通过提供一种类似 PivotTable 的交互方式，来应对在 Excel 中创建和管理复杂多维模型时遇到的挑战，从而降低与电子表格相关的风险。

Conclusion: PivotModel 是一种利用 Excel 平台强大功能的新型解决方案，旨在解决复杂多维模型的手动操作和潜在风险问题。

Abstract: Microsoft Excel is the most ubiquitous analytical tool ever built. Companies around the world leverage it for its power, flexibility and ease of use. However, spreadsheets are manually intensive and prone to error, making it difficult for companies to control spreadsheet risk. The following solution is designed to mitigate spreadsheet risk for a set of problems commonly addressed in a spreadsheet defined as "complex multi-dimensional models". "Complex" referring to certain types of applications that require functionality such as sophisticated algorithms, challenging hierarchies and database write-back (i.e. planning, forecasting, etc.) and "multi-dimensional" referring to providing capabilities such as reporting, data input forms and ad hoc analysis on the different attributes associated with the resulting model. The solution is defined as a "PivotModel" because it works similarly to a PivotTable but is designed to leverage the robust capabilities of the Microsoft Excel platform.

</details>


### [147] [Proposed Spreadsheet Transparency Definition and Measures](https://arxiv.org/abs/1802.01628)
*Craig Hatmaker*

Main category: cs.SE

TL;DR: 金融模型透明度缺乏统一定义和量化指标，导致审计师和模型开发者难以评估和改进模型透明度。


<details>
  <summary>Details</summary>
Motivation: 当前金融建模领域缺乏对“模型透明度”的精确定义和可量化的度量标准，这使得审计师无法准确判断模型是否满足透明度要求，也阻碍了模型开发者客观地比较和选择最适合其目标的建模方法。

Method: 提出一个适用于电子表格建模的透明度定义，该定义具体且可操作，能够支持开发量化指标和自动化审计工具。

Result: 1. 提供了金融模型透明度的具体定义；2. 能够基于此定义开发量化指标和自动化工具，供审计师使用；3. 允许模型开发者客观比较不同建模方法，并选择最符合其需求的。

Conclusion: 所提出的透明度定义解决了当前金融建模领域存在的模糊性和缺乏客观评估标准的问题，有助于提高模型的透明度，并为审计师和模型开发者提供有效的支持。

Abstract: Auditors demand financial models be transparent yet no consensus exists on what that means precisely. Without a clear modeling transparency definition we cannot know when our models are "transparent". The financial modeling community debates which methods are more or less transparent as though transparency is a quantifiable entity yet no measures exist. Without a transparency measure modelers cannot objectively evaluate methods and know which improves model transparency.
  This paper proposes a definition for spreadsheet modeling transparency that is specific enough to create measures and automation tools for auditors to determine if a model meets transparency requirements. The definition also provides modelers the ability to objectively compare spreadsheet modeling methods to select which best meets their goals.

</details>


### [148] [Alternative Spreadsheet Model Designs for an Operations Management Model Embedded in a Periodic Business Process](https://arxiv.org/abs/1802.00484)
*Thomas A. Grossman,Vijay Mehrotra,Mouwafac Sidaoui*

Main category: cs.SE

TL;DR: 该论文提出了一种用于供应链和分销规划的操作管理模型，并对其三种不同的Excel实现进行了评估。


<details>
  <summary>Details</summary>
Motivation: 介绍了一种广泛使用的操作管理模型，该模型通常嵌入在需要模型修改和重用的周期性业务流程中。

Method: 比较了三种Excel实现：数据驱动设计、标准设计和一种新的表驱动技术设计，并从准确性、可修改性、可分析性和可转移性等方面进行了评估。

Result: 数据驱动设计揭示了新手建模者在Excel使用中存在的问题；技术设计无需手动编写或编辑单元格公式即可修改以适应新数据和结构元素，从而加快了修改速度并降低了错误风险；技术设计有潜力应用于其他类别的模型。

Conclusion: 技术设计在修改和减少错误风险方面表现出优势，并具有更广泛的应用潜力。

Abstract: We present a widely-used operations management model used in supply and distribution planning, that is typically embedded in a periodic business process that necessitates model modification and reuse. We consider three alternative spreadsheet implementations, a data-driven design, a canonical (textbook) design, and a novel (table-driven) technical design. We evaluate each regarding suitability for accuracy, modification, analysis, and transfer. We consider the degree of training and technical sophistication required to utilize each design. The data-driven design provides insight into poor spreadsheet practices by naïve modelers. The technical design can be modified for new data and new structural elements without manual writing or editing of cell formulas, thus speeding modification and reducing risk of error. The technical design has potential for use with other classes of models. We identify opportunities for future research.

</details>


### [149] [Mitigating Spreadsheet Model Risk with Python Open Source Infrastructure](https://arxiv.org/abs/1801.09771)
*Oliver Beavers*

Main category: cs.SE

TL;DR: 电子表格模型应被视为软件，但电子表格开发者并非软件工程师，因此易出错。本文旨在为电子表格建模专业人士提供使用Python进行可重复审计的工具，并建立明确的模型“神谕”来验证电子表格计算。


<details>
  <summary>Details</summary>
Motivation: 电子表格模型容易出错，缺乏传统软件工程工具和协议。

Method: 利用Python和开源包开发可重复审计工具，并建立模型“神谕”。

Result: 能够开发出用于测试和审计电子表格计算的模型“神谕”。

Conclusion: 通过使用Python和开源工具，可以提高电子表格审计的准确性和可重复性。

Abstract: Across an aggregation of EuSpRIG presentation papers, two maxims hold true: spreadsheets models are akin to software, yet spreadsheet developers are not software engineers. As such, the lack of traditional software engineering tools and protocols invites a higher rate of error in the end result. This paper lays ground work for spreadsheet modelling professionals to develop reproducible audit tools using freely available, open source packages built with the Python programming language, enabling stakeholders to develop clearly defined model "oracles" with which to test and audit spreadsheet calculations against.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [150] [Mathematics of Digital Hyperspace](https://arxiv.org/abs/2103.15203)
*Jeremy Kepner,Timothy Davis,Vijay Gadepally,Hayden Jananthan,Lauren Milechin*

Main category: cs.MS

TL;DR: 社交媒体、电子商务、视频流等构成庞大的数字空间，其非结构化数据可通过超图、超稀疏矩阵和关联数组代数来表示、遍历和转换。本文提出了一种名为“semilink”的新数学概念，它结合了半环对，为图分析、数据库操作和机器学习提供了基本运算。GraphBLAS标准已支持超图、超稀疏矩阵和半链接所需的数学原理，并能无缝执行图、网络和矩阵运算。通过添加基于键的索引（如指向字符串的指针）和semilinks，GraphBLAS可以成为一个更丰富的关联数组代数，并可作为电子表格、数据库表和数据中心操作系统的插件式替代品，从而增强对数字超空间中非结构化数据的导航能力。


<details>
  <summary>Details</summary>
Motivation: 应对数字超空间中海量非结构化数据及其复杂性，并提出一种更有效的表示、遍历和转换方法。

Method: 提出了一种名为‘semilink’的新数学概念，它结合了半环对，为图分析、数据库操作和机器学习提供基本运算。同时，利用GraphBLAS标准及其对超图、超稀疏矩阵和半链接数学原理的支持。

Result: 通过结合基于键的索引和semilinks，GraphBLAS可以成为一个更丰富的关联数组代数，并有望成为电子表格、数据库表和数据中心操作系统的插件式替代品。

Conclusion: GraphBLAS通过引入semilinks和键基索引，能够更好地处理和导航数字超空间中的非结构化数据，并为现有数据处理系统提供增强功能。

Abstract: Social media, e-commerce, streaming video, e-mail, cloud documents, web pages, traffic flows, and network packets fill vast digital lakes, rivers, and oceans that we each navigate daily. This digital hyperspace is an amorphous flow of data supported by continuous streams that stretch standard concepts of type and dimension. The unstructured data of digital hyperspace can be elegantly represented, traversed, and transformed via the mathematics of hypergraphs, hypersparse matrices, and associative array algebra. This paper explores a novel mathematical concept, the semilink, that combines pairs of semirings to provide the essential operations for graph analytics, database operations, and machine learning. The GraphBLAS standard currently supports hypergraphs, hypersparse matrices, the mathematics required for semilinks, and seamlessly performs graph, network, and matrix operations. With the addition of key based indices (such as pointers to strings) and semilinks, GraphBLAS can become a richer associative array algebra and be a plug-in replacement for spreadsheets, database tables, and data centric operating systems, enhancing the navigation of unstructured data found in digital hyperspace.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [151] [Ensuring Adherence to Standards in Experiment-Related Metadata Entered Via Spreadsheets](https://arxiv.org/abs/2409.08897)
*Martin J. O'Connor,Josef Hardi,Marcos Martínez-Romero,Sowmya Somasundaram,Brendan Honick,Stephen A. Fisher,Ajay Pillai,Mark A. Musen*

Main category: cs.DL

TL;DR: 科学家们倾向于使用电子表格来提供元数据，但电子表格在确保元数据一致性和符合正式规范方面存在局限性。本研究提出了一种端到端的方法，支持基于电子表格的元数据输入，同时确保严格遵守社区元数据标准并提供质量控制。


<details>
  <summary>Details</summary>
Motivation: 尽管有现成的工具，但研究人员在提供元数据时仍偏爱使用电子表格，尽管电子表格在保证元数据的一致性和符合正式规范方面存在局限性。

Method: 本研究采用可定制的模板来表示元数据标准，并据此生成供研究人员使用的电子表格；利用受控术语和本体来定义可直接从电子表格访问的元数据值；开发了一个交互式 Web 工具，用于快速识别和修复电子表格元数据中的错误。

Result: 该方法已被部署到 HuBMAP 生物医学联盟中，用于定义和收集各种生物测定的元数据。

Conclusion: 本研究提出了一种端到端的方法，支持基于电子表格的元数据输入，同时确保严格遵守社区元数据标准并提供质量控制。

Abstract: Scientists increasingly recognize the importance of providing rich, standards-adherent metadata to describe their experimental results. Despite the availability of sophisticated tools to assist in the process of data annotation, investigators generally seem to prefer to use spreadsheets when supplying metadata, despite the limitations of spreadsheets in ensuring metadata consistency and compliance with formal specifications. In this paper, we describe an end-to-end approach that supports spreadsheet-based entry of metadata, while ensuring rigorous adherence to community-based metadata standards and providing quality control. Our methods employ several key components, including customizable templates that represent metadata standards and that can inform the spreadsheets that investigators use to author metadata, controlled terminologies and ontologies for defining metadata values that can be accessed directly from a spreadsheet, and an interactive Web-based tool that allows users to rapidly identify and fix errors in their spreadsheet-based metadata. We demonstrate how this approach is being deployed in a biomedical consortium known as HuBMAP to define and collect metadata about a wide range of biological assays.

</details>


### [152] [Trapped in Transformative Agreements? A Multifaceted Analysis of >1,000 Contracts](https://arxiv.org/abs/2409.20224)
*Laura Rothfritz,W. Benedikt Schmal,Ulrich Herb*

Main category: cs.DL

TL;DR: 学术出版商和研究机构之间的转型协议（TA）虽然普遍存在，但导致了“图书管理员陷阱”，阻碍了向完全开放获取的过渡。


<details>
  <summary>Details</summary>
Motivation: 分析学术出版商和研究机构之间转型协议（TA）的特征，并评估它们在推动开放获取方面的作用。

Method: 利用ESAC倡议的数据库，通过网络抓取合同细节，并结合定性和定量方法，对TA进行深入分析。

Result: 研究机构似乎被困在TA中，阻碍了向完全开放获取的过渡，并使传统出版商获得了显著的市场力量，从而提高了进入壁垒、降低了竞争并增加了图书馆和大学的成本。

Conclusion: TA未能实现其作为通往完全开放获取的桥梁的承诺，反而将学术界困在混合系统中，对出版商有利，但对图书馆和大学不利。

Abstract: Transformative agreements between academic publishers and research institutions are ubiquitous. The 'Efficiency and Standards for Article Charges' (ESAC) Initiative lists more than 1,000 contracts in its database. We make use of this unique dataset by web-scraping the details of every contract to substantially expand the overview spreadsheet provided by the ESAC Initiative. Based on that hitherto unused data source, we combine qualitative and quantitative methods to conduct an in-depth analysis of the contract characteristics and the TA landscape. Our analysis demonstrates that research institutions seem to be 'trapped' in transformative agreements. Instead of being a bridge towards a fully Open Access world, academia is stuck in the hybrid system. This endows the legacy (non-Open Access) publishing houses with substantial market power. It raises entry barriers, lowers competition, and increases costs for libraries and universities.

</details>


### [153] [A Comprehensive Approach to Ensuring Quality in Spreadsheet-Based Metadata](https://arxiv.org/abs/2312.09107)
*Martin J. O'Connor,Marcos Martínez-Romero,Mete Ugur Akdogan,Josef Hardi,Mark A. Musen*

Main category: cs.DL

TL;DR: 该研究提出了一种支持电子表格输入的元数据管理方法，通过可定制模板、受控术语和交互式Web工具，确保元数据合规性和质量。


<details>
  <summary>Details</summary>
Motivation: 电子表格是提供元数据信息的常用工具，但存在合规性和质量控制方面的限制。现有工具也存在学习曲线陡峭和定制性差等缺点。

Method: 提出了一种端到端的方法，包括：1. 可定制的模板用于定义元数据；2. 对在模板定义中使用受控术语提供集成支持；3. 提供交互式Web工具，帮助用户快速识别和修复电子表格元数据中的错误。

Result: 该方法已被部署在一个生物医学联盟中，用于定义和收集科学实验的元数据。

Conclusion: 该方法有效解决了电子表格在元数据管理中的局限性，提高了元数据的合规性和质量。

Abstract: While scientists increasingly recognize the importance of metadata in describing their data, spreadsheets remain the preferred tool for supplying this information despite their limitations in ensuring compliance and quality. Various tools have been developed to address these limitations, but they suffer from their own shortcomings, such as steep learning curves and limited customization. In this paper, we describe an end-to-end approach that supports spreadsheet-based entry of metadata while providing rigorous compliance and quality control. Our approach employs several key strategies, including customizable templates for defining metadata, integral support for the use of controlled terminologies when defining these templates, and an interactive Web-based tool that allows users to rapidly identify and fix errors in the spreadsheet-based metadata they supply. We demonstrate how this approach is being deployed in a biomedical consortium to define and collect metadata about scientific experiments.

</details>


### [154] [Towards Semantic Interoperability in Historical Research: Documenting Research Data and Knowledge with Synthesis](https://arxiv.org/abs/2107.13957)
*Pavlos Fafalios,Konstantina Konsolaki,Lida Charami,Kostas Petrakis,Manos Paterakis,Dimitris Angelakis,Yannis Tzitzikas,Chrysoula Bekiari,Martin Doerr*

Main category: cs.DL

TL;DR: 该研究提出了Synthesis文档系统，一个基于Web的协作系统，以解决历史文物数据管理的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前历史科学领域在文物及其相关证据的记录和研究方面存在诸多问题，例如：多用户协作困难、关系推断细节缺失、数据结构扩展及整合困难、以及数据复用性差。

Method: 本研究描述了Synthesis文档系统，该系统是Web基础的、支持协作的，并利用现有的信息文档和发布标准（CIDOC-CRM, RDF），专注于语义互操作性和高质量、长期有效的数据生产。

Result: Synthesis系统已被大量历史学家在艺术史研究项目中成功使用。

Conclusion: Synthesis系统通过利用现有标准和专注于语义互操作性，有效地解决了历史文物数据管理中的挑战，提高了数据的价值和长期有效性。

Abstract: A vast area of research in historical science concerns the documentation and study of artefacts and related evidence. Current practice mostly uses spreadsheets or simple relational databases to organise the information as rows with multiple columns of related attributes. This form offers itself for data analysis and scholarly interpretation, however it also poses problems including i) the difficulty for collaborative but controlled documentation by a large number of users, ii) the lack of representation of the details from which the documented relations are inferred, iii) the difficulty to extend the underlying data structures as well as to combine and integrate data from multiple and diverse information sources, and iv) the limitation to reuse the data beyond the context of a particular research activity. To support historians to cope with these problems, in this paper we describe the Synthesis documentation system and its use by a large number of historians in the context of an ongoing research project in the field of History of Art. The system is Web-based and collaborative, and makes use of existing standards for information documentation and publication (CIDOC-CRM, RDF), focusing on semantic interoperability and the production of data of high value and long-term validity.

</details>


### [155] [FAST CAT: Collaborative Data Entry and Curation for Semantic Interoperability in Digital Humanities](https://arxiv.org/abs/2105.13733)
*Pavlos Fafalios,Kostas Petrakis,Georgios Samaritakis,Korina Doerr,Athina Kritsotaki,Yannis Tzitzikas,Martin Doerr*

Main category: cs.DL

TL;DR: FAST CAT是一个用于数字人文和经验研究的协作式数据录入和管理系统，解决了传统电子表格和数据库的局限性，并已应用于海事历史项目SeaLiT。


<details>
  <summary>Details</summary>
Motivation: 当前历史学等描述性和经验性科学在数据管理和分析方面依赖电子表格和关系数据库，存在数据依赖研究假设、缺乏细节表征、难以溯源等局限性。FAST CAT旨在解决这些问题。

Method: FAST CAT系统通过支持语义互操作性的方法，为数字人文和经验研究提供协作式数据录入和管理。

Result: FAST CAT系统已被应用于名为SeaLiT的欧洲海事历史项目，该项目研究了1850年代至1920年代蒸汽船引入对地中海地区经济、社会和人口的影响。

Conclusion: FAST CAT系统为数字人文和经验研究提供了一个有效的解决方案，能够克服传统数据管理工具的局限性，并已成功应用于具体研究项目。

Abstract: Descriptive and empirical sciences, such as History, are the sciences that collect, observe and describe phenomena in order to explain them and draw interpretative conclusions about influences, driving forces and impacts under given circumstances. Spreadsheet software and relational database management systems are still the dominant tools for quantitative analysis and overall data management in these these sciences, allowing researchers to directly analyse the gathered data and perform scholarly interpretation. However, this current practice has a set of limitations, including the high dependency of the collected data on the initial research hypothesis, usually useless for other research, the lack of representation of the details from which the registered relations are inferred, and the difficulty to revisit the original data sources for verification, corrections or improvements. To cope with these problems, in this paper we present FAST CAT, a collaborative system for assistive data entry and curation in Digital Humanities and similar forms of empirical research. We describe the related challenges, the overall methodology we follow for supporting semantic interoperability, and discuss the use of FAST CAT in the context of a European (ERC) project of Maritime History, called SeaLiT, which examines economic, social and demographic impacts of the introduction of steamboats in the Mediterranean area between the 1850s and the 1920s.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [156] [Radif Corpus: A Symbolic Dataset for Non-Metric Iranian Classical Music](https://arxiv.org/abs/2507.10456)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.SD

TL;DR: 提供了一个包含伊朗古典音乐radif曲目的数字语料库，包括MIDI文件和详细的音乐数据，用于计算音乐学研究。


<details>
  <summary>Details</summary>
Motivation: 为伊朗古典音乐，特别是其核心的radif曲目，提供一个全面的数字资源，以支持计算方法的研究。

Method: 构建了一个包含伊朗古典音乐radif曲目的数字语料库，涵盖所有13个组成部分，并提供MIDI文件（约281分钟）和包含音符、时长、音程、层级结构等信息的电子表格数据，忠实呈现了包括四分音符在内的调性和非节拍特性，并附带基本统计数据、复杂度和相似性度量。

Result: 创建了一个数字语料库，包含228首音乐作品的MIDI文件和详细数据，能够准确表示伊朗古典音乐的调性和非节拍特性，并提供相关统计分析。

Conclusion: 该语料库为伊朗古典音乐的计算研究提供了一个平台，可用于旋律模式研究、即兴风格探索以及音乐信息检索、音乐理论和计算（民族）音乐学等领域的任务。

Abstract: Non-metric music forms the core of the repertoire in Iranian classical music. Dastgahi music serves as the underlying theoretical system for both Iranian art music and certain folk traditions. At the heart of Iranian classical music lies the radif, a foundational repertoire that organizes melodic material central to performance and pedagogy.
  In this study, we introduce a digital corpus representing the complete non-metrical radif repertoire, covering all 13 existing components of this repertoire. We provide MIDI files (about 281 minutes in total) and data spreadsheets describing notes, note durations, intervals, and hierarchical structures for 228 pieces of music. We faithfully represent the tonality including quarter-tones, and the non-metric aspect. Furthermore, we provide supporting basic statistics, and measures of complexity and similarity over the corpus.
  Our corpus provides a platform for computational studies of Iranian classical music. Researchers might employ it in studying melodic patterns, investigating improvisational styles, or for other tasks in music information retrieval, music theory, and computational (ethno)musicology.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [157] [Dynamic Kernel Matching for Non-conforming Data: A Case Study of T-cell Receptor Datasets](https://arxiv.org/abs/2103.10472)
*Jared Ostmeyer,Scott Christley,Lindsay Cowell*

Main category: q-bio.QM

TL;DR: 该研究提出了一种名为动态核匹配（DKM）的方法，用于修改现有的统计分类器以处理非结构化数据，并成功应用于TCR序列和患者序列数据，以预测疾病和CMV血清状态。


<details>
  <summary>Details</summary>
Motivation: 大多数统计分类器都用于处理表格数据，但现实世界中存在大量非结构化数据，这限制了这些分类器的应用。本研究旨在开发一种能够处理非结构化数据的方法，以发现其中隐藏的模式。

Method: 提出了一种名为动态核匹配（DKM）的方法，该方法通过修改现有的统计分类器来处理非结构化数据。将DKM应用于（i）按疾病抗原标记的T细胞受体（TCR）序列数据集和（ii）按患者巨细胞病毒（CMV）血清状态标记的TCR序列库数据集。使用标准指标和允许不确定诊断的指标评估模型在保留数据上的性能。最后，识别用于生成预测的模式，并与实验研究的观察结果进行比较。

Result: DKM方法成功地应用于两个非结构化数据集，并在保留数据上取得了良好的性能。研究识别出的预测模式与实验研究的观察结果一致。

Conclusion: 动态核匹配（DKM）是一种有效的方法，可以扩展统计分类器的能力，使其能够处理非结构化数据，并在生物医学领域（如疾病诊断）中具有潜在应用价值。识别出的模式为进一步的实验研究提供了依据。

Abstract: Most statistical classifiers are designed to find patterns in data where numbers fit into rows and columns, like in a spreadsheet, but many kinds of data do not conform to this structure. To uncover patterns in non-conforming data, we describe an approach for modifying established statistical classifiers to handle non-conforming data, which we call dynamic kernel matching (DKM). As examples of non-conforming data, we consider (i) a dataset of T-cell receptor (TCR) sequences labelled by disease antigen and (ii) a dataset of sequenced TCR repertoires labelled by patient cytomegalovirus (CMV) serostatus, anticipating that both datasets contain signatures for diagnosing disease. We successfully fit statistical classifiers augmented with DKM to both datasets and report the performance on holdout data using standard metrics and metrics allowing for indeterminant diagnoses. Finally, we identify the patterns used by our statistical classifiers to generate predictions and show that these patterns agree with observations from experimental studies.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [158] [FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining](https://arxiv.org/abs/2109.07323)
*Zhoujun Cheng,Haoyu Dong,Ran Jia,Pengfei Wu,Shi Han,Fan Cheng,Dongmei Zhang*

Main category: cs.IR

TL;DR: Tables store rich numerical data, but numerical reasoning over tables is still a challenge. We find that spreadsheet formulas are strong supervision for numerical reasoning and develop FORTAP, a pretraining method that leverages these formulas. FORTAP achieves state-of-the-art results on downstream tasks like cell type classification and formula prediction.


<details>
  <summary>Details</summary>
Motivation: Numerical reasoning over tables is challenging, despite tables containing rich numerical data. Spreadsheet formulas offer a natural and readily available source of supervision for numerical reasoning.

Method: FORTAP is a novel method for numerical-reasoning-aware table pretraining. It utilizes a large corpus of spreadsheet formulas by designing two formula pretraining tasks to explicitly guide the model to learn numerical reference and calculation in semi-structured tables.

Result: FORTAP achieves state-of-the-art results on two representative downstream tasks: cell type classification and formula prediction.

Conclusion: Leveraging spreadsheet formulas for numerical-reasoning-aware pretraining, as demonstrated by FORTAP, shows great potential for improving performance on table-based reasoning tasks.

Abstract: Tables store rich numerical data, but numerical reasoning over tables is still a challenge. In this paper, we find that the spreadsheet formula, which performs calculations on numerical values in tables, is naturally a strong supervision of numerical reasoning. More importantly, large amounts of spreadsheets with expert-made formulae are available on the web and can be obtained easily. FORTAP is the first method for numerical-reasoning-aware table pretraining by leveraging large corpus of spreadsheet formulae. We design two formula pretraining tasks to explicitly guide FORTAP to learn numerical reference and calculation in semi-structured tables. FORTAP achieves state-of-the-art results on two representative downstream tasks, cell type classification and formula prediction, showing great potential of numerical-reasoning-aware pretraining.

</details>


### [159] [Detecting Layout Templates in Complex Multiregion Files](https://arxiv.org/abs/2109.06630)
*Gerardo Vitagliano,Lan Jiang,Felix Naumann*

Main category: cs.IR

TL;DR: 自动识别电子表格布局模板并提取数据区域。


<details>
  <summary>Details</summary>
Motivation: 电子表格常用于数据管理，但其灵活的结构给自动化分析带来挑战，特别是包含多个独立数据区域的‘多区域’文件。

Method: Mondrian方法，包括将电子表格渲染成图像进行区域元素识别、使用聚类算法对元素进行分组以形成区域，以及将文件布局表示为图并进行比较以发现模板。

Result: Mondrian方法在检测单个文件内的可靠区域边界以及跨文件识别重复布局方面，表现优于现有的表格识别算法。

Conclusion: Mondrian方法能有效处理多区域电子表格，并能识别和提取跨多个文件的共同布局模板。

Abstract: Spreadsheets are among the most commonly used file formats for data management, distribution, and analysis. Their widespread employment makes it easy to gather large collections of data, but their flexible canvas-based structure makes automated analysis difficult without heavy preparation. One of the common problems that practitioners face is the presence of multiple, independent regions in a single spreadsheet, possibly separated by repeated empty cells. We define such files as "multiregion" files. In collections of various spreadsheets, we can observe that some share the same layout. We present the Mondrian approach to automatically identify layout templates across multiple files and systematically extract the corresponding regions. Our approach is composed of three phases: first, each file is rendered as an image and inspected for elements that could form regions; then, using a clustering algorithm, the identified elements are grouped to form regions; finally, every file layout is represented as a graph and compared with others to find layout templates. We compare our method to state-of-the-art table recognition algorithms on two corpora of real-world enterprise spreadsheets. Our approach shows the best performances in detecting reliable region boundaries within each file and can correctly identify recurring layouts across files.

</details>


### [160] [TUTA: Tree-based Transformers for Generally Structured Table Pre-training](https://arxiv.org/abs/2010.12537)
*Zhiruo Wang,Haoyu Dong,Ran Jia,Jia Li,Zhiyi Fu,Shi Han,Dongmei Zhang*

Main category: cs.IR

TL;DR: TUTA是一个统一的预训练框架，用于理解通用结构化表格，通过结合空间、层级和语义信息，在表格理解任务上取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 现有表格理解方法主要关注关系型表格，忽略了其他常见表格结构，因此需要一个能够理解通用表格结构的统一框架。

Method: 提出了一种名为“二维坐标树”的统一树状结构来描述表格的空间和层级信息，并在此基础上设计了基于树的注意力机制和位置嵌入。此外，还设计了三种渐进式的预训练目标，分别用于token、单元格和表格层面的表示学习。TUTA在大量无标签的网页和电子表格数据上进行预训练，并在单元格类型分类和表格类型分类两个任务上进行微调。

Result: TUTA在五种广泛研究的数据集上取得了最先进的性能。

Conclusion: TUTA是一种有效的通用表格理解预训练框架，能够处理各种表格结构，并在关键的表格结构理解任务上取得优异成绩。

Abstract: Tables are widely used with various structures to organize and present data. Recent attempts on table understanding mainly focus on relational tables, yet overlook to other common table structures. In this paper, we propose TUTA, a unified pre-training architecture for understanding generally structured tables. Noticing that understanding a table requires spatial, hierarchical, and semantic information, we enhance transformers with three novel structure-aware mechanisms. First, we devise a unified tree-based structure, called a bi-dimensional coordinate tree, to describe both the spatial and hierarchical information of generally structured tables. Upon this, we propose tree-based attention and position embedding to better capture the spatial and hierarchical information. Moreover, we devise three progressive pre-training objectives to enable representations at the token, cell, and table levels. We pre-train TUTA on a wide range of unlabeled web and spreadsheet tables and fine-tune it on two critical tasks in the field of table structure understanding: cell type classification and table type classification. Experiments show that TUTA is highly effective, achieving state-of-the-art on five widely-studied datasets.

</details>


### [161] [TableSense: Spreadsheet Table Detection with Convolutional Neural Networks](https://arxiv.org/abs/2106.13500)
*Haoyu Dong,Shijie Liu,Shi Han,Zhouyu Fu,Dongmei Zhang*

Main category: cs.IR

TL;DR: TableSense是一个新的端到端框架，用于电子表格表格检测，它利用CNN和主动学习来提高准确性。


<details>
  <summary>Details</summary>
Motivation: 表格检测是电子表格数据智能的关键技术，但面临表格结构和布局多样性的挑战。

Method: TableSense使用有效的单元格特征提取方案、增强的卷积神经网络模型和不确定性度量来指导主动学习，以构建包含22,176个表格的大型训练数据集。

Result: TableSense在EoB-2度量上达到了91.3%的召回率和86.5%的精确率，显著优于现有算法和最先进的计算机视觉CNN模型。

Conclusion: TableSense在电子表格表格检测方面表现出高有效性，并改进了现有方法。

Abstract: Spreadsheet table detection is the task of detecting all tables on a given sheet and locating their respective ranges. Automatic table detection is a key enabling technique and an initial step in spreadsheet data intelligence. However, the detection task is challenged by the diversity of table structures and table layouts on the spreadsheet. Considering the analogy between a cell matrix as spreadsheet and a pixel matrix as image, and encouraged by the successful application of Convolutional Neural Networks (CNN) in computer vision, we have developed TableSense, a novel end-to-end framework for spreadsheet table detection. First, we devise an effective cell featurization scheme to better leverage the rich information in each cell; second, we develop an enhanced convolutional neural network model for table detection to meet the domain-specific requirement on precise table boundary detection; third, we propose an effective uncertainty metric to guide an active learning based smart sampling algorithm, which enables the efficient build-up of a training dataset with 22,176 tables on 10,220 sheets with broad coverage of diverse table structures and layouts. Our evaluation shows that TableSense is highly effective with 91.3\% recall and 86.5\% precision in EoB-2 metric, a significant improvement over both the current detection algorithm that are used in commodity spreadsheet tools and state-of-the-art convolutional neural networks in computer vision.

</details>


### [162] [Recommending Related Tables](https://arxiv.org/abs/1907.03595)
*Shuo Zhang,Krisztian Balog*

Main category: cs.IR

TL;DR: 给定一个输入表，识别并返回相关表的排名列表。


<details>
  <summary>Details</summary>
Motivation: 推荐相关表，为电子表格程序用户主动推荐网络上相关的结构化内容。

Method: 为表匹配开发一个理论上可靠的框架。该方法将表元素表示在多个语义空间中，然后使用区分性学习模型组合元素级相似性。

Result: 使用从维基百科表中专门构建的测试集合，证明所提出的方法提供了最先进的性能。

Conclusion: 所提出的方法在相关表推荐任务上实现了最先进的性能。

Abstract: Tables are an extremely powerful visual and interactive tool for structuring and manipulating data, making spreadsheet programs one of the most popular computer applications. In this paper we introduce and address the task of recommending related tables: given an input table, identifying and returning a ranked list of relevant tables. One of the many possible application scenarios for this task is to provide users of a spreadsheet program proactively with recommendations for related structured content on the Web. At its core, the related table recommendation task boils down to computing the similarity between a pair of tables. We develop a theoretically sound framework for performing table matching. Our approach hinges on the idea of representing table elements in multiple semantic spaces, and then combining element-level similarities using a discriminative learning model. Using a purpose-built test collection from Wikipedia tables, we demonstrate that the proposed approach delivers state-of-the-art performance.

</details>


### [163] [SmartTable: A Spreadsheet Program with Intelligent Assistance](https://arxiv.org/abs/1805.06353)
*Shuo Zhang,Vugar Abdul Zada,Krisztian Balog*

Main category: cs.IR

TL;DR: SmartTable 是一个具有智能辅助功能的在线电子表格应用程序，专注于关系表，提供填充表格（行）和扩展表格（列）的辅助功能。


<details>
  <summary>Details</summary>
Motivation: 介绍 SmartTable，一个具有智能辅助功能的在线电子表格应用程序，并重点介绍其在处理关系表方面的能力。

Method: 介绍 SmartTable 的实现细节，并提供其开源版本。

Result: SmartTable 提供了用于填充表格（添加行）和扩展表格（添加列）的智能辅助功能。

Conclusion: SmartTable 是一个开源的在线电子表格应用程序，具有智能辅助功能，可用于处理关系表。

Abstract: We introduce SmartTable, an online spreadsheet application that is equipped with intelligent assistance capabilities. With a focus on relational tables, describing entities along with their attributes, we offer assistance in two flavors: (i) for populating the table with additional entities (rows) and (ii) for extending it with additional entity attributes (columns). We provide details of our implementation, which is also released as open source. The application is available at http://smarttable.cc.

</details>
