<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Benchmark Dataset Generation and Evaluation for Excel Formula Repair with LLMs](https://arxiv.org/abs/2508.11715)
*Ananya Singha,Harshita Sahijwani,Walt Williams,Emmanuel Aboah Boateng,Nick Hausman,Miguel Di Luca,Keegan Choudhury,Chaya Binet,Vu Le,Tianwei Chen,Oryan Rokeah Chen,Sulaiman Vesal,Sadid Hasan*

Main category: cs.SE

TL;DR: 该论文介绍了一种新颖的方法，用于构建Excel公式修复的基准数据集，以解决现有高质量训练数据匮乏的问题。通过提出一种数据生成流程，结合了少样本提示、LLM作为判断者验证框架和基于执行的检查，该方法生成了一个包含618个高质量样本的数据集。此外，论文还提出了一种上下文感知的基线技术，并评估了各种大型语言模型在此数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: Excel对新手用户来说是一个复杂工具，运行时错误（逻辑错误或函数误解引起）是一个重大挑战。尽管大型语言模型（LLMs）可以解释公式错误，但自动化纠正这些语义运行时错误仍是一个未解决的问题。主要挑战是缺乏高质量、全面的数据集用于训练和严格评估。

Method: 该论文提出了一种数据生成流程，利用少量精心策划的在线论坛种子样本来合成扩展数据集。该流程集成了LLMs的少样本提示和强大的“LLM-as-a-Judge”验证框架，并结合了基于执行的检查，以确保生成数据的正确性和语义忠实性。此外，还提出了一种上下文感知的基线技术，利用LLMs结合错误公式和相关电子表格上下文进行Excel公式修复。

Result: 该过程生成了一个包含618个高质量样本的基准数据集，涵盖了常见的运行时错误。论文还在新生成的基准上使用基于执行的指标评估了各种LLMs（GPT-4o, GPT-4.1, Phi-3, Mistral）的性能。分析通过手动标注展示了数据集的质量，并提供了对错误和函数分布的见解。

Conclusion: 所提出的生成方法具有高度可扩展性，可以很容易地适用于为其他低资源编程语言中类似的代码修复任务创建评估基准。

Abstract: Excel is a pervasive yet often complex tool, particularly for novice users,
where runtime errors arising from logical mistakes or misinterpretations of
functions pose a significant challenge. While large language models (LLMs)
offer promising assistance by explaining formula errors, the automated
correction of these semantic runtime errors remains an open problem. A primary
challenge to advancing models for such scenarios is the severe lack of
high-quality, comprehensive datasets for training and rigorous evaluation. This
paper addresses this gap by introducing a novel approach for constructing a
benchmark dataset specifically designed for Excel formula repair. We propose a
data generation pipeline, which leverages a small set of curated seed samples
from online forums to synthetically expand the dataset. Our pipeline integrates
few-shot prompting with LLMs and employs a robust \textit{LLM-as-a-Judge}
validation framework, combined with execution-based checks to ensure the
correctness and semantic fidelity of the generated data. This process produced
a benchmark dataset of 618 high-quality samples, covering common runtime
errors. Furthermore, we propose a context-aware baseline technique for Excel
formula repair that utilizes LLMs to leverage both the faulty formula, and
relevant spreadsheet context. We evaluate the performance of various LLMs
(GPT-4o, GPT-4.1, Phi-3, Mistral) on our newly generated benchmark using
execution-based metrics. Our analysis demonstrates the dataset's quality
through manual annotation and provides insights into error and function
distributions. The proposed generation methodology is highly scalable and can
be readily adapted to create evaluation benchmarks for similar code repair
tasks in other low-resource programming languages.

</details>
