{"id": "2601.08741", "title": "From Rows to Reasoning: A Retrieval-Augmented Multimodal Framework for Spreadsheet Understanding", "url": "https://arxiv.org/abs/2601.08741", "pdf": "https://arxiv.org/pdf/2601.08741", "abs": "https://arxiv.org/abs/2601.08741", "authors": ["Anmol Gulati", "Sahil Sen", "Waqar Sarguroh", "Kevin Paul"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) struggle to reason over large-scale enterprise spreadsheets containing thousands of numeric rows, multiple linked sheets, and embedded visual content such as charts and receipts. Prior state-of-the-art spreadsheet reasoning approaches typically rely on single-sheet compression or full-context encoding, which limits scalability and fails to reflect how real users interact with complex, multimodal workbooks. We introduce FRTR-Bench, the first large-scale benchmark for multimodal spreadsheet reasoning, comprising 30 enterprise-grade Excel workbooks spanning nearly four million cells and more than 50 embedded images. To address these challenges, we present From Rows to Reasoning (FRTR), an advanced, multimodal retrieval-augmented generation framework that decomposes Excel workbooks into granular row, column, and block embeddings, employs hybrid lexical-dense retrieval with Reciprocal Rank Fusion (RRF), and integrates multimodal embeddings to reason over both numerical and visual information. We tested FRTR on six LLMs, achieving 74% answer accuracy on FRTR-Bench with Claude Sonnet 4.5, a substantial improvement over prior state-of-the-art approaches that reached only 24%. On the SpreadsheetLLM benchmark, FRTR achieved 87% accuracy with GPT-5 while reducing token usage by roughly 50% compared to context-compression methods."}
{"id": "2512.04292", "title": "SQuARE: Structured Query & Adaptive Retrieval Engine For Tabular Formats", "url": "https://arxiv.org/abs/2512.04292", "pdf": "https://arxiv.org/pdf/2512.04292", "abs": "https://arxiv.org/abs/2512.04292", "authors": ["Chinmay Gondhalekar", "Urjitkumar Patel", "Fang-Chun Yeh"], "categories": ["cs.CL"], "comment": "Accepted in The IEEE International Workshop on Large Language Models in Finance, Dec 8-11, Macau, China, 2025, Preprint Copy", "summary": "Accurate question answering over real spreadsheets remains difficult due to multirow headers, merged cells, and unit annotations that disrupt naive chunking, while rigid SQL views fail on files lacking consistent schemas. We present SQuARE, a hybrid retrieval framework with sheet-level, complexity-aware routing. It computes a continuous score based on header depth and merge density, then routes queries either through structure-preserving chunk retrieval or SQL over an automatically constructed relational representation. A lightweight agent supervises retrieval, refinement, or combination of results across both paths when confidence is low. This design maintains header hierarchies, time labels, and units, ensuring that returned values are faithful to the original cells and straightforward to verify. Evaluated on multi-header corporate balance sheets, a heavily merged World Bank workbook, and diverse public datasets, SQuARE consistently surpasses single-strategy baselines and ChatGPT-4o on both retrieval precision and end-to-end answer accuracy while keeping latency predictable. By decoupling retrieval from model choice, the system is compatible with emerging tabular foundation models and offers a practical bridge toward a more robust table understanding."}
{"id": "2507.06171", "title": "Data-Semantics-Aware Recommendation of Diverse Pivot Tables", "url": "https://arxiv.org/abs/2507.06171", "pdf": "https://arxiv.org/pdf/2507.06171", "abs": "https://arxiv.org/abs/2507.06171", "authors": ["Whanhee Cho", "Anna Fariha"], "categories": ["cs.DB"], "comment": "Accepted to SIGMOD 2026", "summary": "Data summarization is essential to discover insights from large datasets. In a spreadsheets, pivot tables offer a convenient way to summarize tabular data by computing aggregates over some attributes, grouped by others. However, identifying attribute combinations that will result in useful pivot tables remains a challenge, especially for high-dimensional datasets. We formalize the problem of automatically recommending insightful and interpretable pivot tables, eliminating the tedious manual process. A crucial aspect of recommending a set of pivot tables is to diversify them. Traditional works inadequately address the table-diversification problem, which leads us to consider the problem of pivot table diversification.\n  We present SAGE, a data-semantics-aware system for recommending k-budgeted diverse pivot tables, overcoming the shortcomings of prior work for top-k recommendations that cause redundancy. SAGE ensures that each pivot table is insightful, interpretable, and adaptive to the user's actions and preferences, while also guaranteeing that the set of pivot tables are different from each other, offering a diverse recommendation. We make two key technical contributions: (1) a data-semantics-aware model to measure the utility of a single pivot table and the diversity of a set of pivot tables, and (2) a scalable greedy algorithm that can efficiently select a set of diverse pivot tables of high utility, by leveraging data semantics to significantly reduce the combinatorial search space. Our extensive experiments on three real-world datasets show that SAGE outperforms alternative approaches, and efficiently scales to accommodate high-dimensional datasets. Additionally, we present several case studies to highlight SAGE's qualitative effectiveness over commercial software and Large Language Models (LLMs)."}
{"id": "2409.08897", "title": "Ensuring Adherence to Standards in Experiment-Related Metadata Entered Via Spreadsheets", "url": "https://arxiv.org/abs/2409.08897", "pdf": "https://arxiv.org/pdf/2409.08897", "abs": "https://arxiv.org/abs/2409.08897", "authors": ["Martin J. O'Connor", "Josef Hardi", "Marcos Martínez-Romero", "Sowmya Somasundaram", "Brendan Honick", "Stephen A. Fisher", "Ajay Pillai", "Mark A. Musen"], "categories": ["cs.DL"], "comment": null, "summary": "Scientists increasingly recognize the importance of providing rich, standards-adherent metadata to describe their experimental results. Despite the availability of sophisticated tools to assist in the process of data annotation, investigators generally seem to prefer to use spreadsheets when supplying metadata, despite the limitations of spreadsheets in ensuring metadata consistency and compliance with formal specifications. In this paper, we describe an end-to-end approach that supports spreadsheet-based entry of metadata, while ensuring rigorous adherence to community-based metadata standards and providing quality control. Our methods employ several key components, including customizable templates that represent metadata standards and that can inform the spreadsheets that investigators use to author metadata, controlled terminologies and ontologies for defining metadata values that can be accessed directly from a spreadsheet, and an interactive Web-based tool that allows users to rapidly identify and fix errors in their spreadsheet-based metadata. We demonstrate how this approach is being deployed in a biomedical consortium known as HuBMAP to define and collect metadata about a wide range of biological assays."}
{"id": "2507.10456", "title": "Radif Corpus: A Symbolic Dataset for Non-Metric Iranian Classical Music", "url": "https://arxiv.org/abs/2507.10456", "pdf": "https://arxiv.org/pdf/2507.10456", "abs": "https://arxiv.org/abs/2507.10456", "authors": ["Maziar Kanani", "Sean O Leary", "James McDermott"], "categories": ["cs.SD", "eess.AS"], "comment": null, "summary": "Non-metric music forms the core of the repertoire in Iranian classical music. Dastgahi music serves as the underlying theoretical system for both Iranian art music and certain folk traditions. At the heart of Iranian classical music lies the radif, a foundational repertoire that organizes melodic material central to performance and pedagogy.\n  In this study, we introduce a digital corpus representing the complete non-metrical radif repertoire, covering all 13 existing components of this repertoire. We provide MIDI files (about 281 minutes in total) and data spreadsheets describing notes, note durations, intervals, and hierarchical structures for 228 pieces of music. We faithfully represent the tonality including quarter-tones, and the non-metric aspect. Furthermore, we provide supporting basic statistics, and measures of complexity and similarity over the corpus.\n  Our corpus provides a platform for computational studies of Iranian classical music. Researchers might employ it in studying melodic patterns, investigating improvisational styles, or for other tasks in music information retrieval, music theory, and computational (ethno)musicology."}
{"id": "2507.16073", "title": "Buckaroo: A Direct Manipulation Visual Data Wrangler", "url": "https://arxiv.org/abs/2507.16073", "pdf": "https://arxiv.org/pdf/2507.16073", "abs": "https://arxiv.org/abs/2507.16073", "authors": ["Annabelle Warner", "Andrew McNutt", "Paul Rosen", "El Kindi Rezig"], "categories": ["cs.HC", "cs.DB"], "comment": "Accepted to VLDB25 Demo track", "summary": "Preparing datasets -- a critical phase known as data wrangling -- constitutes the dominant phase of data science development, consuming upwards of 80% of the total project time. This phase encompasses a myriad of tasks: parsing data, restructuring it for analysis, repairing inaccuracies, merging sources, eliminating duplicates, and ensuring overall data integrity. Traditional approaches, typically through manual coding in languages such as Python or using spreadsheets, are not only laborious but also error-prone. These issues range from missing entries and formatting inconsistencies to data type inaccuracies, all of which can affect the quality of downstream tasks if not properly corrected. To address these challenges, we present Buckaroo, a visualization system to highlight discrepancies in data and enable on-the-spot corrections through direct manipulations of visual objects. Buckaroo (1) automatically finds \"interesting\" data groups that exhibit anomalies compared to the rest of the groups and recommends them for inspection; (2) suggests wrangling actions that the user can choose to repair the anomalies; and (3) allows users to visually manipulate their data by displaying the effects of their wrangling actions and offering the ability to undo or redo these actions, which supports the iterative nature of data wrangling. A video companion is available at https://youtu.be/iXdCYbvpQVE"}
{"id": "2407.10657", "title": "An Empirical Study of Validating Synthetic Data for Formula Generation", "url": "https://arxiv.org/abs/2407.10657", "pdf": "https://arxiv.org/pdf/2407.10657", "abs": "https://arxiv.org/abs/2407.10657", "authors": ["Usneek Singh", "José Cambronero", "Sumit Gulwani", "Aditya Kanade", "Anirudh Khatry", "Vu Le", "Mukul Singh", "Gust Verbruggen"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Findings of NAACL", "summary": "Large language models (LLMs) can be leveraged to help with writing formulas in spreadsheets, but resources on these formulas are scarce, impacting both the base performance of pre-trained models and limiting the ability to fine-tune them. Given a corpus of formulas, we can use a(nother) model to generate synthetic natural language utterances for fine-tuning. However, it is important to validate whether the NL generated by the LLM is indeed accurate to be beneficial for fine-tuning. In this paper, we provide empirical results on the impact of validating these synthetic training examples with surrogate objectives that evaluate the accuracy of the synthetic annotations. We demonstrate that validation improves performance over raw data across four models (2 open and 2 closed weight). Interestingly, we show that although validation tends to prune more challenging examples, it increases the complexity of problems that models can solve after being fine-tuned on validated data."}
{"id": "2506.17330", "title": "Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE", "url": "https://arxiv.org/abs/2506.17330", "pdf": "https://arxiv.org/pdf/2506.17330", "abs": "https://arxiv.org/abs/2506.17330", "authors": ["Simon Thorne"], "categories": ["cs.SE"], "comment": "18 Pages, 10 Tables, 1 Colour Figure", "summary": "Large Language Models (LLMs) have demonstrated some significant capabilities across various domains; however, their effectiveness in spreadsheet related tasks remains underexplored. This study introduces a foundation for a comprehensive benchmark framework to evaluate the performance of leading LLMs in executing spreadsheet functions, formula generation and data manipulation tasks. The benchmark encompasses tasks ranging from basic formula creation to complex, real world spreadsheet scenarios. Our findings reveal that while LLMs exhibit proficiency in straightforward tasks, they often falter in complex, multi step operations, frequently producing plausible yet incorrect outputs. These results underscore the limitations of current LLMs in handling spreadsheet tasks that require precise logical reasoning and highlight the need for integrating symbolic reasoning capabilities into LLM architectures. To support this, we introduce FLARE (Formula Logic, Auditing, Reasoning and Evaluation) a new benchmark for evaluating LLM performance on real-world spreadsheet logic, auditing, and reasoning tasks."}
{"id": "2506.12339", "title": "SheetMind: An End-to-End LLM-Powered Multi-Agent Framework for Spreadsheet Automation", "url": "https://arxiv.org/abs/2506.12339", "pdf": "https://arxiv.org/pdf/2506.12339", "abs": "https://arxiv.org/abs/2506.12339", "authors": ["Ruiyan Zhu", "Xi Cheng", "Ke Liu", "Brian Zhu", "Daniel Jin", "Neeraj Parihar", "Zhoutian Xu", "Oliver Gao"], "categories": ["cs.HC", "cs.AI"], "comment": "Ruiyan Zhu and Xi Cheng contributed equally to this work", "summary": "We present SheetMind, a modular multi-agent framework powered by large language models (LLMs) for spreadsheet automation via natural language instructions. The system comprises three specialized agents: a Manager Agent that decomposes complex user instructions into subtasks; an Action Agent that translates these into structured commands using a Backus Naur Form (BNF) grammar; and a Reflection Agent that validates alignment between generated actions and the user's original intent. Integrated into Google Sheets via a Workspace extension, SheetMind supports real-time interaction without requiring scripting or formula knowledge. Experiments on benchmark datasets demonstrate an 80 percent success rate on single step tasks and approximately 70 percent on multi step instructions, outperforming ablated and baseline variants. Our results highlight the effectiveness of multi agent decomposition and grammar based execution for bridging natural language and spreadsheet functionalities."}
{"id": "2506.09216", "title": "\"How do you even know that stuff?\": Barriers to expertise sharing among spreadsheet users", "url": "https://arxiv.org/abs/2506.09216", "pdf": "https://arxiv.org/pdf/2506.09216", "abs": "https://arxiv.org/abs/2506.09216", "authors": ["Qing", "Xia", "Advait Sarkar", "Duncan Brumby", "Anna Cox"], "categories": ["cs.HC", "cs.CY"], "comment": "Accepted at CSCW 2025", "summary": "Spreadsheet collaboration provides valuable opportunities for learning and expertise sharing between colleagues. Sharing expertise is essential for the retention of important technical skillsets within organisations, but previous studies suggest that spreadsheet experts often fail to disseminate their knowledge to others. We suggest that social norms and beliefs surrounding the value of spreadsheet use significantly influence user engagement in sharing behaviours. To explore this, we conducted 31 semi-structured interviews with professional spreadsheet users from two separate samples. We found that spreadsheet providers face challenges in adapting highly personalised strategies to often subjective standards and evaluating the appropriate social timing of sharing. In addition, conflicted self-evaluations of one's spreadsheet expertise, dismissive normative beliefs about the value of this knowledge, and concerns about the potential disruptions associated with collaboration can further deter sharing. We suggest these observations reflect the challenges of long-term learning in feature-rich software designed primarily with initial learnability in mind. We therefore provide implications for design to navigate this tension. Overall, our findings demonstrate how the complex interaction between technology design and social dynamics can shape collaborative learning behaviours in the context of feature-rich software."}
{"id": "2506.03232", "title": "Pivoting the paradigm: the role of spreadsheets in K-12 data science", "url": "https://arxiv.org/abs/2506.03232", "pdf": "https://arxiv.org/pdf/2506.03232", "abs": "https://arxiv.org/abs/2506.03232", "authors": ["Oren Tirschwell", "Nicholas Jon Horton"], "categories": ["stat.OT", "cs.CY"], "comment": null, "summary": "Spreadsheet tools are widely accessible to and commonly used by K-12 students and teachers. They have an important role in data collection and organization. Beyond data organization, spreadsheets also make data visible and easy to interact with, facilitating student engagement in data exploration and analysis. Though not suitable for all circumstances, spreadsheets can and do help foster data and computing skills for K-12 students. This paper 1) reviews prior frameworks on K-12 data tools; 2) proposes data-driven learning outcomes that can be accomplished by incorporating spreadsheets into the curriculum; and 3) discusses how spreadsheets can help develop data acumen and computational fluency. We provide example class activities, identify challenges and barriers to adoption, suggest pedagogical approaches to ease the learning curve for instructors and students, and discuss the need for professional development to facilitate deeper use of spreadsheets for data science and STEM disciplines."}
{"id": "2505.23667", "title": "Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models", "url": "https://arxiv.org/abs/2505.23667", "pdf": "https://arxiv.org/pdf/2505.23667", "abs": "https://arxiv.org/abs/2505.23667", "authors": ["Lang Cao", "Jingxian Xu", "Hanbing Liu", "Jinyu Wang", "Mengyu Zhou", "Haoyu Dong", "Shi Han", "Dongmei Zhang"], "categories": ["cs.AI"], "comment": null, "summary": "Tables are a fundamental structure for organizing and analyzing data, making effective table understanding a critical capability for intelligent systems. While large language models (LMs) demonstrate strong general reasoning abilities, they continue to struggle with accurate numerical or symbolic reasoning over tabular data, especially in complex scenarios. Spreadsheet formulas provide a powerful and expressive medium for representing executable symbolic operations, encoding rich reasoning patterns that remain largely underutilized. In this paper, we propose Formula Tuning (Fortune), a reinforcement learning (RL) framework that trains LMs to generate executable spreadsheet formulas for question answering over general tabular data. Formula Tuning reduces the reliance on supervised formula annotations by using binary answer correctness as a reward signal, guiding the model to learn formula derivation through reasoning. We provide a theoretical analysis of its advantages and demonstrate its effectiveness through extensive experiments on seven table reasoning benchmarks. Formula Tuning substantially enhances LM performance, particularly on multi-step numerical and symbolic reasoning tasks, enabling a 7B model to outperform OpenAI o1 on table understanding. This highlights the potential of formula-driven RL to advance symbolic table reasoning in LMs."}
{"id": "2505.23296", "title": "Is spreadsheet syntax better than numeric indexing for cell selection?", "url": "https://arxiv.org/abs/2505.23296", "pdf": "https://arxiv.org/pdf/2505.23296", "abs": "https://arxiv.org/abs/2505.23296", "authors": ["Philip Heltweg", "Dirk Riehle", "Georg-Daniel Schwarz"], "categories": ["cs.PL"], "comment": null, "summary": "Selecting a subset of cells is a common task in data engineering, for example, to remove errors or select only specific parts of a table. Multiple approaches to express this selection exist. One option is numeric indexing, commonly found in general programming languages, where a tuple of numbers identifies the cell. Alternatively, the separate dimensions can be referred to using different enumeration schemes like \"A1\" for the first cell, commonly found in software such as spreadsheet systems.\n  In a large-scale controlled experiment with student participants as proxy for data practitioners, we compare the two options with respect to speed and correctness of reading and writing code.\n  The results show that, when reading code, participants make less mistakes using spreadsheet-style syntax. Additionally, when writing code, they make fewer mistakes and are faster when using spreadsheet syntax compared to numeric syntax.\n  From this, a domain-specific syntax, such as spreadsheet syntax for data engineering, appears to be a promising alternative to explore in future tools to support practitioners without a software engineering background."}
{"id": "2504.20681", "title": "Data Encryption Battlefield: A Deep Dive into the Dynamic Confrontations in Ransomware Attacks", "url": "https://arxiv.org/abs/2504.20681", "pdf": "https://arxiv.org/pdf/2504.20681", "abs": "https://arxiv.org/abs/2504.20681", "authors": ["Arash Mahboubi", "Hamed Aboutorab", "Seyit Camtepe", "Hang Thanh Bui", "Khanh Luong", "Keyvan Ansari", "Shenlu Wang", "Bazara Barry"], "categories": ["cs.CR"], "comment": null, "summary": "In the rapidly evolving landscape of cybersecurity threats, ransomware represents a significant challenge. Attackers increasingly employ sophisticated encryption methods, such as entropy reduction through Base64 encoding, and partial or intermittent encryption to evade traditional detection methods. This study explores the dynamic battle between adversaries who continuously refine encryption strategies and defenders developing advanced countermeasures to protect vulnerable data. We investigate the application of online incremental machine learning algorithms designed to predict file encryption activities despite adversaries evolving obfuscation techniques. Our analysis utilizes an extensive dataset of 32.6 GB, comprising 11,928 files across multiple formats, including Microsoft Word documents (doc), PowerPoint presentations (ppt), Excel spreadsheets (xlsx), image formats (jpg, jpeg, png, tif, gif), PDFs (pdf), audio (mp3), and video (mp4) files. These files were encrypted by 75 distinct ransomware families, facilitating a robust empirical evaluation of machine learning classifiers effectiveness against diverse encryption tactics. Results highlight the Hoeffding Tree algorithms superior incremental learning capability, particularly effective in detecting traditional and AES-Base64 encryption methods employed to lower entropy. Conversely, the Random Forest classifier with warm-start functionality excels at identifying intermittent encryption methods, demonstrating the necessity of tailored machine learning solutions to counter sophisticated ransomware strategies."}
{"id": "2504.20657", "title": "Image deidentification in the XNAT ecosystem: use cases and solutions", "url": "https://arxiv.org/abs/2504.20657", "pdf": "https://arxiv.org/pdf/2504.20657", "abs": "https://arxiv.org/abs/2504.20657", "authors": ["Alex Michie", "Simon J Doran"], "categories": ["cs.CV"], "comment": "For submission to MELBA (Machine Learning for Biomedical Imaging) special issue on the MIDI-B deidentification challenge (https://www.synapse.org/Synapse:syn53065760). 11 pages, 1 fig, 2 tables; 1 supplementary data file (supplementary_tables_S1_S2_S3.xlsx) containing three spreadsheet tabs", "summary": "XNAT is a server-based data management platform widely used in academia for curating large databases of DICOM images for research projects. We describe in detail a deidentification workflow for DICOM data using facilities in XNAT, together with independent tools in the XNAT \"ecosystem\". We list different contexts in which deidentification might be needed, based on our prior experience. The starting point for participation in the Medical Image De-Identification Benchmark (MIDI-B) challenge was a set of pre-existing local methodologies, which were adapted during the validation phase of the challenge. Our result in the test phase was 97.91\\%, considerably lower than our peers, due largely to an arcane technical incompatibility of our methodology with the challenge's Synapse platform, which prevented us receiving feedback during the validation phase. Post-submission, additional discrepancy reports from the organisers and via the MIDI-B Continuous Benchmarking facility, enabled us to improve this score significantly to 99.61\\%. An entirely rule-based approach was shown to be capable of removing all name-related information in the test corpus, but exhibited failures in dealing fully with address data. Initial experiments using published machine-learning models to remove addresses were partially successful but showed the models to be \"over-aggressive\" on other types of free-text data, leading to a slight overall degradation in performance to 99.54\\%. Future development will therefore focus on improving address-recognition capabilities, but also on better removal of identifiable data burned into the image pixels. Several technical aspects relating to the \"answer key\" are still under discussion with the challenge organisers, but we estimate that our percentage of genuine deidentification failures on the MIDI-B test corpus currently stands at 0.19\\%. (Abridged from original for arXiv submission)"}
{"id": "2408.12622", "title": "The AI Risk Repository: A Comprehensive Meta-Review, Database, and Taxonomy of Risks From Artificial Intelligence", "url": "https://arxiv.org/abs/2408.12622", "pdf": "https://arxiv.org/pdf/2408.12622", "abs": "https://arxiv.org/abs/2408.12622", "authors": ["Peter Slattery", "Alexander K. Saeri", "Emily A. C. Grundy", "Jess Graham", "Michael Noetel", "Risto Uuk", "James Dao", "Soroush Pour", "Stephen Casper", "Neil Thompson"], "categories": ["cs.AI", "cs.CR", "cs.ET", "cs.LG", "eess.SY"], "comment": null, "summary": "The risks posed by Artificial Intelligence (AI) are of considerable concern to academics, auditors, policymakers, AI companies, and the public. However, a lack of shared understanding of AI risks can impede our ability to comprehensively discuss, research, and react to them. This paper addresses this gap by creating an AI Risk Repository to serve as a common frame of reference. This comprises a living database of 777 risks extracted from 43 taxonomies, which can be filtered based on two overarching taxonomies and easily accessed, modified, and updated via our website and online spreadsheets. We construct our Repository with a systematic review of taxonomies and other structured classifications of AI risk followed by an expert consultation. We develop our taxonomies of AI risk using a best-fit framework synthesis. Our high-level Causal Taxonomy of AI Risks classifies each risk by its causal factors (1) Entity: Human, AI; (2) Intentionality: Intentional, Unintentional; and (3) Timing: Pre-deployment; Post-deployment. Our mid-level Domain Taxonomy of AI Risks classifies risks into seven AI risk domains: (1) Discrimination & toxicity, (2) Privacy & security, (3) Misinformation, (4) Malicious actors & misuse, (5) Human-computer interaction, (6) Socioeconomic & environmental, and (7) AI system safety, failures, & limitations. These are further divided into 23 subdomains. The AI Risk Repository is, to our knowledge, the first attempt to rigorously curate, analyze, and extract AI risk frameworks into a publicly accessible, comprehensive, extensible, and categorized risk database. This creates a foundation for a more coordinated, coherent, and complete approach to defining, auditing, and managing the risks posed by AI systems."}
{"id": "2407.09025", "title": "SpreadsheetLLM: Encoding Spreadsheets for Large Language Models", "url": "https://arxiv.org/abs/2407.09025", "pdf": "https://arxiv.org/pdf/2407.09025", "abs": "https://arxiv.org/abs/2407.09025", "authors": ["Haoyu Dong", "Jianbo Zhao", "Yuzhang Tian", "Junyu Xiong", "Shiyu Xia", "Mengyu Zhou", "Yun Lin", "José Cambronero", "Yeye He", "Shi Han", "Dongmei Zhang"], "categories": ["cs.AI"], "comment": null, "summary": "Spreadsheets are characterized by their extensive two-dimensional grids, flexible layouts, and varied formatting options, which pose significant challenges for large language models (LLMs). In response, we introduce SpreadsheetLLM, pioneering an efficient encoding method designed to unleash and optimize LLMs' powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs' token constraints, making it impractical for most applications. To tackle this challenge, we develop SheetCompressor, an innovative encoding framework that compresses spreadsheets effectively for LLMs. It comprises three modules: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in the spreadsheet table detection task, outperforming the vanilla approach by 25.6% in GPT4's in-context learning setting. Moreover, fine-tuned LLM with SheetCompressor has an average compression ratio of 25 times, and achieves a state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%. Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet understanding and validate it in a new and demanding spreadsheet QA task. We methodically leverage the inherent layout and structure of spreadsheets, demonstrating that SpreadsheetLLM is highly effective across a variety of spreadsheet tasks."}
{"id": "2403.03636", "title": "SheetAgent: Towards A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models", "url": "https://arxiv.org/abs/2403.03636", "pdf": "https://arxiv.org/pdf/2403.03636", "abs": "https://arxiv.org/abs/2403.03636", "authors": ["Yibin Chen", "Yifu Yuan", "Zeyu Zhang", "Yan Zheng", "Jinyi Liu", "Fei Ni", "Jianye Hao", "Hangyu Mao", "Fuzheng Zhang"], "categories": ["cs.AI", "cs.LG"], "comment": "Accepted by International World Wide Web Conference (WWW) 2025 (oral)", "summary": "Spreadsheets are ubiquitous across the World Wide Web, playing a critical role in enhancing work efficiency across various domains. Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce SheetRM, a benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges. To mitigate the above challenges, we further propose SheetAgent, a novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of three collaborative modules: Planner, Informer, and Retriever, achieving both advanced reasoning and accurate manipulation over spreadsheets without human interaction through iterative task reasoning and reflection. Extensive experiments demonstrate that SheetAgent delivers 20--40\\% pass rate improvements on multiple benchmarks over baselines, achieving enhanced precision in spreadsheet manipulation and demonstrating superior table reasoning abilities. More details and visualizations are available at the project website: https://sheetagent.github.io/. The datasets and source code are available at https://anonymous.4open.science/r/SheetAgent."}
{"id": "2403.19318", "title": "TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios", "url": "https://arxiv.org/abs/2403.19318", "pdf": "https://arxiv.org/pdf/2403.19318", "abs": "https://arxiv.org/abs/2403.19318", "authors": ["Xiaokang Zhang", "Sijia Luo", "Bohan Zhang", "Zeyao Ma", "Jing Zhang", "Yang Li", "Guanlin Li", "Zijun Yao", "Kangli Xu", "Jinchang Zhou", "Daniel Zhang-Li", "Jifan Yu", "Shu Zhao", "Juanzi Li", "Jie Tang"], "categories": ["cs.CL"], "comment": "https://tablellm.github.io/", "summary": "We introduce TableLLM, a robust large language model (LLM) with 8 billion parameters, purpose-built for proficiently handling tabular data manipulation tasks, whether they are embedded within documents or spreadsheets, catering to real-world office scenarios. We propose a distant supervision method for training, which comprises a reasoning process extension strategy, aiding in training LLMs to understand reasoning patterns more effectively as well as a cross-way validation strategy, ensuring the quality of the automatically generated data. To evaluate the performance of TableLLM, we have crafted benchmarks tailored to address both document and spreadsheet formats as well as constructed a well-organized evaluation pipeline capable of handling both scenarios. Thorough evaluations underscore the advantages of TableLLM when compared to various existing general-purpose and tabular data-focused LLMs. We have publicly released the model checkpoint, source code, benchmarks, and a web application for user interaction. Our codes and data are publicly available at https://github.com/TableLLM/TableLLM."}
{"id": "2502.11267", "title": "Prompting in the Dark: Assessing Human Performance in Prompt Engineering for Data Labeling When Gold Labels Are Absent", "url": "https://arxiv.org/abs/2502.11267", "pdf": "https://arxiv.org/pdf/2502.11267", "abs": "https://arxiv.org/abs/2502.11267", "authors": ["Zeyu He", "Saniya Naphade", "Ting-Hao 'Kenneth' Huang"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted By CHI 2025", "summary": "Millions of users prompt large language models (LLMs) for various tasks, but how good are people at prompt engineering? Do users actually get closer to their desired outcome over multiple iterations of their prompts? These questions are crucial when no gold-standard labels are available to measure progress. This paper investigates a scenario in LLM-powered data labeling, \"prompting in the dark,\" where users iteratively prompt LLMs to label data without using manually-labeled benchmarks. We developed PromptingSheet, a Google Sheets add-on that enables users to compose, revise, and iteratively label data through spreadsheets. Through a study with 20 participants, we found that prompting in the dark was highly unreliable -- only 9 participants improved labeling accuracy after four or more iterations. Automated prompt optimization tools like DSPy also struggled when few gold labels were available. Our findings highlight the importance of gold labels and the needs, as well as the risks, of automated support in human prompt engineering, providing insights for future tool design."}
{"id": "2502.05113", "title": "GiesKaNe: Bridging Past and Present in Grammatical Theory and Practical Application", "url": "https://arxiv.org/abs/2502.05113", "pdf": "https://arxiv.org/pdf/2502.05113", "abs": "https://arxiv.org/abs/2502.05113", "authors": ["Volker Emmrich"], "categories": ["cs.CL"], "comment": null, "summary": "This article explores the requirements for corpus compilation within the GiesKaNe project (University of Giessen and Kassel, Syntactic Basic Structures of New High German). The project is defined by three central characteristics: it is a reference corpus, a historical corpus, and a syntactically deeply annotated treebank. As a historical corpus, GiesKaNe aims to establish connections with both historical and contemporary corpora, ensuring its relevance across temporal and linguistic contexts. The compilation process strikes the balance between innovation and adherence to standards, addressing both internal project goals and the broader interests of the research community. The methodological complexity of such a project is managed through a complementary interplay of human expertise and machine-assisted processes. The article discusses foundational topics such as tokenization, normalization, sentence definition, tagging, parsing, and inter-annotator agreement, alongside advanced considerations. These include comparisons between grammatical models, annotation schemas, and established de facto annotation standards as well as the integration of human and machine collaboration. Notably, a novel method for machine-assisted classification of texts along the continuum of conceptual orality and literacy is proposed, offering new perspectives on text selection. Furthermore, the article introduces an approach to deriving de facto standard annotations from existing ones, mediating between standardization and innovation. In the course of describing the workflow the article demonstrates that even ambitious projects like GiesKaNe can be effectively implemented using existing research infrastructure, requiring no specialized annotation tools. Instead, it is shown that the workflow can be based on the strategic use of a simple spreadsheet and integrates the capabilities of the existing infrastructure."}
{"id": "2502.04389", "title": "Overcoming Vision Language Model Challenges in Diagram Understanding: A Proof-of-Concept with XML-Driven Large Language Models Solutions", "url": "https://arxiv.org/abs/2502.04389", "pdf": "https://arxiv.org/pdf/2502.04389", "abs": "https://arxiv.org/abs/2502.04389", "authors": ["Shue Shiinoki", "Ryo Koshihara", "Hayato Motegi", "Masumi Morishige"], "categories": ["cs.SE", "cs.AI"], "comment": "The related code is available at \\url{https://github.com/galirage/spreadsheet-intelligence}, which provides the core library developed for this research. The experimental code using this library can be found at \\url{https://github.com/galirage/XMLDriven-Diagram-Understanding}", "summary": "Diagrams play a crucial role in visually conveying complex relationships and processes within business documentation. Despite recent advances in Vision-Language Models (VLMs) for various image understanding tasks, accurately identifying and extracting the structures and relationships depicted in diagrams continues to pose significant challenges. This study addresses these challenges by proposing a text-driven approach that bypasses reliance on VLMs' visual recognition capabilities. Instead, it utilizes the editable source files--such as xlsx, pptx or docx--where diagram elements (e.g., shapes, lines, annotations) are preserved as textual metadata. In our proof-of-concept, we extracted diagram information from xlsx-based system design documents and transformed the extracted shape data into textual input for Large Language Models (LLMs). This approach allowed the LLM to analyze relationships and generate responses to business-oriented questions without the bottleneck of image-based processing. Experimental comparisons with a VLM-based method demonstrated that the proposed text-driven framework yielded more accurate answers for questions requiring detailed comprehension of diagram structures.The results obtained in this study are not limited to the tested .xlsx files but can also be extended to diagrams in other documents with source files, such as Office pptx and docx formats. These findings highlight the feasibility of circumventing VLM constraints through direct textual extraction from original source files. By enabling robust diagram understanding through LLMs, our method offers a promising path toward enhanced workflow efficiency and information analysis in real-world business scenarios."}
{"id": "2501.18268", "title": "Reducing Aleatoric and Epistemic Uncertainty through Multi-modal Data Acquisition", "url": "https://arxiv.org/abs/2501.18268", "pdf": "https://arxiv.org/pdf/2501.18268", "abs": "https://arxiv.org/abs/2501.18268", "authors": ["Arthur Hoarau", "Benjamin Quost", "Sébastien Destercke", "Willem Waegeman"], "categories": ["cs.LG"], "comment": null, "summary": "To generate accurate and reliable predictions, modern AI systems need to combine data from multiple modalities, such as text, images, audio, spreadsheets, and time series. Multi-modal data introduces new opportunities and challenges for disentangling uncertainty: it is commonly assumed in the machine learning community that epistemic uncertainty can be reduced by collecting more data, while aleatoric uncertainty is irreducible. However, this assumption is challenged in modern AI systems when information is obtained from different modalities. This paper introduces an innovative data acquisition framework where uncertainty disentanglement leads to actionable decisions, allowing sampling in two directions: sample size and data modality. The main hypothesis is that aleatoric uncertainty decreases as the number of modalities increases, while epistemic uncertainty decreases by collecting more observations. We provide proof-of-concept implementations on two multi-modal datasets to showcase our data acquisition framework, which combines ideas from active learning, active feature acquisition and uncertainty quantification."}
{"id": "2407.04065", "title": "On the Workflows and Smells of Leaderboard Operations (LBOps): An Exploratory Study of Foundation Model Leaderboards", "url": "https://arxiv.org/abs/2407.04065", "pdf": "https://arxiv.org/pdf/2407.04065", "abs": "https://arxiv.org/abs/2407.04065", "authors": ["Zhimin Zhao", "Abdul Ali Bangash", "Filipe Roseiro Côgo", "Bram Adams", "Ahmed E. Hassan"], "categories": ["cs.SE", "cs.LG"], "comment": "Awesome Foundation Model Leaderboard List: https://github.com/SAILResearch/awesome-foundation-model-leaderboards; Foundation Model Leaderboard Search Toolkit: https://huggingface.co/spaces/zhiminy/awesome-foundation-model-leaderboard-search", "summary": "Foundation models (FM), such as large language models (LLMs), which are large-scale machine learning (ML) models, have demonstrated remarkable adaptability in various downstream software engineering (SE) tasks, such as code completion, code understanding, and software development. As a result, FM leaderboards have become essential tools for SE teams to compare and select the best third-party FMs for their specific products and purposes. However, the lack of standardized guidelines for FM evaluation and comparison threatens the transparency of FM leaderboards and limits stakeholders' ability to perform effective FM selection. As a first step towards addressing this challenge, our research focuses on understanding how these FM leaderboards operate in real-world scenarios (\"leaderboard operations\") and identifying potential pitfalls and areas for improvement (\"leaderboard smells\"). In this regard, we collect up to 1,045 FM leaderboards from five different sources: GitHub, Hugging Face Spaces, Papers With Code, spreadsheet and independent platform, to examine their documentation and engage in direct communication with leaderboard operators to understand their workflows. Through card sorting and negotiated agreement, we identify five distinct workflow patterns and develop a domain model that captures the key components and their interactions within these workflows. We then identify eight unique types of leaderboard smells in LBOps. By mitigating these smells, SE teams can improve transparency, accountability, and collaboration in current LBOps practices, fostering a more robust and responsible ecosystem for FM comparison and selection."}
{"id": "2412.11711", "title": "MiMoTable: A Multi-scale Spreadsheet Benchmark with Meta Operations for Table Reasoning", "url": "https://arxiv.org/abs/2412.11711", "pdf": "https://arxiv.org/pdf/2412.11711", "abs": "https://arxiv.org/abs/2412.11711", "authors": ["Zheng Li", "Yang Du", "Mao Zheng", "Mingyang Song"], "categories": ["cs.CL"], "comment": "Accepted by COLING 2025", "summary": "Extensive research has been conducted to explore the capability of Large Language Models (LLMs) for table reasoning and has significantly improved the performance on existing benchmarks. However, tables and user questions in real-world applications are more complex and diverse, presenting an unignorable gap compared to the existing benchmarks. To fill the gap, we propose a \\textbf{M}ult\\textbf{i}-scale spreadsheet benchmark with \\textbf{M}eta \\textbf{o}perations for \\textbf{Table} reasoning, named as MiMoTable. Specifically, MiMoTable incorporates two key features. First, the tables in MiMoTable are all spreadsheets used in real-world scenarios, which cover seven domains and contain different types. Second, we define a new criterion with six categories of meta operations for measuring the difficulty of each question in MiMoTable, simultaneously as a new perspective for measuring the difficulty of the existing benchmarks. Experimental results show that Claude-3.5-Sonnet achieves the best performance with 77.4\\% accuracy, indicating that there is still significant room to improve for LLMs on MiMoTable. Furthermore, we grade the difficulty of existing benchmarks according to our new criteria. Experiments have shown that the performance of LLMs decreases as the difficulty of benchmarks increases, thereby proving the effectiveness of our proposed new criterion."}
{"id": "2412.15030", "title": "When Copilot Becomes Autopilot: Generative AI's Critical Risk to Knowledge Work and a Critical Solution", "url": "https://arxiv.org/abs/2412.15030", "pdf": "https://arxiv.org/pdf/2412.15030", "abs": "https://arxiv.org/abs/2412.15030", "authors": ["Advait Sarkar", "Xiaotong", "Xu", "Neil Toronto", "Ian Drosos", "Christian Poelitz"], "categories": ["cs.HC"], "comment": null, "summary": "Generative AI, with its tendency to \"hallucinate\" incorrect results, may pose a risk to knowledge work by introducing errors. On the other hand, it may also provide unprecedented opportunities for users, particularly non-experts, to learn and apply advanced software features and greatly increase the scope and complexity of tasks they can successfully achieve.\n  As an example of a complex knowledge workflow that is subject to risks and opportunities from generative AI, we consider the spreadsheet. AI hallucinations are an important challenge, but they are not the greatest risk posed by generative AI to spreadsheet workflows. Rather, as more work can be safely delegated to AI, the risk is that human critical thinking -- the ability to holistically and rigorously evaluate a problem and its solutions -- is degraded in the process. The solution is to design the interfaces of generative AI systems deliberately to foster and encourage critical thinking in knowledge work, building primarily on a long history of research on critical thinking tools for education.\n  We discuss a prototype system for the activity of critical shortlisting in spreadsheets. The system uses generative AI to suggest shortlisting criteria and applies these criteria to sort rows in a spreadsheet. It also generates \"provocations\": short text snippets that critique the AI-generated criteria, highlighting risks, shortcomings, and alternatives. Our prototype opens up a rich and completely unexplored design space of critical thinking tools for modern AI-assisted knowledge work. We outline a research agenda for AI as a critic or provocateur, including questions about where and when provocations should appear, their form and content, and potential design trade-offs."}
{"id": "2412.14062", "title": "Understanding and Evaluating Trust in Generative AI and Large Language Models for Spreadsheets", "url": "https://arxiv.org/abs/2412.14062", "pdf": "https://arxiv.org/pdf/2412.14062", "abs": "https://arxiv.org/abs/2412.14062", "authors": ["Simon Thorne"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Generative AI and Large Language Models (LLMs) hold promise for automating spreadsheet formula creation. However, due to hallucinations, bias and variable user skill, outputs obtained from generative AI cannot be assumed to be accurate or trustworthy. To address these challenges, a trustworthiness framework is proposed based on evaluating the transparency and dependability of the formula. The transparency of the formula is explored through explainability (understanding the formula's reasoning) and visibility (inspecting the underlying algorithms). The dependability of the generated formula is evaluated in terms of reliability (consistency and accuracy) and ethical considerations (bias and fairness). The paper also examines the drivers to these metrics in the form of hallucinations, training data bias and poorly constructed prompts. Finally, examples of mistrust in technology are considered and the consequences explored."}
{"id": "2412.02357", "title": "Dynamic Prompt Middleware: Contextual Prompt Refinement Controls for Comprehension Tasks", "url": "https://arxiv.org/abs/2412.02357", "pdf": "https://arxiv.org/pdf/2412.02357", "abs": "https://arxiv.org/abs/2412.02357", "authors": ["Ian Drosos", "Jack Williams", "Advait Sarkar", "Nicholas Wilson"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Effective prompting of generative AI is challenging for many users, particularly in expressing context for comprehension tasks such as explaining spreadsheet formulas, Python code, and text passages. Prompt middleware aims to address this barrier by assisting in prompt construction, but barriers remain for users in expressing adequate control so that they can receive AI-responses that match their preferences.\n  We conduct a formative survey (n=38) investigating user needs for control over AI-generated explanations in comprehension tasks, which uncovers a trade-off between standardized but predictable support for prompting, and adaptive but unpredictable support tailored to the user and task. To explore this trade-off, we implement two prompt middleware approaches: Dynamic Prompt Refinement Control (Dynamic PRC) and Static Prompt Refinement Control (Static PRC). The Dynamic PRC approach generates context-specific UI elements that provide prompt refinements based on the user's prompt and user needs from the AI, while the Static PRC approach offers a preset list of generally applicable refinements.\n  We evaluate these two approaches with a controlled user study (n=16) to assess the impact of these approaches on user control of AI responses for crafting better explanations. Results show a preference for the Dynamic PRC approach as it afforded more control, lowered barriers to providing context, and encouraged exploration and reflection of the tasks, but that reasoning about the effects of different generated controls on the final output remains challenging. Drawing on participant feedback, we discuss design implications for future Dynamic PRC systems that enhance user control of AI responses. Our findings suggest that dynamic prompt middleware can improve the user experience of generative AI workflows by affording greater control and guide users to a better AI response."}
{"id": "2406.12031", "title": "Large Scale Transfer Learning for Tabular Data via Language Modeling", "url": "https://arxiv.org/abs/2406.12031", "pdf": "https://arxiv.org/pdf/2406.12031", "abs": "https://arxiv.org/abs/2406.12031", "authors": ["Josh Gardner", "Juan C. Perdomo", "Ludwig Schmidt"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "NeurIPS 2024 camera-ready updates", "summary": "Tabular data -- structured, heterogeneous, spreadsheet-style data with rows and columns -- is widely used in practice across many domains. However, while recent foundation models have reduced the need for developing task-specific datasets and predictors in domains such as language modeling and computer vision, this transfer learning paradigm has not had similar impact in the tabular domain. In this work, we seek to narrow this gap and present TabuLa-8B, a language model for tabular prediction. We define a process for extracting a large, high-quality training dataset from the TabLib corpus, proposing methods for tabular data filtering and quality control. Using the resulting dataset, which comprises over 2.1B rows from over 4M unique tables, we fine-tune a Llama 3-8B large language model (LLM) for tabular data prediction (classification and binned regression) using a novel packing and attention scheme for tabular prediction. Through evaluation across a test suite of 329 datasets, we find that TabuLa-8B has zero-shot accuracy on unseen tables that is over 15 percentage points (pp) higher than random guessing, a feat that is not possible with existing state-of-the-art tabular prediction models (e.g. XGBoost, TabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the target datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN models that are explicitly trained on equal, or even up to 16x more data. We release our model, code, and data along with the publication of this paper."}
{"id": "2402.05121", "title": "Large Language Model for Table Processing: A Survey", "url": "https://arxiv.org/abs/2402.05121", "pdf": "https://arxiv.org/pdf/2402.05121", "abs": "https://arxiv.org/abs/2402.05121", "authors": ["Weizheng Lu", "Jing Zhang", "Ju Fan", "Zihao Fu", "Yueguo Chen", "Xiaoyong Du"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, web table question answering, and image table information extraction. Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides a comprehensive overview of table-related tasks, examining both user scenarios and technical aspects. It covers traditional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processing. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought."}
{"id": "2406.14991", "title": "SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation", "url": "https://arxiv.org/abs/2406.14991", "pdf": "https://arxiv.org/pdf/2406.14991", "abs": "https://arxiv.org/abs/2406.14991", "authors": ["Zeyao Ma", "Bohan Zhang", "Jing Zhang", "Jifan Yu", "Xiaokang Zhang", "Xiaohan Zhang", "Sijia Luo", "Xi Wang", "Jie Tang"], "categories": ["cs.CL", "cs.SE"], "comment": "Neurips 2024 (Spotlight); Homepage: https://spreadsheetbench.github.io/", "summary": "We introduce SpreadsheetBench, a challenging spreadsheet manipulation benchmark exclusively derived from real-world scenarios, designed to immerse current large language models (LLMs) in the actual workflow of spreadsheet users. Unlike existing benchmarks that rely on synthesized queries and simplified spreadsheet files, SpreadsheetBench is built from 912 real questions gathered from online Excel forums, which reflect the intricate needs of users. The associated spreadsheets from the forums contain a variety of tabular data such as multiple tables, non-standard relational tables, and abundant non-textual elements. Furthermore, we propose a more reliable evaluation metric akin to online judge platforms, where multiple spreadsheet files are created as test cases for each instruction, ensuring the evaluation of robust solutions capable of handling spreadsheets with varying values. Our comprehensive evaluation of various LLMs under both single-round and multi-round inference settings reveals a substantial gap between the state-of-the-art (SOTA) models and human performance, highlighting the benchmark's difficulty."}
{"id": "2409.20224", "title": "Trapped in Transformative Agreements? A Multifaceted Analysis of >1,000 Contracts", "url": "https://arxiv.org/abs/2409.20224", "pdf": "https://arxiv.org/pdf/2409.20224", "abs": "https://arxiv.org/abs/2409.20224", "authors": ["Laura Rothfritz", "W. Benedikt Schmal", "Ulrich Herb"], "categories": ["cs.DL"], "comment": "37 pages, appendix", "summary": "Transformative agreements between academic publishers and research institutions are ubiquitous. The 'Efficiency and Standards for Article Charges' (ESAC) Initiative lists more than 1,000 contracts in its database. We make use of this unique dataset by web-scraping the details of every contract to substantially expand the overview spreadsheet provided by the ESAC Initiative. Based on that hitherto unused data source, we combine qualitative and quantitative methods to conduct an in-depth analysis of the contract characteristics and the TA landscape. Our analysis demonstrates that research institutions seem to be 'trapped' in transformative agreements. Instead of being a bridge towards a fully Open Access world, academia is stuck in the hybrid system. This endows the legacy (non-Open Access) publishing houses with substantial market power. It raises entry barriers, lowers competition, and increases costs for libraries and universities."}
{"id": "2409.12974", "title": "Exploring Higher Education Competencies through Spreadsheet Self-Assessment and Time", "url": "https://arxiv.org/abs/2409.12974", "pdf": "https://arxiv.org/pdf/2409.12974", "abs": "https://arxiv.org/abs/2409.12974", "authors": ["Maria Csernoch", "Judit T. Kiss", "Viktor Takács", "Domicián Máté"], "categories": ["cs.HC"], "comment": "16 pages, 10 colour figures, 9 tables", "summary": "The present paper aims to explore higher education students' spreadsheet competencies and reliability through self-assessment and real-world problem-solving practices. Digital natives alleged skills and competences allowed us to hypothesize that students perform better in Excel than on paper, but the findings cannot confirm this hypothesis. However, our results indicate that students tend to inaccurately assess their spreadsheet competencies compared to their actual performance in both paper-based and Excel tasks. It has also be found that students need at least twice as much time to achieve the same high scores in the digital environment as they do on paper. The results violated the widely accepted assumption that digital native students do not need computer science education, since they are born with it. This study highlights the importance of accurate self-assessment in digital skill development and time management within higher education contexts, particularly in technology-driven disciplines."}
{"id": "2403.07762", "title": "Supporting Annotators with Affordances for Efficiently Labeling Conversational Data", "url": "https://arxiv.org/abs/2403.07762", "pdf": "https://arxiv.org/pdf/2403.07762", "abs": "https://arxiv.org/abs/2403.07762", "authors": ["Austin Z. Henley", "David Piorkowski"], "categories": ["cs.HC"], "comment": null, "summary": "Without well-labeled ground truth data, machine learning-based systems would not be as ubiquitous as they are today, but these systems rely on substantial amounts of correctly labeled data. Unfortunately, crowdsourced labeling is time consuming and expensive. To address the concerns of effort and tedium, we designed CAL, a novel interface to aid in data labeling. We made several key design decisions for CAL, which include preventing inapt labels from being selected, guiding users in selecting an appropriate label when they need assistance, incorporating labeling documentation into the interface, and providing an efficient means to view previous labels. We implemented a production-quality implementation of CAL and report a user-study evaluation that compares CAL to a standard spreadsheet. Key findings of our study include users using CAL reported lower cognitive load, did not increase task time, users rated CAL to be easier to use, and users preferred CAL over the spreadsheet."}
{"id": "2310.09985", "title": "Prompting for Discovery: Flexible Sense-Making for AI Art-Making with Dreamsheets", "url": "https://arxiv.org/abs/2310.09985", "pdf": "https://arxiv.org/pdf/2310.09985", "abs": "https://arxiv.org/abs/2310.09985", "authors": ["Shm Garanganao Almeda", "J. D. Zamfirescu-Pereira", "Kyu Won Kim", "Pradeep Mani Rathnam", "Bjoern Hartmann"], "categories": ["cs.HC"], "comment": "13 pages, 14 figures, currently under review", "summary": "Design space exploration (DSE) for Text-to-Image (TTI) models entails navigating a vast, opaque space of possible image outputs, through a commensurately vast input space of hyperparameters and prompt text. Minor adjustments to prompt input can surface unexpectedly disparate images. How can interfaces support end-users in reliably steering prompt-space explorations towards interesting results? Our design probe, DreamSheets, supports exploration strategies with LLM-based functions for assisted prompt construction and simultaneous display of generated results, hosted in a spreadsheet interface. The flexible layout and novel generative functions enable experimentation with user-defined workflows. Two studies, a preliminary lab study and a longitudinal study with five expert artists, revealed a set of strategies participants use to tackle the challenges of TTI design space exploration, and the interface features required to support them - like using text-generation to define local \"axes\" of exploration. We distill these insights into a UI mockup to guide future interfaces."}
{"id": "2402.14853", "title": "NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries", "url": "https://arxiv.org/abs/2402.14853", "pdf": "https://arxiv.org/pdf/2402.14853", "abs": "https://arxiv.org/abs/2402.14853", "authors": ["Wei Zhao", "Zhitao Hou", "Siyuan Wu", "Yan Gao", "Haoyu Dong", "Yao Wan", "Hongyu Zhang", "Yulei Sui", "Haidong Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear at EACL 2024", "summary": "Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets, is a widespread practice among users performing data analysis. However, crafting formulas on spreadsheets remains a tedious and error-prone task for many end-users, particularly when dealing with complex operations. To alleviate the burden associated with writing spreadsheet formulas, this paper introduces a novel benchmark task called NL2Formula, with the aim to generate executable formulas that are grounded on a spreadsheet table, given a Natural Language (NL) query as input. To accomplish this, we construct a comprehensive dataset consisting of 70,799 paired NL queries and corresponding spreadsheet formulas, covering 21,670 tables and 37 types of formula functions. We realize the NL2Formula task by providing a sequence-to-sequence baseline implementation called fCoder. Experimental results validate the effectiveness of fCoder, demonstrating its superior performance compared to the baseline models. Furthermore, we also compare fCoder with an initial GPT-3.5 model (i.e., text-davinci-003). Lastly, through in-depth error analysis, we identify potential challenges in the NL2Formula task and advocate for further investigation."}
{"id": "2006.14706", "title": "Will Dynamic Arrays finally change the way Models are built?", "url": "https://arxiv.org/abs/2006.14706", "pdf": "https://arxiv.org/pdf/2006.14706", "abs": "https://arxiv.org/abs/2006.14706", "authors": ["Peter Bartholomew"], "categories": ["cs.SE"], "comment": "11 Pages, 5 Figures, Numerous Spreadsheet Formulae. This version email address update", "summary": "Spreadsheets offer a supremely successful and intuitive means of processing and exchanging numerical content. Its intuitive ad-hoc nature makes it hugely popular for use in diverse areas including business and engineering, yet these very same characteristics make it extraordinarily error-prone; many would question whether it is suitable for serious analysis or modelling tasks. A previous EuSpRIG paper examined the role of Names in increasing solution transparency and providing a readable notation to forge links with the problem domain. Extensive use was made of CSE array formulas, but it is acknowledged that their use makes spreadsheet development a distinctly cumbersome task. Since that time, the new dynamic arrays have been introduced and array calculation is now the default mode of operation for Excel. This paper examines the thesis that their adoption within a more professional development environment could replace traditional techniques where solution integrity is important. A major advantage of fully dynamic models is that they require less manual intervention to keep them updated and so have the potential to reduce the attendant errors and risk."}
{"id": "1704.01142", "title": "A Structured Approach to the development of Solutions in Excel", "url": "https://arxiv.org/abs/1704.01142", "pdf": "https://arxiv.org/pdf/1704.01142", "abs": "https://arxiv.org/abs/1704.01142", "authors": ["Peter Bartholomew"], "categories": ["cs.SE"], "comment": "12 pages, 6 figures. This version updated email address", "summary": "Spreadsheets offer a supremely successful democratisation platform, placing the manipulation and presentation of numbers within the grasp of users that have little or no mathematical expertise or IT experience. What appears to be almost completely lacking within a \"normal\" solution built using Excel default settings is the deployment of any structure that extends beyond a single-cell formula. The structural elements that allow conventional code to scale without escalating errors appear to be absent. This paper considers the use of controversial or lesser-used techniques to create a coherent solution strategy in which the problem is solved by a sequence of formulas resembling the steps of a programmed language."}
{"id": "2402.00069", "title": "Using the Abstract Computer Architecture Description Language to Model AI Hardware Accelerators", "url": "https://arxiv.org/abs/2402.00069", "pdf": "https://arxiv.org/pdf/2402.00069", "abs": "https://arxiv.org/abs/2402.00069", "authors": ["Mika Markus Müller", "Alexander Richard Manfred Borst", "Konstantin Lübeck", "Alexander Louis-Ferdinand Jung", "Oliver Bringmann"], "categories": ["cs.AR", "cs.AI"], "comment": "Accepted Version for: MBMV'24", "summary": "Artificial Intelligence (AI) has witnessed remarkable growth, particularly through the proliferation of Deep Neural Networks (DNNs). These powerful models drive technological advancements across various domains. However, to harness their potential in real-world applications, specialized hardware accelerators are essential. This demand has sparked a market for parameterizable AI hardware accelerators offered by different vendors.\n  Manufacturers of AI-integrated products face a critical challenge: selecting an accelerator that aligns with their product's performance requirements. The decision involves choosing the right hardware and configuring a suitable set of parameters. However, comparing different accelerator design alternatives remains a complex task. Often, engineers rely on data sheets, spreadsheet calculations, or slow black-box simulators, which only offer a coarse understanding of the performance characteristics.\n  The Abstract Computer Architecture Description Language (ACADL) is a concise formalization of computer architecture block diagrams, which helps to communicate computer architecture on different abstraction levels and allows for inferring performance characteristics. In this paper, we demonstrate how to use the ACADL to model AI hardware accelerators, use their ACADL description to map DNNs onto them, and explain the timing simulation semantics to gather performance results."}
{"id": "2401.11042", "title": "Does Using ChatGPT Result in Human Cognitive Augmentation?", "url": "https://arxiv.org/abs/2401.11042", "pdf": "https://arxiv.org/pdf/2401.11042", "abs": "https://arxiv.org/abs/2401.11042", "authors": ["Ron Fulbright", "Miranda Morrison"], "categories": ["cs.HC"], "comment": "12 pages, 5 figures", "summary": "Human cognitive performance is enhanced by the use of tools. For example, a human can produce a much greater, and more accurate, volume of mathematical calculation in a unit of time using a calculator or a spreadsheet application on a computer. Such tools have taken over the burden of lower level cognitive grunt work but the human still serves the role of the expert performing higher level thinking and reasoning. Recently, however, unsupervised, deep, machine learning has produced cognitive systems able to outperform humans in several domains. When humans use these tools in a human cog ensemble, the cognitive ability of the human is augmented. In some cases, even non experts can achieve, and even exceed, the performance of experts in a particular domain, synthetic expertise. A new cognitive system, ChatGPT, has burst onto the scene during the past year. This paper investigates human cognitive augmentation due to using ChatGPT by presenting the results of two experiments comparing responses created using ChatGPT with results created not using ChatGPT. We find using ChatGPT does not always result in cognitive augmentation and does not yet replace human judgement, discernment, and evaluation in certain types of tasks. In fact, ChatGPT was observed to result in misleading users resulting in negative cognitive augmentation."}
{"id": "2301.13779", "title": "FLAME: A small language model for spreadsheet formulas", "url": "https://arxiv.org/abs/2301.13779", "pdf": "https://arxiv.org/pdf/2301.13779", "abs": "https://arxiv.org/abs/2301.13779", "authors": ["Harshit Joshi", "Abishai Ebenezer", "José Cambronero", "Sumit Gulwani", "Aditya Kanade", "Vu Le", "Ivan Radiček", "Gust Verbruggen"], "categories": ["cs.PL", "cs.AI", "cs.SE"], "comment": "Accepted to AAAI 2024", "summary": "Spreadsheets are a vital tool for end-user data management. Using large language models for formula authoring assistance in these environments can be difficult, as these models are expensive to train and challenging to deploy due to their size (up to billions of parameters). We present FLAME, a transformer-based model trained exclusively on Excel formulas that leverages domain insights to achieve competitive performance while being substantially smaller (60M parameters) and training on two orders of magnitude less data. We curate a training dataset using sketch deduplication, introduce an Excel-specific formula tokenizer, and use domain-specific versions of masked span prediction and noisy auto-encoding as pre-training objectives. We evaluate FLAME on formula repair, formula completion, and similarity-based formula retrieval. FLAME can outperform much larger models, such as the Davinci (175B) and Cushman (12B) variants of Codex and CodeT5 (220M), in 10 of 14 evaluation settings for the repair and completion tasks. For formula retrieval, FLAME outperforms CodeT5, CodeBERT, and GraphCodeBERT."}
{"id": "2312.09107", "title": "A Comprehensive Approach to Ensuring Quality in Spreadsheet-Based Metadata", "url": "https://arxiv.org/abs/2312.09107", "pdf": "https://arxiv.org/pdf/2312.09107", "abs": "https://arxiv.org/abs/2312.09107", "authors": ["Martin J. O'Connor", "Marcos Martínez-Romero", "Mete Ugur Akdogan", "Josef Hardi", "Mark A. Musen"], "categories": ["cs.DL"], "comment": null, "summary": "While scientists increasingly recognize the importance of metadata in describing their data, spreadsheets remain the preferred tool for supplying this information despite their limitations in ensuring compliance and quality. Various tools have been developed to address these limitations, but they suffer from their own shortcomings, such as steep learning curves and limited customization. In this paper, we describe an end-to-end approach that supports spreadsheet-based entry of metadata while providing rigorous compliance and quality control. Our approach employs several key strategies, including customizable templates for defining metadata, integral support for the use of controlled terminologies when defining these templates, and an interactive Web-based tool that allows users to rapidly identify and fix errors in the spreadsheet-based metadata they supply. We demonstrate how this approach is being deployed in a biomedical consortium to define and collect metadata about scientific experiments."}
{"id": "2312.06517", "title": "Facilitating Digital Agriculture with Simple Databases", "url": "https://arxiv.org/abs/2312.06517", "pdf": "https://arxiv.org/pdf/2312.06517", "abs": "https://arxiv.org/abs/2312.06517", "authors": ["Dennis Buckmaster", "Sami Basir", "Hanae Sakata"], "categories": ["cs.DB"], "comment": "6 pages, 1 table, 1 figure. Journal of Extension Tools of the Trade, in press", "summary": "As an on-ramp to databases, we offer several well-structured private database templates as open source resources for agriculturalists, particularly those with modest spreadsheet skills. These farmer-oriented Air table databases use simple data-validated forms, with the look and feel of a customized app, to yield operational data that is tidy, machine- and human-readable, editable, and exportable for analysis in other software. Such data can facilitate logistics, provide contextual metadata, and improve enterprise analysis. A recorded workshop explaining how to build a database for activity records is presented. These resources may facilitate infusion of digital agriculture principles through Extension and structured educational programming."}
{"id": "2310.17306", "title": "FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language", "url": "https://arxiv.org/abs/2310.17306", "pdf": "https://arxiv.org/pdf/2310.17306", "abs": "https://arxiv.org/abs/2310.17306", "authors": ["Mukul Singh", "José Cambronero", "Sumit Gulwani", "Vu Le", "Carina Negreanu", "Elnaz Nouri", "Mohammad Raza", "Gust Verbruggen"], "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.PL"], "comment": "Contains inappropriately sourced conjecture of OpenAI's ChatGPT parameter count from www.forbes.com/sites/forbestechcouncil/2023/02/17/is-bigger-better-why-the-chatgpt-vs-gpt-3-vs-gpt-4-battle-is-just-a-family-chat, a citation which was omitted. The authors do not have direct knowledge or verification of this information, and relied solely on this article, which may lead to public confusion", "summary": "Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires them to understand and implement the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system. To evaluate FormaT5 on diverse and real scenarios, we create an extensive benchmark of 1053 CF tasks, containing real-world descriptions collected from four different sources. We release our benchmarks to encourage research in this area. Abstention and filling allow FormaT5 to outperform 8 different neural approaches on our benchmarks, both with and without examples. Our results illustrate the value of building domain-specific learning systems."}
{"id": "2310.20395", "title": "Spreadsheet-based Configuration of Families of Real-Time Specifications", "url": "https://arxiv.org/abs/2310.20395", "pdf": "https://arxiv.org/pdf/2310.20395", "abs": "https://arxiv.org/abs/2310.20395", "authors": ["José Proença", "David Pereira", "Giann Spilere Nandi", "Sina Borrami", "Jonas Melchert"], "categories": ["cs.SE"], "comment": "In Proceedings TiCSA 2023, arXiv:2310.18720", "summary": "Model checking real-time systems is complex, and requires a careful trade-off between including enough detail to be useful and not too much detail to avoid state explosion. This work exploits variability of the formal model being analysed and the requirements being checked, to facilitate the model-checking of variations of real-time specifications.  This work results from the collaboration between academics and Alstom, a railway company with a concrete use-case, in the context of the VALU3S European project. The configuration of the variability of the formal specifications is described in MS Excel spreadsheets with a particular structure, making it easy to use also by developers. These spreadsheets are processed automatically by our prototype tool that generates instances and runs the model checker.  We propose the extension of our previous work by exploiting analysis over valid combination of features, while preserving the simplicity of a spreadsheet-based interface with the model checker."}
{"id": "2305.19308", "title": "SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models", "url": "https://arxiv.org/abs/2305.19308", "pdf": "https://arxiv.org/pdf/2305.19308", "abs": "https://arxiv.org/abs/2305.19308", "authors": ["Hongxin Li", "Jingran Su", "Yuntao Chen", "Qing Li", "Zhaoxiang Zhang"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "Accepted to NeurIPS 2023", "summary": "Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill to automate these burdensome works. With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal. In this work, we propose a SheetCopilot agent that takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for LLMs to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of LLMs in software control tasks. Our SheetCopilot correctly completes 44.3\\% of tasks for a single generation, outperforming the strong code generation baseline by a wide margin. Our project page:https://sheetcopilot.github.io/."}
{"id": "2310.17414", "title": "LEI2JSON: Schema-based Validation and Conversion of Livestock Event Information", "url": "https://arxiv.org/abs/2310.17414", "pdf": "https://arxiv.org/pdf/2310.17414", "abs": "https://arxiv.org/abs/2310.17414", "authors": ["Mahir Habib", "Muhammad Ashad Kabir", "Lihong Zheng"], "categories": ["eess.SY", "cs.SE"], "comment": "20 pages, 6 figures", "summary": "Livestock producers often need help in standardising (i.e., converting and validating) their livestock event data. This article introduces a novel solution, LEI2JSON (Livestock Event Information To JSON). The tool is an add-on for Google Sheets, adhering to the livestock event information (LEI) schema. The core objective of LEI2JSON is to provide livestock producers with an efficient mechanism to standardise their data, leading to substantial savings in time and resources. This is achieved by building the spreadsheet template with the appropriate column headers, notes, and validation rules, converting the spreadsheet data into JSON format, and validating the output against the schema. LEI2JSON facilitates the seamless storage of livestock event information locally or on Google Drive in JSON. Additionally, we have conducted an extensive experimental evaluation to assess the effectiveness of the tool."}
{"id": "2310.16700", "title": "Streamlining Knowledge Graph Construction with a façade: The SPARQL Anything project", "url": "https://arxiv.org/abs/2310.16700", "pdf": "https://arxiv.org/pdf/2310.16700", "abs": "https://arxiv.org/abs/2310.16700", "authors": ["Luigi Asprino", "Enrico Daga", "Justin Dowdy", "Paul Mulholland", "Aldo Gangemi", "Marco Ratta"], "categories": ["cs.DB", "cs.DS"], "comment": "15 pages", "summary": "What should a data integration framework for knowledge engineers look like? Recent research on Knowledge Graph construction proposes the design of a façade, a notion borrowed from object-oriented software engineering. This idea is applied to SPARQL Anything, a system that allows querying heterogeneous resources as-if they were in RDF, in plain SPARQL 1.1, by overloading the SERVICE clause. SPARQL Anything supports a wide variety of file formats, from popular ones (CSV, JSON, XML, Spreadsheets) to others that are not supported by alternative solutions (Markdown, YAML, DOCx, Bibtex). Features include querying Web APIs with high flexibility, parametrised queries, and chaining multiple transformations into complex pipelines. In this paper, we describe the design rationale and software architecture of the SPARQL Anything system. We provide references to an extensive set of reusable, real-world scenarios from various application domains. We report on the value-to-users of the founding assumptions of its design, compared to alternative solutions through a community survey and a field report from the industry."}
{"id": "2310.14495", "title": "InstructExcel: A Benchmark for Natural Language Instruction in Excel", "url": "https://arxiv.org/abs/2310.14495", "pdf": "https://arxiv.org/pdf/2310.14495", "abs": "https://arxiv.org/abs/2310.14495", "authors": ["Justin Payan", "Swaroop Mishra", "Mukul Singh", "Carina Negreanu", "Christian Poelitz", "Chitta Baral", "Subhro Roy", "Rasika Chakravarthy", "Benjamin Van Durme", "Elnaz Nouri"], "categories": ["cs.CL", "cs.AI"], "comment": "Findings of EMNLP 2023, 18 pages", "summary": "With the evolution of Large Language Models (LLMs) we can solve increasingly more complex NLP tasks across various domains, including spreadsheets. This work investigates whether LLMs can generate code (Excel OfficeScripts, a TypeScript API for executing many tasks in Excel) that solves Excel specific tasks provided via natural language user instructions. To do so we introduce a new large-scale benchmark, InstructExcel, created by leveraging the 'Automate' feature in Excel to automatically generate OfficeScripts from users' actions. Our benchmark includes over 10k samples covering 170+ Excel operations across 2,000 publicly available Excel spreadsheets. Experiments across various zero-shot and few-shot settings show that InstructExcel is a hard benchmark for state of the art models like GPT-4. We observe that (1) using GPT-4 over GPT-3.5, (2) providing more in-context examples, and (3) dynamic prompting can help improve performance on this benchmark."}
{"id": "2311.10728", "title": "Improving Feedback from Automated Reviews of Student Spreadsheets", "url": "https://arxiv.org/abs/2311.10728", "pdf": "https://arxiv.org/pdf/2311.10728", "abs": "https://arxiv.org/abs/2311.10728", "authors": ["Sören Aguirre Reid", "Frank Kammer", "Jonas-Ian Kuche", "Pia-Doreen Ritzke", "Markus Siepermann", "Max Stephan", "Armin Wagenknecht"], "categories": ["cs.CY"], "comment": null, "summary": "Spreadsheets are one of the most widely used tools for end users. As a result, spreadsheets such as Excel are now included in many curricula. However, digital solutions for assessing spreadsheet assignments are still scarce in the teaching context. Therefore, we have developed an Intelligent Tutoring System (ITS) to review students' Excel submissions and provide individualized feedback automatically. Although the lecturer only needs to provide one reference solution, the students' submissions are analyzed automatically in several ways: value matching, detailed analysis of the formulas, and quality assessment of the solution. To take the students' learning level into account, we have developed feedback levels for an ITS that provide gradually more information about the error by using one of the different analyses. Feedback at a higher level has been shown to lead to a higher percentage of correct submissions and was also perceived as well understandable and helpful by the students."}
{"id": "2309.02110", "title": "Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!", "url": "https://arxiv.org/abs/2309.02110", "pdf": "https://arxiv.org/pdf/2309.02110", "abs": "https://arxiv.org/abs/2309.02110", "authors": ["James P. Dilger"], "categories": ["math.HO", "cs.CL"], "comment": null, "summary": "Wordle is a popular, online word game offered by the New York Times (nytimes.com). Currently there are some 2 million players of the English version worldwide. Players have 6 attempts to guess the daily word (target word) and after each attempt, the player receives color-coded information about the correctness and position of each letter in the guess. After either a successful completion of the puzzle or the final unsuccessful attempt, software can assess the player's luck and skill using Information Theory and can display data for the first, second, ..., sixth guesses of a random sample of all players. Recently, I discovered that the latter data is presented in a format that can easily be copied and pasted into a spreadsheet. I compiled data on Wordle players' first guesses from May 2023 - August 2023 and inferred some interesting information about Wordle players. A) Every day, about 0.2-0.5% of players solve the puzzle in one attempt. Because the odds of guessing the one of 2,315 possible target words at random is 0.043%, this implies that 4,000 - 10,000 players cheat by obtaining the target word outside of playing the game! B) At least 1/3 of the players have a favorite starting word, or cycle through several. And even though players should be aware that target words are never repeated, most players appear to remain loyal to their starting word even after its appearance as a target word. C) On August 15, 2023, about 30,000 players abruptly changed their starting word, presumably based on a crossword puzzle clue! Wordle players can be influenced! This study goes beyond social media postings, surveys, and Google Trends to provide solid, quantitative evidence about cheating in Wordle."}
{"id": "2310.01297", "title": "Co-audit: tools to help humans double-check AI-generated content", "url": "https://arxiv.org/abs/2310.01297", "pdf": "https://arxiv.org/pdf/2310.01297", "abs": "https://arxiv.org/abs/2310.01297", "authors": ["Andrew D. Gordon", "Carina Negreanu", "José Cambronero", "Rasika Chakravarthy", "Ian Drosos", "Hao Fang", "Bhaskar Mitra", "Hannah Richardson", "Advait Sarkar", "Stephanie Simmons", "Jack Williams", "Ben Zorn"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.PL"], "comment": null, "summary": "Users are increasingly being warned to check AI-generated content for correctness. Still, as LLMs (and other generative models) generate more complex output, such as summaries, tables, or code, it becomes harder for the user to audit or evaluate the output for quality or correctness. Hence, we are seeing the emergence of tool-assisted experiences to help the user double-check a piece of AI-generated content. We refer to these as co-audit tools. Co-audit tools complement prompt engineering techniques: one helps the user construct the input prompt, while the other helps them check the output response. As a specific example, this paper describes recent research on co-audit tools for spreadsheet computations powered by generative models. We explain why co-audit experiences are essential for any application of generative AI where quality is important and errors are consequential (as is common in spreadsheet computations). We propose a preliminary list of principles for co-audit, and outline research challenges."}
{"id": "2309.00115", "title": "Excel as a Turing-complete Functional Programming Environment", "url": "https://arxiv.org/abs/2309.00115", "pdf": "https://arxiv.org/pdf/2309.00115", "abs": "https://arxiv.org/abs/2309.00115", "authors": ["Peter Bartholomew"], "categories": ["cs.SE"], "comment": "14 page, 6 figures", "summary": "Since the calculation engine of Excel was the subject of a major upgrade to accommodate Dynamic Arrays in 2018 there has been a series of seismic changes to the art of building spreadsheet solutions. This paper will show the ad-hoc end user practices of traditional spreadsheets can be replaced by radically different approaches that have far more in common with formal programming. It is too early to guess the extent to which the new functionality will be adopted by the business and engineering communities and the impact that may have upon risk. Nevertheless, some trends are emerging from pioneering work within the Excel community which we will discuss here."}
{"id": "2309.12353", "title": "How Beaufort, Neumann and Gates met? Subject integration with spreadsheeting", "url": "https://arxiv.org/abs/2309.12353", "pdf": "https://arxiv.org/pdf/2309.12353", "abs": "https://arxiv.org/abs/2309.12353", "authors": ["Maria Csernoch", "Julia Csernoch"], "categories": ["cs.CY"], "comment": "17 pages, 14 figures", "summary": "Computational thinking should be the fourth fundamental skill, along with reading, writing, and arithmetic (3R). To reach the level where computational thinking skills, especially digital problem solving have their own schemata, there is a long way to go. In the present paper, a novel approach is detailed to support subject integration and building digital schemata, on the well-known Beaufort scale. The conversion of a traditional, paper-based problem and a data retrieval process are presented within the frame of a Grade 8 action research study. It is found that both students content knowledge and their digital skills developed more efficiently than in traditional course book and decontextualized digital environments. Furthermore, the method presented here can be adapted to any paper-based problems whose solutions would be more effective in a digital environment and which offer various forms for building schemata both in the subject matter and informatics."}
{"id": "2309.00104", "title": "A Use Case-Engineering Resources Taxonomy for Analytical Spreadsheet Models", "url": "https://arxiv.org/abs/2309.00104", "pdf": "https://arxiv.org/pdf/2309.00104", "abs": "https://arxiv.org/abs/2309.00104", "authors": ["Thomas A. Grossman", "Vijay Mehrotra"], "categories": ["cs.SE"], "comment": "13 Pages, 7 Figures, 2 Tables", "summary": "This paper presents a taxonomy for analytical spreadsheet models. It considers both the use case that a spreadsheet is meant to serve, and the engineering resources devoted to its development. We extend a previous three-type taxonomy, to identify nine types of spreadsheet models, that encompass the many analytical spreadsheet models seen in the literature. We connect disparate research literature to distinguish between an \"analytical solution\" and an \"industrial-quality analytical spreadsheet model\". We explore the nature of each of the nine types, propose definitions for some, relate them to the literature, and hypothesize on how they might arise. The taxonomy aids in identifying where various spreadsheet development guidelines are most useful, provides a lens for viewing spreadsheet errors and risk, and offers a structure for understanding how spreadsheets change over time. This taxonomy opens the door to many interesting research questions, including refinements to itself."}
{"id": "2309.00095", "title": "Experimenting with ChatGPT for Spreadsheet Formula Generation: Evidence of Risk in AI Generated Spreadsheets", "url": "https://arxiv.org/abs/2309.00095", "pdf": "https://arxiv.org/pdf/2309.00095", "abs": "https://arxiv.org/abs/2309.00095", "authors": ["Simon Thorne"], "categories": ["cs.SE"], "comment": "15 Pages", "summary": "Large Language Models (LLM) have become sophisticated enough that complex computer programs can be created through interpretation of plain English sentences and implemented in a variety of modern languages such as Python, Java Script, C++ and Spreadsheets. These tools are powerful and relatively accurate and therefore provide broad access to computer programming regardless of the background or knowledge of the individual using them. This paper presents a series of experiments with ChatGPT to explore the tool's ability to produce valid spreadsheet formulae and related computational outputs in situations where ChatGPT has to deduce, infer and problem solve the answer. The results show that in certain circumstances, ChatGPT can produce correct spreadsheet formulae with correct reasoning, deduction and inference. However, when information is limited, uncertain or the problem is too complex, the accuracy of ChatGPT breaks down as does its ability to reason, infer and deduce. This can also result in false statements and \"hallucinations\" that all subvert the process of creating spreadsheet formulae."}
{"id": "2308.14784", "title": "Generating tabular datasets under differential privacy", "url": "https://arxiv.org/abs/2308.14784", "pdf": "https://arxiv.org/pdf/2308.14784", "abs": "https://arxiv.org/abs/2308.14784", "authors": ["Gianluca Truda"], "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DB"], "comment": null, "summary": "Machine Learning (ML) is accelerating progress across fields and industries, but relies on accessible and high-quality training data. Some of the most important datasets are found in biomedical and financial domains in the form of spreadsheets and relational databases. But this tabular data is often sensitive in nature. Synthetic data generation offers the potential to unlock sensitive data, but generative models tend to memorise and regurgitate training data, which undermines the privacy goal. To remedy this, researchers have incorporated the mathematical framework of Differential Privacy (DP) into the training process of deep neural networks. But this creates a trade-off between the quality and privacy of the resulting data. Generative Adversarial Networks (GANs) are the dominant paradigm for synthesising tabular data under DP, but suffer from unstable adversarial training and mode collapse, which are exacerbated by the privacy constraints and challenging tabular data modality. This work optimises the quality-privacy trade-off of generative models, producing higher quality tabular datasets with the same privacy guarantees. We implement novel end-to-end models that leverage attention mechanisms to learn reversible tabular representations. We also introduce TableDiffusion, the first differentially-private diffusion model for tabular data synthesis. Our experiments show that TableDiffusion produces higher-fidelity synthetic datasets, avoids the mode collapse problem, and achieves state-of-the-art performance on privatised tabular data synthesis. By implementing TableDiffusion to predict the added noise, we enabled it to bypass the challenges of reconstructing mixed-type tabular data. Overall, the diffusion paradigm proves vastly more data and privacy efficient than the adversarial paradigm, due to augmented re-use of each data batch and a smoother iterative training process."}
{"id": "2308.10922", "title": "DataVinci: Learning Syntactic and Semantic String Repairs", "url": "https://arxiv.org/abs/2308.10922", "pdf": "https://arxiv.org/pdf/2308.10922", "abs": "https://arxiv.org/abs/2308.10922", "authors": ["Mukul Singh", "José Cambronero", "Sumit Gulwani", "Vu Le", "Carina Negreanu", "Gust Verbruggen"], "categories": ["cs.DB", "cs.AI"], "comment": "13 pages", "summary": "String data is common in real-world datasets: 67.6% of values in a sample of 1.8 million real Excel spreadsheets from the web were represented as text. Systems that successfully clean such string data can have a significant impact on real users. While prior work has explored errors in string data, proposed approaches have often been limited to error detection or require that the user provide annotations, examples, or constraints to fix the errors. Furthermore, these systems have focused independently on syntactic errors or semantic errors in strings, but ignore that strings often contain both syntactic and semantic substrings. We introduce DataVinci, a fully unsupervised string data error detection and repair system. DataVinci learns regular-expression-based patterns that cover a majority of values in a column and reports values that do not satisfy such patterns as data errors. DataVinci can automatically derive edits to the data error based on the majority patterns and constraints learned over other columns without the need for further user interaction. To handle strings with both syntactic and semantic substrings, DataVinci uses an LLM to abstract (and re-concretize) portions of strings that are semantic prior to learning majority patterns and deriving edits. Because not all data can result in majority patterns, DataVinci leverages execution information from an existing program (which reads the target data) to identify and correct data repairs that would not otherwise be identified. DataVinci outperforms 7 baselines on both error detection and repair when evaluated on 4 existing and new benchmarks."}
{"id": "2308.07357", "title": "Demonstration of CORNET: A System For Learning Spreadsheet Formatting Rules By Example", "url": "https://arxiv.org/abs/2308.07357", "pdf": "https://arxiv.org/pdf/2308.07357", "abs": "https://arxiv.org/abs/2308.07357", "authors": ["Mukul Singh", "Jose Cambronero", "Sumit Gulwani", "Vu Le", "Carina Negreanu", "Gust Verbruggen"], "categories": ["cs.SE", "cs.AI", "cs.DB"], "comment": "4 Pages, VLDB 2023 Demonstration Track", "summary": "Data management and analysis tasks are often carried out using spreadsheet software. A popular feature in most spreadsheet platforms is the ability to define data-dependent formatting rules. These rules can express actions such as \"color red all entries in a column that are negative\" or \"bold all rows not containing error or failure.\" Unfortunately, users who want to exercise this functionality need to manually write these conditional formatting (CF) rules. We introduce CORNET, a system that automatically learns such conditional formatting rules from user examples. CORNET takes inspiration from inductive program synthesis and combines symbolic rule enumeration, based on semi-supervised clustering and iterative decision tree learning, with a neural ranker to produce accurate conditional formatting rules. In this demonstration, we show CORNET in action as a simple add-in to Microsoft Excel. After the user provides one or two formatted cells as examples, CORNET generates formatting rule suggestions for the user to apply to the spreadsheet."}
{"id": "2307.14565", "title": "Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples", "url": "https://arxiv.org/abs/2307.14565", "pdf": "https://arxiv.org/pdf/2307.14565", "abs": "https://arxiv.org/abs/2307.14565", "authors": ["Peng Li", "Yeye He", "Cong Yan", "Yue Wang", "Surajit Chaudhuri"], "categories": ["cs.DB", "cs.LG"], "comment": "full version of a paper accepted to VLDB 2023", "summary": "Relational tables, where each row corresponds to an entity and each column corresponds to an attribute, have been the standard for tables in relational databases. However, such a standard cannot be taken for granted when dealing with tables \"in the wild\". Our survey of real spreadsheet-tables and web-tables shows that over 30% of such tables do not conform to the relational standard, for which complex table-restructuring transformations are needed before these tables can be queried easily using SQL-based analytics tools. Unfortunately, the required transformations are non-trivial to program, which has become a substantial pain point for technical and non-technical users alike, as evidenced by large numbers of forum questions in places like StackOverflow and Excel/Power-BI/Tableau forums.\n  We develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations (in Python or other languages), to transform non-relational tables into standard relational forms for downstream analytics, obviating the need for users to manually program transformations. We compile an extensive benchmark for this new task, by collecting 244 real test cases from user spreadsheets and online forums. Our evaluation suggests that Auto-Tables can successfully synthesize transformations for over 70% of test cases at interactive speeds, without requiring any input from users, making this an effective tool for both technical and non-technical users to prepare data for analytics."}
{"id": "2309.12317", "title": "Using a Catenary Trajectory to Reduce Wellbore Friction in Horizontal Extended Reach Drilling", "url": "https://arxiv.org/abs/2309.12317", "pdf": "https://arxiv.org/pdf/2309.12317", "abs": "https://arxiv.org/abs/2309.12317", "authors": ["Vu Nguyen"], "categories": ["cs.RO"], "comment": null, "summary": "Wellbore friction is one of the biggest concerns when drilling due to its relation to the total cost. The catenary concept was introduced to reduce wellbore friction, but it requires detailed analyses. This project would fill this gap. A catenary shape is simply the natural shape of a rope, chain, or drill string. The drill string will then hang freely inside the wellbore. Perfectly, there should be no contact between the hole and the string, and thus no friction. Torque and drag should be minimized this way. A case study is introduced to examine the outcome between Catenary Trajectory Design and traditional 2D Arc design. The calculation procedure of Catenary Trajectory and 2D Arc Design can be found in an MS Excel spreadsheet which is easy to use and reliable for designing catenary well trajectories for extended-reach wells."}
{"id": "2306.12850", "title": "Don't Treat the Symptom, Find the Cause! Efficient Artificial-Intelligence Methods for (Interactive) Debugging", "url": "https://arxiv.org/abs/2306.12850", "pdf": "https://arxiv.org/pdf/2306.12850", "abs": "https://arxiv.org/abs/2306.12850", "authors": ["Patrick Rodler"], "categories": ["cs.AI", "cs.DM", "cs.LO"], "comment": "Habilitation Thesis", "summary": "In the modern world, we are permanently using, leveraging, interacting with, and relying upon systems of ever higher sophistication, ranging from our cars, recommender systems in e-commerce, and networks when we go online, to integrated circuits when using our PCs and smartphones, the power grid to ensure our energy supply, security-critical software when accessing our bank accounts, and spreadsheets for financial planning and decision making. The complexity of these systems coupled with our high dependency on them implies both a non-negligible likelihood of system failures, and a high potential that such failures have significant negative effects on our everyday life. For that reason, it is a vital requirement to keep the harm of emerging failures to a minimum, which means minimizing the system downtime as well as the cost of system repair. This is where model-based diagnosis comes into play.\n  Model-based diagnosis is a principled, domain-independent approach that can be generally applied to troubleshoot systems of a wide variety of types, including all the ones mentioned above, and many more. It exploits and orchestrates i.a. techniques for knowledge representation, automated reasoning, heuristic problem solving, intelligent search, optimization, stochastics, statistics, decision making under uncertainty, machine learning, as well as calculus, combinatorics and set theory to detect, localize, and fix faults in abnormally behaving systems.\n  In this thesis, we will give an introduction to the topic of model-based diagnosis, point out the major challenges in the field, and discuss a selection of approaches from our research addressing these issues."}
{"id": "2304.07303", "title": "Smart Metro: Deep Learning Approaches to Forecasting the MRT Line 3 Ridership", "url": "https://arxiv.org/abs/2304.07303", "pdf": "https://arxiv.org/pdf/2304.07303", "abs": "https://arxiv.org/abs/2304.07303", "authors": ["Jayrald Empino", "Jean Allyson Junsay", "Mary Grace Verzon", "Mideth Abisado", "Shekinah Lor Huyo-a", "Gabriel Avelino Sampedro"], "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": null, "summary": "Since its establishment in 1999, the Metro Rail Transit Line 3 (MRT3) has served as a transportation option for numerous passengers in Metro Manila, Philippines. The Philippine government's transportation department records more than a thousand people using the MRT3 daily and forecasting the daily passenger count may be rather challenging. The MRT3's daily ridership fluctuates owing to variables such as holidays, working days, and other unexpected issues. Commuters do not know how many other commuters are on their route on a given day, which may hinder their ability to plan an efficient itinerary. Currently, the DOTr depends on spreadsheets containing historical data, which might be challenging to examine. This study presents a time series prediction of daily traffic to anticipate future attendance at a particular station on specific days."}
{"id": "2304.06597", "title": "\"What It Wants Me To Say\": Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models", "url": "https://arxiv.org/abs/2304.06597", "pdf": "https://arxiv.org/pdf/2304.06597", "abs": "https://arxiv.org/abs/2304.06597", "authors": ["Michael Xieyang Liu", "Advait Sarkar", "Carina Negreanu", "Ben Zorn", "Jack Williams", "Neil Toronto", "Andrew D. Gordon"], "categories": ["cs.HC"], "comment": null, "summary": "Code-generating large language models translate natural language into code. However, only a small portion of the infinite space of naturalistic utterances is effective at guiding code generation. For non-expert end-user programmers, learning this is the challenge of abstraction matching. We examine this challenge in the specific context of data analysis in spreadsheets, in a system that maps the users natural language query to Python code using the Codex generator, executes the code, and shows the result. We propose grounded abstraction matching, which bridges the abstraction gap by translating the code back into a systematic and predictable naturalistic utterance. In a between-subjects, think-aloud study (n=24), we compare grounded abstraction matching to an ungrounded alternative based on previously established query framing principles. We find that the grounded approach improves end-users' understanding of the scope and capabilities of the code-generating model, and the kind of language needed to use it effectively."}
{"id": "2210.13619", "title": "A Simpler Method for Understanding Emergency Shelter Access Patterns", "url": "https://arxiv.org/abs/2210.13619", "pdf": "https://arxiv.org/pdf/2210.13619", "abs": "https://arxiv.org/abs/2210.13619", "authors": ["Geoffrey G. Messier"], "categories": ["cs.CY"], "comment": null, "summary": "The Simplified Access Metric (SAM) is a new approach for characterizing emergency shelter access patterns as a measure of shelter client vulnerability. The goal of SAM is to provide shelter operators with an intuitive way to understand access patterns that can be implemented by non-technical staff using spreadsheet operations. Client data from a large North American shelter will be used to demonstrate that SAM produces similar results to traditional transitional, episodic and chronic client cluster analysis. Since SAM requires less data than cluster analysis, it is also able to generate a real time picture of how shelter access patterns are affected by external factors. Timelines generated from nine years of shelter client data using SAM demonstrate the impact of Housing First programming and the COVID-19 lockdown on how people access shelter. Finally, SAM allows shelter staff to move beyond assigning transitional, episodic and chronic labels and instead use the \"soft\" output of SAM directly as a measure of vulnerability."}
{"id": "2302.05482", "title": "Efficient and Compact Spreadsheet Formula Graphs", "url": "https://arxiv.org/abs/2302.05482", "pdf": "https://arxiv.org/pdf/2302.05482", "abs": "https://arxiv.org/abs/2302.05482", "authors": ["Dixin Tang", "Fanchao Chen", "Christopher De Leon", "Tana Wattanawaroon", "Jeaseok Yun", "Srinivasan Seshadri", "Aditya G. Parameswaran"], "categories": ["cs.DB"], "comment": null, "summary": "Spreadsheets are one of the most popular data analysis tools, wherein users can express computation as formulae alongside data. The ensuing dependencies are tracked as formula graphs. Efficiently querying and maintaining these formula graphs is critical for interactivity across multiple settings. Unfortunately, formula graphs are often large and complex such that querying and maintaining them is time-consuming, reducing interactivity. We propose TACO, a framework for efficiently compressing formula graphs, thereby reducing the time for querying and maintenance. The efficiency of TACO stems from a key spreadsheet property: tabular locality, which means that cells close to each other are likely to have similar formula structures. We leverage four such tabular locality-based patterns and develop algorithms for compressing formula graphs using these patterns, directly querying the compressed graph without decompression, and incrementally maintaining the graph during updates. We integrate TACO into an open-source spreadsheet system and show that TACO can significantly reduce formula graph sizes. For querying formula graphs, the speedups of TACO over a baseline implemented in our framework and a commercial spreadsheet system are up to 34,972x and 632x, respectively."}
{"id": "2301.11964", "title": "Adversarial Networks and Machine Learning for File Classification", "url": "https://arxiv.org/abs/2301.11964", "pdf": "https://arxiv.org/pdf/2301.11964", "abs": "https://arxiv.org/abs/2301.11964", "authors": ["Ken St. Germain", "Josh Angichiodo"], "categories": ["cs.LG"], "comment": null, "summary": "Correctly identifying the type of file under examination is a critical part of a forensic investigation. The file type alone suggests the embedded content, such as a picture, video, manuscript, spreadsheet, etc. In cases where a system owner might desire to keep their files inaccessible or file type concealed, we propose using an adversarially-trained machine learning neural network to determine a file's true type even if the extension or file header is obfuscated to complicate its discovery. Our semi-supervised generative adversarial network (SGAN) achieved 97.6% accuracy in classifying files across 11 different types. We also compared our network against a traditional standalone neural network and three other machine learning algorithms. The adversarially-trained network proved to be the most precise file classifier especially in scenarios with few supervised samples available. Our implementation of a file classifier using an SGAN is implemented on GitHub (https://ksaintg.github.io/SGAN-File-Classier)."}
{"id": "2208.06032", "title": "CORNET: Learning Table Formatting Rules By Example", "url": "https://arxiv.org/abs/2208.06032", "pdf": "https://arxiv.org/pdf/2208.06032", "abs": "https://arxiv.org/abs/2208.06032", "authors": ["Mukul Singh", "José Cambronero", "Sumit Gulwani", "Vu Le", "Carina Negreanu", "Mohammad Raza", "Gust Verbruggen"], "categories": ["cs.AI", "cs.DB", "cs.SE"], "comment": "12 pages content, 2 pages references", "summary": "Spreadsheets are widely used for table manipulation and presentation. Stylistic formatting of these tables is an important property for both presentation and analysis. As a result, popular spreadsheet software, such as Excel, supports automatically formatting tables based on rules. Unfortunately, writing such formatting rules can be challenging for users as it requires knowledge of the underlying rule language and data logic. We present CORNET, a system that tackles the novel problem of automatically learning such formatting rules from user examples in the form of formatted cells. CORNET takes inspiration from advances in inductive programming and combines symbolic rule enumeration with a neural ranker to learn conditional formatting rules. To motivate and evaluate our approach, we extracted tables with over 450K unique formatting rules from a corpus of over 1.8M real worksheets. Since we are the first to introduce conditional formatting, we compare CORNET to a wide range of symbolic and neural baselines adapted from related domains. Our results show that CORNET accurately learns rules across varying evaluation setups. Additionally, we show that CORNET finds shorter rules than those that a user has written and discovers rules in spreadsheets that users have manually formatted."}
{"id": "2211.04128", "title": "Active Learning with Tabular Language Models", "url": "https://arxiv.org/abs/2211.04128", "pdf": "https://arxiv.org/pdf/2211.04128", "abs": "https://arxiv.org/abs/2211.04128", "authors": ["Martin Ringsquandl", "Aneta Koleva"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "Despite recent advancements in tabular language model research, real-world applications are still challenging. In industry, there is an abundance of tables found in spreadsheets, but acquisition of substantial amounts of labels is expensive, since only experts can annotate the often highly technical and domain-specific tables. Active learning could potentially reduce labeling costs, however, so far there are no works related to active learning in conjunction with tabular language models. In this paper we investigate different acquisition functions in a real-world industrial tabular language model use case for sub-cell named entity recognition. Our results show that cell-level acquisition functions with built-in diversity can significantly reduce the labeling effort, while enforced table diversity is detrimental. We further see open fundamental questions concerning computational efficiency and the perspective of human annotators."}
{"id": "2211.06333", "title": "Excel Spreadsheet Analyzer", "url": "https://arxiv.org/abs/2211.06333", "pdf": "https://arxiv.org/pdf/2211.06333", "abs": "https://arxiv.org/abs/2211.06333", "authors": ["Amir Nassereldine", "Patrick Chen", "Jinjun Xiong"], "categories": ["cs.SE", "cs.PL"], "comment": "10 pages, 9 figures", "summary": "Spreadsheets are widely used in various fields to do large numerical analysis. While several companies have relied on spreadsheets for decades, data scientists are going in the direction of using scientific programming languages such as python to do their data analysis due to the support, community, and vast amount of libraries. While using python to analyze a company's spreadsheets, some information such as the formulas and dependencies of a cell are lost. We propose a tool that creates an abstract intermediate representation (AIR) of a spreadsheet. This representation facilitates the transfer from spreadsheets into scientific programming languages while preserving inter-dependency information about data. In addition to that, we build a python library on top of our tool to perform some data analysis in python."}
{"id": "2210.09928", "title": "Team OS's System for Dialogue Robot Competition 2022", "url": "https://arxiv.org/abs/2210.09928", "pdf": "https://arxiv.org/pdf/2210.09928", "abs": "https://arxiv.org/abs/2210.09928", "authors": ["Yuki Kubo", "Ryo Yanagimoto", "Hayato Futase", "Mikio Nakano", "Zhaojie Luo", "Kazunori Komatani"], "categories": ["cs.HC"], "comment": "This paper is part of the proceedings of the Dialogue Robot Competition 2022", "summary": "This paper describes our dialogue robot system, OSbot, developed for Dialogue Robot Competition 2022. The dialogue flow is based on state transitions described manually and the transition conditions use the results of keyword extraction and sentiment analysis. The transitions can be easily viewed and edited by managing them on a spreadsheet. The keyword extraction is based on named entity extraction and our predefined keyword set. The sentiment analysis is text-based and uses SVM, which was trained with the multimodal dialogue corpus Hazumi. We quickly checked and edited a dialogue flow by using a logging function. In the competition's preliminary round, our system ended up in third place."}
{"id": "2210.09162", "title": "Table-To-Text generation and pre-training with TabT5", "url": "https://arxiv.org/abs/2210.09162", "pdf": "https://arxiv.org/pdf/2210.09162", "abs": "https://arxiv.org/abs/2210.09162", "authors": ["Ewa Andrejczuk", "Julian Martin Eisenschlos", "Francesco Piccinno", "Syrine Krichene", "Yasemin Altun"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to Findings of EMNLP 2022", "summary": "Encoder-only transformer models have been successfully applied to different table understanding tasks, as in TAPAS (Herzig et al., 2020). A major limitation of these architectures is that they are constrained to classification-like tasks such as cell selection or entailment detection. We present TABT5, an encoder-decoder model that generates natural language text based on tables and textual inputs. TABT5 overcomes the encoder-only limitation by incorporating a decoder component and leverages the input structure with table specific embeddings and pre-training. TABT5 achieves new state-of-the-art results on several domains, including spreadsheet formula prediction with a 15% increase in sequence accuracy, QA with a 2.5% increase in sequence accuracy and data-to-text generation with a 2.5% increase in BLEU."}
{"id": "2208.06213", "title": "What is it like to program with artificial intelligence?", "url": "https://arxiv.org/abs/2208.06213", "pdf": "https://arxiv.org/pdf/2208.06213", "abs": "https://arxiv.org/abs/2208.06213", "authors": ["Advait Sarkar", "Andrew D. Gordon", "Carina Negreanu", "Christian Poelitz", "Sruti Srinivasa Ragavan", "Ben Zorn"], "categories": ["cs.HC", "cs.AI", "cs.PL"], "comment": "Proceedings of the 33rd Annual Conference of the Psychology of Programming Interest Group (PPIG 2022)", "summary": "Large language models, such as OpenAI's codex and Deepmind's AlphaCode, can generate code to solve a variety of problems expressed in natural language. This technology has already been commercialised in at least one widely-used programming editor extension: GitHub Copilot.\n  In this paper, we explore how programming with large language models (LLM-assisted programming) is similar to, and differs from, prior conceptualisations of programmer assistance. We draw upon publicly available experience reports of LLM-assisted programming, as well as prior usability and design studies. We find that while LLM-assisted programming shares some properties of compilation, pair programming, and programming via search and reuse, there are fundamental differences both in the technical possibilities as well as the practical experience. Thus, LLM-assisted programming ought to be viewed as a new way of programming with its own distinct properties and challenges.\n  Finally, we draw upon observations from a user study in which non-expert end user programmers use LLM-assisted tools for solving data tasks in spreadsheets. We discuss the issues that might arise, and open research challenges, in applying large language models to end-user programming, particularly with users who have little or no programming expertise."}
{"id": "2209.14812", "title": "Named Entity Recognition in Industrial Tables using Tabular Language Models", "url": "https://arxiv.org/abs/2209.14812", "pdf": "https://arxiv.org/pdf/2209.14812", "abs": "https://arxiv.org/abs/2209.14812", "authors": ["Aneta Koleva", "Martin Ringsquandl", "Mark Buckley", "Rakebul Hasan", "Volker Tresp"], "categories": ["cs.AI", "cs.CL"], "comment": "EMNLP 2022 Industry Track", "summary": "Specialized transformer-based models for encoding tabular data have gained interest in academia. Although tabular data is omnipresent in industry, applications of table transformers are still missing. In this paper, we study how these models can be applied to an industrial Named Entity Recognition (NER) problem where the entities are mentioned in tabular-structured spreadsheets. The highly technical nature of spreadsheets as well as the lack of labeled data present major challenges for fine-tuning transformer-based models. Therefore, we develop a dedicated table data augmentation strategy based on available domain-specific knowledge graphs. We show that this boosts performance in our low-resource scenario considerably. Further, we investigate the benefits of tabular structure as inductive bias compared to tables as linearized sequences. Our experiments confirm that a table transformer outperforms other baselines and that its tabular inductive bias is vital for convergence of transformer-based models."}
{"id": "2209.12560", "title": "Hazard Analysis of Collaborative Automation Systems: A Two-layer Approach based on Supervisory Control and Simulation", "url": "https://arxiv.org/abs/2209.12560", "pdf": "https://arxiv.org/pdf/2209.12560", "abs": "https://arxiv.org/abs/2209.12560", "authors": ["Tom P. Huck", "Yuvaraj Selvaraj", "Constantin Cronrath", "Christoph Ledermann", "Martin Fabian", "Bengt Lennartson", "Torsten Kröger"], "categories": ["cs.RO"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Safety critical systems are typically subjected to hazard analysis before commissioning to identify and analyse potentially hazardous system states that may arise during operation. Currently, hazard analysis is mainly based on human reasoning, past experiences, and simple tools such as checklists and spreadsheets. Increasing system complexity makes such approaches decreasingly suitable. Furthermore, testing-based hazard analysis is often not suitable due to high costs or dangers of physical faults. A remedy for this are model-based hazard analysis methods, which either rely on formal models or on simulation models, each with their own benefits and drawbacks. This paper proposes a two-layer approach that combines the benefits of exhaustive analysis using formal methods with detailed analysis using simulation. Unsafe behaviours that lead to unsafe states are first synthesised from a formal model of the system using Supervisory Control Theory. The result is then input to the simulation where detailed analyses using domain-specific risk metrics are performed. Though the presented approach is generally applicable, this paper demonstrates the benefits of the approach on an industrial human-robot collaboration system."}
{"id": "2208.04738", "title": "Long-Term Mentoring for Computer Science Researchers", "url": "https://arxiv.org/abs/2208.04738", "pdf": "https://arxiv.org/pdf/2208.04738", "abs": "https://arxiv.org/abs/2208.04738", "authors": ["Emily Ruppel", "Sihang Liu", "Elba Garza", "Sukyoung Ryu", "Alexandra Silva", "Talia Ringer"], "categories": ["cs.CY", "cs.GL", "cs.PL"], "comment": null, "summary": "Early in the pandemic, we -- leaders in the research areas of programming languages (PL) and computer architecture (CA) -- realized that we had a problem: the only way to form new lasting connections in the community was to already have lasting connections in the community. Both of our academic communities had wonderful short-term mentoring programs to address this problem, but it was clear that we needed long-term mentoring programs.\n  Those of us in CA approached this scientifically, making an evidence-backed case for community-wide long-term mentoring. In the meantime, one of us in PL had impulsively launched an unofficial long-term mentoring program, founded on chaos and spreadsheets. In January 2021, the latter grew to an official cross-institutional long-term mentoring program called SIGPLAN-M; in January 2022, the former grew to Computer Architecture Long-term Mentoring (CALM).\n  The impacts have been strong: SIGPLAN-M reaches 328 mentees and 234 mentors across 41 countries, and mentees have described it as \"life changing\" and \"a career saver.\" And while CALM is in its pilot phase -- with 13 mentors and 21 mentees across 7 countries -- it has received very positive feedback. The leaders of SIGPLAN-M and CALM shared our designs, impacts, and challenges along the way. Now, we wish to share those with you. We hope this will kick-start a larger long-term mentoring effort across all of computer science."}
{"id": "2209.05739", "title": "MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization", "url": "https://arxiv.org/abs/2209.05739", "pdf": "https://arxiv.org/pdf/2209.05739", "abs": "https://arxiv.org/abs/2209.05739", "authors": ["Lu Ying", "Xinhuan Shu", "Dazhen Deng", "Yuchen Yang", "Tan Tang", "Lingyun Yu", "Yingcai Wu"], "categories": ["cs.HC"], "comment": null, "summary": "Glyph-based visualization achieves an impressive graphic design when associated with comprehensive visual metaphors, which help audiences effectively grasp the conveyed information through revealing data semantics. However, creating such metaphoric glyph-based visualization (MGV) is not an easy task, as it requires not only a deep understanding of data but also professional design skills. This paper proposes MetaGlyph, an automatic system for generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct a qualitative analysis to understand the design of current MGVs from the perspectives of metaphor embodiment and glyph design. Based on the results, we introduce a novel framework for generating MGVs by metaphoric image selection and an MGV construction. Specifically, MetaGlyph automatically selects metaphors with corresponding images from online resources based on the input data semantics. We then integrate a Monte Carlo tree search algorithm that explores the design of an MGV by associating visual elements with data dimensions given the data importance, semantic relevance, and glyph non-overlap. The system also provides editing feedback that allows users to customize the MGVs according to their design preferences. We demonstrate the use of MetaGlyph through a set of examples, one usage scenario, and validate its effectiveness through a series of expert interviews."}
{"id": "2204.03128", "title": "Sigma Workbook: A Spreadsheet for Cloud Data Warehouses", "url": "https://arxiv.org/abs/2204.03128", "pdf": "https://arxiv.org/pdf/2204.03128", "abs": "https://arxiv.org/abs/2204.03128", "authors": ["James Gale", "Max Seiden", "Deepanshu Utkarsh", "Jason Frantz", "Rob Woollen", "Çağatay Demiralp"], "categories": ["cs.DB", "cs.HC"], "comment": "VLDB'22 Demonstrations", "summary": "Cloud data warehouses (CDWs) bring large-scale data and compute power closer to users in enterprises. However, existing tools for analyzing data in CDWs are either limited in ad-hoc transformations or difficult to use for business users. Here we introduce Sigma Workbook, a new interactive system that enables business users to easily perform a visual analysis of data in CDWs at scale. For this, Sigma Workbook provides an accessible spreadsheet-like interface for analysis through direct manipulation. Sigma Workbook dynamically constructs matching SQL queries from user interactions, building on the versatility and expressivity of SQL. Constructed queries are directly executed on CDWs, leveraging the superior characteristics of the new generation CDWs, including scalability. We demonstrate Sigma Workbook through 3 real-life use cases -- cohort analysis, sessionization, and data augmentation -- and underline Workbook's ease of use, scalability, and expressivity."}
{"id": "2204.00598", "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language", "url": "https://arxiv.org/abs/2204.00598", "pdf": "https://arxiv.org/pdf/2204.00598", "abs": "https://arxiv.org/abs/2204.00598", "authors": ["Andy Zeng", "Maria Attarian", "Brian Ichter", "Krzysztof Choromanski", "Adrian Wong", "Stefan Welker", "Federico Tombari", "Aveek Purohit", "Michael Ryoo", "Vikas Sindhwani", "Johnny Lee", "Vincent Vanhoucke", "Pete Florence"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "https://socraticmodels.github.io/", "summary": "Large pretrained (e.g., \"foundation\") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning."}
{"id": "2201.09745", "title": "Table Pre-training: A Survey on Model Architectures, Pre-training Objectives, and Downstream Tasks", "url": "https://arxiv.org/abs/2201.09745", "pdf": "https://arxiv.org/pdf/2201.09745", "abs": "https://arxiv.org/abs/2201.09745", "authors": ["Haoyu Dong", "Zhoujun Cheng", "Xinyi He", "Mengyu Zhou", "Anda Zhou", "Fan Zhou", "Ao Liu", "Shi Han", "Dongmei Zhang"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted by IJCAI'2022 survey track", "summary": "Since a vast number of tables can be easily collected from web pages, spreadsheets, PDFs, and various other document types, a flurry of table pre-training frameworks have been proposed following the success of text and images, and they have achieved new state-of-the-arts on various tasks such as table question answering, table type recognition, column relation classification, table search, formula prediction, etc. To fully use the supervision signals in unlabeled tables, a variety of pre-training objectives have been designed and evaluated, for example, denoising cell values, predicting numerical relationships, and implicitly executing SQLs. And to best leverage the characteristics of (semi-)structured tables, various tabular language models, particularly with specially-designed attention mechanisms, have been explored. Since tables usually appear and interact with free-form text, table pre-training usually takes the form of table-text joint pre-training, which attracts significant research interests from multiple domains. This survey aims to provide a comprehensive review of different model designs, pre-training objectives, and downstream tasks for table pre-training, and we further share our thoughts and vision on existing challenges and future opportunities."}
{"id": "2109.07323", "title": "FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining", "url": "https://arxiv.org/abs/2109.07323", "pdf": "https://arxiv.org/pdf/2109.07323", "abs": "https://arxiv.org/abs/2109.07323", "authors": ["Zhoujun Cheng", "Haoyu Dong", "Ran Jia", "Pengfei Wu", "Shi Han", "Fan Cheng", "Dongmei Zhang"], "categories": ["cs.IR", "cs.LG"], "comment": "Accepted by ACL'22 main track", "summary": "Tables store rich numerical data, but numerical reasoning over tables is still a challenge. In this paper, we find that the spreadsheet formula, which performs calculations on numerical values in tables, is naturally a strong supervision of numerical reasoning. More importantly, large amounts of spreadsheets with expert-made formulae are available on the web and can be obtained easily. FORTAP is the first method for numerical-reasoning-aware table pretraining by leveraging large corpus of spreadsheet formulae. We design two formula pretraining tasks to explicitly guide FORTAP to learn numerical reference and calculation in semi-structured tables. FORTAP achieves state-of-the-art results on two representative downstream tasks, cell type classification and formula prediction, showing great potential of numerical-reasoning-aware pretraining."}
{"id": "2202.13189", "title": "Efficient Specialized Spreadsheet Parsing for Data Science", "url": "https://arxiv.org/abs/2202.13189", "pdf": "https://arxiv.org/pdf/2202.13189", "abs": "https://arxiv.org/abs/2202.13189", "authors": ["Felix Henze", "Haralampos Gavriilidis", "Eleni Tzirita Zacharatou", "Volker Markl"], "categories": ["cs.DB"], "comment": "Accepted at the 24th International Workshop on Design, Optimization, Languages and Analytical Processing of Big Data (DOLAP 2022), March 29, 2022, Edinburgh, UK", "summary": "Spreadsheets are widely used for data exploration. Since spreadsheet systems have limited capabilities, users often need to load spreadsheets to other data science environments to perform advanced analytics. However, current approaches for spreadsheet loading suffer from either high runtime or memory usage, which hinders data exploration on commodity systems. To make spreasheet loading practical on commodity systems, we introduce a novel parser that minimizes memory usage by tightly coupling decompression and parsing. Furthermore, to reduce the runtime, we introduce optimized spreadsheet-specific parsing routines and employ parallelism. To evaluate our approach, we implement a prototype for loading Excel spreadsheets into R environments. Our evaluation shows that our novel approach is up to 3x faster while consuming up to 40x less memory than state-of-the-art approaches."}
{"id": "2203.16346", "title": "Enhanced Spreadsheet Computing with Finite-Domain Constraint Satisfaction", "url": "https://arxiv.org/abs/2203.16346", "pdf": "https://arxiv.org/pdf/2203.16346", "abs": "https://arxiv.org/abs/2203.16346", "authors": ["Ezana N. Beyenne", "Hai-Feng Guo"], "categories": ["cs.PL", "cs.AI"], "comment": null, "summary": "The spreadsheet application is among the most widely used computing tools in modern society. It provides excellent usability and usefulness, and it easily enables a non-programmer to perform programming-like tasks in a visual tabular \"pen and paper\" approach. However, spreadsheets are mostly limited to bookkeeping-like applications due to their mono-directional data flow. This paper shows how the spreadsheet computing paradigm is extended to break this limitation for solving constraint satisfaction problems. We present an enhanced spreadsheet system where finite-domain constraint solving is well supported in a visual environment. Furthermore, a spreadsheet-specific constraint language is constructed for general users to specify constraints among data cells in a declarative and scalable way. The new spreadsheet system significantly simplifies the development of many constraint-based applications using a visual tabular interface. Examples are given to illustrate the usability and usefulness of the extended spreadsheet paradigm.\n  KEYWORDS: Spreadsheet computing, Finite-domain constraint satisfaction, Constraint logic programming"}
{"id": "2203.10944", "title": "Spreadsheet computing with Finite Domain Constraint Enhancements", "url": "https://arxiv.org/abs/2203.10944", "pdf": "https://arxiv.org/pdf/2203.10944", "abs": "https://arxiv.org/abs/2203.10944", "authors": ["Ezana N. Beyenne"], "categories": ["cs.AI"], "comment": "2008 Master's thesis", "summary": "Spreadsheet computing is one of the more popular computing methodologies in today's modern society. The spreadsheet application's ease of use and usefulness has enabled non-programmers to perform programming-like tasks in a familiar setting modeled after the tabular \"pen and paper\" approach. However, spreadsheet applications are limited to bookkeeping-like tasks due to their single-direction data flow. This thesis demonstrates an extension of the spreadsheet computing paradigm in overcoming this limitation to solve constraint satisfaction problems. We present a framework seamlessly incorporating a finite constraint solver with the spreadsheet computing paradigm. This framework allows the individual cells in the spreadsheet to be attached to either a finite domain or a constraint specifying the relationship among the cells. The framework provides an interface for constraint solving and further enhances the spreadsheet computing paradigm by providing a set of spreadsheet-specific constraints that will aid in controlling the scalability of large spreadsheet applications implementations. Finally, we provide examples to demonstrate the usability and usefulness of the extended spreadsheet paradigm.\n  Keywords: Spreadsheet computing, Constraint Logic Programming, Constraint satisfaction, Domain-Specific language, Excel, SWI Prolog, C#"}
{"id": "2202.00454", "title": "TableQuery: Querying tabular data with natural language", "url": "https://arxiv.org/abs/2202.00454", "pdf": "https://arxiv.org/pdf/2202.00454", "abs": "https://arxiv.org/abs/2202.00454", "authors": ["Abhijith Neil Abraham", "Fariz Rahman", "Damanpreet Kaur"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "11 pages, 1 figures", "summary": "This paper presents TableQuery, a novel tool for querying tabular data using deep learning models pre-trained to answer questions on free text. Existing deep learning methods for question answering on tabular data have various limitations, such as having to feed the entire table as input into a neural network model, making them unsuitable for most real-world applications. Since real-world data might contain millions of rows, it may not entirely fit into the memory. Moreover, data could be stored in live databases, which are updated in real-time, and it is impractical to serialize an entire database to a neural network-friendly format each time it is updated. In TableQuery, we use deep learning models pre-trained for question answering on free text to convert natural language queries to structured queries, which can be run against a database or a spreadsheet. This method eliminates the need for fitting the entire data into memory as well as serializing databases. Furthermore, deep learning models pre-trained for question answering on free text are readily available on platforms such as HuggingFace Model Hub (7). TableQuery does not require re-training; when a newly trained model for question answering with better performance is available, it can replace the existing model in TableQuery."}
{"id": "2201.06337", "title": "PoVRPoint: Authoring Presentations in Mobile Virtual Reality", "url": "https://arxiv.org/abs/2201.06337", "pdf": "https://arxiv.org/pdf/2201.06337", "abs": "https://arxiv.org/abs/2201.06337", "authors": ["Verena Biener", "Travis Gesslein", "Daniel Schneider", "Felix Kawala", "Alexander Otte", "Per Ola Kristensson", "Michel Pahud", "Eyal Ofek", "Cuauhtli Campos", "Matjaž Kljun", "Klen Čopič Pucihar", "Jens Grubert"], "categories": ["cs.HC"], "comment": "IEEE VR 2022; to appear in IEEE transactions on visualization and computer graphics, 2022", "summary": "Virtual Reality (VR) has the potential to support mobile knowledge workers by complementing traditional input devices with a large three-dimensional output space and spatial input. Previous research on supporting VR knowledge work explored domains such as text entry using physical keyboards and spreadsheet interaction using combined pen and touch input. Inspired by such work, this paper probes the VR design space for authoring presentations in mobile settings. We propose PoVRPoint -- a set of tools coupling pen- and touch-based editing of presentations on mobile devices, such as tablets, with the interaction capabilities afforded by VR. We study the utility of extended display space to, for example, assist users in identifying target slides, supporting spatial manipulation of objects on a slide, creating animations, and facilitating arrangements of multiple, possibly occluded, shapes. Among other things, our results indicate that 1) the wide field of view afforded by VR results in significantly faster target slide identification times compared to a tablet-only interface for visually salient targets; and 2) the three-dimensional view in VR enables significantly faster object reordering in the presence of occlusion compared to two baseline interfaces. A user study further confirmed that the interaction techniques were found to be usable and enjoyable."}
{"id": "2201.01654", "title": "TableParser: Automatic Table Parsing with Weak Supervision from Spreadsheets", "url": "https://arxiv.org/abs/2201.01654", "pdf": "https://arxiv.org/pdf/2201.01654", "abs": "https://arxiv.org/abs/2201.01654", "authors": ["Susie Xi Rao", "Johannes Rausch", "Peter Egger", "Ce Zhang"], "categories": ["cs.CV"], "comment": "accepted in the AAAI-22 Workshop on Scientific Document Understanding at the Thirty-Sixth AAAI Conference on Artificial Intelligence (SDU@AAAI-22)", "summary": "Tables have been an ever-existing structure to store data. There exist now different approaches to store tabular data physically. PDFs, images, spreadsheets, and CSVs are leading examples. Being able to parse table structures and extract content bounded by these structures is of high importance in many applications. In this paper, we devise TableParser, a system capable of parsing tables in both native PDFs and scanned images with high precision. We have conducted extensive experiments to show the efficacy of domain adaptation in developing such a tool. Moreover, we create TableAnnotator and ExcelAnnotator, which constitute a spreadsheet-based weak supervision mechanism and a pipeline to enable table parsing. We share these resources with the research community to facilitate further research in this interesting direction."}
{"id": "2201.07696", "title": "Exploring Spreadsheet Use and Practices in a Technologically Constrained Setting", "url": "https://arxiv.org/abs/2201.07696", "pdf": "https://arxiv.org/pdf/2201.07696", "abs": "https://arxiv.org/abs/2201.07696", "authors": ["Khwima Mckinley Mkamanga", "Simon Thorne"], "categories": ["cs.SE"], "comment": "23 pages, 7 colour figures, 17 Tables and a Sample Questionnaire", "summary": "This paper explores the impacts of spreadsheets on business operations in a water utility parastatal in Malawi, Sub-Saharan Africa. The organisation is a typical example of a semi-government body operating in a technologically underdeveloped country. The study focused on spreadsheet scope of use and life cycle as well as organisational policy and governance. The results will help define future spreadsheet usage by influencing new approaches for managing potential risks associated with spreadsheets in the organization. Generally, findings indicate that the proliferation of spreadsheets in the organization has provided an enabling environment for business automation. The paper also highlights management, technological and human factor issues contributing to high risks associated with the pervasive spreadsheet use. The conclusions drawn from the research confirms that there is ample room for improvement in many areas such as implementation of comprehensive policies and regulations governing spreadsheet development processes and adoption."}
{"id": "2110.12829", "title": "Spread2RML: Constructing Knowledge Graphs by Predicting RML Mappings on Messy Spreadsheets", "url": "https://arxiv.org/abs/2110.12829", "pdf": "https://arxiv.org/pdf/2110.12829", "abs": "https://arxiv.org/abs/2110.12829", "authors": ["Markus Schröder", "Christian Jilek", "Andreas Dengel"], "categories": ["cs.DB"], "comment": "17 pages, 1 figure, 2 tables, accepted at K-CAP 2021", "summary": "The RDF Mapping Language (RML) allows to map semi-structured data to RDF knowledge graphs. Besides CSV, JSON and XML, this also includes the mapping of spreadsheet tables. Since spreadsheets have a complex data model and can become rather messy, their mapping creation tends to be very time consuming. In order to reduce such efforts, this paper presents Spread2RML which predicts RML mappings on messy spreadsheets. This is done with an extensible set of RML object map templates which are applied for each column based on heuristics. In our evaluation, three datasets are used ranging from very messy synthetic data to spreadsheets from data.gov which are less messy. We obtained first promising results especially with regard to our approach being fully automatic and dealing with rather messy data."}
{"id": "2110.11575", "title": "Methodology for Assessing the State of the Practice for Domain X", "url": "https://arxiv.org/abs/2110.11575", "pdf": "https://arxiv.org/pdf/2110.11575", "abs": "https://arxiv.org/abs/2110.11575", "authors": ["Spencer Smith", "Jacques Carette", "Peter Michalski", "Ao Dong", "Olu Owojaiye"], "categories": ["cs.SE"], "comment": "35 pages, 3 figures", "summary": "To improve software development methods and tools for research software, we first need to understand the current state of the practice. Therefore, we have developed a methodology for assessing the state of the software development practices for a given research software domain. For each domain we wish to answer questions such as: i) What artifacts (documents, code, test cases, etc.) are present? ii) What tools are used? iii) What principles, process and methodologies are used? iv) What are the pain points for developers? v) What actions are used to improve qualities like maintainability and reproducibility? To answer these questions, our methodology prescribes the following steps: i) Identify the domain; ii) Identify a list of candidate software packages; iii) Filter the list to a length of about 30 packages; iv) Gather source code and documentation for each package; v) Collect repository related data on each software package, like number of stars, number of open issues, number of lines of code; vi) Fill in the measurement template (the template consists of 108 questions to assess 9 qualities (including the qualities of installability, usability and visibility)); vii) Interview developers (the interview consists of 20 questions and takes about an hour); viii) Rank the software using the Analytic Hierarchy Process (AHP); and, ix) Analyze the data to answer the questions posed above. A domain expert should be engaged throughout the process, to ensure that implicit information about the domain is properly represented and to assist with conducting an analysis of the commonalities and variabilities between the 30 selected packages. Using our methodology, spreadsheet templates and AHP tool, we estimate (based on our experience with using the process) the time to complete an assessment for a given domain at 173 person hours."}
{"id": "2110.08993", "title": "Typed Image-based Programming with Structure Editing", "url": "https://arxiv.org/abs/2110.08993", "pdf": "https://arxiv.org/pdf/2110.08993", "abs": "https://arxiv.org/abs/2110.08993", "authors": ["Jonathan Edwards", "Tomas Petricek"], "categories": ["cs.PL", "cs.SE"], "comment": "Accepted to: Human Aspects of Types and Reasoning Assistants (HATRA'21), Oct 19, 2021, Chicago, US", "summary": "Many beloved programming systems are image-based: self-contained worlds that persist both code and data in a single file. Examples include Smalltalk, LISP, HyperCard, Flash, and spreadsheets. Image-based programming avoids much of the complexity of modern programming technology stacks and encourages more casual and exploratory programming. However conventional file-based programming has better support for collaboration and deployment. These problems have been blamed for the limited commercial success of Smalltalk. We propose to enable collaboration in image-based programming via types and structure editing.\n  We focus on the problem of schema change on persistent data. We turn to static types, which paradoxically require more schema change but also provide a mechanism to express and execute those changes. To determine those changes we turn to structure editing, so that we can capture changes in type definitions with sufficient fidelity to automatically adapt the data to suit. We conjecture that typical schema changes can be handled through structure editing of static types.\n  That positions us to tackle collaboration with what could be called version control for structure editing. We present a theory realizing this idea, which is our main technical contribution. While we focus here on editing types, if we can extend the approach to cover the entire programming experience then it would offer a new way to collaborate in image-based programming."}
{"id": "2109.06630", "title": "Detecting Layout Templates in Complex Multiregion Files", "url": "https://arxiv.org/abs/2109.06630", "pdf": "https://arxiv.org/pdf/2109.06630", "abs": "https://arxiv.org/abs/2109.06630", "authors": ["Gerardo Vitagliano", "Lan Jiang", "Felix Naumann"], "categories": ["cs.IR"], "comment": null, "summary": "Spreadsheets are among the most commonly used file formats for data management, distribution, and analysis. Their widespread employment makes it easy to gather large collections of data, but their flexible canvas-based structure makes automated analysis difficult without heavy preparation. One of the common problems that practitioners face is the presence of multiple, independent regions in a single spreadsheet, possibly separated by repeated empty cells. We define such files as \"multiregion\" files. In collections of various spreadsheets, we can observe that some share the same layout. We present the Mondrian approach to automatically identify layout templates across multiple files and systematically extract the corresponding regions. Our approach is composed of three phases: first, each file is rendered as an image and inspected for elements that could form regions; then, using a clustering algorithm, the identified elements are grouped to form regions; finally, every file layout is represented as a graph and compared with others to find layout templates. We compare our method to state-of-the-art table recognition algorithms on two corpora of real-world enterprise spreadsheets. Our approach shows the best performances in detecting reliable region boundaries within each file and can correctly identify recurring layouts across files."}
{"id": "2109.07267", "title": "JUBILEE: Secure Debt Relief and Forgiveness", "url": "https://arxiv.org/abs/2109.07267", "pdf": "https://arxiv.org/pdf/2109.07267", "abs": "https://arxiv.org/abs/2109.07267", "authors": ["David Cerezo Sánchez"], "categories": ["cs.CR", "cs.GT", "econ.GN"], "comment": null, "summary": "JUBILEE is a securely computed mechanism for debt relief and forgiveness in a frictionless manner without involving trusted third parties, leading to more harmonious debt settlements by incentivising the parties to truthfully reveal their private information. JUBILEE improves over all previous methods:\n  - individually rational, incentive-compatible, truthful/strategy-proof, ex-post efficient, optimal mechanism for debt relief and forgiveness with private information\n  - by the novel introduction of secure computation techniques to debt relief, the \"blessing of the debtor\" is hereby granted for the first time: debt settlements with higher expected profits and a higher probability of success than without using secure computation\n  A simple and practical implementation is included for \"The Secure Spreadsheet\". Another implementation is realised using Raziel smart contracts on a blockchain with Pravuil consensus."}
{"id": "2108.11525", "title": "Supercomputing Enabled Deployable Analytics for Disaster Response", "url": "https://arxiv.org/abs/2108.11525", "pdf": "https://arxiv.org/pdf/2108.11525", "abs": "https://arxiv.org/abs/2108.11525", "authors": ["Kaira Samuel", "Jeremy Kepner", "Michael Jones", "Lauren Milechin", "Vijay Gadepally", "William Arcand", "David Bestor", "William Bergeron", "Chansup Byun", "Matthew Hubbell", "Michael Houle", "Anna Klein", "Victor Lopez", "Julie Mullen", "Andrew Prout", "Albert Reuther", "Antonio Rosa", "Sid Samsi", "Charles Yee", "Peter Michaleas"], "categories": ["cs.DB", "cs.DC", "cs.GR", "cs.HC", "cs.MM"], "comment": "5 pages, 11 figures, 17 references, accepted to IEEE HPEC 2021", "summary": "First responders and other forward deployed essential workers can benefit from advanced analytics. Limited network access and software security requirements prevent the usage of standard cloud based microservice analytic platforms that are typically used in industry. One solution is to precompute a wide range of analytics as files that can be used with standard preinstalled software that does not require network access or additional software and can run on a wide range of legacy hardware. In response to the COVID-19 pandemic, this approach was tested for providing geo-spatial census data to allow quick analysis of demographic data for better responding to emergencies. These data were processed using the MIT SuperCloud to create several thousand Google Earth and Microsoft Excel files representative of many advanced analytics. The fast mapping of census data using Google Earth and Microsoft Excel has the potential to give emergency responders a powerful tool to improve emergency preparedness. Our approach displays relevant census data (total population, population under 15, population over 65, median age) per census block, sorted by county, through a Microsoft Excel spreadsheet (xlsx file) and Google Earth map (kml file). The spreadsheet interface includes features that allow users to convert between different longitude and latitude coordinate units. For the Google Earth files, a variety of absolute and relative colors maps of population density have been explored to provide an intuitive and meaningful interface. Using several hundred cores on the MIT SuperCloud, new analytics can be generated in a few minutes."}
{"id": "2108.00567", "title": "Agile Elicitation of Scalability Requirements for Open Systems: A Case Study", "url": "https://arxiv.org/abs/2108.00567", "pdf": "https://arxiv.org/pdf/2108.00567", "abs": "https://arxiv.org/abs/2108.00567", "authors": ["Gunnar Brataas", "Antonio Martini", "Geir Kjetil Hanssen", "Georg Ræder"], "categories": ["cs.SE", "cs.PF"], "comment": "36 pages, 7 figures, 6 tables, accepted for publication in Journal of Systems and Software", "summary": "Eliciting scalability requirements during agile software development is complicated and poorly described in previous research. This article presents a lightweight artifact for eliciting scalability requirements during agile software development: the ScrumScale model. The ScrumScale model is a simple spreadsheet. The scalability concepts underlying the ScrumScale model are clarified in this design science research, which also utilizes coordination theory. This paper describes the open banking case study, where a legacy banking system becomes open. This challenges the scalability of this legacy system. The first step in understanding this challenge is to elicit the new scalability requirements. In the open banking case study, key stakeholders from TietoEVRY spent 55 hours eliciting TietoEVRY's open banking project's scalability requirements. According to TietoEVRY, the ScrumScale model provided a systematic way of producing scalability requirements. For TietoEVRY, the scalability concepts behind the ScrumScale model also offered significant advantages in dialogues with other stakeholders."}
{"id": "2107.13957", "title": "Towards Semantic Interoperability in Historical Research: Documenting Research Data and Knowledge with Synthesis", "url": "https://arxiv.org/abs/2107.13957", "pdf": "https://arxiv.org/pdf/2107.13957", "abs": "https://arxiv.org/abs/2107.13957", "authors": ["Pavlos Fafalios", "Konstantina Konsolaki", "Lida Charami", "Kostas Petrakis", "Manos Paterakis", "Dimitris Angelakis", "Yannis Tzitzikas", "Chrysoula Bekiari", "Martin Doerr"], "categories": ["cs.DL", "cs.DB"], "comment": "This is a preprint of an article accepted for publication at the 20th International Semantic Web Conference (ISWC 2021)", "summary": "A vast area of research in historical science concerns the documentation and study of artefacts and related evidence. Current practice mostly uses spreadsheets or simple relational databases to organise the information as rows with multiple columns of related attributes. This form offers itself for data analysis and scholarly interpretation, however it also poses problems including i) the difficulty for collaborative but controlled documentation by a large number of users, ii) the lack of representation of the details from which the documented relations are inferred, iii) the difficulty to extend the underlying data structures as well as to combine and integrate data from multiple and diverse information sources, and iv) the limitation to reuse the data beyond the context of a particular research activity. To support historians to cope with these problems, in this paper we describe the Synthesis documentation system and its use by a large number of historians in the context of an ongoing research project in the field of History of Art. The system is Web-based and collaborative, and makes use of existing standards for information documentation and publication (CIDOC-CRM, RDF), focusing on semantic interoperability and the production of data of high value and long-term validity."}
{"id": "2010.12537", "title": "TUTA: Tree-based Transformers for Generally Structured Table Pre-training", "url": "https://arxiv.org/abs/2010.12537", "pdf": "https://arxiv.org/pdf/2010.12537", "abs": "https://arxiv.org/abs/2010.12537", "authors": ["Zhiruo Wang", "Haoyu Dong", "Ran Jia", "Jia Li", "Zhiyi Fu", "Shi Han", "Dongmei Zhang"], "categories": ["cs.IR", "cs.AI", "cs.DB"], "comment": "KDD'21", "summary": "Tables are widely used with various structures to organize and present data. Recent attempts on table understanding mainly focus on relational tables, yet overlook to other common table structures. In this paper, we propose TUTA, a unified pre-training architecture for understanding generally structured tables. Noticing that understanding a table requires spatial, hierarchical, and semantic information, we enhance transformers with three novel structure-aware mechanisms. First, we devise a unified tree-based structure, called a bi-dimensional coordinate tree, to describe both the spatial and hierarchical information of generally structured tables. Upon this, we propose tree-based attention and position embedding to better capture the spatial and hierarchical information. Moreover, we devise three progressive pre-training objectives to enable representations at the token, cell, and table levels. We pre-train TUTA on a wide range of unlabeled web and spreadsheet tables and fine-tune it on two critical tasks in the field of table structure understanding: cell type classification and table type classification. Experiments show that TUTA is highly effective, achieving state-of-the-art on five widely-studied datasets."}
{"id": "2106.15005", "title": "Untidy Data: The Unreasonable Effectiveness of Tables", "url": "https://arxiv.org/abs/2106.15005", "pdf": "https://arxiv.org/pdf/2106.15005", "abs": "https://arxiv.org/abs/2106.15005", "authors": ["Lyn Bartram", "Michael Correll", "Melanie Tory"], "categories": ["cs.HC"], "comment": null, "summary": "Working with data in table form is usually considered a preparatory and tedious step in the sensemaking pipeline; a way of getting the data ready for more sophisticated visualization and analytical tools. But for many people, spreadsheets -- the quintessential table tool -- remain a critical part of their information ecosystem, allowing them to interact with their data in ways that are hidden or abstracted in more complex tools. This is particularly true for data workers: people who work with data as part of their job but do not identify as professional analysts or data scientists. We report on a qualitative study of how these workers interact with and reason about their data. Our findings show that data tables serve a broader purpose beyond data cleanup at the initial stage of a linear analytic flow: users want to see and \"get their hands on\" the underlying data throughout the analytics process, reshaping and augmenting it to support sensemaking. They reorganize, mark up, layer on levels of detail, and spawn alternatives within the context of the base data. These direct interactions and human-readable table representations form a rich and cognitively important part of building understanding of what the data mean and what they can do with it. We argue that interactive tables are an important visualization idiom in their own right; that the direct data interaction they afford offers a fertile design space for visual analytics; and that sense making can be enriched by more flexible human-data interaction than is currently supported in visual analytics tools."}
{"id": "2008.11015", "title": "Table2Charts: Recommending Charts by Learning Shared Table Representations", "url": "https://arxiv.org/abs/2008.11015", "pdf": "https://arxiv.org/pdf/2008.11015", "abs": "https://arxiv.org/abs/2008.11015", "authors": ["Mengyu Zhou", "Qingtao Li", "Xinyi He", "Yuejiang Li", "Yibo Liu", "Wei Ji", "Shi Han", "Yining Chen", "Daxin Jiang", "Dongmei Zhang"], "categories": ["cs.DB", "cs.CL", "cs.HC"], "comment": "9 + 2(appendix) pages, accepted by KDD'21 conference", "summary": "It is common for people to create different types of charts to explore a multi-dimensional dataset (table). However, to recommend commonly composed charts in real world, one should take the challenges of efficiency, imbalanced data and table context into consideration. In this paper, we propose Table2Charts framework which learns common patterns from a large corpus of (table, charts) pairs. Based on deep Q-learning with copying mechanism and heuristic searching, Table2Charts does table-to-sequence generation, where each sequence follows a chart template. On a large spreadsheet corpus with 165k tables and 266k charts, we show that Table2Charts could learn a shared representation of table fields so that recommendation tasks on different chart types could mutually enhance each other. Table2Charts outperforms other chart recommendation systems in both multi-type task (with doubled recall numbers R@3=0.61 and R@1=0.43) and human evaluations."}
{"id": "2106.15339", "title": "SpreadsheetCoder: Formula Prediction from Semi-structured Context", "url": "https://arxiv.org/abs/2106.15339", "pdf": "https://arxiv.org/pdf/2106.15339", "abs": "https://arxiv.org/abs/2106.15339", "authors": ["Xinyun Chen", "Petros Maniatis", "Rishabh Singh", "Charles Sutton", "Hanjun Dai", "Max Lin", "Denny Zhou"], "categories": ["cs.SE", "cs.LG", "cs.PL"], "comment": "Published in ICML 2021", "summary": "Spreadsheet formula prediction has been an important program synthesis problem with many real-world applications. Previous works typically utilize input-output examples as the specification for spreadsheet formula synthesis, where each input-output pair simulates a separate row in the spreadsheet. However, this formulation does not fully capture the rich context in real-world spreadsheets. First, spreadsheet data entries are organized as tables, thus rows and columns are not necessarily independent from each other. In addition, many spreadsheet tables include headers, which provide high-level descriptions of the cell data. However, previous synthesis approaches do not consider headers as part of the specification. In this work, we present the first approach for synthesizing spreadsheet formulas from tabular context, which includes both headers and semi-structured tabular data. In particular, we propose SpreadsheetCoder, a BERT-based model architecture to represent the tabular context in both row-based and column-based formats. We train our model on a large dataset of spreadsheets, and demonstrate that SpreadsheetCoder achieves top-1 prediction accuracy of 42.51%, which is a considerable improvement over baselines that do not employ rich tabular context. Compared to the rule-based system, SpreadsheetCoder assists 82% more users in composing formulas on Google Sheets."}
{"id": "2106.13500", "title": "TableSense: Spreadsheet Table Detection with Convolutional Neural Networks", "url": "https://arxiv.org/abs/2106.13500", "pdf": "https://arxiv.org/pdf/2106.13500", "abs": "https://arxiv.org/abs/2106.13500", "authors": ["Haoyu Dong", "Shijie Liu", "Shi Han", "Zhouyu Fu", "Dongmei Zhang"], "categories": ["cs.IR"], "comment": null, "summary": "Spreadsheet table detection is the task of detecting all tables on a given sheet and locating their respective ranges. Automatic table detection is a key enabling technique and an initial step in spreadsheet data intelligence. However, the detection task is challenged by the diversity of table structures and table layouts on the spreadsheet. Considering the analogy between a cell matrix as spreadsheet and a pixel matrix as image, and encouraged by the successful application of Convolutional Neural Networks (CNN) in computer vision, we have developed TableSense, a novel end-to-end framework for spreadsheet table detection. First, we devise an effective cell featurization scheme to better leverage the rich information in each cell; second, we develop an enhanced convolutional neural network model for table detection to meet the domain-specific requirement on precise table boundary detection; third, we propose an effective uncertainty metric to guide an active learning based smart sampling algorithm, which enables the efficient build-up of a training dataset with 22,176 tables on 10,220 sheets with broad coverage of diverse table structures and layouts. Our evaluation shows that TableSense is highly effective with 91.3\\% recall and 86.5\\% precision in EoB-2 metric, a significant improvement over both the current detection algorithm that are used in commodity spreadsheet tools and state-of-the-art convolutional neural networks in computer vision."}
{"id": "2106.03096", "title": "TabularNet: A Neural Network Architecture for Understanding Semantic Structures of Tabular Data", "url": "https://arxiv.org/abs/2106.03096", "pdf": "https://arxiv.org/pdf/2106.03096", "abs": "https://arxiv.org/abs/2106.03096", "authors": ["Lun Du", "Fei Gao", "Xu Chen", "Ran Jia", "Junshan Wang", "Jiang Zhang", "Shi Han", "Dongmei Zhang"], "categories": ["cs.LG"], "comment": "10 pages, 7 figures, to be published in the proceedings of KDD 2021", "summary": "Tabular data are ubiquitous for the widespread applications of tables and hence have attracted the attention of researchers to extract underlying information. One of the critical problems in mining tabular data is how to understand their inherent semantic structures automatically. Existing studies typically adopt Convolutional Neural Network (CNN) to model the spatial information of tabular structures yet ignore more diverse relational information between cells, such as the hierarchical and paratactic relationships. To simultaneously extract spatial and relational information from tables, we propose a novel neural network architecture, TabularNet. The spatial encoder of TabularNet utilizes the row/column-level Pooling and the Bidirectional Gated Recurrent Unit (Bi-GRU) to capture statistical information and local positional correlation, respectively. For relational information, we design a new graph construction method based on the WordNet tree and adopt a Graph Convolutional Network (GCN) based encoder that focuses on the hierarchical and paratactic relationships between cells. Our neural network architecture can be a unified neural backbone for different understanding tasks and utilized in a multitask scenario. We conduct extensive experiments on three classification tasks with two real-world spreadsheet data sets, and the results demonstrate the effectiveness of our proposed TabularNet over state-of-the-art baselines."}
{"id": "2105.13733", "title": "FAST CAT: Collaborative Data Entry and Curation for Semantic Interoperability in Digital Humanities", "url": "https://arxiv.org/abs/2105.13733", "pdf": "https://arxiv.org/pdf/2105.13733", "abs": "https://arxiv.org/abs/2105.13733", "authors": ["Pavlos Fafalios", "Kostas Petrakis", "Georgios Samaritakis", "Korina Doerr", "Athina Kritsotaki", "Yannis Tzitzikas", "Martin Doerr"], "categories": ["cs.DL", "cs.DB"], "comment": "This is a preprint of an article accepted for publication at the ACM Journal on Computing and Cultural Heritage (JOCCH)", "summary": "Descriptive and empirical sciences, such as History, are the sciences that collect, observe and describe phenomena in order to explain them and draw interpretative conclusions about influences, driving forces and impacts under given circumstances. Spreadsheet software and relational database management systems are still the dominant tools for quantitative analysis and overall data management in these these sciences, allowing researchers to directly analyse the gathered data and perform scholarly interpretation. However, this current practice has a set of limitations, including the high dependency of the collected data on the initial research hypothesis, usually useless for other research, the lack of representation of the details from which the registered relations are inferred, and the difficulty to revisit the original data sources for verification, corrections or improvements. To cope with these problems, in this paper we present FAST CAT, a collaborative system for assistive data entry and curation in Digital Humanities and similar forms of empirical research. We describe the related challenges, the overall methodology we follow for supporting semantic interoperability, and discuss the use of FAST CAT in the context of a European (ERC) project of Maritime History, called SeaLiT, which examines economic, social and demographic impacts of the introduction of steamboats in the Mediterranean area between the 1850s and the 1920s."}
{"id": "2012.00697", "title": "Sigma Worksheet: Interactive Construction of OLAP Queries", "url": "https://arxiv.org/abs/2012.00697", "pdf": "https://arxiv.org/pdf/2012.00697", "abs": "https://arxiv.org/abs/2012.00697", "authors": ["James Gale", "Max Seiden", "Gretchen Atwood", "Jason Frantz", "Rob Woollen", "Çağatay Demiralp"], "categories": ["cs.DB", "cs.HC"], "comment": null, "summary": "The new generation of cloud data warehouses (CDWs) brings large amounts of data and compute power closer to users in enterprises. The ability to directly access the warehouse data, interactively analyze and explore it at scale can empower users to improve their decision making cycles. However, existing tools for analyzing data in CDWs are either limited in ad-hoc transformations or difficult to use for business users, the largest user segment in enterprises. Here we introduce Sigma Worksheet, a new interactive system that enables users to easily perform ad-hoc visual analysis of data in CDWs at scale. For this, Sigma Worksheet provides an accessible spreadsheet-like interface for data analysis through direct manipulation. Sigma Worksheet dynamically constructs matching SQL queries from user interactions on this familiar interface, building on the versatility and expressivity of SQL. Sigma Worksheet executes constructed queries directly on CDWs, leveraging the superior characteristics of the new generation CDWs, including scalability. To evaluate Sigma Worksheet, we first demonstrate its expressivity through two real life use cases, cohort analysis and sessionization. We then measure the performance of the Worksheet generated queries with a set of experiments using the TPC-H benchmark. Results show the performance of our compiled SQL queries is comparable to that of the reference queries of the benchmark. Finally, to assess the usefulness of Sigma Worksheet in deployment, we elicit feedback through a 100-person survey followed by a semi-structured interview study with 70 participants. We find that Sigma Worksheet is easier to use and learn, improving the productivity of users. Our findings also suggest Sigma Worksheet can further improve user experience by providing guidance to users at various steps of data analysis."}
{"id": "2104.13600", "title": "Mapping Spreadsheets to RDF: Supporting Excel in RML", "url": "https://arxiv.org/abs/2104.13600", "pdf": "https://arxiv.org/pdf/2104.13600", "abs": "https://arxiv.org/abs/2104.13600", "authors": ["Markus Schröder", "Christian Jilek", "Andreas Dengel"], "categories": ["cs.DB"], "comment": "6 pages, submitted to Second International Workshop on Knowledge Graph Construction", "summary": "The RDF Mapping Language (RML) enables, among other formats, the mapping of tabular data as Comma-Separated Values (CSV) files to RDF graphs. Unfortunately, the widely used spreadsheet format is currently neglected by its specification and well-known implementations. Therefore, we extended one of the tools which is RML Mapper to support Microsoft Excel spreadsheet files and demonstrate its capabilities in an interactive online demo. Our approach allows to access various meta data of spreadsheet cells in typical RML maps. Some experimental features for more specific use cases are also provided. The implementation code is publicly available in a GitHub fork."}
{"id": "2104.13576", "title": "Dataset Generation Patterns for Evaluating Knowledge Graph Construction", "url": "https://arxiv.org/abs/2104.13576", "pdf": "https://arxiv.org/pdf/2104.13576", "abs": "https://arxiv.org/abs/2104.13576", "authors": ["Markus Schröder", "Christian Jilek", "Andreas Dengel"], "categories": ["cs.DB"], "comment": "5 pages, submitted to ESWC demo track", "summary": "Confidentiality hinders the publication of authentic, labeled datasets of personal and enterprise data, although they could be useful for evaluating knowledge graph construction approaches in industrial scenarios. Therefore, our plan is to synthetically generate such data in a way that it appears as authentic as possible. Based on our assumption that knowledge workers have certain habits when they produce or manage data, generation patterns could be discovered which can be utilized by data generators to imitate real datasets. In this paper, we initially derived 11 distinct patterns found in real spreadsheets from industry and demonstrate a suitable generator called Data Sprout that is able to reproduce them. We describe how the generator produces spreadsheets in general and what altering effects the implemented patterns have."}
{"id": "2103.15203", "title": "Mathematics of Digital Hyperspace", "url": "https://arxiv.org/abs/2103.15203", "pdf": "https://arxiv.org/pdf/2103.15203", "abs": "https://arxiv.org/abs/2103.15203", "authors": ["Jeremy Kepner", "Timothy Davis", "Vijay Gadepally", "Hayden Jananthan", "Lauren Milechin"], "categories": ["cs.MS", "cs.DB", "cs.DM", "cs.NE", "math.RA"], "comment": "9 pages, 8 figures, 2 tables, accepted to GrAPL 2021. arXiv admin note: text overlap with arXiv:1807.03165, arXiv:2004.01181, arXiv:1909.05631, arXiv:1708.02937", "summary": "Social media, e-commerce, streaming video, e-mail, cloud documents, web pages, traffic flows, and network packets fill vast digital lakes, rivers, and oceans that we each navigate daily. This digital hyperspace is an amorphous flow of data supported by continuous streams that stretch standard concepts of type and dimension. The unstructured data of digital hyperspace can be elegantly represented, traversed, and transformed via the mathematics of hypergraphs, hypersparse matrices, and associative array algebra. This paper explores a novel mathematical concept, the semilink, that combines pairs of semirings to provide the essential operations for graph analytics, database operations, and machine learning. The GraphBLAS standard currently supports hypergraphs, hypersparse matrices, the mathematics required for semilinks, and seamlessly performs graph, network, and matrix operations. With the addition of key based indices (such as pointers to strings) and semilinks, GraphBLAS can become a richer associative array algebra and be a plug-in replacement for spreadsheets, database tables, and data centric operating systems, enhancing the navigation of unstructured data found in digital hyperspace."}
{"id": "2103.10472", "title": "Dynamic Kernel Matching for Non-conforming Data: A Case Study of T-cell Receptor Datasets", "url": "https://arxiv.org/abs/2103.10472", "pdf": "https://arxiv.org/pdf/2103.10472", "abs": "https://arxiv.org/abs/2103.10472", "authors": ["Jared Ostmeyer", "Scott Christley", "Lindsay Cowell"], "categories": ["q-bio.QM", "cs.LG"], "comment": null, "summary": "Most statistical classifiers are designed to find patterns in data where numbers fit into rows and columns, like in a spreadsheet, but many kinds of data do not conform to this structure. To uncover patterns in non-conforming data, we describe an approach for modifying established statistical classifiers to handle non-conforming data, which we call dynamic kernel matching (DKM). As examples of non-conforming data, we consider (i) a dataset of T-cell receptor (TCR) sequences labelled by disease antigen and (ii) a dataset of sequenced TCR repertoires labelled by patient cytomegalovirus (CMV) serostatus, anticipating that both datasets contain signatures for diagnosing disease. We successfully fit statistical classifiers augmented with DKM to both datasets and report the performance on holdout data using standard metrics and metrics allowing for indeterminant diagnoses. Finally, we identify the patterns used by our statistical classifiers to generate predictions and show that these patterns agree with observations from experimental studies."}
{"id": "2103.03537", "title": "Interactively Constructing Knowledge Graphs from Messy User-Generated Spreadsheets", "url": "https://arxiv.org/abs/2103.03537", "pdf": "https://arxiv.org/pdf/2103.03537", "abs": "https://arxiv.org/abs/2103.03537", "authors": ["Markus Schröder", "Christian Jilek", "Michael Schulze", "Andreas Dengel"], "categories": ["cs.DB"], "comment": "15 pages", "summary": "When spreadsheets are filled freely by knowledge workers, they can contain rather unstructured content. For humans and especially machines it becomes difficult to interpret such data properly. Therefore, spreadsheets are often converted to a more explicit, formal and structured form, for example, to a knowledge graph. However, if a data maintenance strategy has been missing and user-generated data becomes \"messy\", the construction of knowledge graphs will be a challenging task. In this paper, we catalog several of those challenges and propose an interactive approach to solve them. Our approach includes a graphical user interface which enables knowledge engineers to bulk-annotate spreadsheet cells with extracted information. Based on the cells' annotations a knowledge graph is ultimately formed. Using five spreadsheets from an industrial scenario, we built a 25k-triple graph during our evaluation. We compared our method with the state-of-the-art RDF Mapping Language (RML) attempt. The comparison highlights contributions of our approach."}
{"id": "2102.09461", "title": "Optimization Helps Scheduling Nursing Staff at the Long-Term Care Homes of the City of Toronto", "url": "https://arxiv.org/abs/2102.09461", "pdf": "https://arxiv.org/pdf/2102.09461", "abs": "https://arxiv.org/abs/2102.09461", "authors": ["Manion Anderson", "Merve Bodur", "Scott Rathwell", "Vahid Sarhangian"], "categories": ["cs.CY", "math.OC"], "comment": null, "summary": "The City of Toronto Long Term Care Homes & Services (LTCH&S) division is one of the largest providers of long-term care in the Canadian province of Ontario, providing care to 2,640 residents at 10 homes across Toronto. Our collaboration with LTCH&S was initiated to facilitate the increasingly challenging task of scheduling nursing staff and reduce high absenteeism rate observed among the part-time nurses. We developed a spreadsheet-based scheduling tool to automate the generation of schedules and incorporate nurses' preferences for different shifts into the schedules. At the core of the scheduling tool is a hierarchical optimization model that generates a feasible schedule with the highest total preference score while satisfying the maximum possible demand. Feasible schedules had to abide by a set of complex seniority requirements which prioritized more senior nurses when allocating the available shifts. Our scheduling tool was implemented in a 391-bed home in Toronto. The tool allowed nursing managers to generate feasible schedules within a fraction of an hour, in contrast to the status-quo manual approach which could took up to tens of hours. In addition, the schedules successfully accounted for preferences with on average above 94% of the allocated shifts ranked as most preferred."}
{"id": "2012.01571", "title": "Online Model Swapping in Architectural Simulation", "url": "https://arxiv.org/abs/2012.01571", "pdf": "https://arxiv.org/pdf/2012.01571", "abs": "https://arxiv.org/abs/2012.01571", "authors": ["Patrick Lavin", "Jeffrey Young", "Rich Vuduc", "Jonathan Beard"], "categories": ["cs.AR"], "comment": null, "summary": "As systems and applications grow more complex, detailed simulation takes an ever increasing amount of time. The prospect of increased simulation time resulting in slower design iteration forces architects to use simpler models, such as spreadsheets, when they want to iterate quickly on a design. However, the task of migrating from a simple simulation to one with more detail often requires multiple executions to find where simple models could be effective, which could be more expensive than running the detailed model in the first place. Also, architects must often rely on intuition to choose these simpler models, further complicating the problem.\n  In this work, we present a method of bridging the gap between simple and detailed simulation by monitoring simulation behavior online and automatically swapping out detailed models with simpler statistical approximations. We demonstrate the potential of our methodology by implementing it in the open-source simulator SVE-Cachesim to swap out the level one data cache (L1D) within a memory hierarchy. This proof of concept demonstrates that our technique can handle a non-trivial use-case in not just approximation of local time-invariant statistics, but also those that vary with time (e.g., the L1D is a form of a time-series function), and downstream side-effects (e.g., the L1D filters accesses for the level two cache). Our simulation swaps out the built-in cache model with only an 8% error in the simulated cycle count while using the approximated cache models for over 90% of the simulation, and our simpler models require two to eight times less computation per \"execution\" of the model"}
{"id": "1909.00855", "title": "Defining and Adopting an End User Computing Policy: A Case Study", "url": "https://arxiv.org/abs/1909.00855", "pdf": "https://arxiv.org/pdf/1909.00855", "abs": "https://arxiv.org/abs/1909.00855", "authors": ["Roger Turner"], "categories": ["cs.HC", "cs.CY"], "comment": "25 Pages, 12 Colour Figures. 1 Table. First presented at the EuSpRIG 2018 Conference at Imperial College, London. Revised and updated following a further presentation at the EuSpRIG 2019 Conference also at Imperial College, London", "summary": "End User Computing carries significant risks if not well controlled. This paper is a case study of the introduction of an updated End User Computing policy at the Wesleyan Assurance Society. The paper outlines the plan and identifies various challenges. The paper explains how these challenges were overcome. We wrote an End User Computing Risk Assessment Application which calculates a risk rating band based on the Complexity, Materiality and Control (or lack of it) pertaining to any given application and the basis of assessment is given in this paper. The policy uses a risk based approach for assessing and mitigating against the highest risks first and obtaining the quickest benefit."}
{"id": "2011.05978", "title": "The Impact of Text Presentation on Translator Performance", "url": "https://arxiv.org/abs/2011.05978", "pdf": "https://arxiv.org/pdf/2011.05978", "abs": "https://arxiv.org/abs/2011.05978", "authors": ["Samuel Läubli", "Patrick Simianer", "Joern Wuebker", "Geza Kovacs", "Rico Sennrich", "Spence Green"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted for publication in Target", "summary": "Widely used computer-aided translation (CAT) tools divide documents into segments such as sentences and arrange them in a side-by-side, spreadsheet-like view. We present the first controlled evaluation of these design choices on translator performance, measuring speed and accuracy in three experimental text processing tasks. We find significant evidence that sentence-by-sentence presentation enables faster text reproduction and within-sentence error identification compared to unsegmented text, and that a top-and-bottom arrangement of source and target sentences enables faster text reproduction compared to a side-by-side arrangement. For revision, on the other hand, our results suggest that presenting unsegmented text results in the highest accuracy and time efficiency. Our findings have direct implications for best practices in designing CAT tools."}
{"id": "2010.09975", "title": "Calliope: Automatic Visual Data Story Generation from a Spreadsheet", "url": "https://arxiv.org/abs/2010.09975", "pdf": "https://arxiv.org/pdf/2010.09975", "abs": "https://arxiv.org/abs/2010.09975", "authors": ["Danqing Shi", "Xinyue Xu", "Fuling Sun", "Yang Shi", "Nan Cao"], "categories": ["cs.HC"], "comment": null, "summary": "Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation."}
{"id": "2009.03520", "title": "Leam: An Interactive System for In-situ Visual Text Analysis", "url": "https://arxiv.org/abs/2009.03520", "pdf": "https://arxiv.org/pdf/2009.03520", "abs": "https://arxiv.org/abs/2009.03520", "authors": ["Sajjadur Rahman", "Peter Griggs", "Çağatay Demiralp"], "categories": ["cs.DB", "cs.CL", "cs.HC"], "comment": null, "summary": "With the increase in scale and availability of digital text generated on the web, enterprises such as online retailers and aggregators often use text analytics to mine and analyze the data to improve their services and products alike. Text data analysis is an iterative, non-linear process with diverse workflows spanning multiple stages, from data cleaning to visualization. Existing text analytics systems usually accommodate a subset of these stages and often fail to address challenges related to data heterogeneity, provenance, workflow reusability and reproducibility, and compatibility with established practices. Based on a set of design considerations we derive from these challenges, we propose Leam, a system that treats the text analysis process as a single continuum by combining advantages of computational notebooks, spreadsheets, and visualization tools. Leam features an interactive user interface for running text analysis workflows, a new data model for managing multiple atomic and composite data types, and an expressive algebra that captures diverse sets of operations representing various stages of text analysis and enables coordination among different components of the system, including data, code, and visualizations. We report our current progress in Leam development while demonstrating its usefulness with usage examples. Finally, we outline a number of enhancements to Leam and identify several research directions for developing an interactive visual text analysis system."}
{"id": "2008.04543", "title": "Pen-based Interaction with Spreadsheets in Mobile Virtual Reality", "url": "https://arxiv.org/abs/2008.04543", "pdf": "https://arxiv.org/pdf/2008.04543", "abs": "https://arxiv.org/abs/2008.04543", "authors": ["Travis Gesslein", "Verena Biener", "Philipp Gagel", "Daniel Schneider", "Per Ola Kristensson", "Eyal Ofek", "Michel Pahud", "Jens Grubert"], "categories": ["cs.HC"], "comment": "10 pages, 11 figures, ISMAR 2020", "summary": "Virtual Reality (VR) can enhance the display and interaction of mobile knowledge work and in particular, spreadsheet applications. While spreadsheets are widely used yet are challenging to interact with, especially on mobile devices, using them in VR has not been explored in depth. A special uniqueness of the domain is the contrast between the immersive and large display space afforded by VR, contrasted by the very limited interaction space that may be afforded for the information worker on the go, such as an airplane seat or a small work-space. To close this gap, we present a tool-set for enhancing spreadsheet interaction on tablets using immersive VR headsets and pen-based input. This combination opens up many possibilities for enhancing the productivity for spreadsheet interaction. We propose to use the space around and in front of the tablet for enhanced visualization of spreadsheet data and meta-data. For example, extending sheet display beyond the bounds of the physical screen, or easier debugging by uncovering hidden dependencies between sheet's cells. Combining the precise on-screen input of a pen with spatial sensing around the tablet, we propose tools for the efficient creation and editing of spreadsheets functions such as off-the-screen layered menus, visualization of sheets dependencies, and gaze-and-touch-based switching between spreadsheet tabs. We study the feasibility of the proposed tool-set using a video-based online survey and an expert-based assessment of indicative human performance potential."}
{"id": "2005.05227", "title": "ObjTables: structured spreadsheets that promote data quality, reuse, and integration", "url": "https://arxiv.org/abs/2005.05227", "pdf": "https://arxiv.org/pdf/2005.05227", "abs": "https://arxiv.org/abs/2005.05227", "authors": ["Jonathan R. Karr", "Wolfram Liebermeister", "Arthur P. Goldberg", "John A. P. Sekar", "Bilal Shaikh"], "categories": ["cs.DB", "q-bio.QM"], "comment": "5 pages, 1 figures, 18 pages of supplementary information, 3 supplementary datasets", "summary": "A central challenge in science is to understand how systems behaviors emerge from complex networks. This often requires aggregating, reusing, and integrating heterogeneous information. Supplementary spreadsheets to articles are a key data source. Spreadsheets are popular because they are easy to read and write. However, spreadsheets are often difficult to reanalyze because they capture data ad hoc without schemas that define the objects, relationships, and attributes that they represent. To help researchers reuse and compose spreadsheets, we developed ObjTables, a toolkit that makes spreadsheets human- and machine-readable by combining spreadsheets with schemas and an object-relational mapping system. ObjTables includes a format for schemas; markup for indicating the class and attribute represented by each spreadsheet and column; numerous data types for scientific information; and high-level software for using schemas to read, write, validate, compare, merge, revision, and analyze spreadsheets. By making spreadsheets easier to reuse, ObjTables could enable unprecedented secondary meta-analyses. By making it easy to build new formats and associated software for new types of data, ObjTables can also accelerate emerging scientific fields."}
{"id": "2007.00003", "title": "EQUS -- helping to see formulae", "url": "https://arxiv.org/abs/2007.00003", "pdf": "https://arxiv.org/pdf/2007.00003", "abs": "https://arxiv.org/abs/2007.00003", "authors": ["Chris Roast"], "categories": ["cs.HC", "cs.SE"], "comment": "12 Pages, 7 Colour Figures", "summary": "Visualisation is often presented as a means of simplifying information and helping people understand complex data. In this paper we describe the design, development and evaluation of an interactive visualisation for spreadsheet formulae (EQUS). The work is justified on the grounds that these are widely used tools for significant numerical processing and modeling, yet the formula developed can be easily misunderstood. The development process was one of iterative refinement engaging an initial target audience of mid-teen learners, involving re-design and formative evaluation. The resulting visualisation techniques have been found to be broadly relevant to spreadsheet users beyond the initial target audience. EQUS has since been developed as fully integrated plug-in for MS Excel."}
{"id": "2006.14694", "title": "From webtables to datatables", "url": "https://arxiv.org/abs/2006.14694", "pdf": "https://arxiv.org/pdf/2006.14694", "abs": "https://arxiv.org/abs/2006.14694", "authors": ["Mária Csernoch"], "categories": ["cs.SE"], "comment": "22 pages, 34 Formulae & 21 Colour Figures", "summary": "Webtables -- tables and table-like structures on webpages -- are excellent sources for teaching spreadsheeting, in commercial and professional organisations by utilizing and developing knowledge-transfer items, presenting and handling various real-world problems and solutions, discussing and debugging, and in general, developing and utilizing computational thinking skills. In the present paper the conversion process of one of the LOL Boards (League of Legends, Riot Games Inc. 2019) is detailed. After presenting the algorithm of the conversion, two solutions are offered -- one in a word processor, the other purely in a spreadsheet application -- leaving space for discussions, inventing other solutions and combining them."}
{"id": "2006.08224", "title": "Needles in the 'Sheet'stack: Augmented Analytics to get Insights from Spreadsheets", "url": "https://arxiv.org/abs/2006.08224", "pdf": "https://arxiv.org/pdf/2006.08224", "abs": "https://arxiv.org/abs/2006.08224", "authors": ["Medha Atre", "Anand Deshpande", "Reshma Godse", "Pooja Deokar", "Sandip Moharir", "Dhruva Ray", "Akshay Chitlangia", "Trupti Phadnis", "Yugansh Goyal"], "categories": ["cs.DB", "cs.HC"], "comment": null, "summary": "Business intelligence (BI) tools for database analytics have come a long way and nowadays also provide ready insights or visual query explorations, e.g. QuickInsights by Microsoft Power BI, SpotIQ by ThoughtSpot, Zenvisage, etc. In this demo, we focus on providing insights by examining periodic spreadsheets of different reports (aka views), without prior knowledge of the schema of the database or reports, or data information. Such a solution is targeted at users without the familiarity with the database schema or resources to conduct analytics in the contemporary way."}
{"id": "2006.05814", "title": "Implementation Strategies for Multidimensional Spreadsheets", "url": "https://arxiv.org/abs/2006.05814", "pdf": "https://arxiv.org/pdf/2006.05814", "abs": "https://arxiv.org/abs/2006.05814", "authors": ["Paul Mireault"], "categories": ["cs.SE"], "comment": "12 Pages, 18 Colour Figures. arXiv admin note: text overlap with arXiv:1801.09777", "summary": "Seasoned Excel developers were invited to participate in a challenge to implement a spreadsheet with multi-dimensional variables. We analyzed their spreadsheet to see the different implement strategies employed. We identified two strategies: most participants used a projection of three or four-dimensional variables on the two-dimensional plane used by Excel. A few participants used a database approach where the multi-dimensional variables are presented in the form of a dataset table with the appropriate primary key. This approach leads to simpler formulas."}
{"id": "2006.04794", "title": "Abstracting spreadsheet data flow through hypergraph redrawing", "url": "https://arxiv.org/abs/2006.04794", "pdf": "https://arxiv.org/pdf/2006.04794", "abs": "https://arxiv.org/abs/2006.04794", "authors": ["David Birch", "Nicolai Stawinoga", "Jack Binks", "Bruno Nicoletti", "Paul Kelly"], "categories": ["cs.SE"], "comment": "23 Pages, 12 Colour Figures", "summary": "We believe the error prone nature of traditional spreadsheets is due to their low level of abstraction. End user programmers are forced to construct their data models from low level cells which we define as \"a data container or manipulator linked by user-intent to model their world and positioned to reflect its structure\". Spreadsheet cells are limited in what they may contain (scalar values) and the links between them are inherently hidden. This paper proposes a method of raising the level of abstraction of spreadsheets by \"redrawing the boundary\" of the cell. To expose the hidden linkage structure we transform spreadsheets into fine-grained graphs with operators and values as nodes. \"cells\" are then represented as hypergraph edges by drawing a boundary \"wall\" around a set of operator/data nodes. To extend what cells may contain and to create a higher level model of the spreadsheet we propose that researchers should seek techniques to redraw these boundaries to create higher level \"cells\" which will more faithfully represent the end-user's real world/mental model. We illustrate this approach via common sub-expression identification and the application of sub-tree isomorphisms for the detection of vector (array) operations."}
{"id": "2006.04793", "title": "Developing Excel Thought Leadership", "url": "https://arxiv.org/abs/2006.04793", "pdf": "https://arxiv.org/pdf/2006.04793", "abs": "https://arxiv.org/abs/2006.04793", "authors": ["David Lyford-Smith"], "categories": ["cs.CY"], "comment": "8 Pages", "summary": "Over a period of five years, the Institute of Chartered Accountants in England and Wales (ICAEW) has developed a suite of three 'thought leadership' papers surrounding good practice in spreadsheet use and spreadsheet work environments. We will review the history of these three papers, the key lessons which each has to teach, and discuss how the process of making them has helped ICAEW to develop its position in the field."}
{"id": "2004.11113", "title": "Human-Machine Collaboration for Democratizing Data Science", "url": "https://arxiv.org/abs/2004.11113", "pdf": "https://arxiv.org/pdf/2004.11113", "abs": "https://arxiv.org/abs/2004.11113", "authors": ["Clément Gautrais", "Yann Dauxais", "Stefano Teso", "Samuel Kolb", "Gust Verbruggen", "Luc De Raedt"], "categories": ["cs.AI", "cs.HC"], "comment": "26 pages", "summary": "Everybody wants to analyse their data, but only few posses the data science expertise to to this. Motivated by this observation we introduce a novel framework and system \\textsc{VisualSynth} for human-machine collaboration in data science.\n  It wants to democratize data science by allowing users to interact with standard spreadsheet software in order to perform and automate various data analysis tasks ranging from data wrangling, data selection, clustering, constraint learning, predictive modeling and auto-completion. \\textsc{VisualSynth} relies on the user providing colored sketches, i.e., coloring parts of the spreadsheet, to partially specify data science tasks, which are then determined and executed using artificial intelligence techniques."}
{"id": "2001.01007", "title": "Automated Discovery of Data Transformations for Robotic Process Automation", "url": "https://arxiv.org/abs/2001.01007", "pdf": "https://arxiv.org/pdf/2001.01007", "abs": "https://arxiv.org/abs/2001.01007", "authors": ["Volodymyr Leno", "Marlon Dumas", "Marcello La Rosa", "Fabrizio Maria Maggi", "Artem Polyvyanyy"], "categories": ["cs.AI"], "comment": "8 pages, 5 figures. To be published in proceedings of AAAI-20 workshop on Intelligent Process Automation", "summary": "Robotic Process Automation (RPA) is a technology for automating repetitive routines consisting of sequences of user interactions with one or more applications. In order to fully exploit the opportunities opened by RPA, companies need to discover which specific routines may be automated, and how. In this setting, this paper addresses the problem of analyzing User Interaction (UI) logs in order to discover routines where a user transfers data from one spreadsheet or (Web) form to another. The paper maps this problem to that of discovering data transformations by example - a problem for which several techniques are available. The paper shows that a naive application of a state-of-the-art technique for data transformation discovery is computationally inefficient. Accordingly, the paper proposes two optimizations that take advantage of the information in the UI log and the fact that data transfers across applications typically involve copying alphabetic and numeric tokens separately. The proposed approach and its optimizations are evaluated using UI logs that replicate a real-life repetitive data transfer routine."}
{"id": "1912.09209", "title": "Comprehensive review for common types of errors using spreadsheets", "url": "https://arxiv.org/abs/1912.09209", "pdf": "https://arxiv.org/pdf/1912.09209", "abs": "https://arxiv.org/abs/1912.09209", "authors": ["Ali Aburas"], "categories": ["cs.SE"], "comment": null, "summary": "Thanks to their flexibility and capability to perform different tasks and organize data in the best form and format, spreadsheets are widely used in different organizations and by different end users. Many business organizations rely on spreadsheets to fulfill their various tasks. On the other hand, the number of spreadsheets that contain errors are very high, thus researchers have developed different tools aimed at the prevention, detection, and correction of errors in spreadsheets. This research work is a comprehensive review that describes and classifies approaches on finding and fixing errors in spreadsheets. The paper discusses up-to-date research work approaches in terms of definition, how they work, and kinds of errors they can find in spreadsheets. The paper looks also for the kinds of errors that end users commonly make in spreadsheets."}
{"id": "1910.05685", "title": "A Coding-free Software Framework of Developing Web Data Management Systems", "url": "https://arxiv.org/abs/1910.05685", "pdf": "https://arxiv.org/pdf/1910.05685", "abs": "https://arxiv.org/abs/1910.05685", "authors": ["Can Yang", "Shiying Pan", "Runmin Li", "Yu Liu", "Lizhang Peng"], "categories": ["cs.SE"], "comment": "16pages, 11 figures, 2 tables", "summary": "More and more enterprises recently intend to deploy data management systems in the cloud. Due to the professionalism of software development, it has still been difficult for non-programmers to develop this kind of systems, even a small one. However, the development of SaaS brings forth the more feasibility of coding-free software development than before. Based on the SaaS architecture, this paper presents a set of theory and method for coding-free construction of a data management system, on which our contributions involve in a practical application platform, a set of construction method and a set of interface on data exchange. By abstracting the common features of data management systems, we design a universal web platform to quickly generate and publish customized system instances. Moreover, we propose a kind of method to develop a data management system using a specific requirements table in spreadsheet. The corresponding platform maps the requirements table into a system instance through parsing the table model and implementing the objective system in the running stage. Finally, we implement the proposed framework and deploy it on web. The empirical result demonstrates the feasibility and availability of the coding-free method in developing web data management systems."}
{"id": "1712.05944", "title": "Taggle: Combining Overview and Details in Tabular Data Visualizations", "url": "https://arxiv.org/abs/1712.05944", "pdf": "https://arxiv.org/pdf/1712.05944", "abs": "https://arxiv.org/abs/1712.05944", "authors": ["Katarina Furmanova", "Samuel Gratzl", "Holger Stitz", "Thomas Zichner", "Miroslava Jaresova", "Alexander Lex", "Marc Streit"], "categories": ["cs.HC"], "comment": null, "summary": "Most tabular data visualization techniques focus on overviews, yet many practical analysis tasks are concerned with investigating individual items of interest. At the same time, relating an item to the rest of a potentially large table is important. In this work we present Taggle, a tabular visualization technique for exploring and presenting large and complex tables. Taggle takes an item-centric, spreadsheet-like approach, visualizing each row in the source data individually using visual encodings for the cells. At the same time, Taggle introduces data-driven aggregation of data subsets. The aggregation strategy is complemented by interaction methods tailored to answer specific analysis questions, such as sorting based on multiple columns and rich data selection and filtering capabilities. We demonstrate Taggle using a case study conducted by a domain expert on complex genomics data analysis for the purpose of drug discovery."}
{"id": "1909.07462", "title": "A Case Study of Spreadsheet Use within the Finance and Academic Registry units within a Higher Education Institution", "url": "https://arxiv.org/abs/1909.07462", "pdf": "https://arxiv.org/pdf/1909.07462", "abs": "https://arxiv.org/abs/1909.07462", "authors": ["Simon Thorne", "Jamie Hancock"], "categories": ["cs.CY"], "comment": "15 Pages, 20 Tables", "summary": "This paper presents the findings of a case study of spreadsheet use in a higher education institution in the UK. The paper considers the use of spreadsheets in two units of the organisation, academic registry and finance. Spreadsheet use is explored in terms of importance, training, experience, purpose, techniques deployed, size of spreadsheets created and sharing of spreadsheets. The implications of the results are then considered in terms of accurate reporting to external funding bodies such the funding councils, internal data integrity and internal data efficiencies. The results show a large volume of spreadsheets being created and used, that the profile of spreadsheet developers is typical of other studies of spreadsheet use and the need for the organisation to have clear principles and guidelines for the development of spreadsheet models in the organisation to ensure data integrity, reduce duplication of effort and to optimise the use of spreadsheets to meet the institutions goals."}
{"id": "1909.00891", "title": "Structured Spreadsheet Modelling and Implementation with Multiple Dimensions -- Part 2: Implementation", "url": "https://arxiv.org/abs/1909.00891", "pdf": "https://arxiv.org/pdf/1909.00891", "abs": "https://arxiv.org/abs/1909.00891", "authors": ["Paul Mireault"], "categories": ["cs.SE"], "comment": "14 Pages, 12 Colour Figures, 3 Tables. First presented at EuSpRIG 2018, Imperial College, London", "summary": "In Part 1, we showed how to develop a conceptual model of a problem involving variables of multiple dimensions, like Products, Regions, Sectors and Months. The conceptual model is presented as a Formula Diagram, giving a global view of the interaction between all the variables, and a Formula List, giving a precise view of the interaction between the variables. In this paper, we present precise steps to implement a multi-dimensional problem in a way that will produce a spreadsheet that is easy to maintain"}
{"id": "1909.00865", "title": "Are digital natives spreadsheet natives?", "url": "https://arxiv.org/abs/1909.00865", "pdf": "https://arxiv.org/pdf/1909.00865", "abs": "https://arxiv.org/abs/1909.00865", "authors": ["Maria Csernoch", "Piroska Biró"], "categories": ["cs.HC"], "comment": "13 Pages, 6 Colour Figures, 9 Tables. First Presented at the EuSpRIG 2018 conference, Imperial College, London", "summary": "The present paper reports the results of testing first year students of Informatics on their algorithmic skills and knowledge transfer abilities in spreadsheet environments. The selection of students plays a crucial role in the project. On the one hand, they have officially finished their spreadsheet training - they know everything - while on the other hand, they do not need any training, since they are digital natives, to whom digital skills are assigned by birth. However, we found that the students had serious difficulties in solving the spreadsheet problems presented: so low were their results that it allowed us to form broad tendencies. Considering computational thinking, algorithmic skills, and knowledge transfer abilities, it is clear that those students performed better who used algorithm-based, multilevel array formulas instead of problem specific, unconnected built-in functions. Furthermore, we can conclude that students, regardless of their birth date and digital generation assigned to them, are in great need of official, high-mathability, algorithm-based training with expert teachers."}
{"id": "1909.02960", "title": "Real-time stock analysis for blending recipes in industrial plants", "url": "https://arxiv.org/abs/1909.02960", "pdf": "https://arxiv.org/pdf/1909.02960", "abs": "https://arxiv.org/abs/1909.02960", "authors": ["Florin Zamfir", "Nicolae Paraschiv", "Emil Pricop"], "categories": ["cs.OH"], "comment": "Accepted for presentation at 23rd International Conference on System Theory, Control and Computing (ICSTCC 2019), October 9-11, 2019, Sinaia, Romania", "summary": "Many companies use Excel spreadsheets to keep stock records and to calculate process-specific data. These spreadsheets are often hard to understand and track. And if the user does not protect them, there is a risk that the user randomly changes or erase formulas. The paper focuses on the stocks of products used in a blending process with a known recipe. Developing an application that can bring this data in a centralized form and that can assist the operator in decide is a necessity. When a programmer implements an application that uses data from plants he needs to consider one fundamental aspect as reading real-time data from the process. The real-time stock analysis application takes into account all the above elements. The application is easy to use by an operator in the command room of installation because of the planning algorithms integrated into it. The algorithms proposed and implemented in this paper have well-defined goals: identifying the ingredients needed to achieve the blending process for required quantities, determine the quantities of the finished product that can be made with the existing ingredients and determine the optimum quantities of the finished product. The application implemented in C# intensively uses these algorithms and gives the user the ability to build the result step by step."}
{"id": "1908.08187", "title": "A CNN toolbox for skin cancer classification", "url": "https://arxiv.org/abs/1908.08187", "pdf": "https://arxiv.org/pdf/1908.08187", "abs": "https://arxiv.org/abs/1908.08187", "authors": ["Fabrizio Nunnari", "Daniel Sonntag"], "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "DFKI Technical Report", "summary": "We describe a software toolbox for the configuration of deep neural networks in the domain of skin cancer classification. The implemented software architecture allows developers to quickly set up new convolutional neural network (CNN) architectures and hyper-parameter configurations. At the same time, the user interface, manageable as a simple spreadsheet, allows non-technical users to explore different configuration settings that need to be explored when switching to different data sets. In future versions, meta leaning frameworks can be added, or AutoML systems that continuously improve over time. Preliminary results, conducted with two CNNs in the context melanoma detection on dermoscopic images, quantify the impact of image augmentation, image resolution, and rescaling filter on the overall detection performance and training time."}
{"id": "1907.03595", "title": "Recommending Related Tables", "url": "https://arxiv.org/abs/1907.03595", "pdf": "https://arxiv.org/pdf/1907.03595", "abs": "https://arxiv.org/abs/1907.03595", "authors": ["Shuo Zhang", "Krisztian Balog"], "categories": ["cs.IR"], "comment": null, "summary": "Tables are an extremely powerful visual and interactive tool for structuring and manipulating data, making spreadsheet programs one of the most popular computer applications. In this paper we introduce and address the task of recommending related tables: given an input table, identifying and returning a ranked list of relevant tables. One of the many possible application scenarios for this task is to provide users of a spreadsheet program proactively with recommendations for related structured content on the Web. At its core, the related table recommendation task boils down to computing the similarity between a pair of tables. We develop a theoretically sound framework for performing table matching. Our approach hinges on the idea of representing table elements in multiple semantic spaces, and then combining element-level similarities using a discriminative learning model. Using a purpose-built test collection from Wikipedia tables, we demonstrate that the proposed approach delivers state-of-the-art performance."}
{"id": "1907.04827", "title": "Hillview: A trillion-cell spreadsheet for big data", "url": "https://arxiv.org/abs/1907.04827", "pdf": "https://arxiv.org/pdf/1907.04827", "abs": "https://arxiv.org/abs/1907.04827", "authors": ["Mihai Budiu", "Parikshit Gopalan", "Lalith Suresh", "Udi Wieder", "Han Kruiger", "Marcos K. Aguilera"], "categories": ["cs.DC"], "comment": null, "summary": "Hillview is a distributed spreadsheet for browsing very large datasets that cannot be handled by a single machine. As a spreadsheet, Hillview provides a high degree of interactivity that permits data analysts to explore information quickly along many dimensions while switching visualizations on a whim. To provide the required responsiveness, Hillview introduces visualization sketches, or vizketches, as a simple idea to produce compact data visualizations. Vizketches combine algorithmic techniques for data summarization with computer graphics principles for efficient rendering. While simple, vizketches are effective at scaling the spreadsheet by parallelizing computation, reducing communication, providing progressive visualizations, and offering precise accuracy guarantees. Using Hillview running on eight servers, we can navigate and visualize datasets of tens of billions of rows and trillions of cells, much beyond the published capabilities of competing systems."}
{"id": "1907.04217", "title": "Streaming 1.9 Billion Hypersparse Network Updates per Second with D4M", "url": "https://arxiv.org/abs/1907.04217", "pdf": "https://arxiv.org/pdf/1907.04217", "abs": "https://arxiv.org/abs/1907.04217", "authors": ["Jeremy Kepner", "Vijay Gadepally", "Lauren Milechin", "Siddharth Samsi", "William Arcand", "David Bestor", "William Bergeron", "Chansup Byun", "Matthew Hubbell", "Michael Houle", "Michael Jones", "Anne Klein", "Peter Michaleas", "Julie Mullen", "Andrew Prout", "Antonio Rosa", "Charles Yee", "Albert Reuther"], "categories": ["cs.DC", "cs.DB", "cs.DS", "cs.IR", "cs.PF"], "comment": "6 pages; 6 figures; accepted to IEEE High Performance Extreme Computing (HPEC) Conference 2019. arXiv admin note: text overlap with arXiv:1807.05308, arXiv:1902.00846", "summary": "The Dynamic Distributed Dimensional Data Model (D4M) library implements associative arrays in a variety of languages (Python, Julia, and Matlab/Octave) and provides a lightweight in-memory database implementation of hypersparse arrays that are ideal for analyzing many types of network data. D4M relies on associative arrays which combine properties of spreadsheets, databases, matrices, graphs, and networks, while providing rigorous mathematical guarantees, such as linearity. Streaming updates of D4M associative arrays put enormous pressure on the memory hierarchy. This work describes the design and performance optimization of an implementation of hierarchical associative arrays that reduces memory pressure and dramatically increases the update rate into an associative array. The parameters of hierarchical associative arrays rely on controlling the number of entries in each level in the hierarchy before an update is cascaded. The parameters are easily tunable to achieve optimal performance for a variety of applications. Hierarchical arrays achieve over 40,000 updates per second in a single instance. Scaling to 34,000 instances of hierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of 1,900,000,000 updates per second. This capability allows the MIT SuperCloud to analyze extremely large streaming network data sets."}
{"id": "1907.02099", "title": "GeoGebra e situações que envolvem modelação numa abordagem STEAM", "url": "https://arxiv.org/abs/1907.02099", "pdf": "https://arxiv.org/pdf/1907.02099", "abs": "https://arxiv.org/abs/1907.02099", "authors": ["J. M. D. S. Dos Santos", "A. P. Silveira", "A. E. S. Trocado"], "categories": ["math.HO", "cs.CY"], "comment": "in Portuguese", "summary": "In order to implement a STEAM approach including the use of technology, namely the use of interactive mathematics software GeoGebra, in mathematics classes, in the lusophone space, the materials presented here were conceived, to be implemented in a first phase among teachers. Later, with the necessary adaptations, these tasks will be applied to the students. The tasks deal with modeling situations, in two- and three-dimensional geometric problems, in order to apply GeoGebra software in its analysis to illustrate its capabilities. The different windows of this software are used, namely the 2D and 3D windows, CAS window, spreadsheet and extra two dimensional windows in order to study cutting planes in solids and some surfaces. The tasks are presented so that any user, regardless of the degree of knowledge they have of the software, can follow them, being supported in scripts with some indications of the tools and commands to use. Designed for the teaching and learning of Mathematics, from a STEAM approach, these tasks allow connections with other Sciences and the Arts, and allow the development of projects using and consolidating relevant mathematical contents. These tasks are part of the proposals of activities of the participants of the Training Courses for Trainers in GeoGebra for Portuguese Speaking Countries, which from 2019 have an impact on the STEAM approach. These courses are carried out with the high sponsorship of the Organization of Ibero-American States for Education, Science and Culture (OEI). Given the interest that the tasks have for the users of the Iberian space, as well as their dissemination at a global level, the materials initially developed in Portuguese language will be adapted for Spanish and English speakers."}
{"id": "1906.04011", "title": "Visual Backpropagation", "url": "https://arxiv.org/abs/1906.04011", "pdf": "https://arxiv.org/pdf/1906.04011", "abs": "https://arxiv.org/abs/1906.04011", "authors": ["Roy S. Freedman"], "categories": ["cs.LG", "cs.PL"], "comment": null, "summary": "We show how a declarative functional programming specification of backpropagation yields a visual and transparent implementation within spreadsheets. We call our method Visual Backpropagation. This backpropagation implementation exploits array worksheet formulas, manual calculation, and has a sequential order of computation similar to the processing of a systolic array. The implementation uses no hidden macros nor user-defined functions; there are no loops, assignment statements, or links to any procedural programs written in conventional languages. As an illustration, we compare a Visual Backpropagation solution to a Tensorflow (Python) solution on a standard regression problem."}
{"id": "1905.13072", "title": "Somewhere Around That Number: An Interview Study of How Spreadsheet Users Manage Uncertainty", "url": "https://arxiv.org/abs/1905.13072", "pdf": "https://arxiv.org/pdf/1905.13072", "abs": "https://arxiv.org/abs/1905.13072", "authors": ["Judith Borghouts", "Andrew D. Gordon", "Advait Sarkar", "Kenton P. O'Hara", "Neil Toronto"], "categories": ["cs.HC"], "comment": null, "summary": "Spreadsheet users regularly deal with uncertainty in their data, for example due to errors and estimates. While an insight into data uncertainty can help in making better informed decisions, prior research suggests that people often use informal heuristics to reason with probabilities, which leads to incorrect conclusions. Moreover, people often ignore or simplify uncertainty. To understand how people currently encounter and deal with uncertainty in spreadsheets, we conducted an interview study with 11 spreadsheet users from a range of domains. We found that how people deal with uncertainty is influenced by the role the spreadsheet plays in people's work and the user's aims. Spreadsheets are used as a database, template, calculation tool, notepad and exploration tool. In doing so, participants' aims were to compute and compare different scenarios, understand something about the nature of the uncertainty in their situation, and translate the complexity of data uncertainty into simplified presentations to other people, usually decision-makers. Spreadsheets currently provide limited tools to support these aims, and participants had various workarounds."}
{"id": "1902.00846", "title": "A Billion Updates per Second Using 30,000 Hierarchical In-Memory D4M Databases", "url": "https://arxiv.org/abs/1902.00846", "pdf": "https://arxiv.org/pdf/1902.00846", "abs": "https://arxiv.org/abs/1902.00846", "authors": ["Jeremy Kepner", "Vijay Gadepally", "Lauren Milechin", "Siddharth Samsi", "William Arcand", "David Bestor", "William Bergeron", "Chansup Byun", "Matthew Hubbell", "Micheal Houle", "Micheal Jones", "Anne Klein", "Peter Michaleas", "Julie Mullen", "Andrew Prout", "Antonio Rosa", "Charles Yee", "Albert Reuther"], "categories": ["cs.DB", "cs.DC", "cs.DS", "cs.NI"], "comment": "Northeast Database Data 2019 (MIT)", "summary": "Analyzing large scale networks requires high performance streaming updates of graph representations of these data. Associative arrays are mathematical objects combining properties of spreadsheets, databases, matrices, and graphs, and are well-suited for representing and analyzing streaming network data. The Dynamic Distributed Dimensional Data Model (D4M) library implements associative arrays in a variety of languages (Python, Julia, and Matlab/Octave) and provides a lightweight in-memory database. Associative arrays are designed for block updates. Streaming updates to a large associative array requires a hierarchical implementation to optimize the performance of the memory hierarchy. Running 34,000 instances of a hierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of 1,900,000,000 updates per second. This capability allows the MIT SuperCloud to analyze extremely large streaming network data sets."}
{"id": "1901.11100", "title": "ExceLint: Automatically Finding Spreadsheet Formula Errors", "url": "https://arxiv.org/abs/1901.11100", "pdf": "https://arxiv.org/pdf/1901.11100", "abs": "https://arxiv.org/abs/1901.11100", "authors": ["Daniel W. Barowy", "Emery D. Berger", "Benjamin Zorn"], "categories": ["cs.PL", "cs.SE"], "comment": "Appeared at OOPSLA 2018", "summary": "Spreadsheets are one of the most widely used programming environments, and are widely deployed in domains like finance where errors can have catastrophic consequences. We present a static analysis specifically designed to find spreadsheet formula errors. Our analysis directly leverages the rectangular character of spreadsheets. It uses an information-theoretic approach to identify formulas that are especially surprising disruptions to nearby rectangular regions. We present ExceLint, an implementation of our static analysis for Microsoft Excel. We demonstrate that ExceLint is fast and effective: across a corpus of 70 spreadsheets, ExceLint takes a median of 5 seconds per spreadsheet, and it significantly outperforms the state of the art analysis."}
{"id": "1807.00018", "title": "Computer Simulation of Neural Networks Using Spreadsheets: The Dawn of the Age of Camelot", "url": "https://arxiv.org/abs/1807.00018", "pdf": "https://arxiv.org/pdf/1807.00018", "abs": "https://arxiv.org/abs/1807.00018", "authors": ["Serhiy O. Semerikov", "Illia O. Teplytskyi", "Yuliia V. Yechkalo", "Arnold E. Kiv"], "categories": ["cs.CY"], "comment": "26 pages, 8 figures; submitted to the 1st International Workshop on Augmented Reality in Education (AREdu 2018)", "summary": "The article substantiates the necessity to develop training methods of computer simulation of neural networks in the spreadsheet environment. The systematic review of their application to simulating artificial neural networks is performed. The authors distinguish basic approaches to solving the problem of network computer simulation training in the spreadsheet environment, joint application of spreadsheets and tools of neural network simulation, application of third-party add-ins to spreadsheets, development of macros using the embedded languages of spreadsheets; use of standard spreadsheet add-ins for non-linear optimization, creation of neural networks in the spreadsheet environment without add-ins and macros. After analyzing a collection of writings of 1890-1950, the research determines the role of the scientific journal \"Bulletin of Mathematical Biophysics\", its founder Nicolas Rashevsky and the scientific community around the journal in creating and developing models and methods of computational neuroscience. There are identified psychophysical basics of creating neural networks, mathematical foundations of neural computing and methods of neuroengineering (image recognition, in particular). The role of Walter Pitts in combining the descriptive and quantitative theories of training is discussed. It is shown that to acquire neural simulation competences in the spreadsheet environment, one should master the models based on the historical and genetic approach. It is indicated that there are three groups of models, which are promising in terms of developing corresponding methods - the continuous two-factor model of Rashevsky, the discrete model of McCulloch and Pitts, and the discrete-continuous models of Householder and Landahl."}
{"id": "1810.04542", "title": "On the Refinement of Spreadsheet Smells by means of Structure Information", "url": "https://arxiv.org/abs/1810.04542", "pdf": "https://arxiv.org/pdf/1810.04542", "abs": "https://arxiv.org/abs/1810.04542", "authors": ["Patrick Koch", "Birgit Hofer", "Franz Wotawa"], "categories": ["cs.SE"], "comment": null, "summary": "Spreadsheet users are often unaware of the risks imposed by poorly designed spreadsheets. One way to assess spreadsheet quality is to detect smells which attempt to identify parts of spreadsheets that are hard to comprehend or maintain and which are more likely to be the root source of bugs. Unfortunately, current spreadsheet smell detection techniques suffer from a number of drawbacks that lead to incorrect or redundant smell reports. For example, the same quality issue is often reported for every copy of a cell, which may overwhelm users. To deal with these issues, we propose to refine spreadsheet smells by exploiting inferred structural information for smell detection. We therefore first provide a detailed description of our static analysis approach to infer clusters and blocks of related cells. We then elaborate on how to improve existing smells by providing three example refinements of existing smells that incorporate information about cell groups and computation blocks. Furthermore, we propose three novel smell detection techniques that make use of the inferred spreadsheet structures. Empirical evaluation of the proposed techniques suggests that the refinements successfully reduce the number of incorrectly and redundantly reported smells, and novel deficits are revealed by the newly introduced smells."}
{"id": "1809.03435", "title": "Now You're Thinking With Structures: A Concept for Structure-based Interactions with Spreadsheets", "url": "https://arxiv.org/abs/1809.03435", "pdf": "https://arxiv.org/pdf/1809.03435", "abs": "https://arxiv.org/abs/1809.03435", "authors": ["Patrick Koch"], "categories": ["cs.SE"], "comment": "In Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (arXiv:1808.09174)", "summary": "Spreadsheets are the go-to tool for computerized calculation and modelling, but are hard to comprehend and adapt after reaching a certain complexity. In general, cognition of complex systems is facilitated by having a higher order mental model of the system in question to work with. We therefore present a concept for structure-aware understanding of and interaction with spreadsheets that extends previous work on structure inference in the domain. Following this concept, structural information is used to enrich visualizations, reactively enhance traditional user actions, and provide tools to proactively alter the overall spreadsheet makeup instead of individual cells The intended systems should, in first approximation, not replace common spreadsheet tools, but provide an additional layer of functionality alongside the established interface. In ongoing work, we therefore implemented a tool for structure inference and visualization along the common spreadsheet layout. Based on this framework, we plan to introduce the envisioned proactive and reactive interaction mechanics, and finally provide structure-aware unctionality as an add-in for common spreadsheet processors. We believe that providing the tools for thinking about and interacting with spreadsheets in this manner will benefit users both in terms of productivity and overall spreadsheet quality."}
{"id": "1804.01186", "title": "Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples", "url": "https://arxiv.org/abs/1804.01186", "pdf": "https://arxiv.org/pdf/1804.01186", "abs": "https://arxiv.org/abs/1804.01186", "authors": ["Ashwin Kalyan", "Abhishek Mohta", "Oleksandr Polozov", "Dhruv Batra", "Prateek Jain", "Sumit Gulwani"], "categories": ["cs.AI", "cs.LG", "cs.PL"], "comment": "Published in ICLR 2018, International Conference on Learning Representations (2018)", "summary": "Synthesizing user-intended programs from a small number of input-output examples is a challenging problem with several important applications like spreadsheet manipulation, data wrangling and code refactoring. Existing synthesis systems either completely rely on deductive logic techniques that are extensively hand-engineered or on purely statistical models that need massive amounts of data, and in general fail to provide real-time synthesis on challenging benchmarks. In this work, we propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique that combines the best of both symbolic logic techniques and statistical models. Thus, it produces programs that satisfy the provided specifications by construction and generalize well on unseen examples, similar to data-driven systems. Our technique effectively utilizes the deductive search framework to reduce the learning problem of the neural component to a simple supervised learning setup. Further, this allows us to both train on sparingly available real-world data and still leverage powerful recurrent neural network encoders. We demonstrate the effectiveness of our method by evaluating on real-world customer scenarios by synthesizing accurate programs with up to 12x speed-up compared to state-of-the-art systems."}
{"id": "1809.02746", "title": "Typed Table Transformations", "url": "https://arxiv.org/abs/1809.02746", "pdf": "https://arxiv.org/pdf/1809.02746", "abs": "https://arxiv.org/abs/1809.02746", "authors": ["Martin Erwig"], "categories": ["cs.SE"], "comment": "In Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (arXiv:1808.09174)", "summary": "Spreadsheet tables are often labeled, and these labels effectively constitute types for the data in the table. In such cases tables can be considered to be built from typed data where the placement of values within the table is controlled by the types used for rows and columns. We present a new approach to the transformations of spreadsheet tables that is based on transformations of row and column types. We illustrate the basic idea of type-based table construction and transformation and lay out a series of research questions that should be addressed in future work."}
{"id": "1809.00025", "title": "Implementing WHERE and ORDER BY as spreadsheet formulas", "url": "https://arxiv.org/abs/1809.00025", "pdf": "https://arxiv.org/pdf/1809.00025", "abs": "https://arxiv.org/abs/1809.00025", "authors": ["Paul Mireault"], "categories": ["cs.SE"], "comment": "In Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (arXiv:1808.09174)", "summary": "The WHERE and ORDER BY clauses of the SQL SELECT statement select a subset of rows in the result of a database query and present the result in the specified order. In a spreadsheet program like Microsoft Excel, one could use the filter and sort buttons, or use its Query or its Pivot Table tools to achieve a similar effect. The disadvantage of using those tools is that they don't react automatically to changes in the calculated values of the spreadsheet. In this paper, we develop spreadsheet formulas that implement SQL's WHERE and ORDER BY clauses."}
{"id": "1808.10642", "title": "The use of Charts, Pivot Tables, and Array Formulas in two Popular Spreadsheet Corpora", "url": "https://arxiv.org/abs/1808.10642", "pdf": "https://arxiv.org/pdf/1808.10642", "abs": "https://arxiv.org/abs/1808.10642", "authors": ["Bas Jansen", "Felienne Hermans"], "categories": ["cs.SE"], "comment": "In Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (arXiv:1808.09174)", "summary": "The use of spreadsheets in industry is widespread. Companies base decisions on information coming from spreadsheets. Unfortunately, spreadsheets are error-prone and this increases the risk that companies base their decisions on inaccurate information, which can lead to incorrect decisions and loss of money. In general, spreadsheet research is aimed to reduce the error-proneness of spreadsheets. Most research is concentrated on the use of formulas. However, there are other constructions in spreadsheets, like charts, pivot tables, and array formulas, that are also used to present decision support information to the user. There is almost no research about how these constructions are used. To improve spreadsheet quality it is important to understand how spreadsheets are used and to obtain a complete understanding, the use of charts, pivot tables, and array formulas should be included in research. In this paper, we analyze two popular spreadsheet corpora: Enron and EUSES on the use of the aforementioned constructions."}
{"id": "1808.10231", "title": "Asheetoxy: A Taxonomy for Classifying Negative Spreadsheet-related Phenomena", "url": "https://arxiv.org/abs/1808.10231", "pdf": "https://arxiv.org/pdf/1808.10231", "abs": "https://arxiv.org/abs/1808.10231", "authors": ["Daniel Kulesz", "Stefan Wagner"], "categories": ["cs.SE"], "comment": "In Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (arXiv:1808.09174)", "summary": "Spreadsheets (sometimes also called Excel programs) are powerful tools which play a business-critical role in many organizations. However, due to faulty spreadsheets many bad decisions have been taken in recent years. Since then, a number of researchers have been studying spreadsheet errors. However, one issue that hinders discussion among researchers and professionals is the lack of a commonly accepted taxonomy.\n  Albeit a number of taxonomies for spreadsheet errors have been proposed in previous work, a major issue is that they use the term error that itself is already ambiguous. Furthermore, to apply most existing taxonomies, detailed knowledge about the underlying process and knowledge about the \"brain state\" of the acting spreadsheet users is required. Due to these limitations, known error-like phenomena in freely available spreadsheet corpora cannot be classified with these taxonomies.\n  We propose Asheetoxy, a simple and phenomenon-oriented taxonomy that avoids the problematic term error altogether. An initial study with 7 participants indicates that even non-spreadsheet researchers similarly classify real-world spreadsheet phenomena using Asheetoxy."}
{"id": "1808.09174", "title": "Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (SEMS'18)", "url": "https://arxiv.org/abs/1808.09174", "pdf": "https://arxiv.org/pdf/1808.09174", "abs": "https://arxiv.org/abs/1808.09174", "authors": ["Birgit Hofer", "Jorge Mendes"], "categories": ["cs.SE"], "comment": null, "summary": "Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (SEMS'18), held on October 1st, 2018, in Lisbon, Portugal, and co-located with the 2018 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)."}
{"id": "1806.04952", "title": "Towards Semantically Enhanced Data Understanding", "url": "https://arxiv.org/abs/1806.04952", "pdf": "https://arxiv.org/pdf/1806.04952", "abs": "https://arxiv.org/abs/1806.04952", "authors": ["Markus Schröder", "Christian Jilek", "Jörn Hees", "Andreas Dengel"], "categories": ["cs.DB", "cs.AI", "cs.HC"], "comment": "4 pages, 3 figures", "summary": "In the field of machine learning, data understanding is the practice of getting initial insights in unknown datasets. Such knowledge-intensive tasks require a lot of documentation, which is necessary for data scientists to grasp the meaning of the data. Usually, documentation is separate from the data in various external documents, diagrams, spreadsheets and tools which causes considerable look up overhead. Moreover, other supporting applications are not able to consume and utilize such unstructured data. That is why we propose a methodology that uses a single semantic model that interlinks data with its documentation. Hence, data scientists are able to directly look up the connected information about the data by simply following links. Equally, they can browse the documentation which always refers to the data. Furthermore, the model can be used by other approaches providing additional support, like searching, comparing, integrating or visualizing data. To showcase our approach we also demonstrate an early prototype."}
{"id": "1805.10493", "title": "Combining Spreadsheet Smells for Improved Fault Prediction", "url": "https://arxiv.org/abs/1805.10493", "pdf": "https://arxiv.org/pdf/1805.10493", "abs": "https://arxiv.org/abs/1805.10493", "authors": ["Patrick Koch", "Konstantin Schekotihin", "Dietmar Jannach", "Birgit Hofer", "Franz Wotawa"], "categories": ["cs.SE"], "comment": "4 pages, 1 figure, to be published in 40th International Conference on Software Engineering: New Ideas and Emerging Results Track", "summary": "Spreadsheets are commonly used in organizations as a programming tool for business-related calculations and decision making. Since faults in spreadsheets can have severe business impacts, a number of approaches from general software engineering have been applied to spreadsheets in recent years, among them the concept of code smells. Smells can in particular be used for the task of fault prediction. An analysis of existing spreadsheet smells, however, revealed that the predictive power of individual smells can be limited. In this work we therefore propose a machine learning based approach which combines the predictions of individual smells by using an AdaBoost ensemble classifier. Experiments on two public datasets containing real-world spreadsheet faults show significant improvements in terms of fault prediction accuracy."}
{"id": "1805.06353", "title": "SmartTable: A Spreadsheet Program with Intelligent Assistance", "url": "https://arxiv.org/abs/1805.06353", "pdf": "https://arxiv.org/pdf/1805.06353", "abs": "https://arxiv.org/abs/1805.06353", "authors": ["Shuo Zhang", "Vugar Abdul Zada", "Krisztian Balog"], "categories": ["cs.IR"], "comment": "The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '18)", "summary": "We introduce SmartTable, an online spreadsheet application that is equipped with intelligent assistance capabilities. With a focus on relational tables, describing entities along with their attributes, we offer assistance in two flavors: (i) for populating the table with additional entities (rows) and (ii) for extending it with additional entity attributes (columns). We provide details of our implementation, which is also released as open source. The application is available at http://smarttable.cc."}
{"id": "1804.04175", "title": "An Easy & Collaborative RDF Data Entry Method using the Spreadsheet Metaphor", "url": "https://arxiv.org/abs/1804.04175", "pdf": "https://arxiv.org/pdf/1804.04175", "abs": "https://arxiv.org/abs/1804.04175", "authors": ["Markus Schröder", "Christian Jilek", "Jörn Hees", "Sven Hertling", "Andreas Dengel"], "categories": ["cs.SE"], "comment": "15 pages", "summary": "Spreadsheets are widely used by knowledge workers, especially in the industrial sector. Their methodology enables a well understood, easy and fast possibility to enter data. As filling out a spreadsheet is more accessible to common knowledge workers than defining RDF statements, in this paper, we propose an easy-to-use, zero-configuration, web-based spreadsheet editor that simultaneously transfers spreadsheet entries into RDF statements. It enables various kinds of users to easily create semantic data whether they are RDF experts or novices. The typical scenario we address focuses on creating instance data starting with an empty knowledge base that is filled incrementally. In a user study, participants were able to create more statements in shorter time, having similar or even significantly outperforming quality, compared to other approaches."}
{"id": "1801.09777", "title": "Structured Spreadsheet Modelling and Implementation with Multiple Dimensions - Part 1: Modelling", "url": "https://arxiv.org/abs/1801.09777", "pdf": "https://arxiv.org/pdf/1801.09777", "abs": "https://arxiv.org/abs/1801.09777", "authors": ["Paul Mireault"], "categories": ["cs.SE"], "comment": "13 Pages, 17 Tables and Figures", "summary": "Dimensions are an integral part of many models we use every day. Without thinking about it, we frequently use the time dimension: many financial and accounting spreadsheets have columns representing months or years. Representing a second dimension is often done by repeating blocs of formulas in a worksheet or creating multiple worksheets with the same structure."}
{"id": "1802.01640", "title": "Mitigating Spreadsheet Risk in Complex Multi-Dimensional Models in Excel", "url": "https://arxiv.org/abs/1802.01640", "pdf": "https://arxiv.org/pdf/1802.01640", "abs": "https://arxiv.org/abs/1802.01640", "authors": ["Steve Litt"], "categories": ["cs.SE"], "comment": "13 Pages, 11 Colour Figures", "summary": "Microsoft Excel is the most ubiquitous analytical tool ever built. Companies around the world leverage it for its power, flexibility and ease of use. However, spreadsheets are manually intensive and prone to error, making it difficult for companies to control spreadsheet risk. The following solution is designed to mitigate spreadsheet risk for a set of problems commonly addressed in a spreadsheet defined as \"complex multi-dimensional models\". \"Complex\" referring to certain types of applications that require functionality such as sophisticated algorithms, challenging hierarchies and database write-back (i.e. planning, forecasting, etc.) and \"multi-dimensional\" referring to providing capabilities such as reporting, data input forms and ad hoc analysis on the different attributes associated with the resulting model. The solution is defined as a \"PivotModel\" because it works similarly to a PivotTable but is designed to leverage the robust capabilities of the Microsoft Excel platform."}
{"id": "1802.01628", "title": "Proposed Spreadsheet Transparency Definition and Measures", "url": "https://arxiv.org/abs/1802.01628", "pdf": "https://arxiv.org/pdf/1802.01628", "abs": "https://arxiv.org/abs/1802.01628", "authors": ["Craig Hatmaker"], "categories": ["cs.SE"], "comment": "13 Pages, 12 Screenshots", "summary": "Auditors demand financial models be transparent yet no consensus exists on what that means precisely. Without a clear modeling transparency definition we cannot know when our models are \"transparent\". The financial modeling community debates which methods are more or less transparent as though transparency is a quantifiable entity yet no measures exist. Without a transparency measure modelers cannot objectively evaluate methods and know which improves model transparency.\n  This paper proposes a definition for spreadsheet modeling transparency that is specific enough to create measures and automation tools for auditors to determine if a model meets transparency requirements. The definition also provides modelers the ability to objectively compare spreadsheet modeling methods to select which best meets their goals."}
{"id": "1802.00496", "title": "Edu-Edition Spreadsheet Competency Framework", "url": "https://arxiv.org/abs/1802.00496", "pdf": "https://arxiv.org/pdf/1802.00496", "abs": "https://arxiv.org/abs/1802.00496", "authors": ["Maria Csernoch", "Piroska Biró"], "categories": ["cs.CY"], "comment": null, "summary": "Based on the Spreadsheet Competency Framework for finance professionals, in the present paper we introduce the Edu-Edition of the Spreadsheet Competency Framework (E2SCF). We claim that building spreadsheet competences should start in education, as early as possible, and this process is a lot more effective if support arrives from expert teachers. The main feature of E2SCF is high mathability computer-supported real world problem solving. This approach is based on - from the very beginning of training - a two-directional knowledge transfer, data and error analysis and handling, and the programming aspect of spreadsheets. Based on these features, E2SCF is set up for basic and general users to build up firm spreadsheet knowledge and to develop transferable problem solving skills and competences."}
{"id": "1802.00484", "title": "Alternative Spreadsheet Model Designs for an Operations Management Model Embedded in a Periodic Business Process", "url": "https://arxiv.org/abs/1802.00484", "pdf": "https://arxiv.org/pdf/1802.00484", "abs": "https://arxiv.org/abs/1802.00484", "authors": ["Thomas A. Grossman", "Vijay Mehrotra", "Mouwafac Sidaoui"], "categories": ["cs.SE"], "comment": "12 Pages, 10 Colour Figures", "summary": "We present a widely-used operations management model used in supply and distribution planning, that is typically embedded in a periodic business process that necessitates model modification and reuse. We consider three alternative spreadsheet implementations, a data-driven design, a canonical (textbook) design, and a novel (table-driven) technical design. We evaluate each regarding suitability for accuracy, modification, analysis, and transfer. We consider the degree of training and technical sophistication required to utilize each design. The data-driven design provides insight into poor spreadsheet practices by naïve modelers. The technical design can be modified for new data and new structural elements without manual writing or editing of cell formulas, thus speeding modification and reducing risk of error. The technical design has potential for use with other classes of models. We identify opportunities for future research."}
{"id": "1801.03829", "title": "Characterizing Scalability Issues in Spreadsheet Software using Online Forums", "url": "https://arxiv.org/abs/1801.03829", "pdf": "https://arxiv.org/pdf/1801.03829", "abs": "https://arxiv.org/abs/1801.03829", "authors": ["Kelly Mack", "John Lee", "Kevin Chang", "Karrie Karahalios", "Aditya Parameswaran"], "categories": ["cs.HC"], "comment": null, "summary": "In traditional usability studies, researchers talk to users of tools to understand their needs and challenges. Insights gained via such interviews offer context, detail, and background. Due to costs in time and money, we are beginning to see a new form of tool interrogation that prioritizes scale, cost, and breadth by utilizing existing data from online forums. In this case study, we set out to apply this method of using online forum data to a specific issue---challenges that users face with Excel spreadsheets. Spreadsheets are a versatile and powerful processing tool if used properly. However, with versatility and power come errors, from both users and the software, which make using spreadsheets less effective. By scraping posts from the website Reddit, we collected a dataset of questions and complaints about Excel. Specifically, we explored and characterized the issues users were facing with spreadsheet software in general, and in particular, as resulting from a large amount of data in their spreadsheets. We discuss the implications of our findings on the design of next-generation spreadsheet software."}
{"id": "1801.10249", "title": "The Reification of an Incorrect and Inappropriate Spreadsheet Model", "url": "https://arxiv.org/abs/1801.10249", "pdf": "https://arxiv.org/pdf/1801.10249", "abs": "https://arxiv.org/abs/1801.10249", "authors": ["Grenville J. Croll"], "categories": ["cs.HC"], "comment": "14 Pages, 4 Colour Figures, 2 Tables", "summary": "Once information is loaded into a spreadsheet, it acquires properties that it may not deserve. These properties include believability, correctness, appropriateness, concreteness, integrity, tangibility, objectivity and authority. The information becomes reified. We describe a case study through which we were able to observe at close hand the reification of a demonstrably incorrect and inappropriate spreadsheet model within a small non profit organisation."}
{"id": "1801.10231", "title": "The Future of Spreadsheets in the Big Data Era", "url": "https://arxiv.org/abs/1801.10231", "pdf": "https://arxiv.org/pdf/1801.10231", "abs": "https://arxiv.org/abs/1801.10231", "authors": ["David Birch", "David Lyford-Smith", "Yike Guo"], "categories": ["cs.CY"], "comment": "13 Pages, 1 Table", "summary": "The humble spreadsheet is the most widely used data storage, manipulation and modelling tool. Its ubiquity over the past 30 years has seen its successful application in every area of life. Surprisingly the spreadsheet has remained fundamentally unchanged over the past three decades. As spreadsheet technology enters its 4th decade a number of drivers of change are beginning to impact upon the spreadsheet. The rise of Big Data, increased end-user computing and mobile computing will undoubtedly increasingly shape the evolution and use of spreadsheet technology.\n  To explore the future of spreadsheet technology a workshop was convened with the aim of \"bringing together academia and industry to examine the future direction of spreadsheet technology and the consequences for users\". This paper records the views of the participants on the reasons for the success of the spreadsheet, the trends driving change and the likely directions of change for the spreadsheet. We then set out key directions for further research in the evolution and use of spreadsheets. Finally we look at the implications of these trends for the end users who after all are the reason for the remarkable success of the spreadsheet."}
{"id": "1801.09771", "title": "Mitigating Spreadsheet Model Risk with Python Open Source Infrastructure", "url": "https://arxiv.org/abs/1801.09771", "pdf": "https://arxiv.org/pdf/1801.09771", "abs": "https://arxiv.org/abs/1801.09771", "authors": ["Oliver Beavers"], "categories": ["cs.SE"], "comment": "14 Pages, 15 Colour Diagrams", "summary": "Across an aggregation of EuSpRIG presentation papers, two maxims hold true: spreadsheets models are akin to software, yet spreadsheet developers are not software engineers. As such, the lack of traditional software engineering tools and protocols invites a higher rate of error in the end result. This paper lays ground work for spreadsheet modelling professionals to develop reproducible audit tools using freely available, open source packages built with the Python programming language, enabling stakeholders to develop clearly defined model \"oracles\" with which to test and audit spreadsheet calculations against."}
