<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [RelServe: Fast LLM Inference Serving on Relational Data](https://arxiv.org/abs/2601.11546)
*Xin Zhang,Shihong Gao,Yanyan Shen,Haoyang Li,Lei Chen*

Main category: cs.DB

TL;DR: RelServe是一个为低延迟relQuery服务的优化LLM引擎，通过动态优先级更新和自适应批次安排解决了HoL阻塞问题，并将平均服务延迟降低了3.1倍。


<details>
  <summary>Details</summary>
Motivation: 随着relQuery服务在AI驱动的电子表格等应用中的广泛采用，在并发查询负载下实现快速响应时间变得越来越重要。然而，当前的LLM引擎面临着来自Head-of-Line（HoL）阻塞的严重延迟瓶颈。

Method: RelServe引擎包含两个核心创新：1.动态优先级更新器（Dynamic Priority Updater），通过统计近似来持续调整优先级并最小化开销。2.自适应批次安排器（Adaptive Batch Arranger），定量评估候选的预填和解码批次，以最小化预测平均延迟。

Result: 在四个真实世界的数据集上，使用13B到70B参数范围内的LLM进行的大量实验表明，与vLLM相比，RelServe将平均服务延迟降低了高达3.1倍。

Conclusion: RelServe通过解决HoL阻塞问题，显著降低了relQuery服务的平均延迟，相比vLLM取得了显著的性能提升。

Abstract: The use of Large Language Models (LLMs) for querying relational data has given rise to relQuery, a workload pattern that applies templated LLM calls to structured tables. As relQuery services become more widely adopted in applications such as AI-powered spreadsheets, fast response times under concurrent query loads are increasingly important. Unfortunately, current LLM engines face severe latency bottlenecks from Head-of-Line (HoL) blocking across three comparable inference phases: waiting, core running, and tail running. Existing static priority scheduling methods only address HoL blocking during the waiting phase, leaving two critical problems unsolved. First, the absence of a priority update mechanism causes inaccurate prioritization and continued HoL blocking during core execution. Second, suboptimal prefill-decode batching exacerbates HoL blocking in tail execution and worsens latency trade-offs between running and waiting relQueries. To address these problems, we propose RelServe, an optimized LLM engine for low-latency relQuery serving. RelServe features two core innovations: a Dynamic Priority Updater that continuously adjusts priorities while minimizing overhead via statistical approximations, and an Adaptive Batch Arranger that quantitatively evaluates candidate prefill and decode batches to minimize projected average latency. Extensive experiments on four real-world datasets using LLMs ranging from 13B to 70B parameters show that RelServe reduces average serving latency by up to 3.1x compared to vLLM.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [2] [The Invisible Mentor: Inferring User Actions from Screen Recordings to Recommend Better Workflows](https://arxiv.org/abs/2509.26557)
*Litao Yan,Andrew Head,Ken Milne,Vu Le,Sumit Gulwani,Chris Parnin,Emerson Murphy-Hill*

Main category: cs.HC

TL;DR: InvisibleMentor是一个通过屏幕录像识别低效工作流并提供改进建议的AI助手，无需用户手动描述问题。


<details>
  <summary>Details</summary>
Motivation: 现有AI助手需要在用户描述问题后才能提供帮助，这可能很费力且不精确。用户在功能丰富的工具（如Excel）中常常忽视更高效的工作流程。

Method: InvisibleMentor通过一个两阶段流程工作：首先，一个视觉-语言模型重建用户操作和上下文；然后，一个语言模型生成结构化、高保真度的建议。该系统直接在屏幕录像上操作，而不是依赖日志、API或用户提示。

Result: InvisibleMentor能够准确识别低效工作流。与基于提示的电子表格助手相比，用户认为其建议更具可操作性、更具针对性，并更有助于学习和改进。

Conclusion: InvisibleMentor可以通过屏幕录像有效识别用户在软件使用中的低效行为，并提供切实可行的改进建议，从而帮助用户提高效率和学习能力。

Abstract: Many users struggle to notice when a more efficient workflow exists in feature-rich tools like Excel. Existing AI assistants offer help only after users describe their goals or problems, which can be effortful and imprecise. We present InvisibleMentor, a system that turns screen recordings of task completion into vision-grounded reflections on tasks. It detects issues such as repetitive edits and recommends more efficient alternatives based on observed behavior. Unlike prior systems that rely on logs, APIs, or user prompts, InvisibleMentor operates directly on screen recordings. It uses a two-stage pipeline: a vision-language model reconstructs actions and context, and a language model generates structured, high-fidelity suggestions. In evaluation, InvisibleMentor accurately identified inefficient workflows, and participants found its suggestions more actionable, tailored, and more helpful for learning and improvement compared to a prompt-based spreadsheet assistant.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [AI Playing Business Games: Benchmarking Large Language Models on Managerial Decision-Making in Dynamic Simulations](https://arxiv.org/abs/2509.26331)
*Berdymyrat Ovezmyradov*

Main category: cs.AI

TL;DR: 该研究提出了一个用于评估大型语言模型（LLM）在长期战略业务决策方面能力的新基准，并使用一个包含零售公司模拟的游戏来评估五个领先的LLM。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在自然语言处理方面表现出色，但它们在需要多步骤、战略性业务决策方面的能力尚未得到充分探索，目前缺乏用于评估这种长期连贯性的基准。

Method: 研究人员开发了一个可复制、开放访问的管理模拟器，LLM 在其中扮演一家模拟零售公司的决策者。该模拟器以动态、月复一月的方式进行，LLM 在每个月都会收到前一期的业务报告，并被要求在定价、订购量、营销预算、招聘、解雇、贷款、培训费用、研发费用、销售预测和收入预测等方面做出关键战略决策。

Result: 该研究使用利润、收入和市场份额等量化指标，以及其他关键绩效指标（KPI），来评估五个领先的LLM（Gemini、ChatGPT、Meta AI、Mistral AI 和 Grok）的表现。此外，还分析了LLM在战略连贯性、对市场变化的适应性以及其决策的合理性。

Conclusion: 这项研究为评估LLM的长期决策能力提供了一个新的框架，超越了简单的绩效指标，并通过提供一个开放访问的模拟器，为研究界对LLM进行基准测试做出了贡献。

Abstract: The rapid advancement of LLMs sparked significant interest in their potential to augment or automate managerial functions. One of the most recent trends in AI benchmarking is performance of Large Language Models (LLMs) over longer time horizons. While LLMs excel at tasks involving natural language and pattern recognition, their capabilities in multi-step, strategic business decision-making remain largely unexplored. Few studies demonstrated how results can be different from benchmarks in short-term tasks, as Vending-Bench revealed. Meanwhile, there is a shortage of alternative benchmarks for long-term coherence. This research analyses a novel benchmark using a business game for the decision making in business. The research contributes to the recent literature on AI by proposing a reproducible, open-access management simulator to the research community for LLM benchmarking. This novel framework is used for evaluating the performance of five leading LLMs available in free online interface: Gemini, ChatGPT, Meta AI, Mistral AI, and Grok. LLM makes decisions for a simulated retail company. A dynamic, month-by-month management simulation provides transparently in spreadsheet model as experimental environment. In each of twelve months, the LLMs are provided with a structured prompt containing a full business report from the previous period and are tasked with making key strategic decisions: pricing, order size, marketing budget, hiring, dismissal, loans, training expense, R&D expense, sales forecast, income forecast The methodology is designed to compare the LLMs on quantitative metrics: profit, revenue, and market share, and other KPIs. LLM decisions are analyzed in their strategic coherence, adaptability to market changes, and the rationale provided for their decisions. This approach allows to move beyond simple performance metrics for assessment of the long-term decision-making.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [4] [BookReconciler: An Open-Source Tool for Metadata Enrichment and Work-Level Clustering](https://arxiv.org/abs/2512.10165)
*Matt Miller,Dan Sinykin,Melanie Walsh*

Main category: cs.DL

TL;DR: BookReconciler是一个开源工具，用于增强和聚类图书数据，通过添加ISBN等标识符并聚类同一作品的不同版本（如翻译或版本），从而方便大规模分析图书。该工具作为OpenRefine的扩展，连接了多个主要书目服务，并优先考虑用户判断，允许用户手动评估匹配并定义作品的范围。在对美国获奖图书和当代世界小说的数据集进行评估后，BookReconciler在处理美国图书时准确率接近完美，但在处理全球文本时准确率较低，这反映了非英语和全球文学书目基础设施的结构性弱点。总的来说，BookReconciler支持跨领域和应用程序的书目数据重用。


<details>
  <summary>Details</summary>
Motivation: 该工具旨在解决图书数据分散、缺乏权威标识符以及同一作品不同版本（如翻译或版本）难以关联的问题，从而方便大规模分析图书。

Method: BookReconciler作为OpenRefine的扩展，连接了美国国会图书馆、VIAF、OCLC、HathiTrust、Google Books和Wikidata等主要书目服务。它能够自动为仅包含书名和作者等少量元数据的电子表格添加ISBN等权威、持久的标识符，并聚类同一作品的不同表达和体现（如不同翻译或版本）。该工具还提供了一个交互式界面，允许用户手动评估匹配并定义作品的范围（例如，是否包含翻译）。

Result: BookReconciler在处理美国图书数据集时，准确率接近完美；但在处理全球文本数据集时，准确率较低，这揭示了非英语和全球文学在书目基础设施方面存在的结构性弱点。

Conclusion: BookReconciler通过增强和聚类图书数据，支持跨领域和应用程序的书目数据重用，为数字图书馆和数字人文领域的研究做出了贡献。尽管在处理全球文学数据时存在挑战，但该工具在处理美国图书数据方面表现出色。

Abstract: We present BookReconciler, an open-source tool for enhancing and clustering book data. BookReconciler allows users to take spreadsheets with minimal metadata, such as book title and author, and automatically 1) add authoritative, persistent identifiers like ISBNs 2) and cluster related Expressions and Manifestations of the same Work, e.g., different translations or editions. This enhancement makes it easier to combine related collections and analyze books at scale. The tool is currently designed as an extension for OpenRefine -- a popular software application -- and connects to major bibliographic services including the Library of Congress, VIAF, OCLC, HathiTrust, Google Books, and Wikidata. Our approach prioritizes human judgment. Through an interactive interface, users can manually evaluate matches and define the contours of a Work (e.g., to include translations or not). We evaluate reconciliation performance on datasets of U.S. prize-winning books and contemporary world fiction. BookReconciler achieves near-perfect accuracy for U.S. works but lower performance for global texts, reflecting structural weaknesses in bibliographic infrastructures for non-English and global literature. Overall, BookReconciler supports the reuse of bibliographic data across domains and applications, contributing to ongoing work in digital libraries and digital humanities.

</details>
