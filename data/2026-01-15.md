<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.HC](#cs.HC) [Total: 5]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Vision Language Models for Spreadsheet Understanding: Challenges and Opportunities](https://arxiv.org/abs/2405.16234)
*Shiyu Xia,Junyu Xiong,Haoyu Dong,Jianbo Zhao,Yuzhang Tian,Mengyu Zhou,Yeye He,Shi Han,Dongmei Zhang*

Main category: cs.CV

TL;DR: 本论文旨在评估视觉语言模型（VLM）在电子表格理解方面的能力，提出了一系列自监督挑战和评估指标，以测试其 OCR、空间感知和视觉格式识别能力，并结合表格检测任务评估整体性能。研究还提出了三种微调设置（列宽调整、样式更改、地址增强）和相应的提示变体，以更精细地探测 VLM。为了利用 VLM 擅长理解文本而非二维定位的优势，提出在表格边界检测中解码单元格边界的单元格值。实验结果表明，VLM 具有一定的 OCR 能力，但在单元格遗漏和对齐方面表现不佳，空间和格式识别能力不足，这表明未来的工作需要利用本文提出的方法和多种设置下的电子表格-图像对来增强 VLM 的电子表格数据理解能力。


<details>
  <summary>Details</summary>
Motivation: 评估视觉语言模型（VLM）在电子表格理解方面的能力，并为未来的 VLM 增强提供方向。

Method: 提出三个自监督挑战（OCR、空间感知、视觉格式识别）及其评估指标，并结合电子表格表格检测任务进行评估。此外，提出三种电子表格到图像的设置（列宽调整、样式更改、地址增强）和相应的提示变体，以更精细地探测 VLM。在表格边界检测中，提出解码表格四个边界的单元格值。

Result: VLM 在 OCR 方面表现出有希望的能力，但在单元格遗漏和对齐方面存在不足。VLM 的空间和格式识别能力明显不足。

Conclusion: VLM 在电子表格理解方面仍有提升空间，尤其是在空间和格式识别方面。未来的工作可以通过利用本文提出的方法和生成大量的电子表格-图像对来增强 VLM 的电子表格数据理解能力。

Abstract: This paper explores capabilities of Vision Language Models on spreadsheet comprehension. We propose three self-supervised challenges with corresponding evaluation metrics to comprehensively evaluate VLMs on Optical Character Recognition (OCR), spatial perception, and visual format recognition. Additionally, we utilize the spreadsheet table detection task to assess the overall performance of VLMs by integrating these challenges. To probe VLMs more finely, we propose three spreadsheet-to-image settings: column width adjustment, style change, and address augmentation. We propose variants of prompts to address the above tasks in different settings. Notably, to leverage the strengths of VLMs in understanding text rather than two-dimensional positioning, we propose to decode cell values on the four boundaries of the table in spreadsheet boundary detection. Our findings reveal that VLMs demonstrate promising OCR capabilities but produce unsatisfactory results due to cell omission and misalignment, and they notably exhibit insufficient spatial and format recognition skills, motivating future work to enhance VLMs' spreadsheet data comprehension capabilities using our methods to generate extensive spreadsheet-image pairs in various settings.

</details>


### [2] [High-Throughput Phenotyping using Computer Vision and Machine Learning](https://arxiv.org/abs/2407.06354)
*Vivaan Singhvi,Langalibalele Lunga,Pragya Nidhi,Chris Keum,Varrun Prakash*

Main category: cs.CV

TL;DR: 我们利用OCR、图像分割和机器学习技术，对一项包含1672张杨树图像的数据集进行了高通量表型分析，以识别植物性状并预测处理条件。


<details>
  <summary>Details</summary>
Motivation: 本次研究旨在通过结合光学字符识别（OCR）、图像分割和机器学习算法，改进高通量植物表型分析的效率和准确性，特别是处理包含物理标签的大型数据集。

Method: 本研究使用了一个包含1672张杨树图像的数据集，其中包含白色标签。我们应用光学字符识别（OCR）技术读取标签信息，利用图像分割和机器学习算法进行形态学分类，然后使用机器学习模型预测植物处理（对照或干旱）条件，并通过分析编码的EXIF标签来评估叶片大小和表型相关性。

Result: 我们的OCR模型在非空文本提取方面达到了94.31%的准确率。分类模型在识别叶片形状、颜色和褐斑程度方面平均准确率为62.82%，在预测植物处理条件方面准确率为60.08%。然而，EXIF标签中缺失关键信息，阻碍了对叶片大小和表型与条件之间相关性的评估。

Conclusion: 尽管在叶片大小和表型相关性评估方面存在局限性，但本研究成功地展示了OCR和机器学习在提高植物表型分析效率方面的潜力，并为未来研究提供了改进方向。

Abstract: High-throughput phenotyping refers to the non-destructive and efficient evaluation of plant phenotypes. In recent years, it has been coupled with machine learning in order to improve the process of phenotyping plants by increasing efficiency in handling large datasets and developing methods for the extraction of specific traits. Previous studies have developed methods to advance these challenges through the application of deep neural networks in tandem with automated cameras; however, the datasets being studied often excluded physical labels. In this study, we used a dataset provided by Oak Ridge National Laboratory with 1,672 images of Populus Trichocarpa with white labels displaying treatment (control or drought), block, row, position, and genotype. Optical character recognition (OCR) was used to read these labels on the plants, image segmentation techniques in conjunction with machine learning algorithms were used for morphological classifications, machine learning models were used to predict treatment based on those classifications, and analyzed encoded EXIF tags were used for the purpose of finding leaf size and correlations between phenotypes. We found that our OCR model had an accuracy of 94.31% for non-null text extractions, allowing for the information to be accurately placed in a spreadsheet. Our classification models identified leaf shape, color, and level of brown splotches with an average accuracy of 62.82%, and plant treatment with an accuracy of 60.08%. Finally, we identified a few crucial pieces of information absent from the EXIF tags that prevented the assessment of the leaf size. There was also missing information that prevented the assessment of correlations between phenotypes and conditions. However, future studies could improve upon this to allow for the assessment of these features.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [3] [General Table Question Answering via Answer-Formula Joint Generation](https://arxiv.org/abs/2503.12345)
*Zhongyuan Wang,Richong Zhang,Zhijie Nie,Hangyu Mao*

Main category: cs.CL

TL;DR: LLMs在表格问答（TableQA）中表现出色，但缺乏通用性。本文提出使用电子表格公式作为可执行表示，并构建了FormulaQA数据集，同时提出了TabAF框架，实现了在多种表格问答任务上的高泛化性和优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有TableQA方法主要依赖LLMs生成答案或代码，但缺乏处理特定问题类型或表格结构的多功能性。电子表格公式作为一种成熟的表格数据操作语言，尚未被充分探索用于TableQA。

Method: 构建了一个名为FormulaQA的大型、标注有公式的TableQA数据集，该数据集源自现有数据集。提出了一种名为TabAF的通用表格问答框架，该框架使用单一LLM骨干模型，能够同时解码答案和公式，以解决多种类型的表格问答任务。

Result: 在WikiTableQuestion、HiTab和TabFact等数据集上，TabAF取得了新的最先进性能，证明了其通用性和泛化能力。在模型规模相同的情况下，TabAF的性能优于现有方法。

Conclusion: TabAF框架通过利用电子表格公式作为可执行表示，成功解决了TableQA中的多功能性和泛化性问题，并在多个基准测试中取得了优异的性能，为TableQA领域的研究提供了新的方向。

Abstract: Advanced table question answering (TableQA) methods prompt large language models (LLMs) to generate answer text, SQL query, Python code, or custom operation, which impressively improve the complex reasoning problems in the TableQA task. However, these methods lack the versatility to cope with specific question types or table structures. In contrast, the Spreadsheet Formula, the widely used and well-defined operation language for tabular data, has not been thoroughly explored to solve TableQA. In this paper, we first attempt to use the Formula as the executable representation for solving complex reasoning on tables with different structures. Specifically, we construct \texttt{FromulaQA}, a large Formula-annotated TableQA dataset from existing datasets. In addition, we propose \texttt{TabAF}, a general table answering framework to solve multiple types of tasks over multiple types of tables simultaneously, which decodes answers and Formulas with a single LLM backbone. Extensive experiments demonstrate the versatility and generalization of \texttt{TabAF}. Under the same model size, \texttt{TabAF} achieves new state-of-the-art performance on the WikiTableQuestion, HiTab, and TabFact.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [4] [Knowledge engineering for open science: Building and deploying knowledge bases for metadata standards](https://arxiv.org/abs/2507.22391)
*Mark A. Musen,Martin J. O'Connor,Josef Hardi,Marcos Martinez-Romero*

Main category: cs.DL

TL;DR: 科学家们希望在开放存储库中提供数据集，但需要丰富的、特定于学科的、标准化的元数据。CEDAR（中心扩展数据注释和检索）开发了一种技术，通过元数据模板来捕获和应用这些元数据标准，以促进数据的可发现性、可访问性、互操作性和可重用性（FAIR）。


<details>
  <summary>Details</summary>
Motivation: 使数据集符合 FAIR 原则，特别是通过标准化元数据。

Method: 开发 CEDAR（中心扩展数据注释和检索）技术，使用元数据模板来捕获和应用科学界对元数据标准的要求。

Result: CEDAR 模板已被用于各种科学联盟的元数据标准化，并已成为数据注释系统的基础，支持 Web 表单和电子表格的元数据获取，并有助于确保元数据符合标准。

Conclusion: CEDAR 模板提供了一种机制，使科学界能够创建共享的元数据标准，为其应用编码偏好，并将这些标准部署到各种智能系统中，以促进开放科学。

Abstract: Scientists strive to make their datasets available in open repositories, with the goal that they be findable, accessible, interoperable, and reusable (FAIR). Although it is hard for most investigators to remember all the guiding principles associated with FAIR data, there is one overarching requirement: The data need to be annotated with rich, discipline-specific, standardized metadata. The Center for Expanded Data Annotation and Retrieval (CEDAR) builds technology that enables scientists to encode metadata standards as templates that enumerate the attributes of different kinds of experiments. These metadata templates capture preferences regarding how data should be described and what a third party needs to know to make sense of the datasets. CEDAR templates describing community metadata preferences have been used to standardize metadata for a variety of scientific consortia. They have been used as the basis for data-annotation systems that acquire metadata through Web forms or through spreadsheets, and they can help correct metadata to ensure adherence to standards. Like the declarative knowledge bases that underpinned intelligent systems decades ago, CEDAR templates capture the knowledge in symbolic form, and they allow that knowledge to be applied in a variety of settings. They provide a mechanism for scientific communities to create shared metadata standards and to encode their preferences for the application of those standards, and for deploying those standards in a range of intelligent systems to promote open science.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [5] [Excel: Automated Ledger or Analytics IDE?](https://arxiv.org/abs/2409.12976)
*Andrew Kumiega*

Main category: cs.CY

TL;DR: Excel已从简单的账簿自动化工具演变为集成了多种功能（数据库、OLAP引擎、统计编程语言、第三方库、动态图表、实时数据连接器）的分析IDE。尽管Excel的功能日益强大，但许多用户并未意识到其作为分析IDE的潜力。因此，需要扩展现有的电子表格风险框架，以应对将Excel用作分析IDE所带来的日益增长的风险。


<details>
  <summary>Details</summary>
Motivation: 随着Excel功能从简单的账簿自动化工具发展为集成的分析IDE，需要一个全面的风险框架来管理这种转变带来的日益增长的风险。

Method: 本研究提出扩展现有的电子表格风险框架，以适应将Excel用作分析IDE所带来的风险。

Result: Excel已发展成为一个功能强大的分析IDE，集成了数据库、OLAP引擎、统计编程语言、第三方库、动态图表和实时数据连接器。

Conclusion: 鉴于Excel已演变为一个分析IDE，有必要扩展现有的风险管理框架，以应对其使用所带来的日益增长的风险。

Abstract: Since the inception of VisiCalc over four decades ago, spreadsheets have undergone a gradual transformation, evolving from simple ledger automation tools to the current state of Excel, which can be described as an Integrated Development Environment (IDE) for analytics. The slow evolution of Excel from an automation tool for ledgers to an IDE for analytics explains why many people have not noticed that Excel includes a fully functional database, an OLAP Engine, multiple statistical programming languages, multiple third-party software libraries, dynamic charts, and real time data connectors. The simplicity of accessing these multiple tools is a low-code framework controlled from the Excel tool that is effectively an IDE. Once we acknowledge Excel's shift from a desk top application to an IDE for analytics, the importance of establishing a comprehensive risk framework for managing this distinctive development environment becomes clear. In this paper we will explain how the current risk framework for spreadsheets needs to be expanded to manage the growing risks of using Excel as an IDE for analytics.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [Oh That Looks Familiar: A Novel Similarity Measure for Spreadsheet Template Discovery](https://arxiv.org/abs/2511.06973)
*Anand Krishnakumar,Vengadesh Ravikumaran*

Main category: cs.LG

TL;DR: 提出了一种结合语义嵌入、数据类型和空间位置的混合距离度量方法，用于量化电子表格相似性，并在无监督聚类方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法无法捕捉定义模板的电子表格的空间布局和类型模式。

Method: 将电子表格转换为单元格级别的嵌入，并使用 Chamfer 和 Hausdorff 距离等聚合技术来计算电子表格相似性。

Result: 在 FUSTE 数据集上，该方法在无监督聚类方面表现优于基于图的 Mondrian 基线，实现了完美的模板重建（调整后的 Rand 指数达到 1.00，而基线为 0.90）。

Conclusion: 该方法能够进行大规模的自动化模板发现，并支持下游应用，如表格集合上的检索增强生成、模型训练和批量数据清理。

Abstract: Traditional methods for identifying structurally similar spreadsheets fail to capture the spatial layouts and type patterns defining templates. To quantify spreadsheet similarity, we introduce a hybrid distance metric that combines semantic embeddings, data type information, and spatial positioning. In order to calculate spreadsheet similarity, our method converts spreadsheets into cell-level embeddings and then uses aggregation techniques like Chamfer and Hausdorff distances. Experiments across template families demonstrate superior unsupervised clustering performance compared to the graph-based Mondrian baseline, achieving perfect template reconstruction (Adjusted Rand Index of 1.00 versus 0.90) on the FUSTE dataset. Our approach facilitates large-scale automated template discovery, which in turn enables downstream applications such as retrieval-augmented generation over tabular collections, model training, and bulk data cleaning.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [7] [Billion-files File Systems (BfFS): A Comparison](https://arxiv.org/abs/2408.01805)
*Sohail Shaikh*

Main category: cs.PF

TL;DR: 海量数据需本地处理，本文分析了EXT4、XFS、BtrFS、ZFS和F2FS等Linux文件系统的性能。


<details>
  <summary>Details</summary>
Motivation: 数据爆炸式增长，需要快速处理，因此数据需要靠近计算设备以减少传输延迟。

Method: 通过创建、存储和读取10亿个文件来分析EXT4、XFS、BtrFS、ZFS和F2FS这几个流行的Linux文件系统。研究还捕获和分析了读/写吞吐量、存储块使用情况、磁盘空间利用率和开销等指标，并探讨了大量文件和文件夹创建期间及之后文件系统性能下降等副作用。

Result: 分析了读/写吞吐量、存储块使用情况、磁盘空间利用率和开销等指标，并研究了文件系统性能下降的副作用。

Conclusion: 本地文件系统对于处理海量数据至关重要，需要深入理解其工作原理、性能和局限性。

Abstract: As the volume of data being produced is increasing at an exponential rate that needs to be processed quickly, it is reasonable that the data needs to be available very close to the compute devices to reduce transfer latency. Due to this need, local filesystems are getting close attention to understand their inner workings, performance, and more importantly their limitations. This study analyzes few popular Linux filesystems: EXT4, XFS, BtrFS, ZFS, and F2FS by creating, storing, and then reading back one billion files from the local filesystem. The study also captured and analyzed read/write throughput, storage blocks usage, disk space utilization and overheads, and other metrics useful for system designers and integrators. Furthermore, the study explored other side effects such as filesystem performance degradation during and after these large numbers of files and folders are created.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [8] [TableTalk: Scaffolding Spreadsheet Development with a Language Agent](https://arxiv.org/abs/2502.09787)
*Jenny T. Liang,Aayush Kumar,Yasharth Bajpai,Sumit Gulwani,Vu Le,Chris Parnin,Arjun Radhakrishna,Ashish Tiwari,Emerson Murphy-Hill,Guastavo Soares*

Main category: cs.SE

TL;DR: TableTalk是一个用于电子表格编程的语言代理，通过结构化计划、灵活的步骤建议和增量构建来支持用户，能生成更高质量的电子表格，减轻认知负荷，并为未来的电子表格工具提供设计指导。


<details>
  <summary>Details</summary>
Motivation: 电子表格编程具有挑战性，需要结合编程知识和问题解决技巧。现有的语言代理在电子表格创建方面显示出潜力，但需要更优化的方法。

Method: TableTalk代理基于对7位电子表格程序员和85个Excel模板的研究，遵循了脚手架、灵活性和增量性三大设计原则。它引导用户进行结构化规划，提供三个可能的下一步建议，并使用预定义工具增量地构建电子表格。

Result: 在20位程序员的研究中，TableTalk生成的电子表格质量更高，被选中的几率是基线的2.3倍。此外，它还将认知负荷和思考时间减少了12.6%。

Conclusion: TableTalk在电子表格编程方面取得了显著成效，表明了其在提高电子表格质量、降低用户认知负荷方面的潜力。研究结果为开发类代理的电子表格编程工具提供了设计指南，并对电子表格编程、最终用户编程、AI辅助编程以及人机协作等方面产生了启示。

Abstract: Spreadsheet programming is challenging. Programmers use spreadsheet programming knowledge (e.g., formulas) and problem-solving skills to combine actions into complex tasks. Advancements in large language models have introduced language agents that observe, plan, and perform tasks, showing promise for spreadsheet creation. We present TableTalk, a spreadsheet programming agent embodying three design principles -- scaffolding, flexibility, and incrementality -- derived from studies with seven spreadsheet programmers and 85 Excel templates. TableTalk guides programmers through structured plans based on professional workflows, generating three potential next steps to adapt plans to programmer needs. It uses pre-defined tools to generate spreadsheet components and incrementally build spreadsheets. In a study with 20 programmers, TableTalk produced higher-quality spreadsheets 2.3 times more likely to be preferred than the baseline. It reduced cognitive load and thinking time by 12.6%. From this, we derive design guidelines for agentic spreadsheet programming tools and discuss implications on spreadsheet programming, end-user programming, AI-assisted programming, and human-agent collaboration.

</details>


### [9] [MaxMind: A Memory Loop Network to Enhance Software Productivity based on Large Language Models](https://arxiv.org/abs/2408.03841)
*Yuchen Dong,XiaoXiang Fang,Yuchen Hu,Renshuang Jiang,Zhe Jiang*

Main category: cs.SE

TL;DR: 该论文提出了MaxMind模型，通过结合记忆循环网络和知识精度分段增强的RAG机制，来解决大型语言模型在软件操作和工具生成（SOTG）中的记忆和知识利用问题，并展示了MaxMind4Sheet在提高任务成功率和效率方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前研究在将实时任务经验转化为系统记忆以及区分现有知识价值方面存在不足，限制了大型语言模型在软件操作和工具生成（SOTG）方面的应用和生产力提升。

Method: 提出了记忆循环网络（Memory-Loop Networks）用于及时记忆和经验引用，并增强了具有知识精度分段的检索增强生成（RAG）机制，以根据价值区分来利用记忆。在此基础上，设计了MaxMind模型用于SOTG，并开发了MaxMind4Sheet系统进行实验。

Result: 与SheetCopilot的实验比较表明，任务记忆的积累和回收可将任务成功率提高约3%-6%，并可能随着记忆的增长而产生显著的累积效应。记忆回收还能将任务执行效率提高高达25%，并通过记忆转移解决了大型语言模型在处理专业任务时面临的再训练问题。

Conclusion: MaxMind模型在SOTG领域具有显著潜力，能够提升大型语言模型系统的能力和生产力。

Abstract: The application of large language models to facilitate automated software operations and tool generation (SOTG), thus augmenting software productivity, mirrors the early stages of human evolution when the ability to create and use tools accelerated the progress of civilization. These complex tasks require AI to continuously summarize and improve. Current research often overlooks the importance of converting real-time task experiences into system memory and differentiating the value of existing knowledge for future reference. This paper addresses these issues by evolving external memory models into Memory-Loop Networks for timely memorization and experience referencing. We also enhance a RAG mechanism with knowledge precision segmentation to utilize memory based on value differentiation, and design the MaxMind model for SOTG accordingly.To demonstrate our approach, we developed MaxMind4Sheet, an electronic spreadsheet processing system aligned with the MaxMind philosophy. Comparative experiments with SheetCopilot have demonstrated that the accumulation and recycling of task memories lead to a steady enhancement in task success rate, with an improvement rate of approximately 3%-6% per round in this implementation example. Note that as the memories continue to grow, this cumulative improvement may be substantial. The inclusion of memory recycling can also boost the system's task execution efficiency by up to 25%, and it can address the retraining issue faced by LLMs when handling specialized tasks through memories transfer.These suggest that MaxMind has significant potential to enhance the capabilities and productivity of LLM systems in SOTG.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [10] [Consensus-Free Spreadsheet Integration](https://arxiv.org/abs/2209.14457)
*Brandon Baylor,Eric Daimler,James Hansen,Esteban Montero,Ryan Wisnesky*

Main category: cs.DB

TL;DR: 通过使用范畴论中的代数（方程）理论和模型，将多个电子表格合并为一个


<details>
  <summary>Details</summary>
Motivation: 寻找一种在不要求模型作者达成共识的情况下合并工程模型的方法

Method: 将每个表格的公式表示为代数（方程）理论，将每个表格的值表示为其理论的模型，将表格之间的重叠表示为理论和模型态射，然后执行范畴论中的余极限、提升和 Kan 扩张构造来计算一个规范的通用集成理论和模型

Result: 对一家主要能源公司的一个实际油气计算案例研究进行了描述，包括整合两个由不相关的工程师构建的不同套管压力测试（MASP）计算电子表格时产生的理论和模型。还描述了与验证重叠映射的语义保持以及验证结果集成表格的保守性/一致性相关的自动定理证明负担。

Conclusion: 思考如何将该方法论应用于扩大企业范围的工程工作。

Abstract: We describe a method for merging multiple spreadsheets into one sheet, and/or exchanging data among the sheets, by expressing each sheet's formulae as an algebraic (equational) theory and each sheet's values as a model of its theory, expressing the overlap between the sheets as theory and model morphisms, and then performing colimit, lifting, and Kan-extension constructions from category theory to compute a canonically universal integrated theory and model, which can then be expressed as a spreadsheet. Our motivation is to find methods of merging engineering models that do not require consensus (agreement) among the authors of the models being merged, a condition fulfilled by our method because theory and model morphisms are semantics-preserving. We describe a case study of this methodology on a real-world oil and gas calculation at a major energy company, describing the theories and models that arise when integrating two different casing pressure test (MASP) calculation spreadsheets constructed by two non-interacting engineers. We also describe the automated theorem proving burden associated with both verifying the semantics preservation of the overlap mappings as well as verifying the conservativity/consistency of the resulting integrated sheet. We conclude with thoughts on how to apply the methodology to scale engineering efforts across the enterprise.

</details>


### [11] [A System and Benchmark for LLM-based Q&A on Heterogeneous Data](https://arxiv.org/abs/2409.05735)
*Achille Fokoue,Srideepika Jayaraman,Elham Khabiri,Jeffrey O. Kephart,Yingjie Li,Dhruv Shah,Youssef Drissi,Fenno F. Heath,Anu Bhamidipaty,Fateh A. Tipu,Robert J. Baseman*

Main category: cs.DB

TL;DR: siwarex平台实现了对数据库和API的无缝自然语言访问，解决了数据源异构性问题。


<details>
  <summary>Details</summary>
Motivation: 用户希望能够用自然语言查询结构化数据（如电子表格、数据库、API等），但通常不知道如何访问和组合这些数据源，尤其是在数据源异构且分散的情况下。

Method: 提出siwarex平台，支持对数据库和API进行自然语言访问，并通过扩展Spider数据集和基准测试来验证其有效性，用API替换部分表格。

Result: siwarex平台能够很好地处理数据源异构性问题。

Conclusion: siwarex平台有效解决了工业环境中自然语言查询结构化数据的异构性挑战。

Abstract: In many industrial settings, users wish to ask questions whose answers may be found in structured data sources such as a spreadsheets, databases, APIs, or combinations thereof. Often, the user doesn't know how to identify or access the right data source. This problem is compounded even further if multiple (and potentially siloed) data sources must be assembled to derive the answer. Recently, various Text-to-SQL applications that leverage Large Language Models (LLMs) have addressed some of these problems by enabling users to ask questions in natural language. However, these applications remain impractical in realistic industrial settings because they fail to cope with the data source heterogeneity that typifies such environments. In this paper, we address heterogeneity by introducing the siwarex platform, which enables seamless natural language access to both databases and APIs. To demonstrate the effectiveness of siwarex, we extend the popular Spider dataset and benchmark by replacing some of its tables by data retrieval APIs. We find that siwarex does a good job of coping with data source heterogeneity. Our modified Spider benchmark will soon be available to the research community

</details>


### [12] [Auditable and reusable crosswalks for fast, scaled integration of scattered tabular data](https://arxiv.org/abs/2409.01517)
*Gavin Chait*

Main category: cs.DB

TL;DR: 生成一个“太长不看”的总结


<details>
  <summary>Details</summary>
Motivation: Curating well-structured and interoperable data.

Method: A schema-centric approach with discrete components for auditable restructuring of complex and scattered tabular data. Transformations are captured as high-level sequential scripts describing schema-to-schema mappings. The toolkit is available as a Python package and a 'no-code' visual web application.

Result: The toolkit enables data transformation and restructuring based on schema definitions, facilitating the integration of scattered data into a single database, as demonstrated by a longitudinal study involving hundreds of local councils.

Conclusion: The presented curatorial toolkit provides a flexible and accessible solution for data curation, promoting data interoperability and simplifying the restructuring of complex datasets.

Abstract: This paper presents an open-source curatorial toolkit intended to produce well-structured and interoperable data. Curation is divided into discrete components, with a schema-centric focus for auditable restructuring of complex and scattered tabular data to conform to a destination schema. Task separation allows development of software and analysis without source data being present. Transformations are captured as high-level sequential scripts describing schema-to-schema mappings, reducing complexity and resource requirements. Ultimately, data are transformed, but the objective is that any data meeting a schema definition can be restructured using a crosswalk. The toolkit is available both as a Python package, and as a 'no-code' visual web application. A visual example is presented, derived from a longitudinal study where scattered source data from hundreds of local councils are integrated into a single database.

</details>


### [13] [Auto-Formula: Recommend Formulas in Spreadsheets using Contrastive Learning for Table Representations](https://arxiv.org/abs/2404.12608)
*Sibei Chen,Yeye He,Weiwei Cui,Ju Fan,Song Ge,Haidong Zhang,Dongmei Zhang,Surajit Chaudhuri*

Main category: cs.DB

TL;DR: 该研究提出了一种名为Auto-Formula的系统，利用相似电子表格中已有的公式，通过对比学习技术来预测用户想要在目标单元格中输入的公式，以解决非技术用户编写复杂电子表格公式的难题。


<details>
  <summary>Details</summary>
Motivation: 电子表格是广泛使用的端到端编程工具，但非技术用户在编写复杂公式时会遇到困难。

Method: 利用组织内相似电子表格中存在的计算逻辑，通过对比学习技术（借鉴计算机视觉中的相似人脸识别）来学习和适应现有公式，从而预测用户想要输入的公式。

Result: 在从真实企业电子表格中提取的2000多个测试公式上进行的广泛评估表明，Auto-Formula的效果优于其他方法。

Conclusion: Auto-Formula系统通过学习相似电子表格中的现有公式，能够有效地帮助用户自动生成复杂电子表格公式，解决了非技术用户在公式输入方面的痛点。

Abstract: Spreadsheets are widely recognized as the most popular end-user programming tools, which blend the power of formula-based computation, with an intuitive table-based interface. Today, spreadsheets are used by billions of users to manipulate tables, most of whom are neither database experts nor professional programmers.
  Despite the success of spreadsheets, authoring complex formulas remains challenging, as non-technical users need to look up and understand non-trivial formula syntax. To address this pain point, we leverage the observation that there is often an abundance of similar-looking spreadsheets in the same organization, which not only have similar data, but also share similar computation logic encoded as formulas. We develop an Auto-Formula system that can accurately predict formulas that users want to author in a target spreadsheet cell, by learning and adapting formulas that already exist in similar spreadsheets, using contrastive-learning techniques inspired by "similar-face recognition" from compute vision.
  Extensive evaluations on over 2K test formulas extracted from real enterprise spreadsheets show the effectiveness of Auto-Formula over alternatives. Our benchmark data is available at https://github.com/microsoft/Auto-Formula to facilitate future research.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [14] [The Paradox of Spreadsheet Self-Efficacy: Social Incentives for Informal Knowledge Sharing in End-User Programming](https://arxiv.org/abs/2408.08068)
*Qing,Xia,Advait Sarkar,Duncan P. Brumby,Anna Cox*

Main category: cs.HC

TL;DR: 个人（自我效能感）、社会（声誉收益、同事间的信任）和软件相关（编纂工作量）变量影响电子表格知识共享意愿。


<details>
  <summary>Details</summary>
Motivation: 了解个人、社会和软件相关变量如何影响电子表格知识共享意愿。

Method: 对来自行政和财务部门的电子表格用户（n=100）进行了调查，并进行了多元回归分析。

Result: 高水平的电子表格自我效能感和认为共享会带来声誉收益的看法可以预测更高的知识共享意愿，但那些认为知识编纂工作量大的个体则表现出较低的知识共享意愿。此外，无论职业如何，用户倾向于报告他们普遍的电子表格熟练程度较低，尽管他们在工作相关的背景下使用电子表格时表现出很高的自我效能感。

Conclusion: 承认并设计这些社会和个人变量可以帮助避免有经验的个人不必要地回避共享，这对电子表格设计有启示。

Abstract: Informal Knowledge Sharing (KS) is vital for end-user programmers to gain expertise. To better understand how personal (self-efficacy), social (reputational gains, trust between colleagues), and software-related (codification effort) variables influence spreadsheet KS intention, we conducted a multiple regressions analysis based on survey data from spreadsheet users (n=100) in administrative and finance roles. We found that high levels of spreadsheet self-efficacy and a perception that sharing would result in reputational gains predicted higher KS intention, but individuals who found knowledge codification effortful showed lower KS intention. We also observed that regardless of occupation, users tended to report a lower sense of self-efficacy in their general spreadsheet proficiency, despite also reporting high self-efficacy in spreadsheet use for job-related contexts. Our findings suggest that acknowledging and designing for these social and personal variables can help avoid situations where experienced individuals refrain unnecessarily from sharing, with implications for spreadsheet design.

</details>


### [15] [Subject integration with spreadsheets -- Ignoring education is the greatest risk ever](https://arxiv.org/abs/2409.12975)
*Mária Csernoch,Ádám Gulácsi,Júlia Csernoch*

Main category: cs.HC

TL;DR: Within the framework of Technological Pedagogical and Content Knowledge, subject integration with digital support is a solution for digitalization in schools, contextualizing informatics classes and bridging the gap between 'serious informatics' and 'digital literacy'.


<details>
  <summary>Details</summary>
Motivation: To explore how subject integration, specifically using spreadsheets for traditional Grade 3 tasks, can facilitate meaningful digitalization in schools and develop essential skills in teachers and students.

Method: Detailing the solutions for three traditional Grade 3 tasks within a spreadsheet environment, and analyzing the skills, competencies, and computer science knowledge that can be developed in both teachers and students through this process.

Result: The paper demonstrates that traditional Grade 3 tasks can be solved using spreadsheets, leading to the development of various skills and knowledge for teachers and students. It also highlights the importance of the analytical and planning phases in the learning process.

Conclusion: Subject integration through spreadsheets is a viable method for digitalization in schools, enhancing both technical and analytical skills crucial for students' future careers. The process of analyzing, understanding, planning, and discussing tasks is as vital as the hands-on spreadsheet activity.

Abstract: Within the framework of Technological Pedagogical and Content Knowledge, subject integration is one possible solution for the introduction of meaningful digitalization and digitization in schools. This process incorporates that any school subject can be taught with digital support, informatics (computer) classes can be contextualized, and the gap between 'serious informatics' and 'digital literacy' can be minimized. The present paper details how three traditional Grade 3 tasks can be solved in spreadsheets, what skills, competencies, and computer science knowledge of both teachers and students can be developed. The solutions also reveal that analysing, understanding, planning, and discussing tasks is as important as the activity in the spreadsheets, which process plays a crucial role in the preparation of students for their future jobs.

</details>


### [16] [Data Guards: Challenges and Solutions for Fostering Trust in Data](https://arxiv.org/abs/2407.14042)
*Nicole Sultanum,Dennis Bromley,Michael Correll*

Main category: cs.HC

TL;DR: 数据驱动决策面临数据质量和故意欺骗的威胁，需要建立信任。本研究通过访谈数据生产者和消费者，探讨了建立数据信任的策略和障碍，发现数据验证和核查的需求迫切但缺乏标准。因此，提出了一套“数据卫士”方法和工具来增强数据可信度。


<details>
  <summary>Details</summary>
Motivation: 数据驱动决策的有效性受到数据质量和故意欺骗等威胁，因此在使用数据（尤其是新数据）时需要信任和验证。

Method: 通过访谈数据生产者和消费者，了解他们建立数据信任的策略和遇到的障碍。

Result: 访谈显示，数据消费者尤其缺乏数据验证和核查的标准，但对此有迫切需求。

Conclusion: 提出了一套“数据卫士”方法和工具，旨在增强数据可信度，以应对数据验证和核查方面的挑战。

Abstract: From dirty data to intentional deception, there are many threats to the validity of data-driven decisions. Making use of data, especially new or unfamiliar data, therefore requires a degree of trust or verification. How is this trust established? In this paper, we present the results of a series of interviews with both producers and consumers of data artifacts (outputs of data ecosystems like spreadsheets, charts, and dashboards) aimed at understanding strategies and obstacles to building trust in data. We find a recurring need, but lack of existing standards, for data validation and verification, especially among data consumers. We therefore propose a set of data guards: methods and tools for fostering trust in data artifacts.

</details>


### [17] [Smart Portable Computer](https://arxiv.org/abs/2405.05292)
*Niladri Das*

Main category: cs.HC

TL;DR: 后疫情时代，个人电脑价格昂贵且配置不足，便携式智能计算机应运而生，它在提供桌面级体验的同时，兼具便携性、能效和成本效益，并支持多种编程语言和编译器。


<details>
  <summary>Details</summary>
Motivation: 解决疫情期间个人电脑价格高、配置不足以及传统笔记本电脑使用不便的问题。

Method: 设计并实现了一种便携式智能计算机，该计算机在外观、能耗和成本方面具有优势，同时提供与台式机相当的性能。

Result: 该便携式智能计算机提供了与台式机相当的性能，能够处理文档编辑、多标签浏览、电子表格管理和演示文稿创建等任务，并支持Python、C、C++等编程语言以及Keil和Xilinx等编译器。

Conclusion: 便携式智能计算机为用户提供了一种经济高效且功能强大的计算解决方案，满足了从日常办公到编程的各种需求。

Abstract: Amidst the COVID-19 pandemic, with many organizations, schools, colleges, and universities transitioning to virtual platforms, students encountered difficulties in acquiring PCs such as desktops or laptops. The starting prices, around 15,000 INR, often failed to offer adequate system specifications, posing a challenge for consumers. Additionally, those reliant on laptops for work found the conventional approach cumbersome. Enter the "Portable Smart Computer," a leap into the future of computing. This innovative device boasts speed and performance comparable to traditional desktops but in a compact, energy-efficient, and cost-effective package. It delivers a seamless desktop experience, whether one is editing documents, browsing multiple tabs, managing spreadsheets, or creating presentations. Moreover, it supports programming languages like Python, C, C++, as well as compilers such as Keil and Xilinx, catering to the needs of programmers.

</details>


### [18] ["My toxic trait is thinking I'll remember this": gaps in the learner experience of video tutorials for feature-rich software](https://arxiv.org/abs/2404.07114)
*Ian Drosos,Advait Sarkar,Andrew D. Gordon*

Main category: cs.HC

TL;DR: Learners face 'gaps' (barriers) when using video tutorials for complex software like Excel. This paper categorizes these gaps, analyzes viewer comments and creator interviews, and proposes design solutions to improve the learning experience.


<details>
  <summary>Details</summary>
Motivation: To identify and understand the barriers ('gaps') learners face when using video tutorials for feature-rich software, and to explore how creators address these gaps.

Method: Collected and analyzed 360 viewer comments from 90 Microsoft Excel video tutorials. Conducted contextual interviews with 8 influential tutorial creators. Presented creators with two design concepts to address identified gaps.

Result: Developed a theory and taxonomy of gaps encountered by video tutorial learners. Gained insights into creator challenges and addressed viewer feedback. Proposed design solutions for improving video tutorials.

Conclusion: Gaps in video tutorials hinder learning. Understanding these gaps through viewer comments and creator interviews can inform the design of better learning experiences. Further research and design iterations are needed to fully address these issues.

Abstract: Video tutorials are a popular medium for informal and formal learning. However, when learners attempt to view and follow along with these tutorials, they encounter what we call gaps, that is, issues that can prevent learning. We examine the gaps encountered by users of video tutorials for feature-rich software, such as spreadsheets. We develop a theory and taxonomy of such gaps, identifying how they act as barriers to learning, by collecting and analyzing 360 viewer comments from 90 Microsoft Excel video tutorials published by 43 creators across YouTube, TikTok, and Instagram. We conducted contextual interviews with 8 highly influential tutorial creators to investigate the gaps their viewers experience and how they address them. Further, we obtain insights into their creative process and frustrations when creating video tutorials. Finally, we present creators with two designs that aim to address gaps identified in the comment analysis for feedback and alternative design ideas.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [19] [MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.05587)
*Junjie Xing,Yeye He,Mengyu Zhou,Haoyu Dong,Shi Han,Lingjiao Chen,Dongmei Zhang,Surajit Chaudhuri,H. V. Jagadish*

Main category: cs.AI

TL;DR: LLMs在处理表格数据方面取得了显著进展，但目前缺乏全面的基准测试来评估其在专业用户场景下的能力。本研究提出了MMTU，一个包含25个真实世界表格任务、超过28000个问题的基准测试，旨在全面评估模型在专业级别上理解、推理和操作表格的能力。研究表明，MMTU需要模型具备表格理解、推理和编码等多项技能，这对当前的模型来说仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有的表格相关任务评估主要集中在自然语言到SQL和表格问答，忽略了专业用户面临的更广泛的真实世界任务，这限制了我们对模型能力的理解和进步。

Method: 构建了一个大规模基准测试MMTU，包含25个真实世界表格任务和超过28000个问题，这些任务源自计算机科学领域对表格数据的几十年研究，并侧重于专业用户面临的复杂表格任务。

Result: MMTU基准测试揭示了当前前沿模型（如OpenAI GPT-5和DeepSeek R1）在处理复杂表格任务时仍存在显著的提升空间，得分分别为69%和57%。

Conclusion: MMTU基准测试的提出将推动对结构化数据处理和分析的基础模型的研究和发展。

Abstract: Tables and table-based use cases play a crucial role in many important real-world applications, such as spreadsheets, databases, and computational notebooks, which traditionally require expert-level users like data engineers, data analysts, and database administrators to operate. Although LLMs have shown remarkable progress in working with tables (e.g., in spreadsheet and database copilot scenarios), comprehensive benchmarking of such capabilities remains limited. In contrast to an extensive and growing list of NLP benchmarks, evaluations of table-related tasks are scarce, and narrowly focus on tasks like NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks that professional users face. This gap limits our understanding and model progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 28K questions across 25 real-world table tasks, designed to comprehensively evaluate models ability to understand, reason, and manipulate real tables at the expert-level. These tasks are drawn from decades' worth of computer science research on tabular data, with a focus on complex table tasks faced by professional users. We show that MMTU require a combination of skills -- including table understanding, reasoning, and coding -- that remain challenging for today's frontier models, where even frontier reasoning models like OpenAI GPT-5 and DeepSeek R1 score only around 69\% and 57\% respectively, suggesting significant room for improvement. We highlight key findings in our evaluation using MMTU and hope that this benchmark drives further advances in understanding and developing foundation models for structured data processing and analysis.
  Our code and data are available at https://github.com/MMTU-Benchmark/MMTU and https://huggingface.co/datasets/MMTU-benchmark/MMTU.

</details>


### [20] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: 人工智能应该关注实体而非感知，但目前主要关注像素和词语。这是因为结构化数据（如电子表格和数据库）的复杂性，阻碍了关系学习的发展。本文旨在阐述关系学习未能普及的原因，并提出使其获得应有地位的对策。


<details>
  <summary>Details</summary>
Motivation: 目前的人工智能系统主要关注对像素、词语和音素的建模，而不是对现实世界中的实体、属性及其关系进行建模。然而，现实世界是由实体及其关系构成的，并且企业最有价值的数据通常是结构化的（如电子表格、数据库），而非文本或图像。因此，有必要将研究重点从感知转向实体及其关系。然而，关系学习在实际应用中面临挑战，本文旨在探讨其原因并提出解决方案。

Method: 本文通过分析当前人工智能领域的研究重点（主要集中在像素和词语的建模）与现实世界数据结构（结构化数据和实体关系）之间的脱节，阐述了关系学习（包括统计关系人工智能）未能普及的原因。文章探讨了阻碍关系学习发展的因素，并提出了使其获得应有地位的对策。

Result: 当前人工智能领域的研究重点与现实世界的数据结构之间存在脱节，导致关系学习（包括统计关系人工智能）未能广泛普及，仅在一些关系受限的情况下有所应用。文章指出了阻碍关系学习发展的具体因素，并为提升其地位提出了相应的策略。

Conclusion: 尽管人工智能取得了显著进展，但目前的研究重点（如像素和词语建模）未能充分反映现实世界的实体和关系。结构化数据（如电子表格和数据库）的复杂性是关系学习面临的主要挑战。本文分析了关系学习未能普及的原因，并提出了相应的对策，旨在推动关系学习在人工智能领域获得更广泛的应用和认可。

Abstract: Artificial intelligence seems to be taking over the world with systems that model pixels, words, and phonemes. The world is arguably made up, not of pixels, words, and phonemes but of entities (objects, things, including events) with properties and relations among them. Surely we should model these, not the perception or description of them. You might suspect that concentrating on modeling words and pixels is because all of the (valuable) data in the world is in terms of text and images. If you look into almost any company you will find their most valuable data is in spreadsheets, databases and other relational formats. These are not the form that are studied in introductory machine learning, but are full of product numbers, student numbers, transaction numbers and other identifiers that can't be interpreted naively as numbers. The field that studies this sort of data has various names including relational learning, statistical relational AI, and many others. This paper explains why relational learning is not taking over the world -- except in a few cases with restricted relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [21] [Solving Data-centric Tasks using Large Language Models](https://arxiv.org/abs/2402.11734)
*Shraddha Barke,Christian Poelitz,Carina Suzana Negreanu,Benjamin Zorn,José Cambronero,Andrew D. Gordon,Vu Le,Elnaz Nouri,Nadia Polikarpova,Advait Sarkar,Brian Slininger,Neil Toronto,Jack Williams*

Main category: cs.PL

TL;DR: LLMs are becoming popular for programming help, especially for data-centric tasks. This paper introduces a new prompting technique ('cluster-then-select') to improve LLM performance on these tasks by including representative data rows in the prompt, and presents a new dataset for NL-to-code manipulation of tabular data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of effectively prompting Large Language models (LLMs) for data-centric tasks where natural language descriptions alone are insufficient, especially for non-professional programmers. The paper aims to determine how much and which data to include in prompts to optimize LLM performance.

Method: The paper introduces a 'cluster-then-select' prompting technique. This method involves clustering the input data and then selecting the most representative rows to include in the LLM prompt. Additionally, the authors created a dataset of real-world natural language to code tasks involving tabular data, sourced from StackOverflow.

Result: Experiments show that LLM performance is sensitive to the amount of data included in the prompt. The proposed 'cluster-then-select' technique outperforms a random selection baseline, particularly for tasks with high syntactic variation in the input table.

Conclusion: The 'cluster-then-select' prompting technique is an effective method for improving LLM performance on natural language to code tasks involving tabular data, outperforming random data selection by including representative data rows. The creation of a new dataset also aids in the study of these tasks.

Abstract: Large language models (LLMs) are rapidly replacing help forums like StackOverflow, and are especially helpful for non-professional programmers and end users. These users are often interested in data-centric tasks, such as spreadsheet manipulation and data wrangling, which are hard to solve if the intent is only communicated using a natural-language description, without including the data. But how do we decide how much data and which data to include in the prompt? This paper makes two contributions towards answering this question. First, we create a dataset of real-world NL-to-code tasks manipulating tabular data, mined from StackOverflow posts. Second, we introduce a cluster-then-select prompting technique, which adds the most representative rows from the input data to the LLM prompt. Our experiments show that LLM performance is indeed sensitive to the amount of data passed in the prompt, and that for tasks with a lot of syntactic variation in the input table, our cluster-then-select technique outperforms a random selection baseline.

</details>
