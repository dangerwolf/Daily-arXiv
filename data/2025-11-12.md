<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [General Table Question Answering via Answer-Formula Joint Generation](https://arxiv.org/abs/2503.12345)
*Zhongyuan Wang,Richong Zhang,Zhijie Nie,Hangyu Mao*

Main category: cs.CL

TL;DR: 本文提出了TabAF框架，它使用电子表格公式作为可执行表示来解决表格问答（TableQA）中的复杂推理问题，并在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有先进的表格问答（TableQA）方法虽然提高了复杂推理问题的解决能力，但缺乏处理特定问题类型或表格结构的通用性。电子表格公式作为一种广泛使用且定义明确的表格数据操作语言，尚未被充分探索用于解决TableQA问题。

Method: 本文首先尝试使用公式作为可执行表示来解决具有不同结构的表格上的复杂推理问题。具体来说，构建了一个名为FormulaQA的大型公式标注TableQA数据集。此外，提出了一种通用的表格回答框架TabAF，它能同时解决多种类型的任务和多种类型的表格，并使用单一的LLM骨干解码答案和公式。

Result: 广泛的实验表明TabAF具有通用性和泛化能力。在相同的模型规模下，TabAF在WikiTableQuestion、HiTab和TabFact数据集上取得了新的最先进性能。

Conclusion: TabAF框架通过利用电子表格公式作为可执行表示，有效解决了TableQA任务中复杂推理和通用性不足的问题，并在多个基准测试中表现出卓越的性能和广泛的适用性。

Abstract: Advanced table question answering (TableQA) methods prompt large language models (LLMs) to generate answer text, SQL query, Python code, or custom operation, which impressively improve the complex reasoning problems in the TableQA task. However, these methods lack the versatility to cope with specific question types or table structures. In contrast, the Spreadsheet Formula, the widely used and well-defined operation language for tabular data, has not been thoroughly explored to solve TableQA. In this paper, we first attempt to use the Formula as the executable representation for solving complex reasoning on tables with different structures. Specifically, we construct \texttt{FromulaQA}, a large Formula-annotated TableQA dataset from existing datasets. In addition, we propose \texttt{TabAF}, a general table answering framework to solve multiple types of tasks over multiple types of tables simultaneously, which decodes answers and Formulas with a single LLM backbone. Extensive experiments demonstrate the versatility and generalization of \texttt{TabAF}. Under the same model size, \texttt{TabAF} achieves new state-of-the-art performance on the WikiTableQuestion, HiTab, and TabFact.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [TableTalk: Scaffolding Spreadsheet Development with a Language Agent](https://arxiv.org/abs/2502.09787)
*Jenny T. Liang,Aayush Kumar,Yasharth Bajpai,Sumit Gulwani,Vu Le,Chris Parnin,Arjun Radhakrishna,Ashish Tiwari,Emerson Murphy-Hill,Guastavo Soares*

Main category: cs.SE

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 503 - [{'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Spreadsheet programming is challenging. Programmers use spreadsheet programming knowledge (e.g., formulas) and problem-solving skills to combine actions into complex tasks. Advancements in large language models have introduced language agents that observe, plan, and perform tasks, showing promise for spreadsheet creation. We present TableTalk, a spreadsheet programming agent embodying three design principles -- scaffolding, flexibility, and incrementality -- derived from studies with seven spreadsheet programmers and 85 Excel templates. TableTalk guides programmers through structured plans based on professional workflows, generating three potential next steps to adapt plans to programmer needs. It uses pre-defined tools to generate spreadsheet components and incrementally build spreadsheets. In a study with 20 programmers, TableTalk produced higher-quality spreadsheets 2.3 times more likely to be preferred than the baseline. It reduced cognitive load and thinking time by 12.6%. From this, we derive design guidelines for agentic spreadsheet programming tools and discuss implications on spreadsheet programming, end-user programming, AI-assisted programming, and human-agent collaboration.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: 当前AI应更关注实体、属性和关系建模，而非仅像素、词和音素，因为大量有价值数据是关系型的。本文旨在解释关系学习未能普及的原因，并提出使其获得应有地位的对策。


<details>
  <summary>Details</summary>
Motivation: AI目前过度关注像素、词和音素，而现实世界由实体、属性及关系构成。企业中多数有价值数据为关系型（如电子表格、数据库），但此类数据在机器学习中受关注不足。因此，本论文旨在强调关系学习的重要性并探讨其发展瓶颈。

Method: 本文通过分析当前AI对像素、词和音素的侧重、企业数据的关系型本质以及关系学习领域的发展现状，来解释关系学习未能普及的原因，并探讨其未来发展方向。

Result: 本文解释了关系学习未能像其他AI领域一样普及的原因，并指出其在少数受限关系案例中的应用情况。

Conclusion: 关系学习在人工智能领域应具有重要地位，但目前尚未充分发展。本文旨在指出实现其应有地位所需的关键步骤和改进方向。

Abstract: Artificial intelligence seems to be taking over the world with systems that model pixels, words, and phonemes. The world is arguably made up, not of pixels, words, and phonemes but of entities (objects, things, including events) with properties and relations among them. Surely we should model these, not the perception or description of them. You might suspect that concentrating on modeling words and pixels is because all of the (valuable) data in the world is in terms of text and images. If you look into almost any company you will find their most valuable data is in spreadsheets, databases and other relational formats. These are not the form that are studied in introductory machine learning, but are full of product numbers, student numbers, transaction numbers and other identifiers that can't be interpreted naively as numbers. The field that studies this sort of data has various names including relational learning, statistical relational AI, and many others. This paper explains why relational learning is not taking over the world -- except in a few cases with restricted relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


### [4] [MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.05587)
*Junjie Xing,Yeye He,Mengyu Zhou,Haoyu Dong,Shi Han,Lingjiao Chen,Dongmei Zhang,Surajit Chaudhuri,H. V. Jagadish*

Main category: cs.AI

TL;DR: 本文介绍了MMTU，一个大规模基准测试，包含25个真实世界表格任务的30K多个问题，旨在全面评估模型理解、推理和操作真实表格的能力。结果显示，即使是前沿模型（如OpenAI o4-mini和DeepSeek R1）也仅能达到60%左右的得分，表明仍有很大的改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有的表格相关任务评估基准非常稀缺，且主要集中在NL-to-SQL和Table-QA等狭窄领域，忽视了专业用户面临的更广泛的真实世界任务，这限制了我们对该重要领域的理解和模型进展。

Method: 我们引入了MMTU，这是一个大规模基准测试，包含25个真实世界表格任务的30K多个问题。这些任务源于数十年的表格数据计算机科学研究，专注于专业用户面临的复杂表格任务，旨在全面评估模型在专家级别理解、推理和操作真实表格的能力。

Result: MMTU需要表格理解、推理和编码等多种技能的结合，这对当前的前沿模型来说仍然具有挑战性，即使是像OpenAI o4-mini和DeepSeek R1这样的前沿推理模型也只获得了大约60%的得分，这表明有很大的改进空间。

Conclusion: 我们希望MMTU能推动对结构化数据处理和分析的基础模型的理解和开发取得进一步进展。

Abstract: Tables and table-based use cases play a crucial role in many important real-world applications, such as spreadsheets, databases, and computational notebooks, which traditionally require expert-level users like data engineers, data analysts, and database administrators to operate. Although LLMs have shown remarkable progress in working with tables (e.g., in spreadsheet and database copilot scenarios), comprehensive benchmarking of such capabilities remains limited. In contrast to an extensive and growing list of NLP benchmarks, evaluations of table-related tasks are scarce, and narrowly focus on tasks like NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks that professional users face. This gap limits our understanding and model progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 30K questions across 25 real-world table tasks, designed to comprehensively evaluate models ability to understand, reason, and manipulate real tables at the expert-level. These tasks are drawn from decades' worth of computer science research on tabular data, with a focus on complex table tasks faced by professional users. We show that MMTU require a combination of skills -- including table understanding, reasoning, and coding -- that remain challenging for today's frontier models, where even frontier reasoning models like OpenAI o4-mini and DeepSeek R1 score only around 60%, suggesting significant room for improvement. We highlight key findings in our evaluation using MMTU and hope that this benchmark drives further advances in understanding and developing foundation models for structured data processing and analysis. Our code and data are available at https://github.com/MMTU-Benchmark/MMTU and https://huggingface.co/datasets/MMTU-benchmark/MMTU.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Oh That Looks Familiar: A Novel Similarity Measure for Spreadsheet Template Discovery](https://arxiv.org/abs/2511.06973)
*Anand Krishnakumar,Vengadesh Ravikumaran*

Main category: cs.LG

TL;DR: 该论文提出了一种混合距离度量来量化电子表格相似性，结合了语义嵌入、数据类型和空间定位。它通过将电子表格转换为单元格级嵌入，并使用Chamfer和Hausdorff距离等聚合技术。在FUSTE数据集上，该方法在无监督聚类和模板重建方面优于现有方法，实现了完美的模板重建，并有助于大规模自动化模板发现以及后续应用。


<details>
  <summary>Details</summary>
Motivation: 传统识别结构相似电子表格的方法未能捕捉到定义模板的空间布局和类型模式。

Method: 引入了一种结合语义嵌入、数据类型信息和空间定位的混合距离度量。该方法将电子表格转换为单元格级嵌入，然后使用Chamfer和Hausdorff距离等聚合技术来计算电子表格相似性。

Result: 实验表明，与基于图的Mondrian基线相比，该方法在无监督聚类性能方面表现出色，并在FUSTE数据集上实现了完美的模板重建（Adjusted Rand Index为1.00，而基线为0.90）。

Conclusion: 该方法促进了大规模自动化模板发现，进而支持下游应用，如表格集合上的检索增强生成、模型训练和批量数据清洗。

Abstract: Traditional methods for identifying structurally similar spreadsheets fail to capture the spatial layouts and type patterns defining templates. To quantify spreadsheet similarity, we introduce a hybrid distance metric that combines semantic embeddings, data type information, and spatial positioning. In order to calculate spreadsheet similarity, our method converts spreadsheets into cell-level embeddings and then uses aggregation techniques like Chamfer and Hausdorff distances. Experiments across template families demonstrate superior unsupervised clustering performance compared to the graph-based Mondrian baseline, achieving perfect template reconstruction (Adjusted Rand Index of 1.00 versus 0.90) on the FUSTE dataset. Our approach facilitates large-scale automated template discovery, which in turn enables downstream applications such as retrieval-augmented generation over tabular collections, model training, and bulk data cleaning.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [6] [The Paradox of Spreadsheet Self-Efficacy: Social Incentives for Informal Knowledge Sharing in End-User Programming](https://arxiv.org/abs/2408.08068)
*Qing,Xia,Advait Sarkar,Duncan P. Brumby,Anna Cox*

Main category: cs.HC

TL;DR: 非正式知识共享对最终用户程序员获取专业知识至关重要。较高的自我效能感和声誉收益预示着更高的知识共享意愿，而知识编码工作量大会降低知识共享意愿。用户在通用电子表格熟练度方面自我效能感较低，但在与工作相关的电子表格使用方面自我效能感较高。


<details>
  <summary>Details</summary>
Motivation: 了解个人（自我效能感）、社会（声誉收益、同事间的信任）和软件相关（编码工作量）变量如何影响最终用户程序员的电子表格知识共享意愿。

Method: 基于对100名行政和财务角色的电子表格用户的调查数据进行多元回归分析。

Result: 高水平的电子表格自我效能感和认为分享会带来声誉收益预示着更高的知识共享意愿。知识编码工作量大的个体表现出较低的知识共享意愿。无论职业如何，用户普遍报告其通用电子表格熟练度自我效能感较低，但对与工作相关的电子表格使用却有较高的自我效能感。

Conclusion: 承认并设计考虑这些社会和个人变量，有助于避免经验丰富的个体不必要地避免分享，这对电子表格设计具有启示意义。

Abstract: Informal Knowledge Sharing (KS) is vital for end-user programmers to gain expertise. To better understand how personal (self-efficacy), social (reputational gains, trust between colleagues), and software-related (codification effort) variables influence spreadsheet KS intention, we conducted a multiple regressions analysis based on survey data from spreadsheet users (n=100) in administrative and finance roles. We found that high levels of spreadsheet self-efficacy and a perception that sharing would result in reputational gains predicted higher KS intention, but individuals who found knowledge codification effortful showed lower KS intention. We also observed that regardless of occupation, users tended to report a lower sense of self-efficacy in their general spreadsheet proficiency, despite also reporting high self-efficacy in spreadsheet use for job-related contexts. Our findings suggest that acknowledging and designing for these social and personal variables can help avoid situations where experienced individuals refrain unnecessarily from sharing, with implications for spreadsheet design.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [7] [Knowledge engineering for open science: Building and deploying knowledge bases for metadata standards](https://arxiv.org/abs/2507.22391)
*Mark A. Musen,Martin J. O'Connor,Josef Hardi,Marcos Martinez-Romero*

Main category: cs.DL

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: Error code: 503 - [{'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}]

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Scientists strive to make their datasets available in open repositories, with the goal that they be findable, accessible, interoperable, and reusable (FAIR). Although it is hard for most investigators to remember all the guiding principles associated with FAIR data, there is one overarching requirement: The data need to be annotated with rich, discipline-specific, standardized metadata. The Center for Expanded Data Annotation and Retrieval (CEDAR) builds technology that enables scientists to encode metadata standards as templates that enumerate the attributes of different kinds of experiments. These metadata templates capture preferences regarding how data should be described and what a third party needs to know to make sense of the datasets. CEDAR templates describing community metadata preferences have been used to standardize metadata for a variety of scientific consortia. They have been used as the basis for data-annotation systems that acquire metadata through Web forms or through spreadsheets, and they can help correct metadata to ensure adherence to standards. Like the declarative knowledge bases that underpinned intelligent systems decades ago, CEDAR templates capture the knowledge in symbolic form, and they allow that knowledge to be applied in a variety of settings. They provide a mechanism for scientific communities to create shared metadata standards and to encode their preferences for the application of those standards, and for deploying those standards in a range of intelligent systems to promote open science.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [8] [Consensus-Free Spreadsheet Integration](https://arxiv.org/abs/2209.14457)
*Brandon Baylor,Eric Daimler,James Hansen,Esteban Montero,Ryan Wisnesky*

Main category: cs.DB

TL;DR: 该论文提出了一种利用范畴论（包括上极限、提升和Kan-扩展）将多个电子表格合并为一个的方法，通过将每个表格的公式表示为代数理论，值表示为模型，重叠部分表示为理论和模型态射，从而生成一个规范的通用集成理论和模型。这种方法无需原始模型作者达成共识，特别适用于工程模型的合并。


<details>
  <summary>Details</summary>
Motivation: 找到一种合并工程模型的方法，该方法无需被合并模型作者之间达成共识，因为理论和模型态射是语义保持的。

Method: 将每个电子表格的公式表示为代数（等式）理论，将其值表示为其理论的模型；将表格之间的重叠表示为理论和模型态射；然后执行范畴论中的上极限、提升和Kan-扩展构建，以计算出一个规范的通用集成理论和模型，该模型可以再表示为一个电子表格。

Result: 通过在一个大型能源公司真实世界的油气计算案例研究中验证了该方法，描述了集成两个由非交互工程师构建的不同套管压力测试（MASP）计算电子表格时产生的理论和模型。同时描述了验证重叠映射的语义保持性和集成表格的保守性/一致性所需的自动定理证明负担。

Conclusion: 展望了如何将该方法应用于企业范围内的工程工作扩展。

Abstract: We describe a method for merging multiple spreadsheets into one sheet, and/or exchanging data among the sheets, by expressing each sheet's formulae as an algebraic (equational) theory and each sheet's values as a model of its theory, expressing the overlap between the sheets as theory and model morphisms, and then performing colimit, lifting, and Kan-extension constructions from category theory to compute a canonically universal integrated theory and model, which can then be expressed as a spreadsheet. Our motivation is to find methods of merging engineering models that do not require consensus (agreement) among the authors of the models being merged, a condition fulfilled by our method because theory and model morphisms are semantics-preserving. We describe a case study of this methodology on a real-world oil and gas calculation at a major energy company, describing the theories and models that arise when integrating two different casing pressure test (MASP) calculation spreadsheets constructed by two non-interacting engineers. We also describe the automated theorem proving burden associated with both verifying the semantics preservation of the overlap mappings as well as verifying the conservativity/consistency of the resulting integrated sheet. We conclude with thoughts on how to apply the methodology to scale engineering efforts across the enterprise.

</details>
