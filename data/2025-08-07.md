<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 5]
- [cs.CL](#cs.CL) [Total: 13]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.FL](#cs.FL) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [math.HO](#math.HO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 32]
- [cs.SE](#cs.SE) [Total: 44]
- [cs.AR](#cs.AR) [Total: 2]
- [stat.OT](#stat.OT) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.DB](#cs.DB) [Total: 27]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.DL](#cs.DL) [Total: 5]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.MS](#cs.MS) [Total: 1]
- [cs.CY](#cs.CY) [Total: 12]
- [cs.OH](#cs.OH) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.PL](#cs.PL) [Total: 9]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Image deidentification in the XNAT ecosystem: use cases and solutions](https://arxiv.org/abs/2504.20657)
*Alex Michie,Simon J Doran*

Main category: cs.CV

TL;DR: 本研究提出了一个基于XNAT的DICOM数据去标识化流程，并尝试使用规则和机器学习方法处理敏感信息。虽然在MIDI-B挑战赛中取得了显著的性能提升，但仍需进一步优化地址信息和图像内嵌信息的去标识化处理。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在对DICOM数据进行去标识化处理，以应对医学图像去标识化基准（MIDI-B）挑战。研究过程中，作者遇到了技术不兼容问题，并需要改进现有方法以提高去标识化性能。

Method: 本研究描述了一个使用XNAT及其生态系统内独立工具对DICOM数据进行去标识化工作的详细流程。作者采用了基于规则的方法，并结合了机器学习模型来处理地址信息。

Result: 研究的初始得分为97.91%，后通过组织者提供的反馈和MIDI-B持续基准测试改进至99.61%。最终的去标识化失败率估计为0.19%。

Conclusion: 虽然基于规则的方法能够移除姓名信息，但未能完全处理地址信息。初步尝试使用机器学习模型移除地址，但因模型在其他自由文本数据上表现过于激进，导致整体性能略有下降。未来的工作将集中于改进地址识别能力，并更好地移除图像像素中包含的可识别信息。

Abstract: XNAT is a server-based data management platform widely used in academia for
curating large databases of DICOM images for research projects. We describe in
detail a deidentification workflow for DICOM data using facilities in XNAT,
together with independent tools in the XNAT "ecosystem". We list different
contexts in which deidentification might be needed, based on our prior
experience. The starting point for participation in the Medical Image
De-Identification Benchmark (MIDI-B) challenge was a set of pre-existing local
methodologies, which were adapted during the validation phase of the challenge.
Our result in the test phase was 97.91\%, considerably lower than our peers,
due largely to an arcane technical incompatibility of our methodology with the
challenge's Synapse platform, which prevented us receiving feedback during the
validation phase. Post-submission, additional discrepancy reports from the
organisers and via the MIDI-B Continuous Benchmarking facility, enabled us to
improve this score significantly to 99.61\%. An entirely rule-based approach
was shown to be capable of removing all name-related information in the test
corpus, but exhibited failures in dealing fully with address data. Initial
experiments using published machine-learning models to remove addresses were
partially successful but showed the models to be "over-aggressive" on other
types of free-text data, leading to a slight overall degradation in performance
to 99.54\%. Future development will therefore focus on improving
address-recognition capabilities, but also on better removal of identifiable
data burned into the image pixels. Several technical aspects relating to the
"answer key" are still under discussion with the challenge organisers, but we
estimate that our percentage of genuine deidentification failures on the MIDI-B
test corpus currently stands at 0.19\%. (Abridged from original for arXiv
submission)

</details>


### [2] [Vision Language Models for Spreadsheet Understanding: Challenges and Opportunities](https://arxiv.org/abs/2405.16234)
*Shiyu Xia,Junyu Xiong,Haoyu Dong,Jianbo Zhao,Yuzhang Tian,Mengyu Zhou,Yeye He,Shi Han,Dongmei Zhang*

Main category: cs.CV

TL;DR: 该研究评估了VLMs在电子表格理解方面的能力，发现它们在OCR方面有潜力，但在空间和格式识别方面存在不足，并提出了一种新的评估方法来促进未来的研究。


<details>
  <summary>Details</summary>
Motivation: 为了全面评估VLMs在电子表格理解方面的能力，并为未来增强VLMs的电子表格数据理解能力提供方向。

Method: 提出了三个自监督的挑战和相应的评估指标来全面评估VLMs在OCR、空间感知和视觉格式识别方面的能力。还利用电子表格表格检测任务来评估VLMs的整体性能。为了更精细地探测VLMs，提出了三种电子表格到图像的设置：列宽调整、样式更改和地址增强。并提出了提示的变体来处理不同设置下的任务。值得注意的是，为了利用VLMs在理解文本而非二维定位方面的优势，我们提出在电子表格边界检测中解码表格四边界的单元格值。

Result: VLMs在OCR方面表现出有希望的能力，但在单元格遗漏和错位方面表现不佳，并且在空间和格式识别方面能力不足。

Conclusion: VLMs在电子表格理解方面展现出有希望的光学字符识别（OCR）能力，但在单元格遗漏和错位方面表现不佳，并且在空间和格式识别方面能力不足。未来的工作需要利用提出的方法来增强VLMs的电子表格数据理解能力。

Abstract: This paper explores capabilities of Vision Language Models on spreadsheet
comprehension. We propose three self-supervised challenges with corresponding
evaluation metrics to comprehensively evaluate VLMs on Optical Character
Recognition (OCR), spatial perception, and visual format recognition.
Additionally, we utilize the spreadsheet table detection task to assess the
overall performance of VLMs by integrating these challenges. To probe VLMs more
finely, we propose three spreadsheet-to-image settings: column width
adjustment, style change, and address augmentation. We propose variants of
prompts to address the above tasks in different settings. Notably, to leverage
the strengths of VLMs in understanding text rather than two-dimensional
positioning, we propose to decode cell values on the four boundaries of the
table in spreadsheet boundary detection. Our findings reveal that VLMs
demonstrate promising OCR capabilities but produce unsatisfactory results due
to cell omission and misalignment, and they notably exhibit insufficient
spatial and format recognition skills, motivating future work to enhance VLMs'
spreadsheet data comprehension capabilities using our methods to generate
extensive spreadsheet-image pairs in various settings.

</details>


### [3] [High-Throughput Phenotyping using Computer Vision and Machine Learning](https://arxiv.org/abs/2407.06354)
*Vivaan Singhvi,Langalibalele Lunga,Pragya Nidhi,Chris Keum,Varrun Prakash*

Main category: cs.CV

TL;DR: 研究人员开发了一种自动化方法，利用 OCR 和机器学习来分析植物图像中的标签和表型特征，以预测植物对干旱的反应。虽然该方法在提取标签和进行分类方面取得了一定的成功，但仍存在准确性问题和数据缺失，限制了对叶片大小和表型相关性的全面评估。未来的研究需要解决这些局限性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决高通量植物表型分析中的关键挑战，特别是如何高效、准确地处理和分析包含大量数据的图像数据集。通过将机器学习技术与自动化成像相结合，研究人员希望提高表型评估的效率，并开发能够提取特定植物性状（如叶形、叶色、褐斑病等）的方法。此外，该研究还着重于利用图像数据中的标签和元数据（如 EXIF 标签）来预测植物的生长条件（如干旱胁迫）以及探索不同表型特征之间的相关性。

Method: 本研究采用了一种结合光学字符识别（OCR）、图像分割、机器学习算法和 EXIF 标签分析的多方面方法。首先，使用 OCR 技术从植物图像中提取物理标签信息（如处理、基因型等），并将其整理到电子表格中。然后，利用图像分割技术和机器学习算法对植物进行形态学分类（如叶形、叶色、褐斑病等）。接着，训练机器学习模型，根据这些分类预测植物所处的处理环境（对照或干旱）。此外，还分析了 EXIF 标签中的编码信息，以确定叶片大小和表型之间的相关性。

Result: 研究结果显示，所提出的 OCR 模型在非空文本提取方面达到了 94.31% 的准确率，能够有效地将标签信息录入电子表格。在形态学分类方面，包括叶形、叶色和褐斑病在内的分类模型的平均准确率为 62.82%。植物处理（对照或干旱）的预测准确率为 60.08%。研究还发现，EXIF 标签中存在缺失的关键信息，阻碍了对叶片大小的准确评估以及表型与生长条件之间相关性的分析。尽管如此，这些结果为未来研究提供了改进的方向。

Conclusion: 该研究表明，结合 OCR、图像分割和机器学习的自动化方法可以有效地处理植物表型数据的标签，并进行形态分类和治疗预测。尽管在精确分类和全面评估某些特征（如叶片大小和表型相关性）方面存在挑战，但该方法为高通量植物表型分析提供了一个有前景的框架，并为未来的改进指明了方向。

Abstract: High-throughput phenotyping refers to the non-destructive and efficient
evaluation of plant phenotypes. In recent years, it has been coupled with
machine learning in order to improve the process of phenotyping plants by
increasing efficiency in handling large datasets and developing methods for the
extraction of specific traits. Previous studies have developed methods to
advance these challenges through the application of deep neural networks in
tandem with automated cameras; however, the datasets being studied often
excluded physical labels. In this study, we used a dataset provided by Oak
Ridge National Laboratory with 1,672 images of Populus Trichocarpa with white
labels displaying treatment (control or drought), block, row, position, and
genotype. Optical character recognition (OCR) was used to read these labels on
the plants, image segmentation techniques in conjunction with machine learning
algorithms were used for morphological classifications, machine learning models
were used to predict treatment based on those classifications, and analyzed
encoded EXIF tags were used for the purpose of finding leaf size and
correlations between phenotypes. We found that our OCR model had an accuracy of
94.31% for non-null text extractions, allowing for the information to be
accurately placed in a spreadsheet. Our classification models identified leaf
shape, color, and level of brown splotches with an average accuracy of 62.82%,
and plant treatment with an accuracy of 60.08%. Finally, we identified a few
crucial pieces of information absent from the EXIF tags that prevented the
assessment of the leaf size. There was also missing information that prevented
the assessment of correlations between phenotypes and conditions. However,
future studies could improve upon this to allow for the assessment of these
features.

</details>


### [4] [Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language](https://arxiv.org/abs/2204.00598)
*Andy Zeng,Maria Attarian,Brian Ichter,Krzysztof Choromanski,Adrian Wong,Stefan Welker,Federico Tombari,Aveek Purohit,Michael Ryoo,Vikas Sindhwani,Johnny Lee,Vincent Vanhoucke,Pete Florence*

Main category: cs.CV

TL;DR: Socratic Models (SMs) use multiple pretrained models together without retraining to gain new multimodal abilities, achieving strong results in tasks like image captioning and enabling new applications in video understanding and robotics.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the observation that large pretrained models, like visual-language models (VLMs) and language models (LMs), store different forms of commonsense knowledge based on their training data domains (e.g., image captions vs. text without images). This diversity, though potentially having minimal overlap, is hypothesized to be symbiotic and can be leveraged.

Method: The paper proposes Socratic Models (SMs), a modular framework that composes multiple pretrained models zero-shot using multimodal-informed prompting. These models exchange information to capture new multimodal capabilities without requiring finetuning.

Result: SMs achieve competitive performance with state-of-the-art zero-shot methods in image captioning and video-to-text retrieval. Additionally, SMs enable novel applications, including answering questions about egocentric video, engaging in multimodal assistive dialogue by interfacing with external APIs and databases, and facilitating robot perception and planning.

Conclusion: Socratic Models (SMs) are a modular framework that composes multiple pretrained models zero-shot, leveraging their diverse domain knowledge to capture new multimodal capabilities without finetuning. SMs are competitive with state-of-the-art zero-shot methods and enable new applications like egocentric video question answering, multimodal assistive dialogue, and robot perception and planning.

Abstract: Large pretrained (e.g., "foundation") models exhibit distinct capabilities
depending on the domain of data they are trained on. While these domains are
generic, they may only barely overlap. For example, visual-language models
(VLMs) are trained on Internet-scale image captions, but large language models
(LMs) are further trained on Internet-scale text with no images (e.g.,
spreadsheets, SAT questions, code). As a result, these models store different
forms of commonsense knowledge across different domains. In this work, we show
that this diversity is symbiotic, and can be leveraged through Socratic Models
(SMs): a modular framework in which multiple pretrained models may be composed
zero-shot i.e., via multimodal-informed prompting, to exchange information with
each other and capture new multimodal capabilities, without requiring
finetuning. With minimal engineering, SMs are not only competitive with
state-of-the-art zero-shot image captioning and video-to-text retrieval, but
also enable new applications such as (i) answering free-form questions about
egocentric video, (ii) engaging in multimodal assistive dialogue with people
(e.g., for cooking recipes) by interfacing with external APIs and databases
(e.g., web search), and (iii) robot perception and planning.

</details>


### [5] [TableParser: Automatic Table Parsing with Weak Supervision from Spreadsheets](https://arxiv.org/abs/2201.01654)
*Susie Xi Rao,Johannes Rausch,Peter Egger,Ce Zhang*

Main category: cs.CV

TL;DR: A system called TableParser is introduced for high-precision table parsing in PDFs and scanned images, utilizing domain adaptation and a weak supervision mechanism.


<details>
  <summary>Details</summary>
Motivation: The motivation is the ever-existing structure of tables for data storage and the importance of parsing table structures and extracting content from various formats like PDFs, images, spreadsheets, and CSVs.

Method: The paper devises a system called TableParser for parsing tables in native PDFs and scanned images. It also creates TableAnnotator and ExcelAnnotator, which form a spreadsheet-based weak supervision mechanism and a pipeline to enable table parsing.

Result: The system, TableParser, achieves high precision in parsing tables from native PDFs and scanned images. The paper also demonstrates the efficacy of domain adaptation in developing such a tool.

Conclusion: The paper presents TableParser, a system for parsing tables in PDFs and scanned images, and highlights the effectiveness of domain adaptation. It also introduces TableAnnotator and ExcelAnnotator, a weak supervision mechanism for table parsing.

Abstract: Tables have been an ever-existing structure to store data. There exist now
different approaches to store tabular data physically. PDFs, images,
spreadsheets, and CSVs are leading examples. Being able to parse table
structures and extract content bounded by these structures is of high
importance in many applications. In this paper, we devise TableParser, a system
capable of parsing tables in both native PDFs and scanned images with high
precision. We have conducted extensive experiments to show the efficacy of
domain adaptation in developing such a tool. Moreover, we create TableAnnotator
and ExcelAnnotator, which constitute a spreadsheet-based weak supervision
mechanism and a pipeline to enable table parsing. We share these resources with
the research community to facilitate further research in this interesting
direction.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [6] [An Empirical Study of Validating Synthetic Data for Formula Generation](https://arxiv.org/abs/2407.10657)
*Usneek Singh,José Cambronero,Sumit Gulwani,Aditya Kanade,Anirudh Khatry,Vu Le,Mukul Singh,Gust Verbruggen*

Main category: cs.CL

TL;DR: 由于电子表格公式的资源稀缺，本研究提出了一种通过验证 LLM 生成的合成数据来提高模型性能的方法，结果表明该方法可以提高性能并增加模型可解决的问题的复杂性。


<details>
  <summary>Details</summary>
Motivation: 由于电子表格公式的资源稀缺，影响了预训练模型的性能并限制了微调能力，因此需要生成和验证合成的自然语言描述以用于微调。

Method: 通过代理目标对 LLM 生成的自然语言进行验证，以评估合成注释的准确性。

Result: 在四种模型（两种开源和两种闭源）上，与原始数据相比，验证可以提高性能。验证会修剪更具挑战性的例子，但会增加微调后模型可以解决的问题的复杂性。

Conclusion: 验证合成训练样本的准确性可以提高基于 LLM 的电子表格公式写作模型的性能，即使它会修剪更具挑战性的例子，也能提高微调后模型能够解决的问题的复杂性。

Abstract: Large language models (LLMs) can be leveraged to help with writing formulas
in spreadsheets, but resources on these formulas are scarce, impacting both the
base performance of pre-trained models and limiting the ability to fine-tune
them. Given a corpus of formulas, we can use a(nother) model to generate
synthetic natural language utterances for fine-tuning. However, it is important
to validate whether the NL generated by the LLM is indeed accurate to be
beneficial for fine-tuning. In this paper, we provide empirical results on the
impact of validating these synthetic training examples with surrogate
objectives that evaluate the accuracy of the synthetic annotations. We
demonstrate that validation improves performance over raw data across four
models (2 open and 2 closed weight). Interestingly, we show that although
validation tends to prune more challenging examples, it increases the
complexity of problems that models can solve after being fine-tuned on
validated data.

</details>


### [7] [General Table Question Answering via Answer-Formula Joint Generation](https://arxiv.org/abs/2503.12345)
*Zhongyuan Wang,Richong Zhang,Zhijie Nie*

Main category: cs.CL

TL;DR: 本研究提出TabAF框架和FormulaQA数据集，使用电子表格公式解决TableQA问题，实现了跨表格和问答类型的通用性，并在多个基准测试中取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在TableQA任务中虽然能生成文本、SQL或Python代码，但缺乏处理特定类型问题或表格结构的通用性。电子表格公式作为一种广泛使用且定义明确的表格数据操作语言，尚未被充分探索用于TableQA。

Method: 本研究提出了TabAF框架，该框架使用电子表格公式作为可执行表示，并利用单个大型语言模型（LLM）后端来解码答案和公式，以解决TableQA任务。此外，研究构建了FormulaQA数据集，用于训练和评估该框架。

Result: TabAF框架基于Llama3.1-70B模型，在WikiTableQuestion、HiTab和TabFact等多个数据集上实现了新的最先进性能，证明了其在处理不同表格结构和问题类型方面的通用性和泛化能力。

Conclusion: 该研究提出了FormulaQA数据集和TabAF框架，以解决表格问答（TableQA）任务中现有方法缺乏通用性的问题。TabAF框架使用电子表格公式作为可执行表示，能够同时处理不同结构和类型的表格及问答任务，并在多个基准测试中取得了新的最先进性能。

Abstract: Advanced table question answering (TableQA) methods prompt large language
models (LLMs) to generate answer text, SQL query, Python code, or custom
operations, which impressively improve the complex reasoning problems in the
TableQA task. However, these methods lack the versatility to cope with specific
question types or table structures. In contrast, the Spreadsheet Formula, the
widely used and well-defined operation language for tabular data, has not been
thoroughly explored to solve TableQA. In this paper, we first attempt to use
the Formula as the executable representation for solving complex reasoning on
tables with different structures. Specifically, we construct
\texttt{FromulaQA}, a large Formula-annotated TableQA dataset from existing
datasets. In addition, we propose \texttt{TabAF}, a general table answering
framework to solve multiple types of tasks over multiple types of tables
simultaneously. Unlike existing methods, \texttt{TabAF} decodes answers and
Formulas with a single LLM backbone, demonstrating great versatility and
generalization. \texttt{TabAF} based on Llama3.1-70B achieves new
state-of-the-art performance on the WikiTableQuestion, HiTab, and TabFact.

</details>


### [8] [TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios](https://arxiv.org/abs/2403.19318)
*Xiaokang Zhang,Sijia Luo,Bohan Zhang,Zeyao Ma,Jing Zhang,Yang Li,Guanlin Li,Zijun Yao,Kangli Xu,Jinchang Zhou,Daniel Zhang-Li,Jifan Yu,Shu Zhao,Juanzi Li,Jie Tang*

Main category: cs.CL

TL;DR: TableLLM 是一个强大的 80 亿参数大语言模型，专门用于处理文档和电子表格中的表格数据。它采用远程监督训练方法，并在相关基准测试中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决现实办公场景中，文档或电子表格中表格数据处理的挑战。

Method: 提出了一种包含推理过程扩展策略和交叉验证策略的远程监督训练方法。

Result: TableLLM 在针对文档和电子表格格式定制的基准测试中表现出优势，并提供了模型检查点、源代码、基准测试和 Web 应用程序。

Conclusion: TableLLM 在处理文档和电子表格中的表格数据方面表现出色，优于现有的通用和专用大语言模型。

Abstract: We introduce TableLLM, a robust large language model (LLM) with 8 billion
parameters, purpose-built for proficiently handling tabular data manipulation
tasks, whether they are embedded within documents or spreadsheets, catering to
real-world office scenarios. We propose a distant supervision method for
training, which comprises a reasoning process extension strategy, aiding in
training LLMs to understand reasoning patterns more effectively as well as a
cross-way validation strategy, ensuring the quality of the automatically
generated data. To evaluate the performance of TableLLM, we have crafted
benchmarks tailored to address both document and spreadsheet formats as well as
constructed a well-organized evaluation pipeline capable of handling both
scenarios. Thorough evaluations underscore the advantages of TableLLM when
compared to various existing general-purpose and tabular data-focused LLMs. We
have publicly released the model checkpoint, source code, benchmarks, and a web
application for user interaction. Our codes and data are publicly available at
https://github.com/TableLLM/TableLLM.

</details>


### [9] [GiesKaNe: Bridging Past and Present in Grammatical Theory and Practical Application](https://arxiv.org/abs/2502.05113)
*Volker Emmrich*

Main category: cs.CL

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: 'NoneType' object has no attribute 'model_dump'

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: This article explores the requirements for corpus compilation within the
GiesKaNe project (University of Giessen and Kassel, Syntactic Basic Structures
of New High German). The project is defined by three central characteristics:
it is a reference corpus, a historical corpus, and a syntactically deeply
annotated treebank. As a historical corpus, GiesKaNe aims to establish
connections with both historical and contemporary corpora, ensuring its
relevance across temporal and linguistic contexts. The compilation process
strikes the balance between innovation and adherence to standards, addressing
both internal project goals and the broader interests of the research
community. The methodological complexity of such a project is managed through a
complementary interplay of human expertise and machine-assisted processes. The
article discusses foundational topics such as tokenization, normalization,
sentence definition, tagging, parsing, and inter-annotator agreement, alongside
advanced considerations. These include comparisons between grammatical models,
annotation schemas, and established de facto annotation standards as well as
the integration of human and machine collaboration. Notably, a novel method for
machine-assisted classification of texts along the continuum of conceptual
orality and literacy is proposed, offering new perspectives on text selection.
Furthermore, the article introduces an approach to deriving de facto standard
annotations from existing ones, mediating between standardization and
innovation. In the course of describing the workflow the article demonstrates
that even ambitious projects like GiesKaNe can be effectively implemented using
existing research infrastructure, requiring no specialized annotation tools.
Instead, it is shown that the workflow can be based on the strategic use of a
simple spreadsheet and integrates the capabilities of the existing
infrastructure.

</details>


### [10] [MiMoTable: A Multi-scale Spreadsheet Benchmark with Meta Operations for Table Reasoning](https://arxiv.org/abs/2412.11711)
*Zheng Li,Yang Du,Mao Zheng,Mingyang Song*

Main category: cs.CL

TL;DR: 介绍MiMoTable，一个包含真实世界电子表格和新难度评估标准的新基准，旨在改进LLM的表格推理能力。


<details>
  <summary>Details</summary>
Motivation: 填补现有表格推理基准与真实世界应用中更复杂多样的表格和用户问题之间的差距。

Method: 提出了一种名为MiMoTable的多尺度电子表格基准测试，其中包含真实世界的电子表格和包含六种元操作的难度评估标准。

Result: MiMoTable包含七个领域、不同类型的真实世界电子表格。新评估标准可衡量问题难度，并用于评估现有基准测试。实验证明了该标准的有效性。

Conclusion: Claude-3.5-Sonnet在MiMoTable上取得了77.4%的准确率，但仍有很大的提升空间，表明LLM在表格推理方面仍需改进。所提出的新评估标准被证明是有效的，因为LLM在更难的基准测试上的表现会下降。

Abstract: Extensive research has been conducted to explore the capability of Large
Language Models (LLMs) for table reasoning and has significantly improved the
performance on existing benchmarks. However, tables and user questions in
real-world applications are more complex and diverse, presenting an unignorable
gap compared to the existing benchmarks. To fill the gap, we propose a
\textbf{M}ult\textbf{i}-scale spreadsheet benchmark with \textbf{M}eta
\textbf{o}perations for \textbf{Table} reasoning, named as MiMoTable.
Specifically, MiMoTable incorporates two key features. First, the tables in
MiMoTable are all spreadsheets used in real-world scenarios, which cover seven
domains and contain different types. Second, we define a new criterion with six
categories of meta operations for measuring the difficulty of each question in
MiMoTable, simultaneously as a new perspective for measuring the difficulty of
the existing benchmarks. Experimental results show that Claude-3.5-Sonnet
achieves the best performance with 77.4\% accuracy, indicating that there is
still significant room to improve for LLMs on MiMoTable. Furthermore, we grade
the difficulty of existing benchmarks according to our new criteria.
Experiments have shown that the performance of LLMs decreases as the difficulty
of benchmarks increases, thereby proving the effectiveness of our proposed new
criterion.

</details>


### [11] [SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation](https://arxiv.org/abs/2406.14991)
*Zeyao Ma,Bohan Zhang,Jing Zhang,Jifan Yu,Xiaokang Zhang,Xiaohan Zhang,Sijia Luo,Xi Wang,Jie Tang*

Main category: cs.CL

TL;DR: 介绍了一个名为SpreadsheetBench的新基准测试，该测试集包含来自真实Excel论坛问题的912个真实用户查询和相关电子表格，并提出了一种更可靠的评估指标。评估显示，当前LLMs在处理真实电子表格任务方面与人类相比存在巨大差距。


<details>
  <summary>Details</summary>
Motivation: 为了弥合现有基准测试与真实用户在电子表格操作需求之间的差距，从而更准确地评估大型语言模型（LLMs）在处理真实电子表格工作流方面的能力。

Method: 提出了一个名为SpreadsheetBench的新基准测试，该测试集包含912个真实用户在Excel论坛上提出的问题，并与之匹配了包含多种表格、非标准关系表和非文本元素的电子表格文件。同时，提出了一种类似在线评判平台的更可靠的评估指标，为每个指令创建多个电子表格文件作为测试用例，以评估模型处理不同数值电子表格的鲁棒性。

Result: 在单轮和多轮推理设置下对各种LLMs进行了全面评估，结果表明，即使是目前最先进的模型，在处理SpreadsheetBench基准测试中的任务时，其表现也远不及人类水平。

Conclusion: 现有的大型语言模型（LLMs）在处理真实世界的电子表格操作任务方面与人类水平存在显著差距，凸显了该基准测试的挑战性。

Abstract: We introduce SpreadsheetBench, a challenging spreadsheet manipulation
benchmark exclusively derived from real-world scenarios, designed to immerse
current large language models (LLMs) in the actual workflow of spreadsheet
users. Unlike existing benchmarks that rely on synthesized queries and
simplified spreadsheet files, SpreadsheetBench is built from 912 real questions
gathered from online Excel forums, which reflect the intricate needs of users.
The associated spreadsheets from the forums contain a variety of tabular data
such as multiple tables, non-standard relational tables, and abundant
non-textual elements. Furthermore, we propose a more reliable evaluation metric
akin to online judge platforms, where multiple spreadsheet files are created as
test cases for each instruction, ensuring the evaluation of robust solutions
capable of handling spreadsheets with varying values. Our comprehensive
evaluation of various LLMs under both single-round and multi-round inference
settings reveals a substantial gap between the state-of-the-art (SOTA) models
and human performance, highlighting the benchmark's difficulty.

</details>


### [12] [NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries](https://arxiv.org/abs/2402.14853)
*Wei Zhao,Zhitao Hou,Siyuan Wu,Yan Gao,Haoyu Dong,Yao Wan,Hongyu Zhang,Yulei Sui,Haidong Zhang*

Main category: cs.CL

TL;DR: 本研究提出NL2Formula任务和fCoder模型，用于将自然语言转换为电子表格公式，旨在简化数据分析过程。


<details>
  <summary>Details</summary>
Motivation: 为了减轻用户在电子表格中编写复杂公式的负担，特别是在处理复杂操作时，本研究引入了NL2Formula任务。

Method: 本研究构建了一个包含70,799个自然语言查询和相应电子表格公式的数据集，涵盖21,670个表格和37种公式函数。实现了fCoder作为序列到序列的基线模型来完成NL2Formula任务，并通过实验评估其性能。

Result: fCoder在NL2Formula任务上取得了优于基线模型的性能，并且与GPT-3.5（text-davinci-003）进行了比较。通过详细的错误分析，确定了NL2Formula任务中的潜在挑战。

Conclusion: 本研究介绍了NL2Formula这一新的基准任务，旨在根据自然语言查询生成电子表格公式，并提供了一个名为fCoder的序列到序列基线实现。实验结果表明fCoder在NL2Formula任务上表现优于基线模型，并与GPT-3.5进行了比较。通过错误分析，研究还指出了该任务的挑战并鼓励进一步研究。

Abstract: Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets,
is a widespread practice among users performing data analysis. However,
crafting formulas on spreadsheets remains a tedious and error-prone task for
many end-users, particularly when dealing with complex operations. To alleviate
the burden associated with writing spreadsheet formulas, this paper introduces
a novel benchmark task called NL2Formula, with the aim to generate executable
formulas that are grounded on a spreadsheet table, given a Natural Language
(NL) query as input. To accomplish this, we construct a comprehensive dataset
consisting of 70,799 paired NL queries and corresponding spreadsheet formulas,
covering 21,670 tables and 37 types of formula functions. We realize the
NL2Formula task by providing a sequence-to-sequence baseline implementation
called fCoder. Experimental results validate the effectiveness of fCoder,
demonstrating its superior performance compared to the baseline models.
Furthermore, we also compare fCoder with an initial GPT-3.5 model (i.e.,
text-davinci-003). Lastly, through in-depth error analysis, we identify
potential challenges in the NL2Formula task and advocate for further
investigation.

</details>


### [13] [InstructExcel: A Benchmark for Natural Language Instruction in Excel](https://arxiv.org/abs/2310.14495)
*Justin Payan,Swaroop Mishra,Mukul Singh,Carina Negreanu,Christian Poelitz,Chitta Baral,Subhro Roy,Rasika Chakravarthy,Benjamin Van Durme,Elnaz Nouri*

Main category: cs.CL

TL;DR: LLMs can generate Excel OfficeScripts from natural language 


<details>
  <summary>Details</summary>
Motivation: Investigate whether LLMs can generate code (Excel 

Method: Introduced a new large-scale benchmark, InstructExcel, by 

Result: InstructExcel is a hard benchmark for state of the art 

Conclusion: LLMs can generate code for Excel tasks, but it

Abstract: With the evolution of Large Language Models (LLMs) we can solve increasingly
more complex NLP tasks across various domains, including spreadsheets. This
work investigates whether LLMs can generate code (Excel OfficeScripts, a
TypeScript API for executing many tasks in Excel) that solves Excel specific
tasks provided via natural language user instructions. To do so we introduce a
new large-scale benchmark, InstructExcel, created by leveraging the 'Automate'
feature in Excel to automatically generate OfficeScripts from users' actions.
Our benchmark includes over 10k samples covering 170+ Excel operations across
2,000 publicly available Excel spreadsheets. Experiments across various
zero-shot and few-shot settings show that InstructExcel is a hard benchmark for
state of the art models like GPT-4. We observe that (1) using GPT-4 over
GPT-3.5, (2) providing more in-context examples, and (3) dynamic prompting can
help improve performance on this benchmark.

</details>


### [14] [Active Learning with Tabular Language Models](https://arxiv.org/abs/2211.04128)
*Martin Ringsquandl,Aneta Koleva*

Main category: cs.CL

TL;DR: 为了解决现实世界中表格语言模型标注成本高的问题，本研究探索了主动学习在亚单元格命名实体识别任务中的应用。通过对比不同的采集函数，我们发现单元格级别的采集函数结合多样性策略可以有效减少标注工作量，但强制表格多样性反而会降低模型性能。研究还强调了计算效率和标注者体验的重要性。


<details>
  <summary>Details</summary>
Motivation: 在表格语言模型研究取得进展的同时，其在现实世界的应用仍面临挑战。工业领域存在大量电子表格数据，但由于表格技术性强、领域特定性高，需要专家进行标注，导致获取大量标签成本高昂。主动学习有望降低标注成本，但目前尚无结合主动学习与表格语言模型的研究。

Method: 本研究在真实世界的工业表格语言模型应用场景中，针对亚单元格命名实体识别任务，研究了不同的采集函数。

Result: 细胞级采集函数与内置多样性可显著降低标注成本，而强制表格多样性则是有害的。

Conclusion: 实验结果表明，具有内置多样性的单元格级别采集函数能够显著降低标注成本，而强制表格多样性则会损害模型性能。此外，研究还指出了计算效率和人类标注者视角方面存在的根本性问题。

Abstract: Despite recent advancements in tabular language model research, real-world
applications are still challenging. In industry, there is an abundance of
tables found in spreadsheets, but acquisition of substantial amounts of labels
is expensive, since only experts can annotate the often highly technical and
domain-specific tables. Active learning could potentially reduce labeling
costs, however, so far there are no works related to active learning in
conjunction with tabular language models. In this paper we investigate
different acquisition functions in a real-world industrial tabular language
model use case for sub-cell named entity recognition. Our results show that
cell-level acquisition functions with built-in diversity can significantly
reduce the labeling effort, while enforced table diversity is detrimental. We
further see open fundamental questions concerning computational efficiency and
the perspective of human annotators.

</details>


### [15] [Table-To-Text generation and pre-training with TabT5](https://arxiv.org/abs/2210.09162)
*Ewa Andrejczuk,Julian Martin Eisenschlos,Francesco Piccinno,Syrine Krichene,Yasemin Altun*

Main category: cs.CL

TL;DR: TABT5是一个创新的表格理解模型，通过引入解码器并优化预训练，显著提高了在公式预测、问答和数据到文本生成等任务上的性能。


<details>
  <summary>Details</summary>
Motivation: Encoder-only transformer模型在表格理解任务中表现出色，但受限于分类类任务。需要一种能够处理更广泛任务（如生成式任务）的模型。

Method: TABT5是一个编码器-解码器模型，它基于表格和文本输入生成自然语言文本。该模型通过整合解码器组件并利用表格特定嵌入和预训练来克服了仅编码器模型的局限性。

Result: TABT5在电子表格公式预测方面提高了15%的序列准确率，在问答方面提高了2.5%的序列准确率，在数据到文本生成方面提高了2.5%的BLEU分数，在多个领域取得了新的最先进成果。

Conclusion: TABT5通过整合解码器组件并利用表格特定嵌入和预训练来克服仅编码器模型的局限性，在包括电子表格公式预测、问答和数据到文本生成在内的多个领域取得了新的最先进成果。

Abstract: Encoder-only transformer models have been successfully applied to different
table understanding tasks, as in TAPAS (Herzig et al., 2020). A major
limitation of these architectures is that they are constrained to
classification-like tasks such as cell selection or entailment detection. We
present TABT5, an encoder-decoder model that generates natural language text
based on tables and textual inputs. TABT5 overcomes the encoder-only limitation
by incorporating a decoder component and leverages the input structure with
table specific embeddings and pre-training. TABT5 achieves new state-of-the-art
results on several domains, including spreadsheet formula prediction with a 15%
increase in sequence accuracy, QA with a 2.5% increase in sequence accuracy and
data-to-text generation with a 2.5% increase in BLEU.

</details>


### [16] [Table Pre-training: A Survey on Model Architectures, Pre-training Objectives, and Downstream Tasks](https://arxiv.org/abs/2201.09745)
*Haoyu Dong,Zhoujun Cheng,Xinyi He,Mengyu Zhou,Anda Zhou,Fan Zhou,Ao Liu,Shi Han,Dongmei Zhang*

Main category: cs.CL

TL;DR: 对表格预训练框架进行了全面回顾，包括模型设计、预训练目标和下游任务，并讨论了挑战和机遇。


<details>
  <summary>Details</summary>
Motivation: 随着从网页、电子表格、PDF等各种文档类型中收集大量表格数据的能力增强，以及文本和图像预训练的成功，表格预训练框架受到广泛关注，并在表格问答、表格类型识别、列关系分类、表格搜索、公式预测等任务上取得了新的进展。

Method: 对表格预训练模型设计、预训练目标和下游任务进行了全面的回顾和分析。

Result: 表格预训练在各种下游任务中取得了新的最先进成果，研究了多种预训练目标和表格语言模型。

Conclusion: 本文对表格预训练框架进行了全面的回顾，涵盖了模型设计、预训练目标和下游任务，并对现有挑战和未来机遇分享了见解。

Abstract: Since a vast number of tables can be easily collected from web pages,
spreadsheets, PDFs, and various other document types, a flurry of table
pre-training frameworks have been proposed following the success of text and
images, and they have achieved new state-of-the-arts on various tasks such as
table question answering, table type recognition, column relation
classification, table search, formula prediction, etc. To fully use the
supervision signals in unlabeled tables, a variety of pre-training objectives
have been designed and evaluated, for example, denoising cell values,
predicting numerical relationships, and implicitly executing SQLs. And to best
leverage the characteristics of (semi-)structured tables, various tabular
language models, particularly with specially-designed attention mechanisms,
have been explored. Since tables usually appear and interact with free-form
text, table pre-training usually takes the form of table-text joint
pre-training, which attracts significant research interests from multiple
domains. This survey aims to provide a comprehensive review of different model
designs, pre-training objectives, and downstream tasks for table pre-training,
and we further share our thoughts and vision on existing challenges and future
opportunities.

</details>


### [17] [TableQuery: Querying tabular data with natural language](https://arxiv.org/abs/2202.00454)
*Abhijith Neil Abraham,Fariz Rahman,Damanpreet Kaur*

Main category: cs.CL

TL;DR: TableQuery est un nouvel outil qui utilise des modèles d'apprentissage profond pré-entraînés pour la réponse aux questions sur des données tabulaires, résolvant les limitations des méthodes existantes qui nécessitent que l'intégralité du tableau soit chargée en mémoire.


<details>
  <summary>Details</summary>
Motivation: Les méthodes d'apprentissage profond existantes pour la réponse aux questions sur les données tabulaires ne conviennent pas à la plupart des applications réelles, car elles nécessitent l'alimentation de l'intégralité du tableau en entrée d'un modèle de réseau neuronal, ce qui les rend inadaptées aux applications réelles.

Method: Utilise des modèles d'apprentissage profond pré-entraînés pour la réponse aux questions sur le texte libre afin de convertir les requêtes en langage naturel en requêtes structurées qui peuvent être exécutées contre une base de données ou une feuille de calcul.

Result: TableQuery peut être remplacé par un nouveau modèle d'apprentissage profond pré-entraîné pour la réponse aux questions sur le texte libre avec de meilleures performances lorsqu'il devient disponible.

Conclusion: TableQuery élimine le besoin d'adapter toutes les données à la mémoire ainsi que de sérialiser les bases de données.

Abstract: This paper presents TableQuery, a novel tool for querying tabular data using
deep learning models pre-trained to answer questions on free text. Existing
deep learning methods for question answering on tabular data have various
limitations, such as having to feed the entire table as input into a neural
network model, making them unsuitable for most real-world applications. Since
real-world data might contain millions of rows, it may not entirely fit into
the memory. Moreover, data could be stored in live databases, which are updated
in real-time, and it is impractical to serialize an entire database to a neural
network-friendly format each time it is updated. In TableQuery, we use deep
learning models pre-trained for question answering on free text to convert
natural language queries to structured queries, which can be run against a
database or a spreadsheet. This method eliminates the need for fitting the
entire data into memory as well as serializing databases. Furthermore, deep
learning models pre-trained for question answering on free text are readily
available on platforms such as HuggingFace Model Hub (7). TableQuery does not
require re-training; when a newly trained model for question answering with
better performance is available, it can replace the existing model in
TableQuery.

</details>


### [18] [The Impact of Text Presentation on Translator Performance](https://arxiv.org/abs/2011.05978)
*Samuel Läubli,Patrick Simianer,Joern Wuebker,Geza Kovacs,Rico Sennrich,Spence Green*

Main category: cs.CL

TL;DR: 逐句呈现和顶部/底部布局可提高翻译速度，但非分段文本在修订任务中更准确高效。


<details>
  <summary>Details</summary>
Motivation: 评估计算机辅助翻译（CAT）工具中常用的逐句呈现和并排视图等设计选择对翻译性能的影响。

Method: 通过三项实验文本处理任务，测量翻译速度和准确性，以评估分段和布局设计选择。

Result: 与非分段文本相比，逐句呈现可提高文本复制速度和句内错误识别能力。顶部/底部布局比并排布局能更快地复制文本。然而，在修订任务中，非分段文本在准确性和时间效率方面表现最佳。

Conclusion: 设计选择，如逐句呈现和顶部/底部布局，会影响翻译速度和准确性。非分段文本在修订任务中表现最佳。

Abstract: Widely used computer-aided translation (CAT) tools divide documents into
segments such as sentences and arrange them in a side-by-side, spreadsheet-like
view. We present the first controlled evaluation of these design choices on
translator performance, measuring speed and accuracy in three experimental text
processing tasks. We find significant evidence that sentence-by-sentence
presentation enables faster text reproduction and within-sentence error
identification compared to unsegmented text, and that a top-and-bottom
arrangement of source and target sentences enables faster text reproduction
compared to a side-by-side arrangement. For revision, on the other hand, our
results suggest that presenting unsegmented text results in the highest
accuracy and time efficiency. Our findings have direct implications for best
practices in designing CAT tools.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [19] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: AI应关注实体关系而非感知数据，关系学习有潜力但需克服现有局限性。


<details>
  <summary>Details</summary>
Motivation: 当前AI主要集中于像素和词语等感知数据的建模，而忽视了现实世界是由实体、属性及关系构成的。作者认为，应将重点放在对这些实体及其关系的建模上，而非对感知数据的描述。

Method: 本文通过分析关系学习在现实世界中的应用情况，解释了其未被广泛采用的原因，并提出了相应的解决方案。

Result: 尽管关系学习在某些受限场景下取得了一定的成功，但它并未像像素和词语模型那样普及。文章旨在阐述关系学习未被广泛应用的原因，并提出促进其发展和普及的策略。

Conclusion: 关系学习未能主导世界，但通过解决其局限性，它可以达到应有的重要地位。

Abstract: AI seems to be taking over the world with systems that model pixels, words,
and phonemes. The world is arguably made up, not of pixels, words, and phonemes
but of entities (objects, things, including events) with properties and
relations among them. Surely we should model these, not the perception or
description of them. You might suspect that concentrating on modeling words and
pixels is because all of the (valuable) data in the world is in terms of text
and images. If you look into almost any company you will find their most
valuable data is in spreadsheets, databases and other relational formats. These
are not the form that are studied in introductory machine learning, but are
full of product numbers, student numbers, transaction numbers and other
identifiers that can't be interpreted naively as numbers. The field that
studies this sort of data has various names including relational learning,
statistical relational AI, and many others. This paper explains why relational
learning is not taking over the world -- except in a few cases with restricted
relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


### [20] [MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.05587)
*Junjie Xing,Yeye He,Mengyu Zhou,Haoyu Dong,Shi Han,Lingjiao Chen,Dongmei Zhang,Surajit Chaudhuri,H. V. Jagadish*

Main category: cs.AI

TL;DR: MMTU 是一个评估模型处理真实表格数据的基准测试，包含 30K+ 问题和 25 个任务。现有模型表现不佳（约 60%），表明在该领域存在改进空间。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在处理表格数据方面取得了显著进展，但相关的基准测试仍然有限，并且主要集中在自然语言到 SQL 和表格问答等任务上，忽视了专业用户面临的更广泛的真实世界任务。这种不足限制了我们对该领域模型能力和进展的理解。

Method: 本研究引入了一个名为 MMTU 的大型基准测试，其中包含 25 个真实的表格任务和超过 30,000 个问题。这些任务源于计算科学中关于表格数据的研究，侧重于专业用户面临的复杂表格任务。研究评估了包括表格理解、推理和编码在内的多种能力。

Result: MMTU 基准测试表明，当前的模型（包括 OpenAI o4-mini 和 DeepSeek R1）在处理表格数据方面仍存在挑战，得分仅为 60% 左右。这表明在表格理解、推理和编码等方面的能力仍有待提高。

Conclusion: MMTU 作为一个包含超过 30,000 个问题和 25 个真实世界表格任务的大型基准测试，全面评估了模型理解、推理和操作真实表格的能力，达到了专家级别。目前的模型（如 OpenAI o4-mini 和 DeepSeek R1）在该基准上的得分仅为 60% 左右，表明在这一领域仍有很大的改进空间。该基准测试旨在推动在处理和分析结构化数据方面的基础模型的研究和发展。

Abstract: Tables and table-based use cases play a crucial role in many important
real-world applications, such as spreadsheets, databases, and computational
notebooks, which traditionally require expert-level users like data engineers,
data analysts, and database administrators to operate. Although LLMs have shown
remarkable progress in working with tables (e.g., in spreadsheet and database
copilot scenarios), comprehensive benchmarking of such capabilities remains
limited. In contrast to an extensive and growing list of NLP benchmarks,
evaluations of table-related tasks are scarce, and narrowly focus on tasks like
NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks
that professional users face. This gap limits our understanding and model
progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 30K
questions across 25 real-world table tasks, designed to comprehensively
evaluate models ability to understand, reason, and manipulate real tables at
the expert-level. These tasks are drawn from decades' worth of computer science
research on tabular data, with a focus on complex table tasks faced by
professional users. We show that MMTU require a combination of skills --
including table understanding, reasoning, and coding -- that remain challenging
for today's frontier models, where even frontier reasoning models like OpenAI
o4-mini and DeepSeek R1 score only around 60%, suggesting significant room for
improvement. We highlight key findings in our evaluation using MMTU and hope
that this benchmark drives further advances in understanding and developing
foundation models for structured data processing and analysis. Our code and
data are available at https://github.com/MMTU-Benchmark/MMTU and
https://huggingface.co/datasets/MMTU-benchmark/MMTU.

</details>


### [21] [Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models](https://arxiv.org/abs/2505.23667)
*Lang Cao,Jingxian Xu,Hanbing Liu,Jinyu Wang,Mengyu Zhou,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: Formula Tuning (Fortune) 通过强化学习和电子表格公式，提升了大型语言模型在表格问答和推理任务上的能力，甚至超越了OpenAI o1。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型（LM）在处理复杂表格数据时，尤其是在数值或符号推理方面存在的挑战，并利用电子表格公式这一强大而富有表现力的媒介来编码丰富的推理模式。

Method: Formula Tuning (Fortune) 是一个强化学习框架，通过奖励模型生成可执行的电子表格公式。

Result: Formula Tuning 显著提高了语言模型在表格理解任务上的性能，特别是在多步数值和符号推理任务上，使得一个7B模型在表格理解能力上超越了OpenAI o1。

Conclusion: Formula Tuning (Fortune)框架通过使用二进制答案正确性作为奖励信号，利用强化学习训练语言模型生成可执行的电子表格公式，以解决表格问答问题，减少了对监督公式注释的依赖。

Abstract: Tables are a fundamental structure for organizing and analyzing data, making
effective table understanding a critical capability for intelligent systems.
While large language models (LMs) demonstrate strong general reasoning
abilities, they continue to struggle with accurate numerical or symbolic
reasoning over tabular data, especially in complex scenarios. Spreadsheet
formulas provide a powerful and expressive medium for representing executable
symbolic operations, encoding rich reasoning patterns that remain largely
underutilized. In this paper, we propose Formula Tuning (Fortune), a
reinforcement learning (RL) framework that trains LMs to generate executable
spreadsheet formulas for question answering over general tabular data. Formula
Tuning reduces the reliance on supervised formula annotations by using binary
answer correctness as a reward signal, guiding the model to learn formula
derivation through reasoning. We provide a theoretical analysis of its
advantages and demonstrate its effectiveness through extensive experiments on
seven table reasoning benchmarks. Formula Tuning substantially enhances LM
performance, particularly on multi-step numerical and symbolic reasoning tasks,
enabling a 7B model to outperform OpenAI o1 on table understanding. This
highlights the potential of formula-driven RL to advance symbolic table
reasoning in LMs.

</details>


### [22] [The AI Risk Repository: A Comprehensive Meta-Review, Database, and Taxonomy of Risks From Artificial Intelligence](https://arxiv.org/abs/2408.12622)
*Peter Slattery,Alexander K. Saeri,Emily A. C. Grundy,Jess Graham,Michael Noetel,Risto Uuk,James Dao,Soroush Pour,Stephen Casper,Neil Thompson*

Main category: cs.AI

TL;DR: 该论文创建了一个包含777个AI风险的知识库，并进行了分类，以解决对AI风险缺乏共同理解的问题，从而为AI风险管理奠定基础。


<details>
  <summary>Details</summary>
Motivation: 目前学术界、审计界、政策制定界、AI公司和公众对人工智能（AI）的风险表示担忧，但缺乏对AI风险的共同理解，这阻碍了对其进行全面讨论、研究和应对。

Method: 通过系统性地审查43种AI风险分类法和其他结构化分类，并进行专家咨询，构建了一个包含777个风险的“AI风险知识库”。该知识库可根据两个总体分类法进行筛选，并通过网站和在线电子表格进行访问、修改和更新。研究者还开发了两个AI风险分类法：一个基于因果因素（实体、意图性、时序）的因果分类法，以及一个包含七个领域（歧视与毒性、隐私与安全、错误信息、恶意行为者与滥用、人机交互、社会经济与环境、AI系统安全、故障与局限性）的领域分类法。

Result: 创建了一个“AI风险知识库”，包含777个从43种分类法中提取的AI风险，并提供了两个层级的分类法（因果分类法和领域分类法）以及23个子领域，旨在为AI风险管理提供一个共同的参考框架。

Conclusion: 该研究通过创建一个包含777个AI风险的“AI风险知识库”，并对其进行分类和分析，为更协调、更连贯、更完整地定义、审计和管理AI风险奠定了基础。

Abstract: The risks posed by Artificial Intelligence (AI) are of considerable concern
to academics, auditors, policymakers, AI companies, and the public. However, a
lack of shared understanding of AI risks can impede our ability to
comprehensively discuss, research, and react to them. This paper addresses this
gap by creating an AI Risk Repository to serve as a common frame of reference.
This comprises a living database of 777 risks extracted from 43 taxonomies,
which can be filtered based on two overarching taxonomies and easily accessed,
modified, and updated via our website and online spreadsheets. We construct our
Repository with a systematic review of taxonomies and other structured
classifications of AI risk followed by an expert consultation. We develop our
taxonomies of AI risk using a best-fit framework synthesis. Our high-level
Causal Taxonomy of AI Risks classifies each risk by its causal factors (1)
Entity: Human, AI; (2) Intentionality: Intentional, Unintentional; and (3)
Timing: Pre-deployment; Post-deployment. Our mid-level Domain Taxonomy of AI
Risks classifies risks into seven AI risk domains: (1) Discrimination &
toxicity, (2) Privacy & security, (3) Misinformation, (4) Malicious actors &
misuse, (5) Human-computer interaction, (6) Socioeconomic & environmental, and
(7) AI system safety, failures, & limitations. These are further divided into
23 subdomains. The AI Risk Repository is, to our knowledge, the first attempt
to rigorously curate, analyze, and extract AI risk frameworks into a publicly
accessible, comprehensive, extensible, and categorized risk database. This
creates a foundation for a more coordinated, coherent, and complete approach to
defining, auditing, and managing the risks posed by AI systems.

</details>


### [23] [SpreadsheetLLM: Encoding Spreadsheets for Large Language Models](https://arxiv.org/abs/2407.09025)
*Haoyu Dong,Jianbo Zhao,Yuzhang Tian,Junyu Xiong,Shiyu Xia,Mengyu Zhou,Yun Lin,José Cambronero,Yeye He,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: SpreadsheetLLM introduces SheetCompressor, an efficient encoding method to help LLMs understand spreadsheets better. It significantly improves performance on tasks like table detection and question answering, achieving state-of-the-art results with a 25x compression ratio.


<details>
  <summary>Details</summary>
Motivation: Spreadsheets' complex structure (grids, flexible layouts, varied formatting) poses challenges for LLMs. SpreadsheetLLM aims to optimize LLMs' understanding and reasoning capabilities on spreadsheets through an efficient encoding method.

Method: SpreadsheetLLM uses an encoding framework called SheetCompressor, which includes structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. This framework compresses spreadsheets effectively for LLMs.

Result: SheetCompressor improves spreadsheet table detection by 25.6% compared to a vanilla approach in GPT4's in-context learning setting. Fine-tuned LLM with SheetCompressor achieves a 25x average compression ratio and a state-of-the-art 78.9% F1 score on spreadsheet understanding tasks, outperforming existing models by 12.3%.

Conclusion: SpreadsheetLLM is highly effective across a variety of spreadsheet tasks, including table detection and question answering, by leveraging the inherent layout and structure of spreadsheets.

Abstract: Spreadsheets are characterized by their extensive two-dimensional grids,
flexible layouts, and varied formatting options, which pose significant
challenges for large language models (LLMs). In response, we introduce
SpreadsheetLLM, pioneering an efficient encoding method designed to unleash and
optimize LLMs' powerful understanding and reasoning capability on spreadsheets.
Initially, we propose a vanilla serialization approach that incorporates cell
addresses, values, and formats. However, this approach was limited by LLMs'
token constraints, making it impractical for most applications. To tackle this
challenge, we develop SheetCompressor, an innovative encoding framework that
compresses spreadsheets effectively for LLMs. It comprises three modules:
structural-anchor-based compression, inverse index translation, and
data-format-aware aggregation. It significantly improves performance in the
spreadsheet table detection task, outperforming the vanilla approach by 25.6%
in GPT4's in-context learning setting. Moreover, fine-tuned LLM with
SheetCompressor has an average compression ratio of 25 times, and achieves a
state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%.
Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet
understanding and validate it in a new and demanding spreadsheet QA task. We
methodically leverage the inherent layout and structure of spreadsheets,
demonstrating that SpreadsheetLLM is highly effective across a variety of
spreadsheet tasks.

</details>


### [24] [SheetAgent: Towards A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models](https://arxiv.org/abs/2403.03636)
*Yibin Chen,Yifu Yuan,Zeyu Zhang,Yan Zheng,Jinyi Liu,Fei Ni,Jianye Hao,Hangyu Mao,Fuzheng Zhang*

Main category: cs.AI

TL;DR: SheetAgent是一个强大的自主代理，利用LLM处理复杂的电子表格任务，通过其创新的模块和推理机制，显著提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前大型语言模型在处理具有挑战性的、现实世界的电子表格操作任务（如需要长期、多步骤推理和处理模糊需求）方面的不足，填补与实际需求之间的差距。

Method: 提出了一种名为SheetAgent的自主代理，该代理包含规划师、信息员和检索员三个协作模块，通过迭代式任务推理和反思来实现高级推理和精确的电子表格操作，无需人工干预。

Result: SheetAgent在多个基准测试上比现有方法提高了20%-40%的通过率，在电子表格操作方面实现了更高的精度，并展示了更强的表格推理能力。

Conclusion: SheetAgent通过其独特的模块化设计和迭代式任务推理能力，在处理复杂、现实世界的电子表格操作任务方面表现出色，显著提高了多项基准测试的通过率和准确性，证明了其在电子表格处理和表格推理方面的优越性。

Abstract: Spreadsheets are ubiquitous across the World Wide Web, playing a critical
role in enhancing work efficiency across various domains. Large language model
(LLM) has been recently attempted for automatic spreadsheet manipulation but
has not yet been investigated in complicated and realistic tasks where
reasoning challenges exist (e.g., long horizon manipulation with multi-step
reasoning and ambiguous requirements). To bridge the gap with the real-world
requirements, we introduce SheetRM, a benchmark featuring long-horizon and
multi-category tasks with reasoning-dependent manipulation caused by real-life
challenges. To mitigate the above challenges, we further propose SheetAgent, a
novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of
three collaborative modules: Planner, Informer, and Retriever, achieving both
advanced reasoning and accurate manipulation over spreadsheets without human
interaction through iterative task reasoning and reflection. Extensive
experiments demonstrate that SheetAgent delivers 20--40\% pass rate
improvements on multiple benchmarks over baselines, achieving enhanced
precision in spreadsheet manipulation and demonstrating superior table
reasoning abilities. More details and visualizations are available at the
project website: https://sheetagent.github.io/. The datasets and source code
are available at https://anonymous.4open.science/r/SheetAgent.

</details>


### [25] [Large Language Model for Table Processing: A Survey](https://arxiv.org/abs/2402.05121)
*Weizheng Lu,Jing Zhang,Ju Fan,Zihao Fu,Yueguo Chen,Xiaoyong Du*

Main category: cs.AI

TL;DR: This survey covers table-related tasks, LLM/VLM training, and prompt engineering, highlighting challenges in automating table data processing.


<details>
  <summary>Details</summary>
Motivation: Tables are essential in daily activities, and automating table-centric tasks with LLMs or VLMs garners interest from academia and industry.

Method: This survey provides a comprehensive overview of table-related tasks, examining user scenarios and technical aspects. It summarizes training techniques for LLMs and VLMs tailored for table processing and discusses prompt engineering, including LLM-powered agents.

Result: The survey covers traditional tasks like table question answering and emerging fields such as spreadsheet manipulation and table data analysis, along with training techniques and prompt engineering strategies.

Conclusion: Automating table-centric tasks with LLMs and VLMs offers significant public benefits, with ongoing research in areas like spreadsheet manipulation and table data analysis. Challenges remain, including diverse user input and slow thinking in chain-of-thought processes.

Abstract: Tables, typically two-dimensional and structured to store large amounts of
data, are essential in daily activities like database queries, spreadsheet
manipulations, web table question answering, and image table information
extraction. Automating these table-centric tasks with Large Language Models
(LLMs) or Visual Language Models (VLMs) offers significant public benefits,
garnering interest from academia and industry. This survey provides a
comprehensive overview of table-related tasks, examining both user scenarios
and technical aspects. It covers traditional tasks like table question
answering as well as emerging fields such as spreadsheet manipulation and table
data analysis. We summarize the training techniques for LLMs and VLMs tailored
for table processing. Additionally, we discuss prompt engineering, particularly
the use of LLM-powered agents, for various table-related tasks. Finally, we
highlight several challenges, including diverse user input when serving and
slow thinking using chain-of-thought.

</details>


### [26] [FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language](https://arxiv.org/abs/2310.17306)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Elnaz Nouri,Mohammad Raza,Gust Verbruggen*

Main category: cs.AI

TL;DR: FormaT5是一个AI模型，可以根据自然语言描述自动生成电子表格中的条件格式规则。它通过预测占位符来处理模糊的指令，并能比其他AI方法更好地完成任务，尤其是在处理真实世界的数据时。


<details>
  <summary>Details</summary>
Motivation: 电子表格软件中的条件格式（CF）规则对用户来说难以编写，因为需要理解底层逻辑。FormaT5旨在通过自然语言描述自动生成CF规则，以解决这一挑战。

Method: FormaT5是一个基于Transformer的模型，它通过预测占位符来处理不完整或模糊的用户描述，这些占位符可以由第二个模型或编程示例系统来填充。我们创建了一个包含1053个条件格式任务的基准来评估FormaT5。

Result: FormaT5在包含1053个CF任务的基准测试中，其性能优于8种不同的神经方法，即使在没有示例的情况下也能获得更好的结果。这证明了其在处理不完整用户描述方面的有效性。

Conclusion: FormaT5通过引入占位符和使用两个模型（或一个编程示例系统）来处理不完整的用户描述，在生成的条件格式规则方面优于其他8种神经方法。研究结果强调了构建特定领域学习系统的重要性。

Abstract: Formatting is an important property in tables for visualization,
presentation, and analysis. Spreadsheet software allows users to automatically
format their tables by writing data-dependent conditional formatting (CF)
rules. Writing such rules is often challenging for users as it requires them to
understand and implement the underlying logic. We present FormaT5, a
transformer-based model that can generate a CF rule given the target table and
a natural language description of the desired formatting logic. We find that
user descriptions for these tasks are often under-specified or ambiguous,
making it harder for code generation systems to accurately learn the desired
rule in a single step. To tackle this problem of under-specification and
minimise argument errors, FormaT5 learns to predict placeholders though an
abstention objective. These placeholders can then be filled by a second model
or, when examples of rows that should be formatted are available, by a
programming-by-example system. To evaluate FormaT5 on diverse and real
scenarios, we create an extensive benchmark of 1053 CF tasks, containing
real-world descriptions collected from four different sources. We release our
benchmarks to encourage research in this area. Abstention and filling allow
FormaT5 to outperform 8 different neural approaches on our benchmarks, both
with and without examples. Our results illustrate the value of building
domain-specific learning systems.

</details>


### [27] [Don't Treat the Symptom, Find the Cause! Efficient Artificial-Intelligence Methods for (Interactive) Debugging](https://arxiv.org/abs/2306.12850)
*Patrick Rodler*

Main category: cs.AI

TL;DR: Model-based diagnosis is essential for troubleshooting complex modern systems due to our high dependency on them. This thesis introduces the concept, discusses challenges, and presents research approaches to minimize harm from failures.


<details>
  <summary>Details</summary>
Motivation: Modern systems are highly sophisticated and heavily relied upon, leading to a non-negligible likelihood of failures that can have significant negative effects. Minimizing downtime and repair costs is vital, making model-based diagnosis a crucial approach.

Method: Model-based diagnosis leverages techniques like knowledge representation, automated reasoning, heuristic problem solving, intelligent search, optimization, stochastics, statistics, decision making under uncertainty, machine learning, calculus, combinatorics, and set theory to detect, localize, and fix faults.

Result: The thesis aims to contribute to the understanding and application of model-based diagnosis by discussing challenges and presenting research-based solutions.

Conclusion: The thesis will provide an introduction to model-based diagnosis, highlight key challenges in the field, and discuss research approaches to address these issues.

Abstract: In the modern world, we are permanently using, leveraging, interacting with,
and relying upon systems of ever higher sophistication, ranging from our cars,
recommender systems in e-commerce, and networks when we go online, to
integrated circuits when using our PCs and smartphones, the power grid to
ensure our energy supply, security-critical software when accessing our bank
accounts, and spreadsheets for financial planning and decision making. The
complexity of these systems coupled with our high dependency on them implies
both a non-negligible likelihood of system failures, and a high potential that
such failures have significant negative effects on our everyday life. For that
reason, it is a vital requirement to keep the harm of emerging failures to a
minimum, which means minimizing the system downtime as well as the cost of
system repair. This is where model-based diagnosis comes into play.
  Model-based diagnosis is a principled, domain-independent approach that can
be generally applied to troubleshoot systems of a wide variety of types,
including all the ones mentioned above, and many more. It exploits and
orchestrates i.a. techniques for knowledge representation, automated reasoning,
heuristic problem solving, intelligent search, optimization, stochastics,
statistics, decision making under uncertainty, machine learning, as well as
calculus, combinatorics and set theory to detect, localize, and fix faults in
abnormally behaving systems.
  In this thesis, we will give an introduction to the topic of model-based
diagnosis, point out the major challenges in the field, and discuss a selection
of approaches from our research addressing these issues.

</details>


### [28] [CORNET: Learning Table Formatting Rules By Example](https://arxiv.org/abs/2208.06032)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Mohammad Raza,Gust Verbruggen*

Main category: cs.AI

TL;DR: CORNET is a new system that automatically learns spreadsheet formatting rules from user examples, making it easier for users to format tables effectively.


<details>
  <summary>Details</summary>
Motivation: Writing stylistic formatting rules in spreadsheets is challenging for users due to the need for knowledge of rule languages and data logic. CORNET aims to automatically learn these rules from user examples.

Method: CORNET learns conditional formatting rules by combining symbolic rule enumeration with a neural ranker, taking inspiration from inductive programming and user-provided examples (formatted cells).

Result: The system was evaluated on over 450K unique formatting rules extracted from 1.8M real worksheets. CORNET demonstrated accurate rule learning and outperformed various symbolic and neural baselines.

Conclusion: CORNET can accurately learn rules across varying evaluation setups, finds shorter rules than those written by users, and discovers rules in manually formatted spreadsheets.

Abstract: Spreadsheets are widely used for table manipulation and presentation.
Stylistic formatting of these tables is an important property for both
presentation and analysis. As a result, popular spreadsheet software, such as
Excel, supports automatically formatting tables based on rules. Unfortunately,
writing such formatting rules can be challenging for users as it requires
knowledge of the underlying rule language and data logic. We present CORNET, a
system that tackles the novel problem of automatically learning such formatting
rules from user examples in the form of formatted cells. CORNET takes
inspiration from advances in inductive programming and combines symbolic rule
enumeration with a neural ranker to learn conditional formatting rules. To
motivate and evaluate our approach, we extracted tables with over 450K unique
formatting rules from a corpus of over 1.8M real worksheets. Since we are the
first to introduce conditional formatting, we compare CORNET to a wide range of
symbolic and neural baselines adapted from related domains. Our results show
that CORNET accurately learns rules across varying evaluation setups.
Additionally, we show that CORNET finds shorter rules than those that a user
has written and discovers rules in spreadsheets that users have manually
formatted.

</details>


### [29] [Named Entity Recognition in Industrial Tables using Tabular Language Models](https://arxiv.org/abs/2209.14812)
*Aneta Koleva,Martin Ringsquandl,Mark Buckley,Rakebul Hasan,Volker Tresp*

Main category: cs.AI

TL;DR: Table transformers work well for spreadsheet NER, especially with data augmentation from knowledge graphs. Their structure helps models learn faster when data is limited.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of industrial applications for specialized transformer-based models designed for tabular data, focusing on an industrial NER problem involving spreadsheets. The challenges identified include the technical nature of spreadsheets and the scarcity of labeled data, motivating the development of an augmentation strategy.

Method: A table data augmentation strategy based on domain-specific knowledge graphs was developed and applied to fine-tune transformer-based models for NER on spreadsheets. The benefits of tabular structure as inductive bias were investigated by comparing table transformers with linearized sequence models.

Result: The developed augmentation strategy significantly boosted performance in a low-resource scenario. Experiments confirmed that the table transformer outperformed other baselines, and its tabular inductive bias proved vital for the convergence of transformer-based models.

Conclusion: Transformer-based models for tabular data are effective for industrial NER tasks, especially when combined with a domain-specific knowledge graph augmentation strategy. The tabular inductive bias is crucial for model convergence in low-resource scenarios.

Abstract: Specialized transformer-based models for encoding tabular data have gained
interest in academia. Although tabular data is omnipresent in industry,
applications of table transformers are still missing. In this paper, we study
how these models can be applied to an industrial Named Entity Recognition (NER)
problem where the entities are mentioned in tabular-structured spreadsheets.
The highly technical nature of spreadsheets as well as the lack of labeled data
present major challenges for fine-tuning transformer-based models. Therefore,
we develop a dedicated table data augmentation strategy based on available
domain-specific knowledge graphs. We show that this boosts performance in our
low-resource scenario considerably. Further, we investigate the benefits of
tabular structure as inductive bias compared to tables as linearized sequences.
Our experiments confirm that a table transformer outperforms other baselines
and that its tabular inductive bias is vital for convergence of
transformer-based models.

</details>


### [30] [Spreadsheet computing with Finite Domain Constraint Enhancements](https://arxiv.org/abs/2203.10944)
*Ezana N. Beyenne*

Main category: cs.AI

TL;DR: 本论文扩展了电子表格计算范式，通过集成约束求解器来解决约束满足问题，克服了传统电子表格的局限性。


<details>
  <summary>Details</summary>
Motivation: 电子表格因其易用性而广受欢迎，但其单向数据流限制了其应用范围，主要局限于类似簿记的任务。本论文旨在扩展电子表格计算范式，以解决约束满足问题。

Method: 通过将有限约束求解器无缝集成到电子表格计算范式中，允许单元格绑定到有限域或指定单元格之间关系的约束。该框架提供了一个约束求解接口，并包含一组特定于电子表格的约束来控制大型电子表格应用程序的可扩展性。

Result: 该框架允许单元格包含约束，并通过示例证明了扩展电子表格范式的可用性和实用性。

Conclusion: 本论文提出了一种将有限约束求解器与电子表格计算范式相结合的框架，以克服电子表格在处理约束满足问题方面的局限性。

Abstract: Spreadsheet computing is one of the more popular computing methodologies in
today's modern society. The spreadsheet application's ease of use and
usefulness has enabled non-programmers to perform programming-like tasks in a
familiar setting modeled after the tabular "pen and paper" approach. However,
spreadsheet applications are limited to bookkeeping-like tasks due to their
single-direction data flow. This thesis demonstrates an extension of the
spreadsheet computing paradigm in overcoming this limitation to solve
constraint satisfaction problems. We present a framework seamlessly
incorporating a finite constraint solver with the spreadsheet computing
paradigm. This framework allows the individual cells in the spreadsheet to be
attached to either a finite domain or a constraint specifying the relationship
among the cells. The framework provides an interface for constraint solving and
further enhances the spreadsheet computing paradigm by providing a set of
spreadsheet-specific constraints that will aid in controlling the scalability
of large spreadsheet applications implementations. Finally, we provide examples
to demonstrate the usability and usefulness of the extended spreadsheet
paradigm.
  Keywords: Spreadsheet computing, Constraint Logic Programming, Constraint
satisfaction, Domain-Specific language, Excel, SWI Prolog, C#

</details>


### [31] [Human-Machine Collaboration for Democratizing Data Science](https://arxiv.org/abs/2004.11113)
*Clément Gautrais,Yann Dauxais,Stefano Teso,Samuel Kolb,Gust Verbruggen,Luc De Raedt*

Main category: cs.AI

TL;DR: VisualSynth is a novel framework and system for human-machine collaboration in data science that allows users to interact with spreadsheet software to perform and automate data analysis tasks using colored sketches and artificial intelligence.


<details>
  <summary>Details</summary>
Motivation: Many people want to analyze their data but lack the data science expertise.

Method: VisualSynth relies on the user providing colored sketches to partially specify data science tasks, which are then determined and executed using artificial intelligence techniques.

Result: VisualSynth allows users to perform and automate various data analysis tasks ranging from data wrangling, data selection, clustering, constraint learning, predictive modeling and auto-completion.

Conclusion: VisualSynth can democratize data science by allowing users to interact with standard spreadsheet software to perform and automate various data analysis tasks.

Abstract: Everybody wants to analyse their data, but only few posses the data science
expertise to to this. Motivated by this observation we introduce a novel
framework and system \textsc{VisualSynth} for human-machine collaboration in
data science.
  It wants to democratize data science by allowing users to interact with
standard spreadsheet software in order to perform and automate various data
analysis tasks ranging from data wrangling, data selection, clustering,
constraint learning, predictive modeling and auto-completion.
\textsc{VisualSynth} relies on the user providing colored sketches, i.e.,
coloring parts of the spreadsheet, to partially specify data science tasks,
which are then determined and executed using artificial intelligence
techniques.

</details>


### [32] [Automated Discovery of Data Transformations for Robotic Process Automation](https://arxiv.org/abs/2001.01007)
*Volodymyr Leno,Marlon Dumas,Marcello La Rosa,Fabrizio Maria Maggi,Artem Polyvyanyy*

Main category: cs.AI

TL;DR: 本文提出了一种从UI日志中发现RPA数据传输例程的优化方法，解决了现有技术效率不高的问题。


<details>
  <summary>Details</summary>
Motivation: 为了充分利用机器人流程自动化（RPA）提供的机会，公司需要发现哪些具体的例程可以自动化以及如何自动化。

Method: 将用户界面（UI）日志分析的问题映射到通过示例发现数据转换的问题，并提出了利用UI日志信息和跨应用程序数据传输涉及单独复制字母和数字标记的事实的两种优化方法。

Result: 所提出的方法和优化能够更有效地从UI日志中发现数据传输例程。

Conclusion: 本文提出了一种用于从用户界面日志中发现跨应用程序数据传输例程的优化方法，并通过UI日志进行了评估。

Abstract: Robotic Process Automation (RPA) is a technology for automating repetitive
routines consisting of sequences of user interactions with one or more
applications. In order to fully exploit the opportunities opened by RPA,
companies need to discover which specific routines may be automated, and how.
In this setting, this paper addresses the problem of analyzing User Interaction
(UI) logs in order to discover routines where a user transfers data from one
spreadsheet or (Web) form to another. The paper maps this problem to that of
discovering data transformations by example - a problem for which several
techniques are available. The paper shows that a naive application of a
state-of-the-art technique for data transformation discovery is computationally
inefficient. Accordingly, the paper proposes two optimizations that take
advantage of the information in the UI log and the fact that data transfers
across applications typically involve copying alphabetic and numeric tokens
separately. The proposed approach and its optimizations are evaluated using UI
logs that replicate a real-life repetitive data transfer routine.

</details>


### [33] [Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples](https://arxiv.org/abs/1804.01186)
*Ashwin Kalyan,Abhishek Mohta,Oleksandr Polozov,Dhruv Batra,Prateek Jain,Sumit Gulwani*

Main category: cs.AI

TL;DR: NGDS是一种结合了符号逻辑和深度学习的程序合成技术，它通过神经引导归纳搜索，在处理用户提供的输入输出示例时，能够实现比现有技术更快的合成速度和更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有合成系统在依赖手工设计的演绎逻辑技术或需要海量数据的统计模型时存在的不足，即无法在具有挑战性的基准上实现实时合成。

Method: 提出了一种名为神经引导归纳搜索（NGDS）的混合合成技术，该技术结合了符号逻辑技术和统计模型。该技术利用归纳搜索框架来简化神经组件的学习问题，将其转化为监督学习设置，从而可以在数据稀疏的情况下进行训练，并利用循环神经网络编码器。

Result: NGDS技术能够生成满足给定规范且能很好地泛化到未见示例的程序，并且在实际客户场景中，合成准确程序的速[度]比现有最先进的系统快12倍。

Conclusion: NGDS技术通过结合符号逻辑和统计模型，在合成用户程序方面表现出色，能够在实际应用中实现高达12倍的加速。

Abstract: Synthesizing user-intended programs from a small number of input-output
examples is a challenging problem with several important applications like
spreadsheet manipulation, data wrangling and code refactoring. Existing
synthesis systems either completely rely on deductive logic techniques that are
extensively hand-engineered or on purely statistical models that need massive
amounts of data, and in general fail to provide real-time synthesis on
challenging benchmarks. In this work, we propose Neural Guided Deductive Search
(NGDS), a hybrid synthesis technique that combines the best of both symbolic
logic techniques and statistical models. Thus, it produces programs that
satisfy the provided specifications by construction and generalize well on
unseen examples, similar to data-driven systems. Our technique effectively
utilizes the deductive search framework to reduce the learning problem of the
neural component to a simple supervised learning setup. Further, this allows us
to both train on sparingly available real-world data and still leverage
powerful recurrent neural network encoders. We demonstrate the effectiveness of
our method by evaluating on real-world customer scenarios by synthesizing
accurate programs with up to 12x speed-up compared to state-of-the-art systems.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [34] [Polynomial-Time Proactive Synthesis of Tree-to-String Functions from Examples](https://arxiv.org/abs/1701.04288)
*Mikaël Mayer,Jad Hamza,Viktor Kuncak*

Main category: cs.FL

TL;DR: 该论文研究了从示例中合成将代数数据类型映射到字符串的递归函数。它将问题形式化为学习单状态树到字符串变换器，并提出了一种即使在NP-完全的一般情况下也可以有效解决的方法，尤其是在主动学习场景中。


<details>
  <summary>Details</summary>
Motivation: 为了使非专业用户能够通过指定行为示例来生成程序，该研究致力于合成技术的基础，特别是用于生成将代数数据类型映射到字符串的递归函数。

Method: 该研究将问题形式化为学习具有单状态的确定性顺序自顶向下树到字符串变换器。研究了从用户提供的输入/输出示例中学习树到字符串变换器的问题，并证明了该问题通常是NP-完全的，但在某些有用的闭包条件下可以多项式时间内解决。此外，还研究了一种主动学习场景，算法选择输入，用户提供输出，以在代数数据类型定义的函数大小的线性数量的查询内唯一确定变换器。

Result: 在某些有用的闭包条件下，可以从输入/输出示例中学习树到字符串变换器（NP-完全问题）。在主动学习场景中，该算法可以在代数数据类型定义的函数大小的线性数量的查询内确定地学习变换器。

Conclusion: 该研究为通过结构递归在代数数据类型上定义的一类函数（将代数数据类型映射到字符串）的合成提供了基础。

Abstract: Synthesis from examples enables non-expert users to generate programs by
specifying examples of their behavior. A domain-specific form of such synthesis
has been recently deployed in a widely used spreadsheet software product. In
this paper we contribute to foundations of such techniques and present a
complete algorithm for synthesis of a class of recursive functions defined by
structural recursion over a given algebraic data type definition. The functions
we consider map an algebraic data type to a string; they are useful for, e.g.,
pretty printing and serialization of programs and data. We formalize our
problem as learning deterministic sequential top-down tree-to-string
transducers with a single state.
  The first problem we consider is learning a tree-to-string transducer from
any set of input/output examples provided by the user. We show that this
problem is NP-complete in general, but can be solved in polynomial time under a
(practically useful) closure condition that each subtree of a tree in the
input/output example set is also part of the input/output examples.
  Because coming up with relevant input/output examples may be difficult for
the user while creating hard constraint problems for the synthesizer, we also
study a more automated active learning scenario in which the algorithm chooses
the inputs for which the user provides the outputs. Our algorithm asks a
worst-case linear number of queries as a function of the size of the algebraic
data type definition to determine a unique transducer.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [35] [LEI2JSON: Schema-based Validation and Conversion of Livestock Event Information](https://arxiv.org/abs/2310.17414)
*Mahir Habib,Muhammad Ashad Kabir,Lihong Zheng*

Main category: eess.SY

TL;DR: LEI2JSON是一个谷歌表格附加组件，可以帮助家畜生产者使用LEI模式标准化他们的事件数据，并将其转换为JSON格式。


<details>
  <summary>Details</summary>
Motivation: 该工具旨在为家畜生产者提供一种高效的机制来标准化其数据，从而节省大量时间和资源。

Method: LEI2JSON通过构建包含适当列标题、注释和验证规则的电子表格模板，将电子表格数据转换为JSON格式，并根据LEI模式验证输出来实现其目标。

Result: LEI2JSON促进了家畜事件信息的无缝本地存储或在谷歌云端硬盘中以JSON格式存储。此外，该工具的有效性已通过广泛的实验评估得到证明。

Conclusion: LEI2JSON是一个创新的解决方案，它作为一个附加组件，可以与谷歌表格一起使用，以帮助家畜生产者标准化他们的数据。

Abstract: Livestock producers often need help in standardising (i.e., converting and
validating) their livestock event data. This article introduces a novel
solution, LEI2JSON (Livestock Event Information To JSON). The tool is an add-on
for Google Sheets, adhering to the livestock event information (LEI) schema.
The core objective of LEI2JSON is to provide livestock producers with an
efficient mechanism to standardise their data, leading to substantial savings
in time and resources. This is achieved by building the spreadsheet template
with the appropriate column headers, notes, and validation rules, converting
the spreadsheet data into JSON format, and validating the output against the
schema. LEI2JSON facilitates the seamless storage of livestock event
information locally or on Google Drive in JSON. Additionally, we have conducted
an extensive experimental evaluation to assess the effectiveness of the tool.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [36] [Using a Catenary Trajectory to Reduce Wellbore Friction in Horizontal Extended Reach Drilling](https://arxiv.org/abs/2309.12317)
*Vu Nguyen*

Main category: cs.RO

TL;DR: The paper discusses using the catenary concept to reduce drill string friction in wells. It compares this method to traditional designs using a case study and provides an Excel tool for calculations, aiming to minimize drilling costs.


<details>
  <summary>Details</summary>
Motivation: Wellbore friction is a major concern in drilling due to its impact on total cost. The catenary concept offers a potential solution to reduce this friction, but requires detailed analysis.

Method: The paper introduces the catenary concept as a method to reduce wellbore friction by allowing the drill string to hang freely in the wellbore, minimizing contact and thus friction, torque, and drag. A case study is presented to compare this design with the traditional 2D Arc design.

Result: A case study is introduced to examine the outcome between Catenary Trajectory Design and traditional 2D Arc design. The calculations for both designs are available in an MS Excel spreadsheet.

Conclusion: This project aims to fill the gap in detailed analyses of the catenary concept for reducing wellbore friction. A case study compares Catenary Trajectory Design with traditional 2D Arc design, using an MS Excel spreadsheet for calculations to design catenary well trajectories for extended-reach wells.

Abstract: Wellbore friction is one of the biggest concerns when drilling due to its
relation to the total cost. The catenary concept was introduced to reduce
wellbore friction, but it requires detailed analyses. This project would fill
this gap. A catenary shape is simply the natural shape of a rope, chain, or
drill string. The drill string will then hang freely inside the wellbore.
Perfectly, there should be no contact between the hole and the string, and thus
no friction. Torque and drag should be minimized this way. A case study is
introduced to examine the outcome between Catenary Trajectory Design and
traditional 2D Arc design. The calculation procedure of Catenary Trajectory and
2D Arc Design can be found in an MS Excel spreadsheet which is easy to use and
reliable for designing catenary well trajectories for extended-reach wells.

</details>


### [37] [Hazard Analysis of Collaborative Automation Systems: A Two-layer Approach based on Supervisory Control and Simulation](https://arxiv.org/abs/2209.12560)
*Tom P. Huck,Yuvaraj Selvaraj,Constantin Cronrath,Christoph Ledermann,Martin Fabian,Bengt Lennartson,Torsten Kröger*

Main category: cs.RO

TL;DR: 针对复杂安全关键系统，提出一种结合形式化方法和仿真方法（基于监督控制理论）的两层危险分析新方法，并以人机协作系统为例进行了验证。


<details>
  <summary>Details</summary>
Motivation: 现有的基于人为推理、经验和简单工具（如清单和电子表格）的危险分析方法，在面对日益复杂的系统时显得越来越不适用。而基于测试的危险分析方法由于成本高昂或物理故障的危险性而不适合。因此，需要一种新的方法来应对这些挑战。

Method: 提出一种两层方法，首先使用监督控制理论从形式化模型合成导致不安全状态的不安全行为，然后将结果输入仿真环境，并使用特定领域的风险指标进行详细分析。

Result: 该方法能够综合利用形式化方法的穷尽分析能力和仿真方法的详细分析能力，对复杂系统进行有效的危险分析。

Conclusion: 该方法结合了形式化方法和仿真方法的优点，能够对复杂系统进行详尽且细致的安全分析，并已在人机协作系统中得到验证。

Abstract: Safety critical systems are typically subjected to hazard analysis before
commissioning to identify and analyse potentially hazardous system states that
may arise during operation. Currently, hazard analysis is mainly based on human
reasoning, past experiences, and simple tools such as checklists and
spreadsheets. Increasing system complexity makes such approaches decreasingly
suitable. Furthermore, testing-based hazard analysis is often not suitable due
to high costs or dangers of physical faults. A remedy for this are model-based
hazard analysis methods, which either rely on formal models or on simulation
models, each with their own benefits and drawbacks. This paper proposes a
two-layer approach that combines the benefits of exhaustive analysis using
formal methods with detailed analysis using simulation. Unsafe behaviours that
lead to unsafe states are first synthesised from a formal model of the system
using Supervisory Control Theory. The result is then input to the simulation
where detailed analyses using domain-specific risk metrics are performed.
Though the presented approach is generally applicable, this paper demonstrates
the benefits of the approach on an industrial human-robot collaboration system.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [38] [Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!](https://arxiv.org/abs/2309.02110)
*James P. Dilger*

Main category: math.HO

TL;DR: Wordle 玩家数据分析揭示了作弊、对起始词的忠诚度以及玩家易受外部因素影响的现象。


<details>
  <summary>Details</summary>
Motivation: 为了深入了解 Wordle 玩家的行为模式，特别是为了量化分析玩家的作弊行为、对起始词的偏好以及受外部因素影响的程度，超越了传统的社交媒体和调查方法。

Method: 通过收集和分析 2023 年 5 月至 8 月的 Wordle 玩家起始猜测词数据，利用信息论评估玩家的运气和技巧。

Result: 研究发现，约有 0.2-0.5% 的玩家在一局游戏中成功，这表明可能存在作弊行为；至少有三分之一的玩家有固定的起始词，并且即使该词已作为目标词出现过，他们仍继续使用；在 2023 年 8 月 15 日，约有 30,000 名玩家突然更换了起始词，这可能与填字游戏线索有关，证明玩家易受影响。

Conclusion: 该研究通过分析 Wordle 玩家的起始猜测词数据，揭示了玩家行为的几个关键方面，包括作弊现象、玩家对起始词的偏好以及受外部因素影响的可能性。

Abstract: Wordle is a popular, online word game offered by the New York Times
(nytimes.com). Currently there are some 2 million players of the English
version worldwide. Players have 6 attempts to guess the daily word (target
word) and after each attempt, the player receives color-coded information about
the correctness and position of each letter in the guess. After either a
successful completion of the puzzle or the final unsuccessful attempt, software
can assess the player's luck and skill using Information Theory and can display
data for the first, second, ..., sixth guesses of a random sample of all
players. Recently, I discovered that the latter data is presented in a format
that can easily be copied and pasted into a spreadsheet. I compiled data on
Wordle players' first guesses from May 2023 - August 2023 and inferred some
interesting information about Wordle players. A) Every day, about 0.2-0.5% of
players solve the puzzle in one attempt. Because the odds of guessing the one
of 2,315 possible target words at random is 0.043%, this implies that 4,000 -
10,000 players cheat by obtaining the target word outside of playing the game!
B) At least 1/3 of the players have a favorite starting word, or cycle through
several. And even though players should be aware that target words are never
repeated, most players appear to remain loyal to their starting word even after
its appearance as a target word. C) On August 15, 2023, about 30,000 players
abruptly changed their starting word, presumably based on a crossword puzzle
clue! Wordle players can be influenced! This study goes beyond social media
postings, surveys, and Google Trends to provide solid, quantitative evidence
about cheating in Wordle.

</details>


### [39] [GeoGebra e situações que envolvem modelação numa abordagem STEAM](https://arxiv.org/abs/1907.02099)
*J. M. D. S. Dos Santos,A. P. Silveira,A. E. S. Trocado*

Main category: math.HO

TL;DR: This paper presents GeoGebra-based STEAM activities for mathematics education, initially for teacher training in lusophone countries, with plans for wider adaptation and translation.


<details>
  <summary>Details</summary>
Motivation: To implement a STEAM approach in mathematics classes within the lusophone space by integrating interactive software like GeoGebra, initially targeting teacher training and subsequently student application.

Method: The paper introduces tasks that utilize GeoGebra software for modeling and analyzing 2D and 3D geometric problems. It details the use of various GeoGebra windows (2D, 3D, CAS, spreadsheet) and provides supporting scripts with tool and command indications.

Result: The developed tasks facilitate the exploration of geometric concepts using GeoGebra, supporting a STEAM approach and demonstrating the software's capabilities in analyzing geometric problems. These tasks are part of ongoing training courses sponsored by the Organization of Ibero-American States.

Conclusion: The tasks are designed to be adaptable for various users regardless of their GeoGebra knowledge, fostering connections with other STEAM disciplines and allowing for project-based learning. They have a broad impact, with plans for translation into Spanish and English.

Abstract: In order to implement a STEAM approach including the use of technology,
namely the use of interactive mathematics software GeoGebra, in mathematics
classes, in the lusophone space, the materials presented here were conceived,
to be implemented in a first phase among teachers. Later, with the necessary
adaptations, these tasks will be applied to the students. The tasks deal with
modeling situations, in two- and three-dimensional geometric problems, in order
to apply GeoGebra software in its analysis to illustrate its capabilities. The
different windows of this software are used, namely the 2D and 3D windows, CAS
window, spreadsheet and extra two dimensional windows in order to study cutting
planes in solids and some surfaces. The tasks are presented so that any user,
regardless of the degree of knowledge they have of the software, can follow
them, being supported in scripts with some indications of the tools and
commands to use. Designed for the teaching and learning of Mathematics, from a
STEAM approach, these tasks allow connections with other Sciences and the Arts,
and allow the development of projects using and consolidating relevant
mathematical contents. These tasks are part of the proposals of activities of
the participants of the Training Courses for Trainers in GeoGebra for
Portuguese Speaking Countries, which from 2019 have an impact on the STEAM
approach. These courses are carried out with the high sponsorship of the
Organization of Ibero-American States for Education, Science and Culture (OEI).
Given the interest that the tasks have for the users of the Iberian space, as
well as their dissemination at a global level, the materials initially
developed in Portuguese language will be adapted for Spanish and English
speakers.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [40] [The Paradox of Spreadsheet Self-Efficacy: Social Incentives for Informal Knowledge Sharing in End-User Programming](https://arxiv.org/abs/2408.08068)
*Qing,Xia,Advait Sarkar,Duncan P. Brumby,Anna Cox*

Main category: cs.HC

TL;DR: 本研究调查了100名电子表格用户，发现自我效能感和声誉收益能提升知识共享意愿，而知识编纂的困难会降低这种意愿。用户普遍在通用电子表格技能上自信心不足，但在工作场景则不然。研究建议在电子表格设计中考虑这些因素。


<details>
  <summary>Details</summary>
Motivation: 为了深入理解个人、社交和软件相关因素如何影响终端用户程序员（特别是电子表格用户）在知识共享（KS）方面的意愿，以提升他们的专业能力。

Method: 本研究通过对100名行政和财务领域的电子表格用户进行问卷调查，并运用多元回归分析的方法，探讨了个人（自我效能感）、社交（声誉收益、同事间的信任）和软件相关（知识编纂的努力程度）等变量对电子表格知识共享意愿的影响。

Result: 研究发现，较高的电子表格自我效能感和认为分享知识能带来声誉收益是知识共享意愿的积极预测因子，而认为知识编纂费力的用户则表现出较低的知识共享意愿。此外，无论何种职业，用户普遍报告在通用电子表格熟练度方面的自我效能感较低，但在工作相关的情境下自我效能感较高。

Conclusion: 研究结果表明，提高个人在电子表格方面的自我效能感，并强调分享知识带来的声誉效益，可以有效提升知识共享的意愿。同时，降低知识编纂的门槛也能促进知识共享。此外，大多数用户倾向于认为自己在通用电子表格熟练度方面的自我效能感较低，但在工作相关场景下则表现出较高的自我效能感。这些发现对电子表格的设计具有启示意义，应重视并设计相应的社交和个人因素，以避免有经验的用户不必要地回避分享。

Abstract: Informal Knowledge Sharing (KS) is vital for end-user programmers to gain
expertise. To better understand how personal (self-efficacy), social
(reputational gains, trust between colleagues), and software-related
(codification effort) variables influence spreadsheet KS intention, we
conducted a multiple regressions analysis based on survey data from spreadsheet
users (n=100) in administrative and finance roles. We found that high levels of
spreadsheet self-efficacy and a perception that sharing would result in
reputational gains predicted higher KS intention, but individuals who found
knowledge codification effortful showed lower KS intention. We also observed
that regardless of occupation, users tended to report a lower sense of
self-efficacy in their general spreadsheet proficiency, despite also reporting
high self-efficacy in spreadsheet use for job-related contexts. Our findings
suggest that acknowledging and designing for these social and personal
variables can help avoid situations where experienced individuals refrain
unnecessarily from sharing, with implications for spreadsheet design.

</details>


### [41] [Buckaroo: A Direct Manipulation Visual Data Wrangler](https://arxiv.org/abs/2507.16073)
*Annabelle Warner,Andrew McNutt,Paul Rosen,El Kindi Rezig*

Main category: cs.HC

TL;DR: Buckaroo是一个创新的可视化系统，旨在通过自动化和可视化操作简化数据整理过程，提高数据质量和处理效率。


<details>
  <summary>Details</summary>
Motivation: 传统的数据整理方法（手动编码或电子表格）劳动密集且容易出错，影响下游任务的数据质量。本文旨在解决这些挑战。

Method: Buckaroo通过（1）自动查找异常数据组并推荐检查；（2）建议用户修复异常的操作；（3）通过显示整理操作的效果并提供撤销或重做功能，支持用户进行可视化数据整理。

Result: Buckaroo能够高亮显示数据中的差异，并通过直接操作视觉对象进行即时修复，支持迭代式数据整理。

Conclusion: Buckaroo是一个可视化系统，可以通过直接操作视觉对象来修复数据差异，从而解决手动数据整理的痛点。

Abstract: Preparing datasets -- a critical phase known as data wrangling -- constitutes
the dominant phase of data science development, consuming upwards of 80% of the
total project time. This phase encompasses a myriad of tasks: parsing data,
restructuring it for analysis, repairing inaccuracies, merging sources,
eliminating duplicates, and ensuring overall data integrity. Traditional
approaches, typically through manual coding in languages such as Python or
using spreadsheets, are not only laborious but also error-prone. These issues
range from missing entries and formatting inconsistencies to data type
inaccuracies, all of which can affect the quality of downstream tasks if not
properly corrected. To address these challenges, we present Buckaroo, a
visualization system to highlight discrepancies in data and enable on-the-spot
corrections through direct manipulations of visual objects. Buckaroo (1)
automatically finds "interesting" data groups that exhibit anomalies compared
to the rest of the groups and recommends them for inspection; (2) suggests
wrangling actions that the user can choose to repair the anomalies; and (3)
allows users to visually manipulate their data by displaying the effects of
their wrangling actions and offering the ability to undo or redo these actions,
which supports the iterative nature of data wrangling. A video companion is
available at https://youtu.be/iXdCYbvpQVE

</details>


### [42] [SheetMind: An End-to-End LLM-Powered Multi-Agent Framework for Spreadsheet Automation](https://arxiv.org/abs/2506.12339)
*Ruiyan Zhu,Xi Cheng,Ke Liu,Brian Zhu,Daniel Jin,Neeraj Parihar,Zhoutian Xu,Oliver Gao*

Main category: cs.HC

TL;DR: SheetMind 是一个基于 LLMs 的多智能体框架，用于通过自然语言自动化电子表格。它使用三个智能体（管理器、动作、反思）将指令分解、转换为命令并进行验证。SheetMind 集成到 Google Sheets 中，无需用户具备脚本或公式知识。实验显示其在单步任务上成功率为 80%，多步任务上成功率为 70%。


<details>
  <summary>Details</summary>
Motivation: SheetMind 旨在通过自然语言指令实现电子表格自动化，解决用户在电子表格操作中遇到的挑战。

Method: SheetMind 是一个模块化的多智能体框架，利用大型语言模型（LLMs）通过自然语言指令实现电子表格自动化。该系统包含三个智能体：负责将复杂用户指令分解为子任务的管理器智能体；使用巴科斯范式（BNF）语法将子任务转换为结构化命令的动作智能体；以及负责验证生成动作与用户原始意图之间一致性的反思智能体。SheetMind 通过 Workspace 扩展集成到 Google Sheets 中，支持实时交互，无需编写脚本或了解公式。

Result: SheetMind 在基准数据集上的实验表明，在单步任务上的成功率为 80%，在多步指令上的成功率约为 70%，优于经过剥离和基线变体。

Conclusion: SheetMind 通过多智能体分解和基于语法的执行，在连接自然语言和电子表格功能方面效果显著。

Abstract: We present SheetMind, a modular multi-agent framework powered by large
language models (LLMs) for spreadsheet automation via natural language
instructions. The system comprises three specialized agents: a Manager Agent
that decomposes complex user instructions into subtasks; an Action Agent that
translates these into structured commands using a Backus Naur Form (BNF)
grammar; and a Reflection Agent that validates alignment between generated
actions and the user's original intent. Integrated into Google Sheets via a
Workspace extension, SheetMind supports real-time interaction without requiring
scripting or formula knowledge. Experiments on benchmark datasets demonstrate
an 80 percent success rate on single step tasks and approximately 70 percent on
multi step instructions, outperforming ablated and baseline variants. Our
results highlight the effectiveness of multi agent decomposition and grammar
based execution for bridging natural language and spreadsheet functionalities.

</details>


### [43] ["How do you even know that stuff?": Barriers to expertise sharing among spreadsheet users](https://arxiv.org/abs/2506.09216)
*Qing,Xia,Advait Sarkar,Duncan Brumby,Anna Cox*

Main category: cs.HC

TL;DR: 电子表格的分享行为受到技术设计和社交动态的复杂交互影响，这给协作学习带来了挑战。


<details>
  <summary>Details</summary>
Motivation: 分享专业知识对于在组织内保留重要的技术技能至关重要，但以往的研究表明，电子表格专家往往未能将其知识传播给他人。我们认为，围绕电子表格使用价值的社会规范和信念显著影响用户参与分享行为。

Method: 通过对来自两个不同样本的31位专业电子表格用户进行半结构化访谈。

Result: 电子表格提供者在制定高度个性化的策略以适应主观标准和评估适当的社交时机方面面临挑战。此外，对自身电子表格专业知识的矛盾评价、对这些知识价值的轻视性规范性信念以及对协作潜在干扰的担忧会进一步阻碍分享。这些观察结果反映了为满足主要以初始可学性设计的富含功能性软件而进行的长期学习所面临的挑战。

Conclusion: 电子表格的复杂交互技术设计和社交媒体塑造了人们在富含功能性软件中的协作学习行为，尤其是在电子表格方面。

Abstract: Spreadsheet collaboration provides valuable opportunities for learning and
expertise sharing between colleagues. Sharing expertise is essential for the
retention of important technical skillsets within organisations, but previous
studies suggest that spreadsheet experts often fail to disseminate their
knowledge to others. We suggest that social norms and beliefs surrounding the
value of spreadsheet use significantly influence user engagement in sharing
behaviours. To explore this, we conducted 31 semi-structured interviews with
professional spreadsheet users from two separate samples. We found that
spreadsheet providers face challenges in adapting highly personalised
strategies to often subjective standards and evaluating the appropriate social
timing of sharing. In addition, conflicted self-evaluations of one's
spreadsheet expertise, dismissive normative beliefs about the value of this
knowledge, and concerns about the potential disruptions associated with
collaboration can further deter sharing. We suggest these observations reflect
the challenges of long-term learning in feature-rich software designed
primarily with initial learnability in mind. We therefore provide implications
for design to navigate this tension. Overall, our findings demonstrate how the
complex interaction between technology design and social dynamics can shape
collaborative learning behaviours in the context of feature-rich software.

</details>


### [44] [Prompting in the Dark: Assessing Human Performance in Prompt Engineering for Data Labeling When Gold Labels Are Absent](https://arxiv.org/abs/2502.11267)
*Zeyu He,Saniya Naphade,Ting-Hao 'Kenneth' Huang*

Main category: cs.HC

TL;DR: 用户在没有参考标签的情况下，通过反复修改提示来指导大型语言模型（LLM）进行数据标注，效果不佳。即使是自动化工具也难以在缺乏参考标签的情况下取得好效果。这说明参考标签很重要，但同时设计工具来帮助用户改进提示也是有益的，不过要小心潜在的风险。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决在没有黄金标准标签的情况下，用户在提示大型语言模型（LLM）工程方面的能力问题，特别是用户通过多轮迭代提示是否能更接近期望结果。这对于LLM驱动的数据标注场景至关重要，因为手动标注的基准可能不可用。

Method: 本研究通过一个包含20名参与者的研究，调查了在LLM赋能的数据标注场景中“暗中提示”的有效性。研究人员开发了一个名为PromptingSheet的Google表格插件，允许用户通过电子表格撰写、修改和迭代标注数据。同时，研究还评估了像DSPy这样的自动化提示优化工具在黄金标签有限情况下的表现。

Result: 研究发现，“暗中提示”的效率非常低下且不可靠，只有20名参与者中的9名（不到一半）在四次或更多次的迭代后提高了标注准确率。此外，即使是像DSPy这样的自动化提示优化工具，在黄金标签数量有限的情况下也遇到了困难。这些结果表明，在提示工程中，黄金标签至关重要，并揭示了自动化支持的需求和风险。

Conclusion: 在没有人工标注基准的情况下，用户迭代式提示大型语言模型（LLM）进行数据标注（“暗中提示”）的效率低下且不可靠，只有不到一半的参与者在四次或更多次迭代后提高了标注准确率。像DSPy这样的自动化提示优化工具在缺乏黄金标签时也面临挑战。研究强调了黄金标签的重要性，以及在未来工具设计中对自动化支持的需求和风险。

Abstract: Millions of users prompt large language models (LLMs) for various tasks, but
how good are people at prompt engineering? Do users actually get closer to
their desired outcome over multiple iterations of their prompts? These
questions are crucial when no gold-standard labels are available to measure
progress. This paper investigates a scenario in LLM-powered data labeling,
"prompting in the dark," where users iteratively prompt LLMs to label data
without using manually-labeled benchmarks. We developed PromptingSheet, a
Google Sheets add-on that enables users to compose, revise, and iteratively
label data through spreadsheets. Through a study with 20 participants, we found
that prompting in the dark was highly unreliable-only 9 participants improved
labeling accuracy after four or more iterations. Automated prompt optimization
tools like DSPy also struggled when few gold labels were available. Our
findings highlight the importance of gold labels and the needs, as well as the
risks, of automated support in human prompt engineering, providing insights for
future tool design.

</details>


### [45] [When Copilot Becomes Autopilot: Generative AI's Critical Risk to Knowledge Work and a Critical Solution](https://arxiv.org/abs/2412.15030)
*Advait Sarkar,Xiaotong,Xu,Neil Toronto,Ian Drosos,Christian Poelitz*

Main category: cs.HC

TL;DR: 生成式AI在提高效率的同时，也可能削弱人类的批判性思维。本文提出设计AI接口以培养批判性思维，并展示了一个在电子表格中通过“挑衅”来批判AI筛选标准的系统原型，为AI作为批评者的研究开辟了新方向。


<details>
  <summary>Details</summary>
Motivation: 生成式AI可能通过引入错误来危害知识工作，但也可能为用户（尤其是非专家）提供前所未有的机会来学习和应用高级软件功能，从而扩大他们能够成功完成的任务的范围和复杂性。本文认为，生成式AI对电子表格工作流的最大风险不是AI幻觉，而是随着更多工作可以安全地委托给AI，人类的批判性思维能力可能会在过程中退化。

Method: 本文提出了一个利用生成式AI的系统，该系统能够提出筛选数据透视表的标准，并根据这些标准对数据透视表中的行进行排序。此外，该系统还能生成“挑衅”性文本，用于批判AI生成的标准，指出其风险、不足之处以及替代方案。

Result: 本文讨论了一个用于电子表格中关键筛选活动的系统原型，该原型利用生成式AI提出筛选标准并应用这些标准对电子表格中的行进行排序，同时生成“挑衅”来批判AI生成的标准。该原型为现代AI辅助知识工作开启了一个丰富且完全未被探索的关键性思维工具的设计空间。

Conclusion: 设计生成式AI的接口以培养和鼓励知识工作中的批判性思维，可以利用教育领域对批判性思维工具的深入研究。本文提出的原型系统为AI辅助的知识工作探索了一个新的设计空间，并提出了关于AI作为批评者或挑衅者的研究议程。

Abstract: Generative AI, with its tendency to "hallucinate" incorrect results, may pose
a risk to knowledge work by introducing errors. On the other hand, it may also
provide unprecedented opportunities for users, particularly non-experts, to
learn and apply advanced software features and greatly increase the scope and
complexity of tasks they can successfully achieve.
  As an example of a complex knowledge workflow that is subject to risks and
opportunities from generative AI, we consider the spreadsheet. AI
hallucinations are an important challenge, but they are not the greatest risk
posed by generative AI to spreadsheet workflows. Rather, as more work can be
safely delegated to AI, the risk is that human critical thinking -- the ability
to holistically and rigorously evaluate a problem and its solutions -- is
degraded in the process. The solution is to design the interfaces of generative
AI systems deliberately to foster and encourage critical thinking in knowledge
work, building primarily on a long history of research on critical thinking
tools for education.
  We discuss a prototype system for the activity of critical shortlisting in
spreadsheets. The system uses generative AI to suggest shortlisting criteria
and applies these criteria to sort rows in a spreadsheet. It also generates
"provocations": short text snippets that critique the AI-generated criteria,
highlighting risks, shortcomings, and alternatives. Our prototype opens up a
rich and completely unexplored design space of critical thinking tools for
modern AI-assisted knowledge work. We outline a research agenda for AI as a
critic or provocateur, including questions about where and when provocations
should appear, their form and content, and potential design trade-offs.

</details>


### [46] [Understanding and Evaluating Trust in Generative AI and Large Language Models for Spreadsheets](https://arxiv.org/abs/2412.14062)
*Simon Thorne*

Main category: cs.HC

TL;DR: A framework is proposed to ensure the trustworthiness of AI-generated spreadsheet formulas by evaluating their transparency and dependability, addressing issues like hallucinations and bias.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of accuracy and trustworthiness in spreadsheet formulas generated by Generative AI and LLMs, stemming from issues like hallucinations, bias, and varying user skills.

Method: The paper proposes a trustworthiness framework for evaluating AI-generated spreadsheet formulas. This framework assesses transparency through explainability and visibility, and dependability through reliability and ethical considerations. It also examines factors influencing these metrics, including hallucinations, training data bias, and prompt construction.

Result: The paper examines the transparency and dependability of AI-generated formulas, considering factors like hallucinations, bias, and prompt quality. It also explores examples and consequences of mistrust in technology.

Conclusion: Generative AI, particularly LLMs, has the potential to automate spreadsheet formula creation, but its outputs require careful evaluation due to issues like hallucinations and bias. The proposed trustworthiness framework, focusing on transparency (explainability and visibility) and dependability (reliability and ethical considerations), aims to address these challenges. Understanding the drivers of mistrust, such as hallucinations, training data bias, and poor prompts, is crucial for developing reliable and ethical AI-generated formulas.

Abstract: Generative AI and Large Language Models (LLMs) hold promise for automating
spreadsheet formula creation. However, due to hallucinations, bias and variable
user skill, outputs obtained from generative AI cannot be assumed to be
accurate or trustworthy. To address these challenges, a trustworthiness
framework is proposed based on evaluating the transparency and dependability of
the formula. The transparency of the formula is explored through explainability
(understanding the formula's reasoning) and visibility (inspecting the
underlying algorithms). The dependability of the generated formula is evaluated
in terms of reliability (consistency and accuracy) and ethical considerations
(bias and fairness). The paper also examines the drivers to these metrics in
the form of hallucinations, training data bias and poorly constructed prompts.
Finally, examples of mistrust in technology are considered and the consequences
explored.

</details>


### [47] [Dynamic Prompt Middleware: Contextual Prompt Refinement Controls for Comprehension Tasks](https://arxiv.org/abs/2412.02357)
*Ian Drosos,Jack Williams,Advait Sarkar,Nicholas Wilson*

Main category: cs.HC

TL;DR: 提示工程对于许多用户来说具有挑战性，特别是在需要 AI 理解特定上下文（如代码或文本）的任务中。本研究提出了一种名为动态 PRC（Dynamic PRC）的提示中间件方法，以增强用户对 AI 生成解释的控制力。通过用户研究发现，动态 PRC 优于静态 PRC，能提供更多控制并降低用户提供上下文的门槛，尽管用户在理解控制效果方面仍面临挑战。最终，动态 PRC 被证明能够改善用户体验和 AI 响应质量。


<details>
  <summary>Details</summary>
Motivation: 为了解决用户在向生成式 AI 提供提示（prompt）以完成理解任务（如解释电子表格公式、Python 代码和文本段落）时遇到的困难，特别是用户在表达足够的控制以获得符合其偏好的 AI 响应方面存在障碍。

Method: 通过一项包含 38 名参与者的形成性调查，研究用户在理解任务中对 AI 生成的解释进行控制的需求。随后，通过一项包含 16 名参与者的受控用户研究，评估了动态 PRC 和静态 PRC（Static PRC）这两种提示中间件方法对用户控制 AI 响应以生成更好解释的影响。

Result: 用户研究结果显示，参与者更倾向于动态 PRC 方法，因为它提供了更多的控制力，降低了提供上下文的门槛，并鼓励了用户对任务的探索和反思。然而，用户在理解不同生成控件对最终输出的影响方面仍感困难。研究还发现，动态提示中间件可以增强用户对 AI 响应的控制力，并引导用户获得更好的 AI 响应，从而改善生成式 AI 工作流的用户体验。

Conclusion: 动态 PRC（Dynamic PRC）方法在用户控制 AI 响应以生成更好解释方面表现出优势，它降低了提供上下文的门槛，并鼓励用户探索和反思任务，但用户在理解不同生成控件对最终输出的影响方面仍存在挑战。未来的动态 PRC 系统应着重于增强用户对 AI 响应的控制力，从而改善用户体验。

Abstract: Effective prompting of generative AI is challenging for many users,
particularly in expressing context for comprehension tasks such as explaining
spreadsheet formulas, Python code, and text passages. Prompt middleware aims to
address this barrier by assisting in prompt construction, but barriers remain
for users in expressing adequate control so that they can receive AI-responses
that match their preferences.
  We conduct a formative survey (n=38) investigating user needs for control
over AI-generated explanations in comprehension tasks, which uncovers a
trade-off between standardized but predictable support for prompting, and
adaptive but unpredictable support tailored to the user and task. To explore
this trade-off, we implement two prompt middleware approaches: Dynamic Prompt
Refinement Control (Dynamic PRC) and Static Prompt Refinement Control (Static
PRC). The Dynamic PRC approach generates context-specific UI elements that
provide prompt refinements based on the user's prompt and user needs from the
AI, while the Static PRC approach offers a preset list of generally applicable
refinements.
  We evaluate these two approaches with a controlled user study (n=16) to
assess the impact of these approaches on user control of AI responses for
crafting better explanations. Results show a preference for the Dynamic PRC
approach as it afforded more control, lowered barriers to providing context,
and encouraged exploration and reflection of the tasks, but that reasoning
about the effects of different generated controls on the final output remains
challenging. Drawing on participant feedback, we discuss design implications
for future Dynamic PRC systems that enhance user control of AI responses. Our
findings suggest that dynamic prompt middleware can improve the user experience
of generative AI workflows by affording greater control and guide users to a
better AI response.

</details>


### [48] [Subject integration with spreadsheets -- Ignoring education is the greatest risk ever](https://arxiv.org/abs/2409.12975)
*Mária Csernoch,Ádám Gulácsi,Júlia Csernoch*

Main category: cs.HC

TL;DR: Subject integration using spreadsheets can enhance digital literacy and computer science skills in schools, with a focus on analytical and planning as well as practical application.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore subject integration within the Technological Pedagogical and Content Knowledge framework as a means to achieve meaningful digitalization in schools, contextualize informatics classes, and bridge the gap between "serious informatics" and "digital literacy."

Method: The paper analyzes how three traditional Grade 3 tasks can be solved using spreadsheets, focusing on the development of skills, competencies, and computer science knowledge for teachers and students.

Result: The solutions reveal that analyzing, understanding, planning, and discussing tasks are as crucial as the hands-on spreadsheet activities, emphasizing the role of spreadsheets in preparing students for future employment.

Conclusion: The paper demonstrates that spreadsheet-based solutions for traditional Grade 3 tasks can effectively develop essential skills, competencies, and computer science knowledge in both teachers and students, while also highlighting the importance of analytical and planning activities for future job preparedness.

Abstract: Within the framework of Technological Pedagogical and Content Knowledge,
subject integration is one possible solution for the introduction of meaningful
digitalization and digitization in schools. This process incorporates that any
school subject can be taught with digital support, informatics (computer)
classes can be contextualized, and the gap between 'serious informatics' and
'digital literacy' can be minimized. The present paper details how three
traditional Grade 3 tasks can be solved in spreadsheets, what skills,
competencies, and computer science knowledge of both teachers and students can
be developed. The solutions also reveal that analysing, understanding,
planning, and discussing tasks is as important as the activity in the
spreadsheets, which process plays a crucial role in the preparation of students
for their future jobs.

</details>


### [49] [Exploring Higher Education Competencies through Spreadsheet Self-Assessment and Time](https://arxiv.org/abs/2409.12974)
*Maria Csernoch,Judit T. Kiss,Viktor Takács,Domicián Máté*

Main category: cs.HC

TL;DR: 大学生在电子表格能力方面的自我评估不准确，在数字环境中完成任务比纸质任务花费的时间更长，这挑战了“数字原住民”的假设。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索高等教育学生在电子表格方面的能力和可靠性，并检验“数字原住民”在电子表格任务上的表现是否优于纸质任务的假设。

Method: 通过自我评估和现实世界问题解决实践来探索大学生在电子表格方面的能力和可靠性。

Result: 研究结果未能证实大学生在Excel上表现优于纸质任务的假设。相反，研究发现学生倾向于不准确地评估自己的电子表格能力，并且在数字环境中完成任务需要更长的时间才能达到与纸质任务相同的分数。

Conclusion: 该研究结果表明，大学生往往对其电子表格能力进行不准确的自我评估，并且在数字环境中完成任务比在纸质环境中需要更长的时间。这挑战了数字原住民无需计算机科学教育的普遍假设。

Abstract: The present paper aims to explore higher education students' spreadsheet
competencies and reliability through self-assessment and real-world
problem-solving practices. Digital natives alleged skills and competences
allowed us to hypothesize that students perform better in Excel than on paper,
but the findings cannot confirm this hypothesis. However, our results indicate
that students tend to inaccurately assess their spreadsheet competencies
compared to their actual performance in both paper-based and Excel tasks. It
has also be found that students need at least twice as much time to achieve the
same high scores in the digital environment as they do on paper. The results
violated the widely accepted assumption that digital native students do not
need computer science education, since they are born with it. This study
highlights the importance of accurate self-assessment in digital skill
development and time management within higher education contexts, particularly
in technology-driven disciplines.

</details>


### [50] [Data Guards: Challenges and Solutions for Fostering Trust in Data](https://arxiv.org/abs/2407.14042)
*Nicole Sultanum,Dennis Bromley,Michael Correll*

Main category: cs.HC

TL;DR: Researchers interviewed data producers and consumers to understand how to build trust in data. They found a need for data validation but no existing standards, so they propose 'data guards' to help.


<details>
  <summary>Details</summary>
Motivation: The research was motivated by the need to establish trust and verification in data-driven decisions, given the various threats to data validity, from dirty data to intentional deception. This is particularly important when dealing with new or unfamiliar data.

Method: The study involved conducting a series of interviews with both producers and consumers of data artifacts to understand their strategies and obstacles in building trust in data.

Result: The interviews revealed a recurring need for data validation and verification, especially among data consumers, but also highlighted a lack of existing standards in this area.

Conclusion: The paper proposes a set of data guards, which are methods and tools designed to foster trust in data artifacts, addressing the identified need for data validation and verification.

Abstract: From dirty data to intentional deception, there are many threats to the
validity of data-driven decisions. Making use of data, especially new or
unfamiliar data, therefore requires a degree of trust or verification. How is
this trust established? In this paper, we present the results of a series of
interviews with both producers and consumers of data artifacts (outputs of data
ecosystems like spreadsheets, charts, and dashboards) aimed at understanding
strategies and obstacles to building trust in data. We find a recurring need,
but lack of existing standards, for data validation and verification,
especially among data consumers. We therefore propose a set of data guards:
methods and tools for fostering trust in data artifacts.

</details>


### [51] [Smart Portable Computer](https://arxiv.org/abs/2405.05292)
*Niladri Das*

Main category: cs.HC

TL;DR: 便携式智能计算机：一款便携、经济高效的设备，提供强大的桌面体验，适合学生和程序员。


<details>
  <summary>Details</summary>
Motivation: 在 COVID-19 大流行期间，学生难以获得具有足够系统规格的 PC，并且依赖笔记本电脑工作的人发现传统方法很麻烦。

Method: 提出了一种名为“便携式智能计算机”的新型计算设备。

Result: 该设备提供与传统台式机相当的速度和性能，体积小巧，节能且具有成本效益，可用于文档编辑、浏览网页、管理电子表格、创建演示文稿以及支持编程语言和编译器。

Conclusion: Portable Smart Computer 是一款便携、节能、经济高效的设备，提供无缝的桌面体验，并支持多种编程语言和编译器，可满足学生和程序员的需求。

Abstract: Amidst the COVID-19 pandemic, with many organizations, schools, colleges, and
universities transitioning to virtual platforms, students encountered
difficulties in acquiring PCs such as desktops or laptops. The starting prices,
around 15,000 INR, often failed to offer adequate system specifications, posing
a challenge for consumers. Additionally, those reliant on laptops for work
found the conventional approach cumbersome. Enter the "Portable Smart
Computer," a leap into the future of computing. This innovative device boasts
speed and performance comparable to traditional desktops but in a compact,
energy-efficient, and cost-effective package. It delivers a seamless desktop
experience, whether one is editing documents, browsing multiple tabs, managing
spreadsheets, or creating presentations. Moreover, it supports programming
languages like Python, C, C++, as well as compilers such as Keil and Xilinx,
catering to the needs of programmers.

</details>


### [52] ["My toxic trait is thinking I'll remember this": gaps in the learner experience of video tutorials for feature-rich software](https://arxiv.org/abs/2404.07114)
*Ian Drosos,Advait Sarkar,Andrew D. Gordon*

Main category: cs.HC

TL;DR: Video tutorials for software like Excel have 'gaps' that hinder learning. This study analyzes viewer comments and interviews creators to understand these gaps, offering design solutions to improve the learning experience.


<details>
  <summary>Details</summary>
Motivation: The research aims to understand and address 'gaps' that hinder learning when users follow video tutorials for complex software like spreadsheets.

Method: The study analyzes 360 viewer comments from 90 Microsoft Excel video tutorials and conducts interviews with 8 influential creators. It also develops a theory and taxonomy of learning gaps.

Result: The study identifies various gaps encountered by learners, provides insights into creators' processes and frustrations, and proposes design solutions.

Conclusion: The paper presents designs to address identified gaps in video tutorials and gathers feedback from creators.

Abstract: Video tutorials are a popular medium for informal and formal learning.
However, when learners attempt to view and follow along with these tutorials,
they encounter what we call gaps, that is, issues that can prevent learning. We
examine the gaps encountered by users of video tutorials for feature-rich
software, such as spreadsheets. We develop a theory and taxonomy of such gaps,
identifying how they act as barriers to learning, by collecting and analyzing
360 viewer comments from 90 Microsoft Excel video tutorials published by 43
creators across YouTube, TikTok, and Instagram. We conducted contextual
interviews with 8 highly influential tutorial creators to investigate the gaps
their viewers experience and how they address them. Further, we obtain insights
into their creative process and frustrations when creating video tutorials.
Finally, we present creators with two designs that aim to address gaps
identified in the comment analysis for feedback and alternative design ideas.

</details>


### [53] [Supporting Annotators with Affordances for Efficiently Labeling Conversational Data](https://arxiv.org/abs/2403.07762)
*Austin Z. Henley,David Piorkowski*

Main category: cs.HC

TL;DR: CAL是一种新的数据标记接口，旨在减少用户的工作量和单调性。它通过防止不当标签、提供指导、集成文档和提供有效的先前标签查看方式来帮助用户。用户研究表明，CAL比标准电子表格更易于使用，认知负荷更低，且用户更喜欢使用CAL。


<details>
  <summary>Details</summary>
Motivation: 为了解决人力和单调乏味的标记问题，我们设计了CAL。

Method:  CAL是一种新颖的接口，用于辅助数据标记，其设计决策包括防止选择不当的标签，在用户需要帮助时指导用户选择合适的标签，将标记文档集成到接口中，并提供一种查看先前标签的有效方法。

Result:  CAL是一种新颖的接口，用于辅助数据标记。

Conclusion: 用户使用CAL报告的认知负荷较低，任务时间没有增加，用户认为CAL更易于使用，并且用户更喜欢CAL而不是电子表格。

Abstract: Without well-labeled ground truth data, machine learning-based systems would
not be as ubiquitous as they are today, but these systems rely on substantial
amounts of correctly labeled data. Unfortunately, crowdsourced labeling is time
consuming and expensive. To address the concerns of effort and tedium, we
designed CAL, a novel interface to aid in data labeling. We made several key
design decisions for CAL, which include preventing inapt labels from being
selected, guiding users in selecting an appropriate label when they need
assistance, incorporating labeling documentation into the interface, and
providing an efficient means to view previous labels. We implemented a
production-quality implementation of CAL and report a user-study evaluation
that compares CAL to a standard spreadsheet. Key findings of our study include
users using CAL reported lower cognitive load, did not increase task time,
users rated CAL to be easier to use, and users preferred CAL over the
spreadsheet.

</details>


### [54] [Prompting for Discovery: Flexible Sense-Making for AI Art-Making with Dreamsheets](https://arxiv.org/abs/2310.09985)
*Shm Garanganao Almeda,J. D. Zamfirescu-Pereira,Kyu Won Kim,Pradeep Mani Rathnam,Bjoern Hartmann*

Main category: cs.HC

TL;DR: DreamSheets是一个电子表格界面的设计工具，利用LLM辅助用户进行文本到图像的探索，通过实验和用户研究，揭示了用户探索策略和界面需求，并提出了未来界面的设计指导。


<details>
  <summary>Details</summary>
Motivation: 文本到图像（TTI）模型的设计空间探索面临着挑战，因为输入空间的微小变化可能导致输出图像的巨大差异。因此，需要设计能够支持最终用户可靠地将提示空间探索引导至有趣结果的界面。

Method: 本研究采用了设计探讨和用户研究的方法。首先设计了一个名为DreamSheets的界面，该界面集成了一个电子表格界面，并包含基于LLM的辅助提示构建函数和结果的同步显示功能。然后，通过一项初步的实验室研究和一项包含五位专业艺术家的纵向研究，收集用户在TTI设计空间探索中的策略和对界面功能的需求。

Result: 研究发现了用户在TTI设计空间探索中使用的策略，例如利用文本生成来定义探索的局部“轴”。研究还确定了界面功能，如DreamSheets提供的灵活布局和新颖的生成函数，能够支持用户定义的实验工作流程。

Conclusion: 本研究通过设计探讨和用户研究，提出了一个名为DreamSheets的界面，以支持文本到图像（TTI）模型的设计空间探索。研究揭示了用户在TTI设计空间探索中面临的挑战，以及支持这些挑战所需的界面功能，例如使用文本生成来定义探索的局部“轴”。最后，研究提出了一个UI模型来指导未来的界面设计。

Abstract: Design space exploration (DSE) for Text-to-Image (TTI) models entails
navigating a vast, opaque space of possible image outputs, through a
commensurately vast input space of hyperparameters and prompt text. Minor
adjustments to prompt input can surface unexpectedly disparate images. How can
interfaces support end-users in reliably steering prompt-space explorations
towards interesting results? Our design probe, DreamSheets, supports
exploration strategies with LLM-based functions for assisted prompt
construction and simultaneous display of generated results, hosted in a
spreadsheet interface. The flexible layout and novel generative functions
enable experimentation with user-defined workflows. Two studies, a preliminary
lab study and a longitudinal study with five expert artists, revealed a set of
strategies participants use to tackle the challenges of TTI design space
exploration, and the interface features required to support them - like using
text-generation to define local "axes" of exploration. We distill these
insights into a UI mockup to guide future interfaces.

</details>


### [55] [Does Using ChatGPT Result in Human Cognitive Augmentation?](https://arxiv.org/abs/2401.11042)
*Ron Fulbright,Miranda Morrison*

Main category: cs.HC

TL;DR: 本文研究了 ChatGPT 对人类认知能力的影响。通过实验发现，ChatGPT 并不总能提升认知能力，有时还会误导用户，导致认知能力下降，因此它还不能取代人类的判断和评估。


<details>
  <summary>Details</summary>
Motivation: 近年来，无监督深度学习产生的认知系统在多个领域表现优于人类。本文旨在研究 ChatGPT 对人类认知能力的影响，并探讨其是否能实现“合成专业知识”。

Method: 通过两个实验，将使用 ChatGPT 生成的响应与未使用 ChatGPT 生成的响应进行比较，以研究使用 ChatGPT 对人类认知增强的影响。

Result: 研究发现，使用 ChatGPT 并不总能带来认知增强，并且在某些类型的任务中，它还不能取代人类的判断、鉴别和评估。在某些情况下，ChatGPT 甚至会误导用户，导致负面的认知增强。

Conclusion: 使用 ChatGPT 进行一些任务时，并不能总是提高认知能力，也不能取代人类的判断、鉴别和评估。在某些情况下，ChatGPT 可能会误导用户，从而导致认知能力下降。

Abstract: Human cognitive performance is enhanced by the use of tools. For example, a
human can produce a much greater, and more accurate, volume of mathematical
calculation in a unit of time using a calculator or a spreadsheet application
on a computer. Such tools have taken over the burden of lower level cognitive
grunt work but the human still serves the role of the expert performing higher
level thinking and reasoning. Recently, however, unsupervised, deep, machine
learning has produced cognitive systems able to outperform humans in several
domains. When humans use these tools in a human cog ensemble, the cognitive
ability of the human is augmented. In some cases, even non experts can achieve,
and even exceed, the performance of experts in a particular domain, synthetic
expertise. A new cognitive system, ChatGPT, has burst onto the scene during the
past year. This paper investigates human cognitive augmentation due to using
ChatGPT by presenting the results of two experiments comparing responses
created using ChatGPT with results created not using ChatGPT. We find using
ChatGPT does not always result in cognitive augmentation and does not yet
replace human judgement, discernment, and evaluation in certain types of tasks.
In fact, ChatGPT was observed to result in misleading users resulting in
negative cognitive augmentation.

</details>


### [56] [Co-audit: tools to help humans double-check AI-generated content](https://arxiv.org/abs/2310.01297)
*Andrew D. Gordon,Carina Negreanu,José Cambronero,Rasika Chakravarthy,Ian Drosos,Hao Fang,Bhaskar Mitra,Hannah Richardson,Advait Sarkar,Stephanie Simmons,Jack Williams,Ben Zorn*

Main category: cs.HC

TL;DR: AI-generated content is getting complex, making it hard to check. Co-audit tools help users verify AI output, like in spreadsheets, and are essential for quality-focused AI applications. Principles and research challenges for co-audit tools are discussed.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of AI-generated content, such as summaries, tables, and code, makes it difficult for users to audit or evaluate its quality and correctness, necessitating the development of tools to assist in this process.

Method: This paper describes recent research on co-audit tools for generative AI, with a specific focus on spreadsheet computations. It explains the necessity of co-audit experiences in applications where quality and error consequences are significant, and proposes a preliminary list of principles for co-audit while outlining associated research challenges.

Result: The paper highlights the emergence and importance of co-audit tools for generative AI, particularly in high-stakes applications like spreadsheet computations. It lays the groundwork for future research by proposing principles and identifying challenges in the development of these tools.

Conclusion: As generative AI models are used in more complex applications, such as spreadsheet computations, tool-assisted experiences called co-audit tools are emerging to help users check AI-generated content for correctness and quality. These tools complement prompt engineering techniques by assisting users in verifying output responses.

Abstract: Users are increasingly being warned to check AI-generated content for
correctness. Still, as LLMs (and other generative models) generate more complex
output, such as summaries, tables, or code, it becomes harder for the user to
audit or evaluate the output for quality or correctness. Hence, we are seeing
the emergence of tool-assisted experiences to help the user double-check a
piece of AI-generated content. We refer to these as co-audit tools. Co-audit
tools complement prompt engineering techniques: one helps the user construct
the input prompt, while the other helps them check the output response. As a
specific example, this paper describes recent research on co-audit tools for
spreadsheet computations powered by generative models. We explain why co-audit
experiences are essential for any application of generative AI where quality is
important and errors are consequential (as is common in spreadsheet
computations). We propose a preliminary list of principles for co-audit, and
outline research challenges.

</details>


### [57] ["What It Wants Me To Say": Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models](https://arxiv.org/abs/2304.06597)
*Michael Xieyang Liu,Advait Sarkar,Carina Negreanu,Ben Zorn,Jack Williams,Neil Toronto,Andrew D. Gordon*

Main category: cs.HC

TL;DR: 为改善代码生成模型的可用性，提出了一种将代码翻译回自然语言的方法，并在用户研究中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为解决自然语言到代码生成中，非专家用户理解和有效引导代码生成所面临的挑战，特别是在电子表格数据分析的背景下。

Method: 通过将代码翻译回系统化的、可预测的自然语言描述，来弥合抽象鸿沟。在对24名用户进行的方法性、引导性研究中，将基础性匹配方法与基于先前建立的查询框架原则的非基础性方法进行了比较。

Result: 基础性匹配方法提高了终端用户对代码生成模型范围和能力的理解，以及有效使用该模型所需的语言。

Conclusion: 该研究提出的基础性匹配方法能够帮助用户更好地理解代码生成模型的能力和范围，以及更有效地使用该模型的语言。

Abstract: Code-generating large language models translate natural language into code.
However, only a small portion of the infinite space of naturalistic utterances
is effective at guiding code generation. For non-expert end-user programmers,
learning this is the challenge of abstraction matching. We examine this
challenge in the specific context of data analysis in spreadsheets, in a system
that maps the users natural language query to Python code using the Codex
generator, executes the code, and shows the result. We propose grounded
abstraction matching, which bridges the abstraction gap by translating the code
back into a systematic and predictable naturalistic utterance. In a
between-subjects, think-aloud study (n=24), we compare grounded abstraction
matching to an ungrounded alternative based on previously established query
framing principles. We find that the grounded approach improves end-users'
understanding of the scope and capabilities of the code-generating model, and
the kind of language needed to use it effectively.

</details>


### [58] [Team OS's System for Dialogue Robot Competition 2022](https://arxiv.org/abs/2210.09928)
*Yuki Kubo,Ryo Yanagimoto,Hayato Futase,Mikio Nakano,Zhaojie Luo,Kazunori Komatani*

Main category: cs.HC

TL;DR: OSbot是一个基于状态转换的对话机器人，结合了关键词提取和情感分析。该系统在2022年对话机器人竞赛中获得第三名。


<details>
  <summary>Details</summary>
Motivation: 为对话机器人竞赛2022开发对话机器人系统OSbot。

Method: 本系统基于手动描述的状态转换，并结合关键词提取和情感分析的结果来实现对话流。关键词提取基于命名实体提取和预定义的关键词集。情感分析采用基于文本的SVM，并使用多模态对话语料库Hazumi进行训练。通过日志功能快速检查和编辑对话流。

Result: 系统在竞赛初赛中获得第三名。

Conclusion: 该系统在竞赛初赛中获得第三名。

Abstract: This paper describes our dialogue robot system, OSbot, developed for Dialogue
Robot Competition 2022. The dialogue flow is based on state transitions
described manually and the transition conditions use the results of keyword
extraction and sentiment analysis. The transitions can be easily viewed and
edited by managing them on a spreadsheet. The keyword extraction is based on
named entity extraction and our predefined keyword set. The sentiment analysis
is text-based and uses SVM, which was trained with the multimodal dialogue
corpus Hazumi. We quickly checked and edited a dialogue flow by using a logging
function. In the competition's preliminary round, our system ended up in third
place.

</details>


### [59] [What is it like to program with artificial intelligence?](https://arxiv.org/abs/2208.06213)
*Advait Sarkar,Andrew D. Gordon,Carina Negreanu,Christian Poelitz,Sruti Srinivasa Ragavan,Ben Zorn*

Main category: cs.HC

TL;DR: LLM-assisted programming is a new way to program, different from previous methods. It has its own challenges, especially for beginners using tools like spreadsheets.


<details>
  <summary>Details</summary>
Motivation: To explore how LLM-assisted programming is similar to, and differs from, prior conceptualizations of programmer assistance, and to discuss potential issues and challenges in applying LLMs to end-user programming.

Method: Drawing upon publicly available experience reports of LLM-assisted programming, as well as prior usability and design studies, and observations from a user study with non-expert end-user programmers.

Result: LLM-assisted programming shares some properties with compilation, pair programming, and programming via search and reuse, but has fundamental differences in technical possibilities and practical experience. Applying LLMs to end-user programming presents unique issues and challenges, especially for users with little to no programming expertise.

Conclusion: LLM-assisted programming is a new paradigm with distinct properties and challenges, particularly when applied to end-user programming with non-expert users.

Abstract: Large language models, such as OpenAI's codex and Deepmind's AlphaCode, can
generate code to solve a variety of problems expressed in natural language.
This technology has already been commercialised in at least one widely-used
programming editor extension: GitHub Copilot.
  In this paper, we explore how programming with large language models
(LLM-assisted programming) is similar to, and differs from, prior
conceptualisations of programmer assistance. We draw upon publicly available
experience reports of LLM-assisted programming, as well as prior usability and
design studies. We find that while LLM-assisted programming shares some
properties of compilation, pair programming, and programming via search and
reuse, there are fundamental differences both in the technical possibilities as
well as the practical experience. Thus, LLM-assisted programming ought to be
viewed as a new way of programming with its own distinct properties and
challenges.
  Finally, we draw upon observations from a user study in which non-expert end
user programmers use LLM-assisted tools for solving data tasks in spreadsheets.
We discuss the issues that might arise, and open research challenges, in
applying large language models to end-user programming, particularly with users
who have little or no programming expertise.

</details>


### [60] [MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization](https://arxiv.org/abs/2209.05739)
*Lu Ying,Xinhuan Shu,Dazhen Deng,Yuchen Yang,Tan Tang,Lingyun Yu,Yingcai Wu*

Main category: cs.HC

TL;DR: MetaGlyph is an automatic system that generates visually appealing glyph-based visualizations with metaphors from spreadsheets, simplifying the design process for users by leveraging data semantics and a Monte Carlo tree search algorithm.


<details>
  <summary>Details</summary>
Motivation: Creating metaphoric glyph-based visualizations (MGVs) is challenging, requiring deep data understanding and professional design skills. This paper aims to simplify this process by proposing an automatic system.

Method: MetaGlyph employs a framework for generating MGVs by metaphoric image selection and MGV construction. It automatically selects metaphors and corresponding images from online resources based on input data semantics. A Monte Carlo tree search algorithm is used to explore the design by associating visual elements with data dimensions, considering data importance, semantic relevance, and glyph non-overlap. The system also offers editing feedback for user customization.

Result: MetaGlyph automatically generates MGVs from spreadsheets by selecting relevant metaphors and images, and constructing glyphs using a Monte Carlo tree search algorithm. The system allows user customization and has been validated through examples, a usage scenario, and expert interviews.

Conclusion: The paper proposes MetaGlyph, an automatic system for generating metaphoric glyph-based visualizations (MGVs) from spreadsheets, addressing the difficulty of creating them manually. The system uses metaphoric image selection and a Monte Carlo tree search algorithm for MGV construction, incorporating user feedback for customization. Its effectiveness is demonstrated through examples, a usage scenario, and expert interviews.

Abstract: Glyph-based visualization achieves an impressive graphic design when
associated with comprehensive visual metaphors, which help audiences
effectively grasp the conveyed information through revealing data semantics.
However, creating such metaphoric glyph-based visualization (MGV) is not an
easy task, as it requires not only a deep understanding of data but also
professional design skills. This paper proposes MetaGlyph, an automatic system
for generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct
a qualitative analysis to understand the design of current MGVs from the
perspectives of metaphor embodiment and glyph design. Based on the results, we
introduce a novel framework for generating MGVs by metaphoric image selection
and an MGV construction. Specifically, MetaGlyph automatically selects
metaphors with corresponding images from online resources based on the input
data semantics. We then integrate a Monte Carlo tree search algorithm that
explores the design of an MGV by associating visual elements with data
dimensions given the data importance, semantic relevance, and glyph
non-overlap. The system also provides editing feedback that allows users to
customize the MGVs according to their design preferences. We demonstrate the
use of MetaGlyph through a set of examples, one usage scenario, and validate
its effectiveness through a series of expert interviews.

</details>


### [61] [PoVRPoint: Authoring Presentations in Mobile Virtual Reality](https://arxiv.org/abs/2201.06337)
*Verena Biener,Travis Gesslein,Daniel Schneider,Felix Kawala,Alexander Otte,Per Ola Kristensson,Michel Pahud,Eyal Ofek,Cuauhtli Campos,Matjaž Kljun,Klen Čopič Pucihar,Jens Grubert*

Main category: cs.HC

TL;DR: 本研究提出了PoVRPoint，一种结合平板电脑和VR技术的演示文稿创作工具，用于移动场景。研究表明VR的宽视野和三维视图在幻灯片识别和对象重排方面优于纯平板电脑，且该技术用户体验良好。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索VR设计空间，以支持移动场景下的演示文稿创作，并研究扩展显示空间在识别目标幻灯片、空间操作对象、创建动画以及排列多个可能被遮挡的图形等方面的效用。

Method: 本研究探索了在移动设置中创作演示文稿的VR设计空间，提出了PoVRPoint工具集，该工具集将移动设备（如平板电脑）上的演示文稿笔触和触摸编辑与VR提供的交互能力相结合。

Result: 研究结果表明，VR提供的宽视野比纯平板电脑界面能显著缩短目标幻灯片的识别时间（针对视觉显著的目标）；同时，与两个基线界面相比，VR的三维视图在处理遮挡情况下的对象重排方面速度更快。

Conclusion: 本研究提出的PoVRPoint工具集，结合了平板电脑的笔触和触摸编辑功能，以及VR的交互能力，可用于移动场景下的演示文稿创作。用户研究表明，该交互技术可用且令人愉悦。

Abstract: Virtual Reality (VR) has the potential to support mobile knowledge workers by
complementing traditional input devices with a large three-dimensional output
space and spatial input. Previous research on supporting VR knowledge work
explored domains such as text entry using physical keyboards and spreadsheet
interaction using combined pen and touch input. Inspired by such work, this
paper probes the VR design space for authoring presentations in mobile
settings. We propose PoVRPoint -- a set of tools coupling pen- and touch-based
editing of presentations on mobile devices, such as tablets, with the
interaction capabilities afforded by VR. We study the utility of extended
display space to, for example, assist users in identifying target slides,
supporting spatial manipulation of objects on a slide, creating animations, and
facilitating arrangements of multiple, possibly occluded, shapes. Among other
things, our results indicate that 1) the wide field of view afforded by VR
results in significantly faster target slide identification times compared to a
tablet-only interface for visually salient targets; and 2) the
three-dimensional view in VR enables significantly faster object reordering in
the presence of occlusion compared to two baseline interfaces. A user study
further confirmed that the interaction techniques were found to be usable and
enjoyable.

</details>


### [62] [Untidy Data: The Unreasonable Effectiveness of Tables](https://arxiv.org/abs/2106.15005)
*Lyn Bartram,Michael Correll,Melanie Tory*

Main category: cs.HC

TL;DR: Data tables are more than just a preparatory step; data workers use them interactively throughout the analysis process for sensemaking, reorganizing and augmenting data. Interactive tables are a valuable visualization tool that should be further explored in visual analytics.


<details>
  <summary>Details</summary>
Motivation: The paper explores the critical role of spreadsheets and table tools in the information ecosystem for data workers, who use them for interaction beyond simple data preparation.

Method: The study involved a qualitative analysis of how data workers interact with and reason about their data in table form.

Result: The findings indicate that data tables serve multiple purposes throughout the analytics process, allowing users to see, reshape, and augment underlying data for sensemaking by reorganizing, marking up, layering details, and spawning alternatives.

Conclusion: The paper argues that interactive tables are an important visualization idiom in their own right, offering a fertile design space for visual analytics and enriching sensemaking through more flexible human-data interaction.

Abstract: Working with data in table form is usually considered a preparatory and
tedious step in the sensemaking pipeline; a way of getting the data ready for
more sophisticated visualization and analytical tools. But for many people,
spreadsheets -- the quintessential table tool -- remain a critical part of
their information ecosystem, allowing them to interact with their data in ways
that are hidden or abstracted in more complex tools. This is particularly true
for data workers: people who work with data as part of their job but do not
identify as professional analysts or data scientists. We report on a
qualitative study of how these workers interact with and reason about their
data. Our findings show that data tables serve a broader purpose beyond data
cleanup at the initial stage of a linear analytic flow: users want to see and
"get their hands on" the underlying data throughout the analytics process,
reshaping and augmenting it to support sensemaking. They reorganize, mark up,
layer on levels of detail, and spawn alternatives within the context of the
base data. These direct interactions and human-readable table representations
form a rich and cognitively important part of building understanding of what
the data mean and what they can do with it. We argue that interactive tables
are an important visualization idiom in their own right; that the direct data
interaction they afford offers a fertile design space for visual analytics; and
that sense making can be enriched by more flexible human-data interaction than
is currently supported in visual analytics tools.

</details>


### [63] [Defining and Adopting an End User Computing Policy: A Case Study](https://arxiv.org/abs/1909.00855)
*Roger Turner*

Main category: cs.HC

TL;DR: Wesleyan Assurance Society 通过引入更新的最终用户计算策略和风险评估应用程序，成功应对了 EUC 风险。该方法采用基于风险的策略，优先处理最高风险以获得最快效益。


<details>
  <summary>Details</summary>
Motivation: 为了应对最终用户计算（EUC）带来的重大风险，特别是在没有得到妥善控制的情况下，本研究旨在探讨如何在 Wesleyan Assurance Society 引入更新后的 EUC 策略，并识别和克服相关挑战。

Method: 本研究采用了案例研究的方法，重点关注 Wesleyan Assurance Society 引入更新后的最终用户计算策略的过程。研究人员还开发了一款最终用户计算风险评估应用程序，该应用程序根据应用程序的复杂性、重要性和控制程度（或缺乏控制）来计算风险评级。

Result: 通过实施更新后的 EUC 策略和使用风险评估应用程序，Wesleyan Assurance Society 能够首先解决最高风险，从而最快地获得收益。

Conclusion: 该政策的实施和风险评估应用程序的开发应能有效降低最终用户计算带来的风险。

Abstract: End User Computing carries significant risks if not well controlled. This
paper is a case study of the introduction of an updated End User Computing
policy at the Wesleyan Assurance Society. The paper outlines the plan and
identifies various challenges. The paper explains how these challenges were
overcome. We wrote an End User Computing Risk Assessment Application which
calculates a risk rating band based on the Complexity, Materiality and Control
(or lack of it) pertaining to any given application and the basis of assessment
is given in this paper. The policy uses a risk based approach for assessing and
mitigating against the highest risks first and obtaining the quickest benefit.

</details>


### [64] [Calliope: Automatic Visual Data Story Generation from a Spreadsheet](https://arxiv.org/abs/2010.09975)
*Danqing Shi,Xinyue Xu,Fuling Sun,Yang Shi,Nan Cao*

Main category: cs.HC

TL;DR: Calliope is a novel system that automatically creates visual data stories from spreadsheets, overcoming technical barriers and improving efficiency through a logic-oriented Monte Carlo tree search algorithm and an online editor.


<details>
  <summary>Details</summary>
Motivation: Technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult.

Method: Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description.

Result: The proposed technique is evaluated through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. The evaluation shows that Calliope is beneficial to efficient visual data story generation.

Conclusion: Calliope is beneficial to efficient visual data story generation.

Abstract: Visual data stories shown in the form of narrative visualizations such as a
poster or a data video, are frequently used in data-oriented storytelling to
facilitate the understanding and memorization of the story content. Although
useful, technique barriers, such as data analysis, visualization, and
scripting, make the generation of a visual data story difficult. Existing
authoring tools rely on users' skills and experiences, which are usually
inefficient and still difficult. In this paper, we introduce a novel visual
data story generating system, Calliope, which creates visual data stories from
an input spreadsheet through an automatic process and facilities the easy
revision of the generated story based on an online story editor. Particularly,
Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm
that explores the data space given by the input spreadsheet to progressively
generate story pieces (i.e., data facts) and organize them in a logical order.
The importance of data facts is measured based on information theory, and each
data fact is visualized in a chart and captioned by an automatically generated
description. We evaluate the proposed technique through three example stories,
two controlled experiments, and a series of interviews with 10 domain experts.
Our evaluation shows that Calliope is beneficial to efficient visual data story
generation.

</details>


### [65] [Pen-based Interaction with Spreadsheets in Mobile Virtual Reality](https://arxiv.org/abs/2008.04543)
*Travis Gesslein,Verena Biener,Philipp Gagel,Daniel Schneider,Per Ola Kristensson,Eyal Ofek,Michel Pahud,Jens Grubert*

Main category: cs.HC

TL;DR: VR+笔输入=更好的移动电子表格交互。


<details>
  <summary>Details</summary>
Motivation: 电子表格在移动设备上的交互性差，尤其是在有限的物理空间（如飞机座位）中。VR提供了沉浸式的大显示空间，但与有限的交互空间形成对比，因此需要探索如何利用VR来增强移动电子表格的交互性。

Method: 提出了一个结合VR和笔输入来增强平板电脑上电子表格交互性的工具集，并设计了用于扩展表格显示、可视化依赖关系以及通过注视和触摸进行交互的工具。通过视频调查和专家评估来研究可行性。

Result: 通过视频调查和专家评估，初步证明了所提出的工具集在增强电子表格交互性和潜在生产力方面的可行性。

Conclusion: 该研究提出了一个结合VR和笔输入来增强平板电脑上电子表格交互性的工具集，并通过视频调查和专家评估研究了其可行性。

Abstract: Virtual Reality (VR) can enhance the display and interaction of mobile
knowledge work and in particular, spreadsheet applications. While spreadsheets
are widely used yet are challenging to interact with, especially on mobile
devices, using them in VR has not been explored in depth. A special uniqueness
of the domain is the contrast between the immersive and large display space
afforded by VR, contrasted by the very limited interaction space that may be
afforded for the information worker on the go, such as an airplane seat or a
small work-space. To close this gap, we present a tool-set for enhancing
spreadsheet interaction on tablets using immersive VR headsets and pen-based
input. This combination opens up many possibilities for enhancing the
productivity for spreadsheet interaction. We propose to use the space around
and in front of the tablet for enhanced visualization of spreadsheet data and
meta-data. For example, extending sheet display beyond the bounds of the
physical screen, or easier debugging by uncovering hidden dependencies between
sheet's cells. Combining the precise on-screen input of a pen with spatial
sensing around the tablet, we propose tools for the efficient creation and
editing of spreadsheets functions such as off-the-screen layered menus,
visualization of sheets dependencies, and gaze-and-touch-based switching
between spreadsheet tabs. We study the feasibility of the proposed tool-set
using a video-based online survey and an expert-based assessment of indicative
human performance potential.

</details>


### [66] [EQUS -- helping to see formulae](https://arxiv.org/abs/2007.00003)
*Chris Roast*

Main category: cs.HC

TL;DR: EQUS is an interactive visualisation tool for spreadsheet formulae, developed through iterative refinement and evaluation with teen learners, which has proven relevant to a broader audience and is now an MS Excel plug-in.


<details>
  <summary>Details</summary>
Motivation: Spreadsheet formulae are widely used for numerical processing and modeling, but they can be easily misunderstood.

Method: The development process was one of iterative refinement, involving re-design and formative evaluation with an initial target audience of mid-teen learners. The resulting visualisation was developed as a fully integrated plug-in for MS Excel.

Result: An interactive visualisation for spreadsheet formulae (EQUS) was designed, developed, and evaluated.

Conclusion: The developed visualisation techniques are broadly relevant to spreadsheet users beyond the initial target audience of mid-teen learners.

Abstract: Visualisation is often presented as a means of simplifying information and
helping people understand complex data. In this paper we describe the design,
development and evaluation of an interactive visualisation for spreadsheet
formulae (EQUS). The work is justified on the grounds that these are widely
used tools for significant numerical processing and modeling, yet the formula
developed can be easily misunderstood. The development process was one of
iterative refinement engaging an initial target audience of mid-teen learners,
involving re-design and formative evaluation. The resulting visualisation
techniques have been found to be broadly relevant to spreadsheet users beyond
the initial target audience. EQUS has since been developed as fully integrated
plug-in for MS Excel.

</details>


### [67] [Taggle: Combining Overview and Details in Tabular Data Visualizations](https://arxiv.org/abs/1712.05944)
*Katarina Furmanova,Samuel Gratzl,Holger Stitz,Thomas Zichner,Miroslava Jaresova,Alexander Lex,Marc Streit*

Main category: cs.HC

TL;DR: Taggle is a spreadsheet-like tabular visualization technique that focuses on individual items in large datasets, using visual encodings and data-driven aggregation with specific interaction methods for analysis, as shown in a genomics drug discovery case study.


<details>
  <summary>Details</summary>
Motivation: Many practical analysis tasks focus on investigating individual items of interest within large tables, and relating these items to the rest of the data is crucial. Existing tabular visualization techniques often prioritize overviews.

Method: Taggle visualizes each row individually using visual encodings for cells and incorporates data-driven aggregation of data subsets. It also includes interaction methods for sorting by multiple columns and rich data selection/filtering.

Result: The effectiveness of Taggle was demonstrated through a case study involving a domain expert analyzing complex genomics data for drug discovery.

Conclusion: Taggle allows for the exploration and presentation of large and complex tables by employing an item-centric approach with visual encodings for cells and data-driven aggregation. Its interaction methods support specific analysis questions like multi-column sorting and advanced data selection/filtering.

Abstract: Most tabular data visualization techniques focus on overviews, yet many
practical analysis tasks are concerned with investigating individual items of
interest. At the same time, relating an item to the rest of a potentially large
table is important. In this work we present Taggle, a tabular visualization
technique for exploring and presenting large and complex tables. Taggle takes
an item-centric, spreadsheet-like approach, visualizing each row in the source
data individually using visual encodings for the cells. At the same time,
Taggle introduces data-driven aggregation of data subsets. The aggregation
strategy is complemented by interaction methods tailored to answer specific
analysis questions, such as sorting based on multiple columns and rich data
selection and filtering capabilities. We demonstrate Taggle using a case study
conducted by a domain expert on complex genomics data analysis for the purpose
of drug discovery.

</details>


### [68] [Are digital natives spreadsheet natives?](https://arxiv.org/abs/1909.00865)
*Maria Csernoch,Piroska Biró*

Main category: cs.HC

TL;DR: 大学生在电子表格任务中表现不佳，即使他们被认为是“数字原生代”。算法方法优于特定函数。所有学生都需要正规的、高级的算法培训。


<details>
  <summary>Details</summary>
Motivation: 评估“数字原生代”大学生在电子表格环境中的实际算法技能和知识转移能力。

Method: 对信息学专业大一新生进行电子表格环境下的算法技能和知识转移能力测试。

Result: 学生们在解决电子表格问题时遇到了重大困难，表现出明显的趋势。使用基于算法的多层数组公式的学生比使用特定于问题的、不相关的内置函数 Thus 表现更好。

Conclusion: 数字原生代学生在电子表格环境中解决问题时，尽管完成了培训，但仍存在显著困难。基于算法的多层数组公式比特定于问题的、不相关的内置函数更能提升他们的表现。此外，所有学生，无论其数字世代如何，都需要基于高等数学和算法的官方培训，并由专家教师指导。

Abstract: The present paper reports the results of testing first year students of
Informatics on their algorithmic skills and knowledge transfer abilities in
spreadsheet environments. The selection of students plays a crucial role in the
project. On the one hand, they have officially finished their spreadsheet
training - they know everything - while on the other hand, they do not need any
training, since they are digital natives, to whom digital skills are assigned
by birth. However, we found that the students had serious difficulties in
solving the spreadsheet problems presented: so low were their results that it
allowed us to form broad tendencies. Considering computational thinking,
algorithmic skills, and knowledge transfer abilities, it is clear that those
students performed better who used algorithm-based, multilevel array formulas
instead of problem specific, unconnected built-in functions. Furthermore, we
can conclude that students, regardless of their birth date and digital
generation assigned to them, are in great need of official, high-mathability,
algorithm-based training with expert teachers.

</details>


### [69] [Somewhere Around That Number: An Interview Study of How Spreadsheet Users Manage Uncertainty](https://arxiv.org/abs/1905.13072)
*Judith Borghouts,Andrew D. Gordon,Advait Sarkar,Kenton P. O'Hara,Neil Toronto*

Main category: cs.HC

TL;DR: 电子表格用户在处理不确定性时，会受到电子表格的角色和用户目标的影响，而目前的电子表格工具对这些需求的支持有限。


<details>
  <summary>Details</summary>
Motivation: 由于错误和估计等原因，电子表格用户经常会遇到数据不确定性，了解数据不确定性有助于做出更明智的决策，但人们在处理不确定性时常常会忽略或简化它。

Method: 通过对11位不同领域电子表格用户进行访谈研究，了解人们目前在电子表格中如何遇到和处理不确定性。

Result: 电子表格目前提供的支持这些目标的工具有限，并且参与者采用了各种变通方法。

Conclusion: 人们在电子表格中处理不确定性的方式受到电子表格在其工作中所扮演的角色以及用户目标的影响。

Abstract: Spreadsheet users regularly deal with uncertainty in their data, for example
due to errors and estimates. While an insight into data uncertainty can help in
making better informed decisions, prior research suggests that people often use
informal heuristics to reason with probabilities, which leads to incorrect
conclusions. Moreover, people often ignore or simplify uncertainty. To
understand how people currently encounter and deal with uncertainty in
spreadsheets, we conducted an interview study with 11 spreadsheet users from a
range of domains. We found that how people deal with uncertainty is influenced
by the role the spreadsheet plays in people's work and the user's aims.
Spreadsheets are used as a database, template, calculation tool, notepad and
exploration tool. In doing so, participants' aims were to compute and compare
different scenarios, understand something about the nature of the uncertainty
in their situation, and translate the complexity of data uncertainty into
simplified presentations to other people, usually decision-makers. Spreadsheets
currently provide limited tools to support these aims, and participants had
various workarounds.

</details>


### [70] [Characterizing Scalability Issues in Spreadsheet Software using Online Forums](https://arxiv.org/abs/1801.03829)
*Kelly Mack,John Lee,Kevin Chang,Karrie Karahalios,Aditya Parameswaran*

Main category: cs.HC

TL;DR: 本研究利用Reddit论坛数据分析用户在Excel使用中遇到的问题，为下一代电子表格软件设计提供启示。


<details>
  <summary>Details</summary>
Motivation: 为了以更具规模、成本效益和广度的方式来理解用户需求和挑战，本文采用了利用在线论坛数据来审视工具使用情况的新方法，以应对传统可用性研究中时间和成本的限制。

Method: 通过抓取Reddit帖子，收集了用户关于Excel的提问和抱怨数据，并对用户在使用电子表格软件（尤其是在处理大量数据时）时遇到的问题进行了探索和表征。

Result: 研究对用户在使用电子表格软件时遇到的问题进行了表征，特别是与处理大量数据相关的挑战。

Conclusion: 本研究的发现对下一代电子表格软件的设计具有启示意义。

Abstract: In traditional usability studies, researchers talk to users of tools to
understand their needs and challenges. Insights gained via such interviews
offer context, detail, and background. Due to costs in time and money, we are
beginning to see a new form of tool interrogation that prioritizes scale, cost,
and breadth by utilizing existing data from online forums. In this case study,
we set out to apply this method of using online forum data to a specific
issue---challenges that users face with Excel spreadsheets. Spreadsheets are a
versatile and powerful processing tool if used properly. However, with
versatility and power come errors, from both users and the software, which make
using spreadsheets less effective. By scraping posts from the website Reddit,
we collected a dataset of questions and complaints about Excel. Specifically,
we explored and characterized the issues users were facing with spreadsheet
software in general, and in particular, as resulting from a large amount of
data in their spreadsheets. We discuss the implications of our findings on the
design of next-generation spreadsheet software.

</details>


### [71] [The Reification of an Incorrect and Inappropriate Spreadsheet Model](https://arxiv.org/abs/1801.10249)
*Grenville J. Croll*

Main category: cs.HC

TL;DR: 电子表格会使信息实体化，赋予其不应有的属性。


<details>
  <summary>Details</summary>
Motivation: 信息在电子表格中被实体化，获得了它们可能不配拥有的属性。

Method: 通过一个案例研究，近距离观察了一个小型非营利组织中一个明显不正确和不适当的电子表格模型的实体化过程。

Result: 信息在电子表格中被实体化，获得了它们可能不配拥有的属性，例如可信度、正确性、适当性、具体性、完整性、实质性、客观性和权威性。

Conclusion: 信息在电子表格中被实体化，获得了它们可能不配拥有的属性，例如可信度、正确性、适当性、具体性、完整性、实质性、客观性和权威性。

Abstract: Once information is loaded into a spreadsheet, it acquires properties that it
may not deserve. These properties include believability, correctness,
appropriateness, concreteness, integrity, tangibility, objectivity and
authority. The information becomes reified. We describe a case study through
which we were able to observe at close hand the reification of a demonstrably
incorrect and inappropriate spreadsheet model within a small non profit
organisation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [72] [Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE](https://arxiv.org/abs/2506.17330)
*Simon Thorne*

Main category: cs.SE

TL;DR: LLMs are not yet adept at complex spreadsheet tasks. They make mistakes that look right. We need better LLMs for spreadsheets, and we've made a new test called FLARE to check them.


<details>
  <summary>Details</summary>
Motivation: While LLMs show promise in many domains, their effectiveness in spreadsheet-related tasks is underexplored. This study aims to establish a benchmark for evaluating LLMs in this area.

Method: The study introduces a comprehensive benchmark framework to evaluate LLM performance on spreadsheet tasks, including formula generation, data manipulation, and complex real-world scenarios. FLARE (Formula Logic, Auditing, Reasoning and Evaluation) is proposed as a new benchmark specifically for evaluating LLM performance on real-world spreadsheet logic, auditing, and reasoning.

Result: LLMs perform well on simple spreadsheet tasks but struggle with complex, multi-step operations. They tend to generate outputs that appear correct but are factually wrong.

Conclusion: LLMs currently struggle with complex, multi-step spreadsheet operations, often producing plausible but incorrect outputs, highlighting the need for enhanced logical reasoning capabilities and the integration of symbolic reasoning into their architectures.

Abstract: Large Language Models (LLMs) have demonstrated some significant capabilities
across various domains; however, their effectiveness in spreadsheet related
tasks remains underexplored. This study introduces a foundation for a
comprehensive benchmark framework to evaluate the performance of leading LLMs
in executing spreadsheet functions, formula generation and data manipulation
tasks. The benchmark encompasses tasks ranging from basic formula creation to
complex, real world spreadsheet scenarios. Our findings reveal that while LLMs
exhibit proficiency in straightforward tasks, they often falter in complex,
multi step operations, frequently producing plausible yet incorrect outputs.
These results underscore the limitations of current LLMs in handling
spreadsheet tasks that require precise logical reasoning and highlight the need
for integrating symbolic reasoning capabilities into LLM architectures. To
support this, we introduce FLARE (Formula Logic, Auditing, Reasoning and
Evaluation) a new benchmark for evaluating LLM performance on real-world
spreadsheet logic, auditing, and reasoning tasks.

</details>


### [73] [TableTalk: Scaffolding Spreadsheet Development with a Language Agent](https://arxiv.org/abs/2502.09787)
*Jenny T. Liang,Aayush Kumar,Yasharth Bajpai,Sumit Gulwani,Vu Le,Chris Parnin,Arjun Radhakrishna,Ashish Tiwari,Emerson Murphy-Hill,Guastavo Soares*

Main category: cs.SE

TL;DR: TableTalk是一种通过对话帮助程序员构建电子表格的语言代理，它通过提供结构化指导、灵活的工具集成和增量式构建方法，使用户能够更有效地创建电子表格，并降低认知负荷。


<details>
  <summary>Details</summary>
Motivation: 尽管电子表格编程在劳动力中无处不在，但它仍然具有挑战性，因为程序员需要电子表格特定的知识（例如，编写公式的API）和解决问题的能力来创建复杂的电子表格。大型语言模型（LLM）可以通过规划和推理来帮助自动化这一过程，而语言代理能够动态地规划、使用工具和采取迭代行动来完成复杂任务。这些代理通过观察、规划和行动来完成任务，非常适合通过遵循专家流程来支持电子表格编程。

Method: TableTalk是一个语言代理，通过对话帮助程序员构建电子表格。

Result: 一项针对20名程序员的用户研究表明，与基线代理相比，TableTalk生成的电子表格被优先选择的可能性高2.3倍，同时将认知负荷和花费在推理电子表格操作上的时间减少了12.6%。

Conclusion: TableTalk的设计原则为支撑、灵活性和增量性，这源于对七名程序员和62个Excel模板的研究。TableTalk通过生成循序渐进的计划和建议用户可选择的三个后续步骤来构建电子表格。它还集成了支持增量式电子表格构建的工具。一项针对20名程序员的用户研究表明，与基线代理相比，TableTalk生成的电子表格被优先选择的可能性高2.3倍，同时将认知负荷和花费在推理电子表格操作上的时间减少了12.6%。TableTalk的方法对人机协作具有启示意义，包括为停止或撤销代理操作提供持久的直接操作界面，同时确保接受操作的界面可以被禁用。

Abstract: Despite its ubiquity in the workforce, spreadsheet programming remains
challenging as programmers need both spreadsheet-specific knowledge (e.g., APIs
to write formulas) and problem-solving skills to create complex spreadsheets.
Large language models (LLMs) can help automate aspects of this process, and
recent advances in planning and reasoning have enabled language agents, which
dynamically plan, use tools, and take iterative actions to complete complex
tasks. These agents observe, plan, and act, making them well-suited to scaffold
spreadsheet programming by following expert processes.
  We present TableTalk, a language agent that helps programmers build
spreadsheets conversationally. Its design reifies three design principles --
scaffolding, flexibility, and incrementality -- which we derived from two
studies of seven programmers and 62 Excel templates. TableTalk structures
spreadsheet development by generating step-by-step plans and suggesting three
next steps users can choose from. It also integrates tools that enable
incremental spreadsheet construction. A user study with 20 programmers shows
that TableTalk produces spreadsheets 2.3 times more likely to be preferred over
a baseline agent, while reducing cognitive load and time spent reasoning about
spreadsheet actions by 12.6%. TableTalk's approach has implications for
human-agent collaboration. This includes providing persistent direct
manipulation interfaces for stopping or undoing agent actions, while ensuring
that such interfaces for accepting actions can be deactivated.

</details>


### [74] [Overcoming Vision Language Model Challenges in Diagram Understanding: A Proof-of-Concept with XML-Driven Large Language Models Solutions](https://arxiv.org/abs/2502.04389)
*Shue Shiinoki,Ryo Koshihara,Hayato Motegi,Masumi Morishige*

Main category: cs.SE

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: 'NoneType' object has no attribute 'model_dump'

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Diagrams play a crucial role in visually conveying complex relationships and
processes within business documentation. Despite recent advances in
Vision-Language Models (VLMs) for various image understanding tasks, accurately
identifying and extracting the structures and relationships depicted in
diagrams continues to pose significant challenges. This study addresses these
challenges by proposing a text-driven approach that bypasses reliance on VLMs'
visual recognition capabilities. Instead, it utilizes the editable source
files--such as xlsx, pptx or docx--where diagram elements (e.g., shapes, lines,
annotations) are preserved as textual metadata. In our proof-of-concept, we
extracted diagram information from xlsx-based system design documents and
transformed the extracted shape data into textual input for Large Language
Models (LLMs). This approach allowed the LLM to analyze relationships and
generate responses to business-oriented questions without the bottleneck of
image-based processing. Experimental comparisons with a VLM-based method
demonstrated that the proposed text-driven framework yielded more accurate
answers for questions requiring detailed comprehension of diagram
structures.The results obtained in this study are not limited to the tested
.xlsx files but can also be extended to diagrams in other documents with source
files, such as Office pptx and docx formats. These findings highlight the
feasibility of circumventing VLM constraints through direct textual extraction
from original source files. By enabling robust diagram understanding through
LLMs, our method offers a promising path toward enhanced workflow efficiency
and information analysis in real-world business scenarios.

</details>


### [75] [On the Workflows and Smells of Leaderboard Operations (LBOps): An Exploratory Study of Foundation Model Leaderboards](https://arxiv.org/abs/2407.04065)
*Zhimin Zhao,Abdul Ali Bangash,Filipe Roseiro Côgo,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本研究旨在解决基础模型（FM）排行榜缺乏标准化评估指南的问题。通过分析 1,045 个 FM 排行榜，我们确定了五种工作流程模式和八种“排行榜异味”，并提出了改进建议，以提高 FM 选择的透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 基础模型（FM）在软件工程（SE）任务中表现出卓越的适应性，但缺乏标准的 FM 评估和比较指南威胁到 FM 排行榜的透明度，并限制了利益相关者进行有效 FM 选择的能力。本研究旨在了解 FM 排行榜的实际运行情况，并识别其潜在的缺陷和可改进之处。

Method: 通过收集来自五个不同来源（GitHub、Hugging Face Spaces、Papers With Code、电子表格和独立平台）的 1,045 个 FM 排行榜，检查其文档，并与排行榜运营者进行直接沟通，以了解其工作流程。通过卡片分类和协商一致，我们确定了五种不同的工作流程模式，并开发了一个捕获这些工作流程中关键组件及其交互的领域模型。然后，我们在 LBOps 中识别出八种独特的排行榜异味。

Result: 确定了五种不同的工作流程模式，并开发了一个捕获这些工作流程中关键组件及其交互的领域模型。在 LBOps 中识别出八种独特的排行榜异味。

Conclusion: 通过识别和缓解“排行榜异味”，软件工程（SE）团队可以提高“排行榜操作”（LBOps）实践的透明度、可问责性和协作性，从而为基础模型（FM）的比较和选择培养一个更强大、更负责任的生态系统。

Abstract: Foundation models (FM), such as large language models (LLMs), which are
large-scale machine learning (ML) models, have demonstrated remarkable
adaptability in various downstream software engineering (SE) tasks, such as
code completion, code understanding, and software development. As a result, FM
leaderboards have become essential tools for SE teams to compare and select the
best third-party FMs for their specific products and purposes. However, the
lack of standardized guidelines for FM evaluation and comparison threatens the
transparency of FM leaderboards and limits stakeholders' ability to perform
effective FM selection. As a first step towards addressing this challenge, our
research focuses on understanding how these FM leaderboards operate in
real-world scenarios ("leaderboard operations") and identifying potential
pitfalls and areas for improvement ("leaderboard smells"). In this regard, we
collect up to 1,045 FM leaderboards from five different sources: GitHub,
Hugging Face Spaces, Papers With Code, spreadsheet and independent platform, to
examine their documentation and engage in direct communication with leaderboard
operators to understand their workflows. Through card sorting and negotiated
agreement, we identify five distinct workflow patterns and develop a domain
model that captures the key components and their interactions within these
workflows. We then identify eight unique types of leaderboard smells in LBOps.
By mitigating these smells, SE teams can improve transparency, accountability,
and collaboration in current LBOps practices, fostering a more robust and
responsible ecosystem for FM comparison and selection.

</details>


### [76] [MaxMind: A Memory Loop Network to Enhance Software Productivity based on Large Language Models](https://arxiv.org/abs/2408.03841)
*Yuchen Dong,XiaoXiang Fang,Yuchen Hu,Renshuang Jiang,Zhe Jiang*

Main category: cs.SE

TL;DR: 本文提出MaxMind模型，通过Memory-Loop Networks和增强的RAG机制，解决AI在软件自动化操作和工具生成（SOTG）中记忆和知识利用的问题，并在实验中展示了显著的效率和成功率提升。


<details>
  <summary>Details</summary>
Motivation: 当前研究忽视了将实时任务经验转化为系统记忆以及区分现有知识价值的重要性，而这些对于AI在SOTG任务中持续改进至关重要。

Method: 本文提出了Memory-Loop Networks用于及时记忆和经验引用，并增强了RAG机制以进行知识精度分段，结合MaxMind模型来处理SOTG任务。

Result: MaxMind4Sheet（一个电子表格处理系统）的实验表明，任务记忆的积累和循环利用可使任务成功率稳定提升约3%-6%，并能通过记忆迁移将系统任务执行效率提高高达25%，同时解决了LLM在处理专业任务时的再训练问题。

Conclusion: MaxMind模型在SOTG领域具有巨大潜力，可通过记忆积累和循环利用来提升LLM系统的能力和生产力。

Abstract: The application of large language models to facilitate automated software
operations and tool generation (SOTG), thus augmenting software productivity,
mirrors the early stages of human evolution when the ability to create and use
tools accelerated the progress of civilization. These complex tasks require AI
to continuously summarize and improve. Current research often overlooks the
importance of converting real-time task experiences into system memory and
differentiating the value of existing knowledge for future reference. This
paper addresses these issues by evolving external memory models into
Memory-Loop Networks for timely memorization and experience referencing. We
also enhance a RAG mechanism with knowledge precision segmentation to utilize
memory based on value differentiation, and design the MaxMind model for SOTG
accordingly.To demonstrate our approach, we developed MaxMind4Sheet, an
electronic spreadsheet processing system aligned with the MaxMind philosophy.
Comparative experiments with SheetCopilot have demonstrated that the
accumulation and recycling of task memories lead to a steady enhancement in
task success rate, with an improvement rate of approximately 3%-6% per round in
this implementation example. Note that as the memories continue to grow, this
cumulative improvement may be substantial. The inclusion of memory recycling
can also boost the system's task execution efficiency by up to 25%, and it can
address the retraining issue faced by LLMs when handling specialized tasks
through memories transfer.These suggest that MaxMind has significant potential
to enhance the capabilities and productivity of LLM systems in SOTG.

</details>


### [77] [Will Dynamic Arrays finally change the way Models are built?](https://arxiv.org/abs/2006.14706)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: 电子表格的动态数组功能通过减少手动干预，提高了模型的完整性，降低了错误风险。


<details>
  <summary>Details</summary>
Motivation: 电子表格虽然直观易用，但容易出错，不适合严肃的分析和建模任务。需要改进其解决方案的透明度和可读性，以减少错误和风险。

Method: 探讨了电子表格在数据处理中的应用，重点关注了名称、CSE数组公式以及新的动态数组功能，并分析了动态数组在提高专业开发环境中的解决方案完整性方面的潜力。

Result: 新的动态数组功能通过减少手动干预来提高电子表格模型的完整性，从而降低错误和风险。

Conclusion: 电子表格的动态数组模型在提高解决方案的完整性、减少手动干预和降低错误风险方面具有潜力，可以替代传统技术。

Abstract: Spreadsheets offer a supremely successful and intuitive means of processing
and exchanging numerical content. Its intuitive ad-hoc nature makes it hugely
popular for use in diverse areas including business and engineering, yet these
very same characteristics make it extraordinarily error-prone; many would
question whether it is suitable for serious analysis or modelling tasks. A
previous EuSpRIG paper examined the role of Names in increasing solution
transparency and providing a readable notation to forge links with the problem
domain. Extensive use was made of CSE array formulas, but it is acknowledged
that their use makes spreadsheet development a distinctly cumbersome task.
Since that time, the new dynamic arrays have been introduced and array
calculation is now the default mode of operation for Excel. This paper examines
the thesis that their adoption within a more professional development
environment could replace traditional techniques where solution integrity is
important. A major advantage of fully dynamic models is that they require less
manual intervention to keep them updated and so have the potential to reduce
the attendant errors and risk.

</details>


### [78] [A Structured Approach to the development of Solutions in Excel](https://arxiv.org/abs/1704.01142)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: Spreadsheets need better structure. This paper explores using unusual Excel tricks to make formulas work like code, preventing errors and managing errors as spreadsheets get bigger.


<details>
  <summary>Details</summary>
Motivation: Spreadsheets lack structural elements for scalability and error prevention beyond single-cell formulas, despite their success in democratizing data manipulation.

Method: This paper considers the use of controversial or lesser-used techniques to create a coherent solution strategy in which the problem is solved by a sequence of formulas resembling the steps of a programmed language.

Result: The paper aims to demonstrate how to build scalable spreadsheet solutions by structuring formulas like a programming language.

Conclusion: The paper suggests using unconventional spreadsheet techniques to mimic programming language structures for more scalable and error-resistant solutions.

Abstract: Spreadsheets offer a supremely successful democratisation platform, placing
the manipulation and presentation of numbers within the grasp of users that
have little or no mathematical expertise or IT experience. What appears to be
almost completely lacking within a "normal" solution built using Excel default
settings is the deployment of any structure that extends beyond a single-cell
formula. The structural elements that allow conventional code to scale without
escalating errors appear to be absent. This paper considers the use of
controversial or lesser-used techniques to create a coherent solution strategy
in which the problem is solved by a sequence of formulas resembling the steps
of a programmed language.

</details>


### [79] [Spreadsheet-based Configuration of Families of Real-Time Specifications](https://arxiv.org/abs/2310.20395)
*José Proença,David Pereira,Giann Spilere Nandi,Sina Borrami,Jonas Melchert*

Main category: cs.SE

TL;DR: This paper simplifies real-time system model checking by using Excel to manage variations in specifications, making it easier for developers and enabling analysis of feature combinations.


<details>
  <summary>Details</summary>
Motivation: Model checking real-time systems is challenging due to the need to balance detail and state explosion. This work aims to simplify the model checking of variations in real-time specifications by leveraging the variability of formal models and requirements.

Method: The approach exploits the variability of formal models and requirements to facilitate model checking of real-time specification variations. Configuration is managed through structured MS Excel spreadsheets, which are automatically processed by a prototype tool to generate instances and run the model checker.

Result: This work facilitates the model checking of variations of real-time specifications by exploiting variability in formal models and requirements, using a user-friendly spreadsheet-based interface.

Conclusion: The paper proposes an extension to previous work by exploiting analysis over valid feature combinations, while maintaining a simple spreadsheet-based interface for the model checker.

Abstract: Model checking real-time systems is complex, and requires a careful trade-off
between including enough detail to be useful and not too much detail to avoid
state explosion. This work exploits variability of the formal model being
analysed and the requirements being checked, to facilitate the model-checking
of variations of real-time specifications. This work results from the
collaboration between academics and Alstom, a railway company with a concrete
use-case, in the context of the VALU3S European project. The configuration of
the variability of the formal specifications is described in MS Excel
spreadsheets with a particular structure, making it easy to use also by
developers. These spreadsheets are processed automatically by our prototype
tool that generates instances and runs the model checker. We propose the
extension of our previous work by exploiting analysis over valid combination of
features, while preserving the simplicity of a spreadsheet-based interface with
the model checker.

</details>


### [80] [SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models](https://arxiv.org/abs/2305.19308)
*Hongxin Li,Jingran Su,Yuntao Chen,Qing Li,Zhaoxiang Zhang*

Main category: cs.SE

TL;DR: SheetCopilot uses LLMs to control spreadsheets via natural language, automating repetitive tasks with a 44.3% success rate, beating code generation methods.


<details>
  <summary>Details</summary>
Motivation: End users spend billions of hours on repetitive and error-prone tasks like tabular data processing and scheduling, often lacking the skills to automate them. LLMs present a promising opportunity to enable natural language control of software for these tasks.

Method: The proposed SheetCopilot agent utilizes a set of atomic actions representing spreadsheet functionalities and a state machine-based task planning framework for LLM-driven software interaction. A dataset of 221 tasks was curated, along with an automated evaluation pipeline for benchmarking.

Result: SheetCopilot achieved a 44.3% task completion rate in a single generation, significantly outperforming a strong code generation baseline.

Conclusion: SheetCopilot, a novel agent leveraging LLMs and a state machine framework, demonstrates significant improvements in automating spreadsheet tasks via natural language commands, outperforming existing code generation baselines.

Abstract: Computer end users have spent billions of hours completing daily tasks like
tabular data processing and project timeline scheduling. Most of these tasks
are repetitive and error-prone, yet most end users lack the skill to automate
these burdensome works. With the advent of large language models (LLMs),
directing software with natural language user requests become a reachable goal.
In this work, we propose a SheetCopilot agent that takes natural language task
and control spreadsheet to fulfill the requirements. We propose a set of atomic
actions as an abstraction of spreadsheet software functionalities. We further
design a state machine-based task planning framework for LLMs to robustly
interact with spreadsheets. We curate a representative dataset containing 221
spreadsheet control tasks and establish a fully automated evaluation pipeline
for rigorously benchmarking the ability of LLMs in software control tasks. Our
SheetCopilot correctly completes 44.3\% of tasks for a single generation,
outperforming the strong code generation baseline by a wide margin. Our project
page:https://sheetcopilot.github.io/.

</details>


### [81] [Excel as a Turing-complete Functional Programming Environment](https://arxiv.org/abs/2309.00115)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: Excel's Dynamic Arrays feature allows for more formal programming approaches to spreadsheet solutions, with emerging trends showing a shift from traditional practices.


<details>
  <summary>Details</summary>
Motivation: To show that the ad-hoc end user practices of traditional spreadsheets can be replaced by radically different approaches due to the major upgrade of Excel's calculation engine to accommodate Dynamic Arrays.

Method: Discussing trends emerging from pioneering work within the Excel community.

Result: It is too early to guess the extent to which the new functionality will be adopted by the business and engineering communities and the impact that may have upon risk, but some trends are emerging.

Conclusion: The ad-hoc end user practices of traditional spreadsheets can be replaced by radically different approaches that have far more in common with formal programming.

Abstract: Since the calculation engine of Excel was the subject of a major upgrade to
accommodate Dynamic Arrays in 2018 there has been a series of seismic changes
to the art of building spreadsheet solutions. This paper will show the ad-hoc
end user practices of traditional spreadsheets can be replaced by radically
different approaches that have far more in common with formal programming. It
is too early to guess the extent to which the new functionality will be adopted
by the business and engineering communities and the impact that may have upon
risk. Nevertheless, some trends are emerging from pioneering work within the
Excel community which we will discuss here.

</details>


### [82] [A Use Case-Engineering Resources Taxonomy for Analytical Spreadsheet Models](https://arxiv.org/abs/2309.00104)
*Thomas A. Grossman,Vijay Mehrotra*

Main category: cs.SE

TL;DR: 这篇论文提出了一个包含九种类型的分析电子表格模型分类法，考虑了用例和工程资源，有助于识别开发指南的适用性，并为审视电子表格错误和风险提供视角。


<details>
  <summary>Details</summary>
Motivation: 为了对分析电子表格模型进行分类，扩展了先前三类分类法，以识别九种类型的电子表格模型，并区分“分析解决方案”和“工业级分析电子表格模型”。

Method: 提出了一个包括九种类型的分析电子表格模型分类法，该分类法考虑了电子表格的用例和开发过程中投入的工程资源。

Result: 该分类法能够对分析电子表格模型进行分类，并为审视电子表格错误和风险提供视角。

Conclusion: 该分类法有助于识别各种电子表格开发指南的适用性，为审视电子表格错误和风险提供了一个视角，并为理解电子表格如何随时间变化提供了一个结构。此分类法为包括其自身在内的许多有趣的研究问题开辟了道路。

Abstract: This paper presents a taxonomy for analytical spreadsheet models. It
considers both the use case that a spreadsheet is meant to serve, and the
engineering resources devoted to its development. We extend a previous
three-type taxonomy, to identify nine types of spreadsheet models, that
encompass the many analytical spreadsheet models seen in the literature. We
connect disparate research literature to distinguish between an "analytical
solution" and an "industrial-quality analytical spreadsheet model". We explore
the nature of each of the nine types, propose definitions for some, relate them
to the literature, and hypothesize on how they might arise. The taxonomy aids
in identifying where various spreadsheet development guidelines are most
useful, provides a lens for viewing spreadsheet errors and risk, and offers a
structure for understanding how spreadsheets change over time. This taxonomy
opens the door to many interesting research questions, including refinements to
itself.

</details>


### [83] [Experimenting with ChatGPT for Spreadsheet Formula Generation: Evidence of Risk in AI Generated Spreadsheets](https://arxiv.org/abs/2309.00095)
*Simon Thorne*

Main category: cs.SE

TL;DR: ChatGPT 在生成电子表格公式方面表现出潜力，但在信息有限或问题复杂的情况下准确性会下降，并可能出现幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）已变得足够复杂，可以通过解释纯英文句子来创建复杂的计算机程序，并以 Python、Java Script、C++ 和电子表格等多种现代语言实现。这些工具功能强大且相对准确，因此，无论使用者背景或知识如何，都能广泛接触计算机编程。

Method: 本论文提出了一系列使用 ChatGPT 的实验，以探索该工具在需要推断、推理和解决问题的场景中生成有效电子表格公式及相关计算输出的能力。

Result: 结果表明，在某些情况下，ChatGPT 可以在正确的推理、推断和演绎下生成正确的电子表格公式。

Conclusion: 当信息有限、不确定或问题过于复杂时，ChatGPT 的准确性会下降，其推理、推断和演绎能力也会随之下降。此外，这还会导致错误的陈述和“幻觉”，从而破坏创建电子表格公式的过程。

Abstract: Large Language Models (LLM) have become sophisticated enough that complex
computer programs can be created through interpretation of plain English
sentences and implemented in a variety of modern languages such as Python, Java
Script, C++ and Spreadsheets. These tools are powerful and relatively accurate
and therefore provide broad access to computer programming regardless of the
background or knowledge of the individual using them. This paper presents a
series of experiments with ChatGPT to explore the tool's ability to produce
valid spreadsheet formulae and related computational outputs in situations
where ChatGPT has to deduce, infer and problem solve the answer. The results
show that in certain circumstances, ChatGPT can produce correct spreadsheet
formulae with correct reasoning, deduction and inference. However, when
information is limited, uncertain or the problem is too complex, the accuracy
of ChatGPT breaks down as does its ability to reason, infer and deduce. This
can also result in false statements and "hallucinations" that all subvert the
process of creating spreadsheet formulae.

</details>


### [84] [Demonstration of CORNET: A System For Learning Spreadsheet Formatting Rules By Example](https://arxiv.org/abs/2308.07357)
*Mukul Singh,Jose Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Gust Verbruggen*

Main category: cs.SE

TL;DR: CORNET is an Excel add-in that automatically learns and suggests conditional formatting rules from user examples.


<details>
  <summary>Details</summary>
Motivation: Users currently need to manually write conditional formatting rules in spreadsheet software, which is inconvenient.

Method: CORNET combines symbolic rule enumeration (using semi-supervised clustering and iterative decision tree learning) with a neural ranker to synthesize conditional formatting rules.

Result: CORNET generates accurate conditional formatting rule suggestions based on user-provided examples, enhancing the usability of spreadsheet software.

Conclusion: CORNET can automatically learn conditional formatting rules from user examples, demonstrated as an Excel add-in that suggests rules after the user provides a few formatted cells.

Abstract: Data management and analysis tasks are often carried out using spreadsheet
software. A popular feature in most spreadsheet platforms is the ability to
define data-dependent formatting rules. These rules can express actions such as
"color red all entries in a column that are negative" or "bold all rows not
containing error or failure." Unfortunately, users who want to exercise this
functionality need to manually write these conditional formatting (CF) rules.
We introduce CORNET, a system that automatically learns such conditional
formatting rules from user examples. CORNET takes inspiration from inductive
program synthesis and combines symbolic rule enumeration, based on
semi-supervised clustering and iterative decision tree learning, with a neural
ranker to produce accurate conditional formatting rules. In this demonstration,
we show CORNET in action as a simple add-in to Microsoft Excel. After the user
provides one or two formatted cells as examples, CORNET generates formatting
rule suggestions for the user to apply to the spreadsheet.

</details>


### [85] [Excel Spreadsheet Analyzer](https://arxiv.org/abs/2211.06333)
*Amir Nassereldine,Patrick Chen,Jinjun Xiong*

Main category: cs.SE

TL;DR: A tool and Python library are introduced to transfer spreadsheet data to Python while preserving formulas and dependencies, enabling data analysis in Python.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the loss of information such as formulas and cell dependencies when transferring spreadsheet data to scientific programming languages like Python, despite the growing trend of data scientists moving towards these languages for their analytical capabilities.

Method: The proposed method involves creating an Abstract Intermediate Representation (AIR) of a spreadsheet to preserve inter-dependency information during the transfer to scientific programming languages. A Python library is built on top of this tool to enable data analysis in Python.

Result: The tool creates an AIR of a spreadsheet, preserving inter-dependency information. A Python library is built on top of this tool to perform data analysis, facilitating the transfer from spreadsheets to Python.

Conclusion: The paper proposes a tool that creates an Abstract Intermediate Representation (AIR) of a spreadsheet to facilitate the transfer of spreadsheet data and its inter-dependencies into scientific programming languages like Python, along with a Python library for data analysis.

Abstract: Spreadsheets are widely used in various fields to do large numerical
analysis. While several companies have relied on spreadsheets for decades, data
scientists are going in the direction of using scientific programming languages
such as python to do their data analysis due to the support, community, and
vast amount of libraries. While using python to analyze a company's
spreadsheets, some information such as the formulas and dependencies of a cell
are lost. We propose a tool that creates an abstract intermediate
representation (AIR) of a spreadsheet. This representation facilitates the
transfer from spreadsheets into scientific programming languages while
preserving inter-dependency information about data. In addition to that, we
build a python library on top of our tool to perform some data analysis in
python.

</details>


### [86] [Exploring Spreadsheet Use and Practices in a Technologically Constrained Setting](https://arxiv.org/abs/2201.07696)
*Khwima Mckinley Mkamanga,Simon Thorne*

Main category: cs.SE

TL;DR: 电子表格在马拉维一家水务公司的普及促进了业务自动化，但也带来了管理、技术和人为方面的风险。需要制定更全面的政策和法规来改进电子表格的开发和使用。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在了解电子表格在水务公司中的使用情况及其对业务运营的影响，以期为未来管理电子表格相关风险提供指导。

Method: 本研究探讨了电子表格对马拉维（撒哈拉以南非洲）一家水务国有企业业务运营的影响。该组织是一家在技术欠发达国家运营的半政府机构的典型代表。本研究侧重于电子表格的适用范围和生命周期以及组织政策和治理。

Result: 总体而言，研究结果表明，电子表格在该组织的普及为业务自动化提供了有利环境。该研究还强调了导致电子表格广泛使用相关高风险的管理、技术和人为因素问题。

Conclusion: 该研究证实，在实施旨在规范电子表格开发流程和采用的全面政策和法规的许多方面，都有很大的改进空间。

Abstract: This paper explores the impacts of spreadsheets on business operations in a
water utility parastatal in Malawi, Sub-Saharan Africa. The organisation is a
typical example of a semi-government body operating in a technologically
underdeveloped country. The study focused on spreadsheet scope of use and life
cycle as well as organisational policy and governance. The results will help
define future spreadsheet usage by influencing new approaches for managing
potential risks associated with spreadsheets in the organization. Generally,
findings indicate that the proliferation of spreadsheets in the organization
has provided an enabling environment for business automation. The paper also
highlights management, technological and human factor issues contributing to
high risks associated with the pervasive spreadsheet use. The conclusions drawn
from the research confirms that there is ample room for improvement in many
areas such as implementation of comprehensive policies and regulations
governing spreadsheet development processes and adoption.

</details>


### [87] [Methodology for Assessing the State of the Practice for Domain X](https://arxiv.org/abs/2110.11575)
*Spencer Smith,Jacques Carette,Peter Michalski,Ao Dong,Olu Owojaiye*

Main category: cs.SE

TL;DR: 研究人员提出了一种评估研究软件开发实践的新方法，通过收集和分析代码、文档、仓库数据以及开发者访谈，并使用AHP进行排名，以了解各领域的实践现状、使用的工具、开发者的痛点以及为提高软件质量所采取的措施。


<details>
  <summary>Details</summary>
Motivation: 为了改进研究软件的开发方法和工具，首先需要了解该领域的实践现状。

Method: 该研究提出了一种评估研究软件开发实践状态的方法论，包括识别领域、筛选候选软件包、收集源代码和文档、收集仓库相关数据、填写包含108个问题的测量模板、进行包含20个问题的开发者访谈、使用分析层次法（AHP）对软件进行排名，以及分析数据以回答关于构件、工具、原则、流程、方法论、痛点和改进质量的实践问题。该方法论强调在整个过程中让领域专家参与，以确保隐性信息的正确表示并分析软件包之间的共性和差异性。

Result: 该方法论提供了一套系统化的步骤和工具（电子表格模板和AHP工具）来评估研究软件开发实践的现状，并估计完成一个领域的评估大约需要173个人时。

Conclusion: 该方法论能够系统地评估研究软件开发实践的现状。

Abstract: To improve software development methods and tools for research software, we
first need to understand the current state of the practice. Therefore, we have
developed a methodology for assessing the state of the software development
practices for a given research software domain. For each domain we wish to
answer questions such as: i) What artifacts (documents, code, test cases, etc.)
are present? ii) What tools are used? iii) What principles, process and
methodologies are used? iv) What are the pain points for developers? v) What
actions are used to improve qualities like maintainability and reproducibility?
To answer these questions, our methodology prescribes the following steps: i)
Identify the domain; ii) Identify a list of candidate software packages; iii)
Filter the list to a length of about 30 packages; iv) Gather source code and
documentation for each package; v) Collect repository related data on each
software package, like number of stars, number of open issues, number of lines
of code; vi) Fill in the measurement template (the template consists of 108
questions to assess 9 qualities (including the qualities of installability,
usability and visibility)); vii) Interview developers (the interview consists
of 20 questions and takes about an hour); viii) Rank the software using the
Analytic Hierarchy Process (AHP); and, ix) Analyze the data to answer the
questions posed above. A domain expert should be engaged throughout the
process, to ensure that implicit information about the domain is properly
represented and to assist with conducting an analysis of the commonalities and
variabilities between the 30 selected packages. Using our methodology,
spreadsheet templates and AHP tool, we estimate (based on our experience with
using the process) the time to complete an assessment for a given domain at 173
person hours.

</details>


### [88] [Agile Elicitation of Scalability Requirements for Open Systems: A Case Study](https://arxiv.org/abs/2108.00567)
*Gunnar Brataas,Antonio Martini,Geir Kjetil Hanssen,Georg Ræder*

Main category: cs.SE

TL;DR: ScrumScale模型是一个用于敏捷软件开发中获取可扩展性需求的轻量级工具，通过案例研究证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 敏捷软件开发中，引发可扩展性需求是复杂且研究不足的。

Method: 设计科学研究，利用协调理论，并以开放银行案例研究的形式进行。

Result: ScrumScale模型被证明是一个轻量级的可扩展性需求获取工具，在开放银行案例研究中，用户（TietoEVRY）花费了55小时使用该模型，并认为它提供了系统化的需求产出方式和显著的沟通优势。

Conclusion: ScrumScale模型为敏捷软件开发中的可扩展性需求提供了系统化的方法，并为理解和沟通可扩展性概念提供了优势。

Abstract: Eliciting scalability requirements during agile software development is
complicated and poorly described in previous research. This article presents a
lightweight artifact for eliciting scalability requirements during agile
software development: the ScrumScale model. The ScrumScale model is a simple
spreadsheet. The scalability concepts underlying the ScrumScale model are
clarified in this design science research, which also utilizes coordination
theory. This paper describes the open banking case study, where a legacy
banking system becomes open. This challenges the scalability of this legacy
system. The first step in understanding this challenge is to elicit the new
scalability requirements. In the open banking case study, key stakeholders from
TietoEVRY spent 55 hours eliciting TietoEVRY's open banking project's
scalability requirements. According to TietoEVRY, the ScrumScale model provided
a systematic way of producing scalability requirements. For TietoEVRY, the
scalability concepts behind the ScrumScale model also offered significant
advantages in dialogues with other stakeholders.

</details>


### [89] [SpreadsheetCoder: Formula Prediction from Semi-structured Context](https://arxiv.org/abs/2106.15339)
*Xinyun Chen,Petros Maniatis,Rishabh Singh,Charles Sutton,Hanjun Dai,Max Lin,Denny Zhou*

Main category: cs.SE

TL;DR: SpreadsheetCoder是一种新的基于BERT的模型，可以利用电子表格的表头和表格数据来合成公式。它比以往的方法更能帮助用户编写公式。


<details>
  <summary>Details</summary>
Motivation: 之前的研究主要利用输入输出示例作为电子表格公式合成的规范，但这种方法未能完全捕捉真实电子表格中丰富的上下文，例如表格组织、行列依赖性以及表头信息

Method: 提出了一种名为SpreadsheetCoder的基于BERT的模型架构，该架构能够以基于行和基于列的格式来表示表格上下文

Result: SpreadsheetCoder实现了42.51%的预测准确率，相比之下，不使用丰富表格上下文的基线方法的准确率有显著提高

Conclusion: SpreadsheetCoder在Google表格上协助了82%更多的用户编写公式，与不使用丰富表格上下文的基线相比，其预测准确率提高了

Abstract: Spreadsheet formula prediction has been an important program synthesis
problem with many real-world applications. Previous works typically utilize
input-output examples as the specification for spreadsheet formula synthesis,
where each input-output pair simulates a separate row in the spreadsheet.
However, this formulation does not fully capture the rich context in real-world
spreadsheets. First, spreadsheet data entries are organized as tables, thus
rows and columns are not necessarily independent from each other. In addition,
many spreadsheet tables include headers, which provide high-level descriptions
of the cell data. However, previous synthesis approaches do not consider
headers as part of the specification. In this work, we present the first
approach for synthesizing spreadsheet formulas from tabular context, which
includes both headers and semi-structured tabular data. In particular, we
propose SpreadsheetCoder, a BERT-based model architecture to represent the
tabular context in both row-based and column-based formats. We train our model
on a large dataset of spreadsheets, and demonstrate that SpreadsheetCoder
achieves top-1 prediction accuracy of 42.51%, which is a considerable
improvement over baselines that do not employ rich tabular context. Compared to
the rule-based system, SpreadsheetCoder assists 82% more users in composing
formulas on Google Sheets.

</details>


### [90] [From webtables to datatables](https://arxiv.org/abs/2006.14694)
*Mária Csernoch*

Main category: cs.SE

TL;DR: This paper shows how to convert tables from webpages (like LOL Boards) into spreadsheets, offering two ways to do it for educational and problem-solving purposes.


<details>
  <summary>Details</summary>
Motivation: Webtables are valuable for teaching computational thinking, spreadsheet skills, and problem-solving in various professional contexts. This paper focuses on a specific application using data from LOL Boards.

Method: The paper presents an algorithm for converting Webtables, specifically from LOL Boards, and provides two implementation solutions: one in a word processor and another purely in a spreadsheet application.

Result: Two solutions for converting and utilizing Webtables from LOL Boards are presented, one in a word processor and one in a spreadsheet, allowing for further discussion and development.

Conclusion: The paper details the conversion process of a LOL Boards table into spreadsheet format, offering two distinct solutions for implementation and further discussion.

Abstract: Webtables -- tables and table-like structures on webpages -- are excellent
sources for teaching spreadsheeting, in commercial and professional
organisations by utilizing and developing knowledge-transfer items, presenting
and handling various real-world problems and solutions, discussing and
debugging, and in general, developing and utilizing computational thinking
skills. In the present paper the conversion process of one of the LOL Boards
(League of Legends, Riot Games Inc. 2019) is detailed. After presenting the
algorithm of the conversion, two solutions are offered -- one in a word
processor, the other purely in a spreadsheet application -- leaving space for
discussions, inventing other solutions and combining them.

</details>


### [91] [Implementation Strategies for Multidimensional Spreadsheets](https://arxiv.org/abs/2006.05814)
*Paul Mireault*

Main category: cs.SE

TL;DR: Excel专家通过投影或数据库方法处理多维变量，数据库方法公式更简单。


<details>
  <summary>Details</summary>
Motivation: 了解Excel开发人员在处理多维变量时采用的实现策略。

Method: 通过邀请有经验的Excel开发人员参与挑战，实现包含多维变量的电子表格，并分析他们所采用的不同实现策略。

Result: 大多数参与者将多维变量投影到二维平面上，少数参与者采用了数据库方法，这种方法简化了公式。

Conclusion: Excel开发人员在实现多维变量电子表格时，主要采用两种策略：一种是将多维变量投影到Excel的二维平面上，另一种是采用数据库方法，将多维变量表示为带有主键的数据集表格，这种方法能简化公式。

Abstract: Seasoned Excel developers were invited to participate in a challenge to
implement a spreadsheet with multi-dimensional variables. We analyzed their
spreadsheet to see the different implement strategies employed. We identified
two strategies: most participants used a projection of three or
four-dimensional variables on the two-dimensional plane used by Excel. A few
participants used a database approach where the multi-dimensional variables are
presented in the form of a dataset table with the appropriate primary key. This
approach leads to simpler formulas.

</details>


### [92] [Abstracting spreadsheet data flow through hypergraph redrawing](https://arxiv.org/abs/2006.04794)
*David Birch,Nicolai Stawinoga,Jack Binks,Bruno Nicoletti,Paul Kelly*

Main category: cs.SE

TL;DR: Spreadsheets are error-prone because they use low-level cells. This paper suggests representing spreadsheets as graphs with cells as hypergraph edges, raising the abstraction level to better model the real world and detect operations like vector calculations.


<details>
  <summary>Details</summary>
Motivation: Traditional spreadsheets are error-prone due to their low level of abstraction, forcing end-user programmers to construct data models from low-level cells containing only scalar values with hidden linkages.

Method: The proposed method involves transforming spreadsheets into fine-grained graphs where operators and values are nodes, and cells are represented as hypergraph edges by drawing a boundary around a set of operator/data nodes. Common sub-expression identification and sub-tree isomorphisms are used to detect vector (array) operations.

Result: The approach illustrates how redrawing cell boundaries can lead to higher-level abstractions, enabling the detection of common sub-expressions and vector operations through techniques like sub-tree isomorphism.

Conclusion: The paper proposes a method to raise the level of abstraction of spreadsheets by transforming them into fine-grained graphs with operators and values as nodes, and representing cells as hypergraph edges. This approach aims to expose hidden linkage structures and allow for higher-level cell definitions that better represent the end-user's mental model.

Abstract: We believe the error prone nature of traditional spreadsheets is due to their
low level of abstraction. End user programmers are forced to construct their
data models from low level cells which we define as "a data container or
manipulator linked by user-intent to model their world and positioned to
reflect its structure". Spreadsheet cells are limited in what they may contain
(scalar values) and the links between them are inherently hidden. This paper
proposes a method of raising the level of abstraction of spreadsheets by
"redrawing the boundary" of the cell. To expose the hidden linkage structure we
transform spreadsheets into fine-grained graphs with operators and values as
nodes. "cells" are then represented as hypergraph edges by drawing a boundary
"wall" around a set of operator/data nodes. To extend what cells may contain
and to create a higher level model of the spreadsheet we propose that
researchers should seek techniques to redraw these boundaries to create higher
level "cells" which will more faithfully represent the end-user's real
world/mental model. We illustrate this approach via common sub-expression
identification and the application of sub-tree isomorphisms for the detection
of vector (array) operations.

</details>


### [93] [Comprehensive review for common types of errors using spreadsheets](https://arxiv.org/abs/1912.09209)
*Ali Aburas*

Main category: cs.SE

TL;DR: 本文对电子表格错误查找和修复的方法进行了全面的回顾和分类。


<details>
  <summary>Details</summary>
Motivation: 由于电子表格的广泛使用和其中错误的高发率，需要对预防、检测和纠正电子表格错误的方法进行综述。

Method: 对电子表格错误查找和修复的现有方法进行了全面的回顾和分类。

Result: 对电子表格错误查找和修复的最新研究方法进行了描述和分类，并讨论了这些方法所能发现的错误类型以及用户常犯的错误类型。

Conclusion: 研究工作对电子表格错误查找和修复的方法进行了全面的回顾和分类，并讨论了该领域最新的研究方法及其发现电子表格中错误的类型，同时还探讨了终端用户在电子表格中常犯的错误类型。

Abstract: Thanks to their flexibility and capability to perform different tasks and
organize data in the best form and format, spreadsheets are widely used in
different organizations and by different end users. Many business organizations
rely on spreadsheets to fulfill their various tasks. On the other hand, the
number of spreadsheets that contain errors are very high, thus researchers have
developed different tools aimed at the prevention, detection, and correction of
errors in spreadsheets. This research work is a comprehensive review that
describes and classifies approaches on finding and fixing errors in
spreadsheets. The paper discusses up-to-date research work approaches in terms
of definition, how they work, and kinds of errors they can find in
spreadsheets. The paper looks also for the kinds of errors that end users
commonly make in spreadsheets.

</details>


### [94] [A Coding-free Software Framework of Developing Web Data Management Systems](https://arxiv.org/abs/1910.05685)
*Can Yang,Shiying Pan,Runmin Li,Yu Liu,Lizhang Peng*

Main category: cs.SE

TL;DR: This paper presents a coding-free method for constructing data management systems on the cloud, leveraging SaaS architecture. It offers a universal platform and a method using spreadsheet requirements to develop customized systems, demonstrating feasibility and availability through empirical results.


<details>
  <summary>Details</summary>
Motivation: More and more enterprises intend to deploy data management systems in the cloud. However, it is still difficult for non-programmers to develop these systems due to the professionalism of software development. The development of SaaS brings forth the feasibility of coding-free software development.

Method: Based on the SaaS architecture, this paper presents a set of theory and method for coding-free construction of a data management system, which involves a practical application platform, a set of construction method, and a set of interface on data exchange. The platform maps the requirements table into a system instance by parsing the table model and implementing the objective system in the running stage.

Result: A universal web platform is designed to quickly generate and publish customized system instances. A method is proposed to develop a data management system using a specific requirements table in a spreadsheet.

Conclusion: The empirical result demonstrates the feasibility and availability of the coding-free method in developing web data management systems.

Abstract: More and more enterprises recently intend to deploy data management systems
in the cloud. Due to the professionalism of software development, it has still
been difficult for non-programmers to develop this kind of systems, even a
small one. However, the development of SaaS brings forth the more feasibility
of coding-free software development than before. Based on the SaaS
architecture, this paper presents a set of theory and method for coding-free
construction of a data management system, on which our contributions involve in
a practical application platform, a set of construction method and a set of
interface on data exchange. By abstracting the common features of data
management systems, we design a universal web platform to quickly generate and
publish customized system instances. Moreover, we propose a kind of method to
develop a data management system using a specific requirements table in
spreadsheet. The corresponding platform maps the requirements table into a
system instance through parsing the table model and implementing the objective
system in the running stage. Finally, we implement the proposed framework and
deploy it on web. The empirical result demonstrates the feasibility and
availability of the coding-free method in developing web data management
systems.

</details>


### [95] [Structured Spreadsheet Modelling and Implementation with Multiple Dimensions -- Part 2: Implementation](https://arxiv.org/abs/1909.00891)
*Paul Mireault*

Main category: cs.SE

TL;DR: This paper details the steps to create a maintainable spreadsheet from a multi-dimensional problem's conceptual model (Formula Diagram and List).


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a way to implement multi-dimensional problems in a spreadsheet that is easy to maintain, building upon previous work on conceptual models.

Method: The method involves presenting a conceptual model as a Formula Diagram for a global view and a Formula List for a precise view of variable interactions, then implementing this model into a spreadsheet.

Result: The result is a spreadsheet implementation of a multi-dimensional problem that is easy to maintain.

Conclusion: The paper provides precise steps to implement a multi-dimensional problem into a spreadsheet for easy maintenance.

Abstract: In Part 1, we showed how to develop a conceptual model of a problem involving
variables of multiple dimensions, like Products, Regions, Sectors and Months.
The conceptual model is presented as a Formula Diagram, giving a global view of
the interaction between all the variables, and a Formula List, giving a precise
view of the interaction between the variables. In this paper, we present
precise steps to implement a multi-dimensional problem in a way that will
produce a spreadsheet that is easy to maintain

</details>


### [96] [On the Refinement of Spreadsheet Smells by means of Structure Information](https://arxiv.org/abs/1810.04542)
*Patrick Koch,Birgit Hofer,Franz Wotawa*

Main category: cs.SE

TL;DR: 通过利用推断出的结构信息来改进电子表格异味检测，减少了错误报告并发现了新问题。


<details>
  <summary>Details</summary>
Motivation: 当前电子表格异味检测技术存在缺陷，会导致不正确或冗余的报告，使电子表格用户不知所措。

Method: 提出了一种静态分析方法来推断相关单元格的集群和块，并在此基础上改进了现有的异味检测技术，同时提出了三种利用推断出的电子表格结构的新型异味检测技术。

Result: 实验评估表明，改进后的技术成功减少了不正确和冗余的异味报告数量，并且新引入的异味揭示了新的缺陷。

Conclusion: 该研究通过利用推断出的结构信息来改进电子表格异味检测技术，成功减少了不正确和冗余的异味报告，并揭示了新的缺陷。

Abstract: Spreadsheet users are often unaware of the risks imposed by poorly designed
spreadsheets. One way to assess spreadsheet quality is to detect smells which
attempt to identify parts of spreadsheets that are hard to comprehend or
maintain and which are more likely to be the root source of bugs.
Unfortunately, current spreadsheet smell detection techniques suffer from a
number of drawbacks that lead to incorrect or redundant smell reports. For
example, the same quality issue is often reported for every copy of a cell,
which may overwhelm users. To deal with these issues, we propose to refine
spreadsheet smells by exploiting inferred structural information for smell
detection. We therefore first provide a detailed description of our static
analysis approach to infer clusters and blocks of related cells. We then
elaborate on how to improve existing smells by providing three example
refinements of existing smells that incorporate information about cell groups
and computation blocks. Furthermore, we propose three novel smell detection
techniques that make use of the inferred spreadsheet structures. Empirical
evaluation of the proposed techniques suggests that the refinements
successfully reduce the number of incorrectly and redundantly reported smells,
and novel deficits are revealed by the newly introduced smells.

</details>


### [97] [Now You're Thinking With Structures: A Concept for Structure-based Interactions with Spreadsheets](https://arxiv.org/abs/1809.03435)
*Patrick Koch*

Main category: cs.SE

TL;DR: Spreadsheets are complex; a new tool uses structure-aware functionality to enhance understanding and interaction, improving productivity and quality.


<details>
  <summary>Details</summary>
Motivation: Cognition of complex systems is facilitated by having a higher order mental model of the system in question to work with. This concept extends previous work on structure inference in the domain.

Method: Implemented a tool for structure inference and visualization along the common spreadsheet layout, with plans to introduce proactive and reactive interaction mechanics and provide structure-aware functionality as an add-in for common spreadsheet processors.

Result: Structural information is used to enrich visualizations, reactively enhance traditional user actions, and provide tools to proactively alter the overall spreadsheet makeup instead of individual cells.

Conclusion: Spreadsheets are hard to comprehend and adapt after reaching a certain complexity, but structure-aware functionality can help. The intended systems should provide an additional layer of functionality alongside the established interface, benefiting users in productivity and overall spreadsheet quality.

Abstract: Spreadsheets are the go-to tool for computerized calculation and modelling,
but are hard to comprehend and adapt after reaching a certain complexity. In
general, cognition of complex systems is facilitated by having a higher order
mental model of the system in question to work with. We therefore present a
concept for structure-aware understanding of and interaction with spreadsheets
that extends previous work on structure inference in the domain. Following this
concept, structural information is used to enrich visualizations, reactively
enhance traditional user actions, and provide tools to proactively alter the
overall spreadsheet makeup instead of individual cells The intended systems
should, in first approximation, not replace common spreadsheet tools, but
provide an additional layer of functionality alongside the established
interface. In ongoing work, we therefore implemented a tool for structure
inference and visualization along the common spreadsheet layout. Based on this
framework, we plan to introduce the envisioned proactive and reactive
interaction mechanics, and finally provide structure-aware unctionality as an
add-in for common spreadsheet processors. We believe that providing the tools
for thinking about and interacting with spreadsheets in this manner will
benefit users both in terms of productivity and overall spreadsheet quality.

</details>


### [98] [Typed Table Transformations](https://arxiv.org/abs/1809.02746)
*Martin Erwig*

Main category: cs.SE

TL;DR: A new method for transforming spreadsheet tables using row and column types is introduced, with future research directions outlined.


<details>
  <summary>Details</summary>
Motivation: Spreadsheet tables often have labels that act as types for the data. Tables can be viewed as being built from typed data where the placement of values is controlled by row and column types.

Method: The method involves transformations of row and column types, illustrating basic ideas for type-based table construction and transformation.

Result: The paper illustrates a new approach to spreadsheet table transformations and identifies future research questions.

Conclusion: This paper presents a new approach to the transformations of spreadsheet tables based on transformations of row and column types, laying out research questions for future work.

Abstract: Spreadsheet tables are often labeled, and these labels effectively constitute
types for the data in the table. In such cases tables can be considered to be
built from typed data where the placement of values within the table is
controlled by the types used for rows and columns. We present a new approach to
the transformations of spreadsheet tables that is based on transformations of
row and column types. We illustrate the basic idea of type-based table
construction and transformation and lay out a series of research questions that
should be addressed in future work.

</details>


### [99] [Implementing WHERE and ORDER BY as spreadsheet formulas](https://arxiv.org/abs/1809.00025)
*Paul Mireault*

Main category: cs.SE

TL;DR: Spreadsheet formulas now mimic SQL's WHERE and ORDER BY for dynamic data filtering and sorting.


<details>
  <summary>Details</summary>
Motivation: To provide automatic reaction to changes in calculated values in spreadsheets, similar to SQL's WHERE and ORDER BY clauses, which is a disadvantage of current spreadsheet tools like Excel.

Method: Develop spreadsheet formulas to implement SQL's WHERE and ORDER BY clauses.

Result: Spreadsheet formulas that implement SQL's WHERE and ORDER BY clauses.

Conclusion: We develop spreadsheet formulas that implement SQL's WHERE and ORDER BY clauses.

Abstract: The WHERE and ORDER BY clauses of the SQL SELECT statement select a subset of
rows in the result of a database query and present the result in the specified
order. In a spreadsheet program like Microsoft Excel, one could use the filter
and sort buttons, or use its Query or its Pivot Table tools to achieve a
similar effect. The disadvantage of using those tools is that they don't react
automatically to changes in the calculated values of the spreadsheet. In this
paper, we develop spreadsheet formulas that implement SQL's WHERE and ORDER BY
clauses.

</details>


### [100] [The use of Charts, Pivot Tables, and Array Formulas in two Popular Spreadsheet Corpora](https://arxiv.org/abs/1808.10642)
*Bas Jansen,Felienne Hermans*

Main category: cs.SE

TL;DR: 电子表格广泛用于商业决策，但容易出错。现有研究多集中于公式，忽略了图表、数据透视表和数组公式。本文分析了Enron和EUSES语料库中的这些构造，以期全面提升电子表格质量。


<details>
  <summary>Details</summary>
Motivation: 现有电子表格研究主要集中在公式的错误预防，而忽略了图表、数据透视表和数组公式等其他同样重要的信息呈现方式，这可能导致对公司决策支持信息准确性的片面理解。因此，有必要对这些被忽视的构造进行研究，以全面理解电子表格的实际使用情况，从而提高电子表格的整体质量。

Method: 对Enron和EUSES两个电子表格语料库进行分析，重点研究了图表、数据透视表和数组公式等特定构造的使用情况。

Result: 研究分析了两个大型电子表格语料库（Enron和EUSES），重点关注了图表、数据透视表和数组公式等构造的使用情况。

Conclusion: 通过对Enron和EUSES这两个流行的电子表格语料库的分析，揭示了在数据透视表、图表和数组公式等方面的实际使用情况，为提高电子表格质量的研究提供了新的视角。

Abstract: The use of spreadsheets in industry is widespread. Companies base decisions
on information coming from spreadsheets. Unfortunately, spreadsheets are
error-prone and this increases the risk that companies base their decisions on
inaccurate information, which can lead to incorrect decisions and loss of
money. In general, spreadsheet research is aimed to reduce the error-proneness
of spreadsheets. Most research is concentrated on the use of formulas. However,
there are other constructions in spreadsheets, like charts, pivot tables, and
array formulas, that are also used to present decision support information to
the user. There is almost no research about how these constructions are used.
To improve spreadsheet quality it is important to understand how spreadsheets
are used and to obtain a complete understanding, the use of charts, pivot
tables, and array formulas should be included in research. In this paper, we
analyze two popular spreadsheet corpora: Enron and EUSES on the use of the
aforementioned constructions.

</details>


### [101] [Asheetoxy: A Taxonomy for Classifying Negative Spreadsheet-related Phenomena](https://arxiv.org/abs/1808.10231)
*Daniel Kulesz,Stefan Wagner*

Main category: cs.SE

TL;DR: 电子表格错误分类混乱，现有方法存在不足。我们提出了Asheetoxy分类法，它更简单且避免了“错误”一词的歧义。初步实验表明，该分类法易于使用且有效。


<details>
  <summary>Details</summary>
Motivation: 为了解决电子表格错误分类缺乏统一标准的问题，并克服现有分类法中术语模糊和需要深入了解用户状态的局限性。

Method: 提出了一种名为Asheetoxy的简单且面向现象的分类法，用于对电子表格中的现象进行分类。

Result: 一项有7名参与者的初步研究表明，非电子表格研究人员也能使用Asheetoxy对现实世界中的电子表格现象进行分类，表明了该分类法的可用性。

Conclusion: Asheetoxy是一种简单且面向现象的分类法，它避免了使用本身就具有歧义的“错误”一词。初步研究表明，即使是非电子表格研究人员，也可以使用Asheetoxy对现实世界中的电子表格现象进行分类。

Abstract: Spreadsheets (sometimes also called Excel programs) are powerful tools which
play a business-critical role in many organizations. However, due to faulty
spreadsheets many bad decisions have been taken in recent years. Since then, a
number of researchers have been studying spreadsheet errors. However, one issue
that hinders discussion among researchers and professionals is the lack of a
commonly accepted taxonomy.
  Albeit a number of taxonomies for spreadsheet errors have been proposed in
previous work, a major issue is that they use the term error that itself is
already ambiguous. Furthermore, to apply most existing taxonomies, detailed
knowledge about the underlying process and knowledge about the "brain state" of
the acting spreadsheet users is required. Due to these limitations, known
error-like phenomena in freely available spreadsheet corpora cannot be
classified with these taxonomies.
  We propose Asheetoxy, a simple and phenomenon-oriented taxonomy that avoids
the problematic term error altogether. An initial study with 7 participants
indicates that even non-spreadsheet researchers similarly classify real-world
spreadsheet phenomena using Asheetoxy.

</details>


### [102] [Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (SEMS'18)](https://arxiv.org/abs/1808.09174)
*Birgit Hofer,Jorge Mendes*

Main category: cs.SE

TL;DR: A collection of papers on software engineering in spreadsheets from SEMS'18.


<details>
  <summary>Details</summary>
Motivation: The motivation is to present and discuss recent research on software engineering methods in spreadsheets.

Method: The paper is a collection of research papers from a workshop.

Result: The result is a proceedings document containing various papers on the topic.

Conclusion: This paper is a collection of research papers presented at the 5th International Workshop on Software Engineering Methods in Spreadsheets (SEMS'18).

Abstract: Proceedings of the 5th International Workshop on Software Engineering Methods
in Spreadsheets (SEMS'18), held on October 1st, 2018, in Lisbon, Portugal, and
co-located with the 2018 IEEE Symposium on Visual Languages and Human-Centric
Computing (VL/HCC).

</details>


### [103] [Combining Spreadsheet Smells for Improved Fault Prediction](https://arxiv.org/abs/1805.10493)
*Patrick Koch,Konstantin Schekotihin,Dietmar Jannach,Birgit Hofer,Franz Wotawa*

Main category: cs.SE

TL;DR: Spreadsheet smells lack predictive power alone; an AdaBoost ensemble classifier improves fault prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing spreadsheet smells have limited predictive power individually, necessitating a method to combine their predictions for better fault prediction.

Method: A machine learning based approach using an AdaBoost ensemble classifier to combine predictions of individual spreadsheet smells.

Result: Significant improvements in fault prediction accuracy were observed on two public datasets containing real-world spreadsheet faults.

Conclusion: The proposed machine learning approach combining individual smell predictions using an AdaBoost ensemble classifier shows significant improvements in fault prediction accuracy for spreadsheets.

Abstract: Spreadsheets are commonly used in organizations as a programming tool for
business-related calculations and decision making. Since faults in spreadsheets
can have severe business impacts, a number of approaches from general software
engineering have been applied to spreadsheets in recent years, among them the
concept of code smells. Smells can in particular be used for the task of fault
prediction. An analysis of existing spreadsheet smells, however, revealed that
the predictive power of individual smells can be limited. In this work we
therefore propose a machine learning based approach which combines the
predictions of individual smells by using an AdaBoost ensemble classifier.
Experiments on two public datasets containing real-world spreadsheet faults
show significant improvements in terms of fault prediction accuracy.

</details>


### [104] [An Easy & Collaborative RDF Data Entry Method using the Spreadsheet Metaphor](https://arxiv.org/abs/1804.04175)
*Markus Schröder,Christian Jilek,Jörn Hees,Sven Hertling,Andreas Dengel*

Main category: cs.SE

TL;DR: 一种新的基于 Web 的电子表格编辑器，可将电子表格数据转换为 RDF 语句，使用户能够更轻松、更快速、更有效地创建语义数据。


<details>
  <summary>Details</summary>
Motivation: 为了让非 RDF 专家也能轻松创建语义数据。

Method: 提出一种易于使用的、零配置的、基于 Web 的电子表格编辑器，该编辑器可同时将电子表格条目转换为 RDF 语句。

Result: 在用户研究中，与采用其他方法的用户相比，使用该编辑器的用户能够更快、更有效地创建更多语义数据，并且数据质量相当或更高。

Conclusion: 所提出的基于电子表格的编辑器使用户能够比其他方法更快、更有效地创建语义数据，而不会影响数据质量。

Abstract: Spreadsheets are widely used by knowledge workers, especially in the
industrial sector. Their methodology enables a well understood, easy and fast
possibility to enter data. As filling out a spreadsheet is more accessible to
common knowledge workers than defining RDF statements, in this paper, we
propose an easy-to-use, zero-configuration, web-based spreadsheet editor that
simultaneously transfers spreadsheet entries into RDF statements. It enables
various kinds of users to easily create semantic data whether they are RDF
experts or novices. The typical scenario we address focuses on creating
instance data starting with an empty knowledge base that is filled
incrementally. In a user study, participants were able to create more
statements in shorter time, having similar or even significantly outperforming
quality, compared to other approaches.

</details>


### [105] [Structured Spreadsheet Modelling and Implementation with Multiple Dimensions - Part 1: Modelling](https://arxiv.org/abs/1801.09777)
*Paul Mireault*

Main category: cs.SE

TL;DR: Models use dimensions like time. Current methods for adding a second dimension (like repeating formulas or multiple worksheets) are likely inefficient, and this paper probably offers better solutions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiencies or limitations of current methods for representing a second dimension in models, which are often handled by repeating formula blocks or creating multiple worksheets.

Method: The abstract doesn't provide specific methods, but it implies that current methods involve formula repetition or multiple worksheets.

Result: The abstract doesn't detail the results, but it sets the stage for presenting improved or alternative ways to manage multi-dimensional data.

Conclusion: This paper likely discusses methods for handling multi-dimensional data in models, possibly spreadsheets, given the mention of time as a dimension and the common practice of repeating formulas or creating multiple worksheets for additional dimensions.

Abstract: Dimensions are an integral part of many models we use every day. Without
thinking about it, we frequently use the time dimension: many financial and
accounting spreadsheets have columns representing months or years. Representing
a second dimension is often done by repeating blocs of formulas in a worksheet
or creating multiple worksheets with the same structure.

</details>


### [106] [Mitigating Spreadsheet Risk in Complex Multi-Dimensional Models in Excel](https://arxiv.org/abs/1802.01640)
*Steve Litt*

Main category: cs.SE

TL;DR: Microsoft Excel is powerful but risky due to manual processes. This paper introduces PivotModel, which works like a PivotTable to reduce risks in complex Excel models.


<details>
  <summary>Details</summary>
Motivation: Spreadsheets like Microsoft Excel are widely used but prone to errors and manual intensity, creating spreadsheet risk that is difficult to control, especially in complex applications.

Method: The solution, named PivotModel, leverages Microsoft Excel's capabilities and functions similarly to a PivotTable to address risks associated with manual data entry and potential errors in spreadsheets.

Result: The proposed PivotModel aims to mitigate spreadsheet risk by providing enhanced functionality for complex multi-dimensional models, including sophisticated algorithms, challenging hierarchies, and database write-back, while also supporting reporting, data input forms, and ad hoc analysis.

Conclusion: The paper proposes PivotModel, a solution designed to mitigate spreadsheet risk in complex multi-dimensional models within Microsoft Excel.

Abstract: Microsoft Excel is the most ubiquitous analytical tool ever built. Companies
around the world leverage it for its power, flexibility and ease of use.
However, spreadsheets are manually intensive and prone to error, making it
difficult for companies to control spreadsheet risk. The following solution is
designed to mitigate spreadsheet risk for a set of problems commonly addressed
in a spreadsheet defined as "complex multi-dimensional models". "Complex"
referring to certain types of applications that require functionality such as
sophisticated algorithms, challenging hierarchies and database write-back (i.e.
planning, forecasting, etc.) and "multi-dimensional" referring to providing
capabilities such as reporting, data input forms and ad hoc analysis on the
different attributes associated with the resulting model. The solution is
defined as a "PivotModel" because it works similarly to a PivotTable but is
designed to leverage the robust capabilities of the Microsoft Excel platform.

</details>


### [107] [Proposed Spreadsheet Transparency Definition and Measures](https://arxiv.org/abs/1802.01628)
*Craig Hatmaker*

Main category: cs.SE

TL;DR: 该研究提出了一个明确的财务模型透明度定义，旨在解决审计师和模型开发者在理解和评估模型透明度方面存在的模糊性。


<details>
  <summary>Details</summary>
Motivation: 审计师要求财务模型透明，但目前缺乏明确的定义和衡量标准，导致无法确定模型的透明度水平，也无法客观评估和选择不同的建模方法。

Method: 提出了一个具体的模型透明度定义，该定义可用于创建测量工具和自动化工具。

Result: 提出了一个具体的模型透明度定义，该定义可用于创建测量工具和自动化工具，以判断模型是否满足透明度要求，并帮助模型开发者客观地比较和选择最适合其目标的方法。

Conclusion: 该研究提出了一个具体的模型透明度定义，能够用于创建评估工具和自动化工具，以判断模型是否满足透明度要求，并帮助模型开发者客观地选择最适合其目标的方法。

Abstract: Auditors demand financial models be transparent yet no consensus exists on
what that means precisely. Without a clear modeling transparency definition we
cannot know when our models are "transparent". The financial modeling community
debates which methods are more or less transparent as though transparency is a
quantifiable entity yet no measures exist. Without a transparency measure
modelers cannot objectively evaluate methods and know which improves model
transparency.
  This paper proposes a definition for spreadsheet modeling transparency that
is specific enough to create measures and automation tools for auditors to
determine if a model meets transparency requirements. The definition also
provides modelers the ability to objectively compare spreadsheet modeling
methods to select which best meets their goals.

</details>


### [108] [Alternative Spreadsheet Model Designs for an Operations Management Model Embedded in a Periodic Business Process](https://arxiv.org/abs/1802.00484)
*Thomas A. Grossman,Vijay Mehrotra,Mouwafac Sidaoui*

Main category: cs.SE

TL;DR: 技术设计在修改和重用方面优于数据驱动设计和规范设计。


<details>
  <summary>Details</summary>
Motivation: 本研究提出的一个广泛使用的运营管理模型，用于供应链和分销规划，该模型通常嵌入在一个需要模型修改和重用的周期性业务流程中。

Method: 本研究评估了三种不同的电子表格实现：数据驱动设计、规范（教科书）设计和新颖（表格驱动）技术设计。

Result: 技术设计可以在没有手动编写或编辑单元格公式的情况下进行修改，以适应新数据和新结构元素，从而加快修改速度并降低出错风险。

Conclusion: 该技术设计有潜力用于其他类别的模型。

Abstract: We present a widely-used operations management model used in supply and
distribution planning, that is typically embedded in a periodic business
process that necessitates model modification and reuse. We consider three
alternative spreadsheet implementations, a data-driven design, a canonical
(textbook) design, and a novel (table-driven) technical design. We evaluate
each regarding suitability for accuracy, modification, analysis, and transfer.
We consider the degree of training and technical sophistication required to
utilize each design. The data-driven design provides insight into poor
spreadsheet practices by na\"ive modelers. The technical design can be modified
for new data and new structural elements without manual writing or editing of
cell formulas, thus speeding modification and reducing risk of error. The
technical design has potential for use with other classes of models. We
identify opportunities for future research.

</details>


### [109] [Mitigating Spreadsheet Model Risk with Python Open Source Infrastructure](https://arxiv.org/abs/1801.09771)
*Oliver Beavers*

Main category: cs.SE

TL;DR: 由于缺乏传统的软件工程工具和协议，电子表格建模容易出错。本研究提出使用 Python 和开源包来开发可重现的审计工具，以提高电子表格的准确性。


<details>
  <summary>Details</summary>
Motivation: 电子表格模型被视为软件，但电子表格开发者并非软件工程师，这导致错误率较高。

Method: 该论文使用 Python 编程语言和免费、开源包来开发可重现的审计工具。

Result: 利益相关者能够开发明确定义的模型“预言家”来测试和审计电子表格计算。

Conclusion: 该论文为电子表格建模专业人士奠定了基础，使他们能够使用 Python 编程语言构建的免费、开源包来开发可重现的审计工具，使利益相关者能够开发明确定义的模型“预言家”来测试和审计电子表格计算。

Abstract: Across an aggregation of EuSpRIG presentation papers, two maxims hold true:
spreadsheets models are akin to software, yet spreadsheet developers are not
software engineers. As such, the lack of traditional software engineering tools
and protocols invites a higher rate of error in the end result. This paper lays
ground work for spreadsheet modelling professionals to develop reproducible
audit tools using freely available, open source packages built with the Python
programming language, enabling stakeholders to develop clearly defined model
"oracles" with which to test and audit spreadsheet calculations against.

</details>


### [110] [Structuring Spreadsheets with the "Lish" Data Model](https://arxiv.org/abs/1801.08603)
*Alan Hall,Michel Wermelinger,Tony Hirst,Santi Phithakkitnukoon*

Main category: cs.SE

TL;DR: lish：一种用于电子表格的新型嵌套列表数据模型，通过模板支持结构化数据，旨在提高可理解性和减少错误。


<details>
  <summary>Details</summary>
Motivation: 传统的电子表格单元格缺乏对数据整体结构的认知，这会影响理解并增加公式复制的错误风险。

Method: 通过将单元格组织成嵌套列表，并允许用户在列表中使用模板来定义重复结构，从而在电子表格环境中引入新的数据模型。

Result: 研究提出了一种名为“lish”的新型数据模型，其特点是单元格组织成嵌套列表，并可通过模板定义重复结构，已通过演示应用程序进行了展示。

Conclusion: 该研究探索了一种名为“lish”的新型数据模型，旨在为电子表格环境提供一种替代传统网格的方案，以捕捉更高层级的数据结构，同时保持电子表格的简洁性。

Abstract: A spreadsheet is remarkably flexible in representing various forms of
structured data, but the individual cells have no knowledge of the larger
structures of which they may form a part. This can hamper comprehension and
increase formula replication, increasing the risk of error on both scores. We
explore a novel data model (called the "lish") that could form an alternative
to the traditional grid in a spreadsheet-like environment. Its aim is to
capture some of these higher structures while preserving the simplicity that
makes a spreadsheet so attractive. It is based on cells organised into nested
lists, in each of which the user may optionally employ a template to prototype
repeating structures. These template elements can be likened to the marginal
"cells" in the borders of a traditional worksheet, but are proper members of
the sheet and may themselves contain internal structure. A small demonstration
application shows the "lish" in operation.

</details>


### [111] [Automated Refactoring of Nested-IF Formulae in Spreadsheets](https://arxiv.org/abs/1712.09797)
*Jie Zhang,Shi Han,Dan Hao,Lu Zhang,Dongmei Zhang*

Main category: cs.SE

TL;DR: 该研究提出了一种自动化方法，用于重构电子表格中难以理解的嵌套IF公式，并取得了显著的改进效果，用户反馈也表明了其有效性和必要性。


<details>
  <summary>Details</summary>
Motivation: 由于嵌套IF表达式具有可读性差、认知成本高、易出错等问题，而终端用户往往缺乏必要的编程知识来解决这些问题，因此需要一种有效且自动化的方法来处理这个问题。

Method: 提出了一种基于抽象语法树（AST）的自动化方法，该方法通过检测和移除AST中的逻辑冗余，并识别和重组被碎片化的更高级别语义到简洁的内置函数中，来系统地重构嵌套IF公式。

Result: 该方法能够处理超过99%的嵌套IF公式，其中超过50%的重构将嵌套IF的嵌套层级减少了一半以上。用户调研也表明，大多数参与者更喜欢重构后的公式，并认为该自动化重构方法是必要且有用的。

Conclusion: 该研究提出了一种基于AST的自动化方法来重构嵌套IF公式，并通过对包含2700万个嵌套IF公式的68000多个电子表格进行的评估，证明了该方法的有效性。该方法能够处理99%以上的嵌套IF公式，并显著降低其嵌套层级。此外，用户调研结果显示，大多数用户偏好重构后的公式，并认为该自动化重构方法是必要且有用的。

Abstract: Spreadsheets are the most popular end-user programming software, where
formulae act like programs and also have smells. One well recognized common
smell of spreadsheet formulae is nest-IF expressions, which have low
readability and high cognitive cost for users, and are error-prone during reuse
or maintenance. However, end users usually lack essential programming language
knowledge and skills to tackle or even realize the problem. The previous
research work has made very initial attempts in this aspect, while no effective
and automated approach is currently available.
  This paper firstly proposes an AST-based automated approach to systematically
refactoring nest-IF formulae. The general idea is two-fold. First, we detect
and remove logic redundancy on the AST. Second, we identify higher-level
semantics that have been fragmented and scattered, and reassemble the syntax
using concise built-in functions. A comprehensive evaluation has been conducted
against a real-world spreadsheet corpus, which is collected in a leading IT
company for research purpose. The results with over 68,000 spreadsheets with 27
million nest-IF formulae reveal that our approach is able to relieve the smell
of over 99\% of nest-IF formulae. Over 50% of the refactorings have reduced
nesting levels of the nest-IFs by more than a half. In addition, a survey
involving 49 participants indicates that for most cases the participants prefer
the refactored formulae, and agree on that such automated refactoring approach
is necessary and helpful.

</details>


### [112] [Spreadsheet Guardian: An Approach to Protecting Semantic Correctness throughout the Evolution of Spreadsheets](https://arxiv.org/abs/1612.03813)
*Daniel Kulesz,Verena Käfer,Stefan Wagner*

Main category: cs.SE

TL;DR: Spreadsheet Guardian is a tool that improves spreadsheet quality by separating test rule specification from execution, automatically detecting errors, and aiding collaboration during maintenance. It's user-friendly and enhances users' awareness of spreadsheet correctness.


<details>
  <summary>Details</summary>
Motivation: Spreadsheets are powerful business tools, but errors in them lead to bad decisions. Collaboration on spreadsheets for maintenance lacks support for ensuring correctness.

Method: Spreadsheet Guardian separates the specification of spreadsheet test rules from their execution, automatically executing user-defined test rules to detect semantic faults. It was implemented as a Microsoft Excel add-in.

Result: Two empirical evaluations with 29 end-users and 42 computer science students showed that Spreadsheet Guardian is easy to learn and apply. Participants using it were more realistic about their spreadsheet correctness after maintenance compared to those using traditional testing techniques.

Conclusion: Spreadsheet Guardian can help detect semantic faults in spreadsheets and protect collaborating users from introducing faults during maintenance. It is easy to learn and apply, and helps users be more realistic about the correctness of their spreadsheets.

Abstract: Spreadsheets are powerful tools which play a business-critical role in many
organizations. However, many bad decisions taken due to faulty spreadsheets
show that these tools need serious quality assurance. Furthermore, while
collaboration on spreadsheets for maintenance tasks is common, there has been
almost no support for ensuring that the spreadsheets remain correct during this
process.
  We have developed an approach named Spreadsheet Guardian which separates the
specification of spreadsheet test rules from their execution. By automatically
executing user-defined test rules, our approach is able to detect semantic
faults. It also protects all collaborating spreadsheet users from introducing
faults during maintenance, even if only few end-users specify test rules. To
evaluate Spreadsheet Guardian, we implemented a representative testing
technique as an add-in for Microsoft Excel.
  We evaluated the testing technique in two empirical evaluations with 29
end-users and 42 computer science students. The results indicate that the
technique is easy to learn and to apply. Furthermore, after finishing
maintenance, participants with spreadsheets "protected" by the technique are
more realistic about the correctness of their spreadsheets than participants
who employ only "classic", non-interactive test rules based on static analysis
techniques. Hence, we believe Spreadsheet Guardian can be of use for
business-critical spreadsheets.

</details>


### [113] [On Evidence-based Risk Management in Requirements Engineering](https://arxiv.org/abs/1707.00144)
*Daniel Méndez Fernández,Michaela Tießler,Marcos Kalinowski,Michael Felderer,Marco Kuhrmann*

Main category: cs.SE

TL;DR: An evidence-based approach using survey data and a probabilistic network was developed to assess risks in RE, improving awareness of risk factors and showing promise for practical application.


<details>
  <summary>Details</summary>
Motivation: The sensitivity of RE to context makes risk management difficult, and there is a lack of empirical knowledge about context-specific RE phenomena for effective context-sensitive risk management.

Method: A survey of 228 companies was conducted, and the data was used to build a probabilistic network for forecasting context-specific RE phenomena. The approach was validated in 6 companies.

Result: The approach increases awareness of individual risk factors in RE, and initial validation in 6 companies has shown positive feedback for practical dissemination.

Conclusion: The proposed evidence-based approach, validated using cross-company data and implemented via spreadsheets, increases awareness of individual risk factors in RE and can be disseminated into practice.

Abstract: Background: The sensitivity of Requirements Engineering (RE) to the context
makes it difficult to efficiently control problems therein, thus, hampering an
effective risk management devoted to allow for early corrective or even
preventive measures. Problem: There is still little empirical knowledge about
context-specific RE phenomena which would be necessary for an effective
context- sensitive risk management in RE. Goal: We propose and validate an
evidence-based approach to assess risks in RE using cross-company data about
problems, causes and effects. Research Method: We use survey data from 228
companies and build a probabilistic network that supports the forecast of
context-specific RE phenomena. We implement this approach using spreadsheets to
support a light-weight risk assessment. Results: Our results from an initial
validation in 6 companies strengthen our confidence that the approach increases
the awareness for individual risk factors in RE, and the feedback further
allows for disseminating our approach into practice.

</details>


### [114] [Tabula: A Language to Model Spreadsheet Tables](https://arxiv.org/abs/1707.02833)
*Jorge Mendes,João Saraiva*

Main category: cs.SE

TL;DR: Tabula 是一种新的、更具表现力的电子表格建模语言，可以处理更复杂的功能并确保模型与电子表格之间的同步。


<details>
  <summary>Details</summary>
Motivation: 电子表格虽然灵活且易于使用，但容易出错。以往的电子表格模型语言表达能力有限，无法对现实世界电子表格中的许多功能进行建模。

Method: 介绍了一种名为 Tabula 的新建模语言，该语言扩展了以前的电子表格模型，增加了类型约束和具有重复的嵌套类等功能。

Result: Tabula 是一种更具表现力的模型语言，可以扩展，并包含一个双向转换引擎，可确保同步。

Conclusion: Tabula 语言比其他模型更具表现力，并且可以扩展更多功能。此外，Tabula 还包含一个双向转换引擎，可确保在模型或电子表格更新后进行同步。

Abstract: Spreadsheets provide a flexible and easy to use software development
environment, but that leads to error proneness. Work has been done to prevent
errors in spreadsheets, including using models to specify distinct parts of a
spreadsheet as it is done with model-driven software development. Previous
model languages for spreadsheets offer a limited expressiveness, and cannot
model several features present in most real world spreadsheets.
  In this paper, the modeling language Tabula is introduced. It extends
previous spreadsheet models with features like type constraints and nested
classes with repetitions. Tabula is not only more expressive than other models
but it can also be extended with more features. Moreover, Tabula includes a
bidirectional transformation engine that guarantees synchronization after an
update either in the model or spreadsheet.

</details>


### [115] [SpreadCluster: Recovering Versioned Spreadsheets through Similarity-Based Clustering](https://arxiv.org/abs/1704.08476)
*Liang Xu,Wensheng Dou,Chushu Gao,Jie Wang,Jun Wei,Hua Zhong,Tao Huang*

Main category: cs.SE

TL;DR: SpreadCluster是一种新的自动聚类算法，通过分析电子表格的共同特征（如相似的表头和工作表名称）来解决版本信息缺失的问题，并在多个语料库的实验中证明了其优于现有方法的聚类精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 由于最终用户很少使用版本控制工具来记录电子表格的版本信息，导致电子表格版本信息缺失，并且不同的电子表格版本以单个相似电子表格的形式存在。现有的方法试图通过聚类这些相似的电子表格来恢复电子表格的版本信息，但由于文件名和邮件对话等必要信息的缺失，其适用性和准确性有限。

Method: 提出了一种名为SpreadCluster的自动聚类算法，该算法通过学习VEnron中的版本化电子表格特征来对具有相似特征的电子表格进行聚类。

Result: SpreadCluster在Enron语料库上的聚类结果显示，其精度和召回率均高于VEnron中使用的基于文件名的聚类方法。此外，基于SpreadCluster的聚类结果创建了一个新的、比VEnron更大的版本化电子表格语料库VEnron2。SpreadCluster在FUSE和EUSES两个电子表格语料库上的应用结果也表明，它可以高精度地聚类这些语料库中的版本化电子表格。

Conclusion: SpreadCluster能够比filename-based方法更精确地聚类版本化的电子表格。

Abstract: Version information plays an important role in spreadsheet understanding,
maintaining and quality improving. However, end users rarely use version
control tools to document spreadsheet version information. Thus, the
spreadsheet version information is missing, and different versions of a
spreadsheet coexist as individual and similar spreadsheets. Existing approaches
try to recover spreadsheet version information through clustering these similar
spreadsheets based on spreadsheet filenames or related email conversation.
However, the applicability and accuracy of existing clustering approaches are
limited due to the necessary information (e.g., filenames and email
conversation) is usually missing. We inspected the versioned spreadsheets in
VEnron, which is extracted from the Enron Corporation. In VEnron, the different
versions of a spreadsheet are clustered into an evolution group. We observed
that the versioned spreadsheets in each evolution group exhibit certain common
features (e.g., similar table headers and worksheet names). Based on this
observation, we proposed an automatic clustering algorithm, SpreadCluster.
SpreadCluster learns the criteria of features from the versioned spreadsheets
in VEnron, and then automatically clusters spreadsheets with the similar
features into the same evolution group. We applied SpreadCluster on all
spreadsheets in the Enron corpus. The evaluation result shows that
SpreadCluster could cluster spreadsheets with higher precision and recall rate
than the filename-based approach used by VEnron. Based on the clustering result
by SpreadCluster, we further created a new versioned spreadsheet corpus
VEnron2, which is much bigger than VEnron. We also applied SpreadCluster on the
other two spreadsheet corpora FUSE and EUSES. The results show that
SpreadCluster can cluster the versioned spreadsheets in these two corpora with
high precision.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [116] [Using the Abstract Computer Architecture Description Language to Model AI Hardware Accelerators](https://arxiv.org/abs/2402.00069)
*Mika Markus Müller,Alexander Richard Manfred Borst,Konstantin Lübeck,Alexander Louis-Ferdinand Jung,Oliver Bringmann*

Main category: cs.AR

TL;DR: This paper introduces ACADL as a tool to help AI product manufacturers choose and configure hardware accelerators by providing a formal way to model accelerators and simulate their performance, overcoming the limitations of current methods like data sheets and slow simulators.


<details>
  <summary>Details</summary>
Motivation: Manufacturers of AI products need to select appropriate hardware accelerators and configure their parameters to meet performance requirements. Comparing different accelerator design alternatives is challenging due to reliance on data sheets, spreadsheets, or slow black-box simulators. ACADL offers a formal and concise way to describe computer architectures and infer performance characteristics, addressing this challenge.

Method: The paper utilizes the Abstract Computer Architecture Description Language (ACADL) to formally describe AI hardware accelerators, map Deep Neural Networks (DNNs) onto these models, and employ timing simulation semantics to derive performance metrics.

Result: The paper shows how ACADL can be used to model AI hardware accelerators, map DNNs to them, and simulate their performance.

Conclusion: This paper demonstrates the use of ACADL to model AI hardware accelerators, map DNNs onto them, and gather performance results through timing simulations.

Abstract: Artificial Intelligence (AI) has witnessed remarkable growth, particularly
through the proliferation of Deep Neural Networks (DNNs). These powerful models
drive technological advancements across various domains. However, to harness
their potential in real-world applications, specialized hardware accelerators
are essential. This demand has sparked a market for parameterizable AI hardware
accelerators offered by different vendors.
  Manufacturers of AI-integrated products face a critical challenge: selecting
an accelerator that aligns with their product's performance requirements. The
decision involves choosing the right hardware and configuring a suitable set of
parameters. However, comparing different accelerator design alternatives
remains a complex task. Often, engineers rely on data sheets, spreadsheet
calculations, or slow black-box simulators, which only offer a coarse
understanding of the performance characteristics.
  The Abstract Computer Architecture Description Language (ACADL) is a concise
formalization of computer architecture block diagrams, which helps to
communicate computer architecture on different abstraction levels and allows
for inferring performance characteristics. In this paper, we demonstrate how to
use the ACADL to model AI hardware accelerators, use their ACADL description to
map DNNs onto them, and explain the timing simulation semantics to gather
performance results.

</details>


### [117] [Online Model Swapping in Architectural Simulation](https://arxiv.org/abs/2012.01571)
*Patrick Lavin,Jeffrey Young,Rich Vuduc,Jonathan Beard*

Main category: cs.AR

TL;DR: 通过用简单的统计模型替换复杂的模型，加快了仿真速度，同时保持了准确性。


<details>
  <summary>Details</summary>
Motivation: 随着系统和应用的日益复杂，详细仿真耗时过长，迫使架构师在快速迭代设计时使用更简单的模型，但从简单模型迁移到详细模型的过程可能比直接运行详细模型更昂贵，且架构师需要依赖直觉选择简单模型。

Method: 通过在线监控仿真行为，自动将详细模型替换为更简单的统计模型。

Result: 在SVE-Cachesim模拟器中实现了该方法，用于替换内存层级中的L1D模型。实验证明，该技术可以处理时间不变和时间变化的统计（例如L1D是时间序列函数），以及下游的副作用（例如L1D过滤L2缓存的访问）。在仿真中使用了90%的近似缓存模型，而模拟周期计数仅有8%的误差，并且更简单的模型每次执行所需的计算量减少了2到8倍。

Conclusion: 该研究提出了一种在线监控仿真行为并自动用更简单的统计模型替换复杂模型的方法，以弥合简单和详细仿真之间的差距。

Abstract: As systems and applications grow more complex, detailed simulation takes an
ever increasing amount of time. The prospect of increased simulation time
resulting in slower design iteration forces architects to use simpler models,
such as spreadsheets, when they want to iterate quickly on a design. However,
the task of migrating from a simple simulation to one with more detail often
requires multiple executions to find where simple models could be effective,
which could be more expensive than running the detailed model in the first
place. Also, architects must often rely on intuition to choose these simpler
models, further complicating the problem.
  In this work, we present a method of bridging the gap between simple and
detailed simulation by monitoring simulation behavior online and automatically
swapping out detailed models with simpler statistical approximations. We
demonstrate the potential of our methodology by implementing it in the
open-source simulator SVE-Cachesim to swap out the level one data cache (L1D)
within a memory hierarchy. This proof of concept demonstrates that our
technique can handle a non-trivial use-case in not just approximation of local
time-invariant statistics, but also those that vary with time (e.g., the L1D is
a form of a time-series function), and downstream side-effects (e.g., the L1D
filters accesses for the level two cache). Our simulation swaps out the
built-in cache model with only an 8% error in the simulated cycle count while
using the approximated cache models for over 90% of the simulation, and our
simpler models require two to eight times less computation per "execution" of
the model

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [118] [Pivoting the paradigm: the role of spreadsheets in K-12 data science](https://arxiv.org/abs/2506.03232)
*Oren Tirschwell,Nicholas Jon Horton*

Main category: stat.OT

TL;DR: Spreadsheets are useful for K-12 data and computing skills, but require pedagogical support and professional development for effective implementation.


<details>
  <summary>Details</summary>
Motivation: Spreadsheet tools are widely accessible to K-12 students and teachers and play a role in data collection, organization, exploration, and analysis, thus having the potential to foster data and computing skills.

Method: The paper reviews prior frameworks on K-12 data tools, proposes data-driven learning outcomes for spreadsheet integration, and discusses how spreadsheets can develop data acumen and computational fluency. It also provides example activities, identifies challenges, suggests pedagogical approaches, and discusses professional development needs.

Result: The paper discusses the potential of spreadsheets in K-12 education for developing data acumen and computational fluency, offering examples and strategies for implementation while acknowledging existing challenges.

Conclusion: Spreadsheet tools can foster data and computing skills for K-12 students and can be integrated into the curriculum to achieve data-driven learning outcomes. However, challenges and barriers to adoption exist, and professional development is needed for deeper use.

Abstract: Spreadsheet tools are widely accessible to and commonly used by K-12 students
and teachers. They have an important role in data collection and organization.
Beyond data organization, spreadsheets also make data visible and easy to
interact with, facilitating student engagement in data exploration and
analysis. Though not suitable for all circumstances, spreadsheets can and do
help foster data and computing skills for K-12 students. This paper 1) reviews
prior frameworks on K-12 data tools; 2) proposes data-driven learning outcomes
that can be accomplished by incorporating spreadsheets into the curriculum; and
3) discusses how spreadsheets can help develop data acumen and computational
fluency. We provide example class activities, identify challenges and barriers
to adoption, suggest pedagogical approaches to ease the learning curve for
instructors and students, and discuss the need for professional development to
facilitate deeper use of spreadsheets for data science and STEM disciplines.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [119] [Radif Corpus: A Symbolic Dataset for Non-Metric Iranian Classical Music](https://arxiv.org/abs/2507.10456)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.SD

TL;DR: 本研究介绍了第一个完整的伊朗非公制radif音乐的数字语料库，包含MIDI文件和详细数据，为计算音乐学研究提供了基础。


<details>
  <summary>Details</summary>
Motivation: 伊朗古典音乐的核心是radif，这是一个组织旋律材料的基石，对表演和教学法至关重要。然而，目前缺乏一个数字化的radif曲库，这阻碍了对这种非公制音乐形式的计算分析。

Method: 我们介绍了第一个代表完整的非公制radif曲库的数字语料库，涵盖了该曲库现有的所有13个组成部分。我们提供了MIDI文件（总计约281分钟）和数据电子表格，描述了228首音乐的音符、音符时值、音程和层次结构。我们忠实地展示了包括四分音符和非公制方面在内的音乐。此外，我们还提供了支持性的基本统计数据以及语料库的复杂性和相似性度量。

Result: 我们成功创建了一个包含228首乐曲的数字语料库，涵盖了完整的非公制radif曲库，并提供了详细的音乐信息和统计数据。

Conclusion: 该语料库为伊朗古典音乐的计算研究提供了一个平台，研究人员可以利用它来研究旋律模式、调查即兴风格，或用于音乐信息检索、音乐理论和计算（民族）音乐学中的其他任务。

Abstract: Non-metric music forms the core of the repertoire in Iranian classical music.
Dastgahi music serves as the underlying theoretical system for both Iranian art
music and certain folk traditions. At the heart of Iranian classical music lies
the radif, a foundational repertoire that organizes melodic material central to
performance and pedagogy.
  In this study, we introduce the first digital corpus representing the
complete non-metrical radif repertoire, covering all 13 existing components of
this repertoire. We provide MIDI files (about 281 minutes in total) and data
spreadsheets describing notes, note durations, intervals, and hierarchical
structures for 228 pieces of music. We faithfully represent the tonality
including quarter-tones, and the non-metric aspect. Furthermore, we provide
supporting basic statistics, and measures of complexity and similarity over the
corpus.
  Our corpus provides a platform for computational studies of Iranian classical
music. Researchers might employ it in studying melodic patterns, investigating
improvisational styles, or for other tasks in music information retrieval,
music theory, and computational (ethno)musicology.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [120] [A CNN toolbox for skin cancer classification](https://arxiv.org/abs/1908.08187)
*Fabrizio Nunnari,Daniel Sonntag*

Main category: eess.IV

TL;DR: A software toolbox simplifies deep neural network configuration for skin cancer classification, featuring a spreadsheet interface for non-technical users and showing promising preliminary results on melanoma detection.


<details>
  <summary>Details</summary>
Motivation: To provide a tool for easy configuration of deep neural networks in skin cancer classification, catering to both technical and non-technical users.

Method: A software toolbox with a spreadsheet-like interface for configuring deep neural networks (CNNs), allowing for quick setup of architectures and hyper-parameter configurations. Future versions may include meta-learning and AutoML.

Result: Preliminary results using two CNNs for melanoma detection demonstrated the impact of image augmentation, resolution, and rescaling filters on detection performance and training time.

Conclusion: The software toolbox enables quick configuration of deep neural networks for skin cancer classification, with a user-friendly interface for both developers and non-technical users. Preliminary results show the impact of various configurations on detection performance and training time.

Abstract: We describe a software toolbox for the configuration of deep neural networks
in the domain of skin cancer classification. The implemented software
architecture allows developers to quickly set up new convolutional neural
network (CNN) architectures and hyper-parameter configurations. At the same
time, the user interface, manageable as a simple spreadsheet, allows
non-technical users to explore different configuration settings that need to be
explored when switching to different data sets. In future versions, meta
leaning frameworks can be added, or AutoML systems that continuously improve
over time. Preliminary results, conducted with two CNNs in the context melanoma
detection on dermoscopic images, quantify the impact of image augmentation,
image resolution, and rescaling filter on the overall detection performance and
training time.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [121] [Data-Semantics-Aware Recommendation of Diverse Pivot Tables](https://arxiv.org/abs/2507.06171)
*Whanhee Cho,Anna Fariha*

Main category: cs.DB

TL;DR: SAGE是一个推荐k个多样化数据透视表的系统，它通过一个数据语义感知模型和一个贪婪算法来确保推荐的数据透视表具有高用户价值和多样性，解决了现有方法在处理高维数据集时推荐冗余的问题。


<details>
  <summary>Details</summary>
Motivation: 识别能够生成有用的数据透视表的属性组合仍然是一个挑战，尤其是在处理高维数据集时。传统的作品未能充分解决数据透视表多样化问题，这促使我们考虑数据透视表多样化问题，旨在自动推荐有见地且可解释的数据透视表，以消除繁琐的手动过程。

Method: 我们提出了一种名为SAGE的数据语义感知系统，用于推荐k个预算多样化的数据透视表。SAGE通过（1）一个数据语义感知模型来衡量单个数据透视表的效用和数据透视表集合的多样性，以及（2）一个可扩展的贪婪算法来高效地选择一组具有高效用的多样化数据透视表，从而克服了现有工作中存在的冗余问题。

Result: SAGE确保每个数据透视表都有见地、可解释并能适应用户的操作和偏好，同时保证推荐的数据透视表集合彼此不同，提供了多样化的推荐。

Conclusion: SAGE系统在三个真实数据集上的广泛实验表明，其性能优于其他方法，并且能够有效地处理高维数据集。此外，我们还展示了几个案例研究，以突出SAGE在定性方面优于商业软件和大型语言模型（LLMs）。

Abstract: Data summarization is essential to discover insights from large datasets. In
a spreadsheets, pivot tables offer a convenient way to summarize tabular data
by computing aggregates over some attributes, grouped by others. However,
identifying attribute combinations that will result in useful pivot tables
remains a challenge, especially for high-dimensional datasets. We formalize the
problem of automatically recommending insightful and interpretable pivot
tables, eliminating the tedious manual process. A crucial aspect of
recommending a set of pivot tables is to diversify them. Traditional works
inadequately address the table-diversification problem, which leads us to
consider the problem of pivot table diversification.
  We present SAGE, a data-semantics-aware system for recommending k-budgeted
diverse pivot tables, overcoming the shortcomings of prior work for top-k
recommendations that cause redundancy. SAGE ensures that each pivot table is
insightful, interpretable, and adaptive to the user's actions and preferences,
while also guaranteeing that the set of pivot tables are different from each
other, offering a diverse recommendation. We make two key technical
contributions: (1) a data-semantics-aware model to measure the utility of a
single pivot table and the diversity of a set of pivot tables, and (2) a
scalable greedy algorithm that can efficiently select a set of diverse pivot
tables of high utility, by leveraging data semantics to significantly reduce
the combinatorial search space. Our extensive experiments on three real-world
datasets show that SAGE outperforms alternative approaches, and efficiently
scales to accommodate high-dimensional datasets. Additionally, we present
several case studies to highlight SAGE's qualitative effectiveness over
commercial software and Large Language Models (LLMs).

</details>


### [122] [A System and Benchmark for LLM-based Q&A on Heterogeneous Data](https://arxiv.org/abs/2409.05735)
*Achille Fokoue,Srideepika Jayaraman,Elham Khabiri,Jeffrey O. Kephart,Yingjie Li,Dhruv Shah,Youssef Drissi,Fenno F. Heath III,Anu Bhamidipaty,Fateh A. Tipu,Robert J. Baseman*

Main category: cs.DB

TL;DR: siwarex 是一个能够处理数据库和 API 异构性的平台，可用于自然语言查询。


<details>
  <summary>Details</summary>
Motivation: 解决现实工业环境中，用户希望通过自然语言查询结构化数据（如电子表格、数据库、API 或其组合），但面临数据源识别、访问以及多数据源集成困难的问题。

Method: 通过扩展 Spider 数据集和基准测试，用数据检索 API 替换其中一些表来演示 siwarex 的有效性。

Result: siwarex 平台能够通过无缝的自然语言访问数据库和 API 来处理数据源异构性。

Conclusion: siwarex 平台能够很好地处理数据源异构性。

Abstract: In many industrial settings, users wish to ask questions whose answers may be
found in structured data sources such as a spreadsheets, databases, APIs, or
combinations thereof. Often, the user doesn't know how to identify or access
the right data source. This problem is compounded even further if multiple (and
potentially siloed) data sources must be assembled to derive the answer.
Recently, various Text-to-SQL applications that leverage Large Language Models
(LLMs) have addressed some of these problems by enabling users to ask questions
in natural language. However, these applications remain impractical in
realistic industrial settings because they fail to cope with the data source
heterogeneity that typifies such environments. In this paper, we address
heterogeneity by introducing the siwarex platform, which enables seamless
natural language access to both databases and APIs. To demonstrate the
effectiveness of siwarex, we extend the popular Spider dataset and benchmark by
replacing some of its tables by data retrieval APIs. We find that siwarex does
a good job of coping with data source heterogeneity. Our modified Spider
benchmark will soon be available to the research community

</details>


### [123] [Auditable and reusable crosswalks for fast, scaled integration of scattered tabular data](https://arxiv.org/abs/2409.01517)
*Gavin Chait*

Main category: cs.DB

TL;DR: 一个用于数据清理和整合的工具包，可以处理各种数据源，并提供代码和无代码两种使用方式。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决在数据策展过程中遇到的挑战，特别是处理结构不良、互操作性差以及分散的数据集。其目标是创建一个能够有效处理复杂且分散的表格数据，并生成结构良好且可互操作的数据的工具。

Method: 本研究提出了一个开源策展工具包，该工具包采用以模式为中心的策略，将零散的表格数据进行结构化调整，以适应目标模式。通过将策展分解为独立的组件并分离任务，允许在没有源数据的情况下进行软件开发和分析。数据转换被记录为描述模式到模式映射的高层脚本，从而简化了复杂性并降低了资源需求。

Result: 研究结果表明，该工具包能够有效地将零散的数据集成到单个数据库中，即使在数据源分散且数量庞大的情况下也是如此。该工具包支持通过“交叉路径”来重构满足模式定义（schema definition）的任何数据，并提供了 Python 包和可视化 Web 应用程序两种形式。

Conclusion: 该工具包旨在提供一个可扩展的解决方案，以应对各种数据集成挑战。

Abstract: This paper presents an open-source curatorial toolkit intended to produce
well-structured and interoperable data. Curation is divided into discrete
components, with a schema-centric focus for auditable restructuring of complex
and scattered tabular data to conform to a destination schema. Task separation
allows development of software and analysis without source data being present.
Transformations are captured as high-level sequential scripts describing
schema-to-schema mappings, reducing complexity and resource requirements.
Ultimately, data are transformed, but the objective is that any data meeting a
schema definition can be restructured using a crosswalk. The toolkit is
available both as a Python package, and as a 'no-code' visual web application.
A visual example is presented, derived from a longitudinal study where
scattered source data from hundreds of local councils are integrated into a
single database.

</details>


### [124] [Auto-Formula: Recommend Formulas in Spreadsheets using Contrastive Learning for Table Representations](https://arxiv.org/abs/2404.12608)
*Sibei Chen,Yeye He,Weiwei Cui,Ju Fan,Song Ge,Haidong Zhang,Dongmei Zhang,Surajit Chaudhuri*

Main category: cs.DB

TL;DR: Auto-Formula系统通过学习相似电子表格中的公式，并借鉴人脸识别的对比学习技术，解决了用户在电子表格中编写复杂公式的难题，并在实际应用中得到了验证。


<details>
  <summary>Details</summary>
Motivation: 电子表格是广受欢迎的工具，但对于非技术用户来说，编写复杂的公式仍然是一个挑战，因为他们需要查找和理解复杂的公式语法。本研究旨在解决这一痛点。

Method: 本研究利用对比学习技术，从相似的电子表格中学习并适应现有的公式，以准确预测用户在目标电子表格单元格中想要输入的公式。这种方法借鉴了计算机视觉中“相似人脸识别”的思路。

Result: 通过在超过2000个来自真实企业电子表格的测试公式上进行的大量评估，证明了Auto-Formula系统的有效性，其效果优于其他同类方法。

Conclusion: 该研究表明，Auto-Formula系统能够有效地预测用户想要在目标电子表格单元格中输入的公式，并且在真实的企业电子表格中进行了广泛的评估，效果优于其他同类方法。

Abstract: Spreadsheets are widely recognized as the most popular end-user programming
tools, which blend the power of formula-based computation, with an intuitive
table-based interface. Today, spreadsheets are used by billions of users to
manipulate tables, most of whom are neither database experts nor professional
programmers.
  Despite the success of spreadsheets, authoring complex formulas remains
challenging, as non-technical users need to look up and understand non-trivial
formula syntax. To address this pain point, we leverage the observation that
there is often an abundance of similar-looking spreadsheets in the same
organization, which not only have similar data, but also share similar
computation logic encoded as formulas. We develop an Auto-Formula system that
can accurately predict formulas that users want to author in a target
spreadsheet cell, by learning and adapting formulas that already exist in
similar spreadsheets, using contrastive-learning techniques inspired by
"similar-face recognition" from compute vision.
  Extensive evaluations on over 2K test formulas extracted from real enterprise
spreadsheets show the effectiveness of Auto-Formula over alternatives. Our
benchmark data is available at https://github.com/microsoft/Auto-Formula to
facilitate future research.

</details>


### [125] [Facilitating Digital Agriculture with Simple Databases](https://arxiv.org/abs/2312.06517)
*Dennis Buckmaster,Sami Basir,Hanae Sakata*

Main category: cs.DB

TL;DR: 该研究提供易于使用的开源数据库模板，帮助农业工作者改进数据管理和分析。


<details>
  <summary>Details</summary>
Motivation: 为了帮助农业工作者（尤其是电子表格技能有限的农业工作者）更好地管理数据，从而改进物流、提供元数据和加强企业分析。

Method: 提供基于Air table的、具有应用程序外观和感觉的、带有数据验证的简单表单的数据库模板。

Result: 开发了易于使用、数据规范、机器可读、可编辑且可导出的数据库模板，并提供了一个关于如何构建活动记录数据库的研讨会。

Conclusion: 该研究提供开源的数据库模板，旨在帮助农业工作者，特别是那些电子表格技能有限的农业工作者，改善数据管理和企业分析能力。

Abstract: As an on-ramp to databases, we offer several well-structured private database
templates as open source resources for agriculturalists, particularly those
with modest spreadsheet skills. These farmer-oriented Air table databases use
simple data-validated forms, with the look and feel of a customized app, to
yield operational data that is tidy, machine- and human-readable, editable, and
exportable for analysis in other software. Such data can facilitate logistics,
provide contextual metadata, and improve enterprise analysis. A recorded
workshop explaining how to build a database for activity records is presented.
These resources may facilitate infusion of digital agriculture principles
through Extension and structured educational programming.

</details>


### [126] [Streamlining Knowledge Graph Construction with a façade: The SPARQL Anything project](https://arxiv.org/abs/2310.16700)
*Luigi Asprino,Enrico Daga,Justin Dowdy,Paul Mulholland,Aldo Gangemi,Marco Ratta*

Main category: cs.DB

TL;DR: SPARQL Anything is a system that allows querying diverse data formats (like CSV, JSON, XML, even DOCx and Markdown) as if they were in RDF, using plain SPARQL. It's flexible, supports web APIs, and allows complex data processing pipelines. Its design and benefits were validated through user surveys and industry feedback.


<details>
  <summary>Details</summary>
Motivation: The paper aims to answer the question: 'What should a data integration framework for knowledge engineers look like?' by presenting the SPARQL Anything system.

Method: The paper analyzes the design of a facade for knowledge graph construction, applied to the SPARQL Anything system which queries heterogeneous resources using SPARQL 1.1 by overloading the SERVICE clause. It supports various file formats, flexible web API querying, parameterized queries, and transformation pipelines.

Result: The SPARQL Anything system supports a wide variety of file formats (CSV, JSON, XML, Spreadsheets, Markdown, YAML, DOCx, Bibtex), flexible web API querying, parameterized queries, and complex transformation pipelines. Its value to users is reported through a community survey and an industry field report.

Conclusion: The paper describes the design rationale and software architecture of the SPARQL Anything system, providing references to real-world scenarios and reporting on its value to users through a community survey and industry field report.

Abstract: What should a data integration framework for knowledge engineers look like?
Recent research on Knowledge Graph construction proposes the design of a
fa\c{c}ade, a notion borrowed from object-oriented software engineering. This
idea is applied to SPARQL Anything, a system that allows querying heterogeneous
resources as-if they were in RDF, in plain SPARQL 1.1, by overloading the
SERVICE clause. SPARQL Anything supports a wide variety of file formats, from
popular ones (CSV, JSON, XML, Spreadsheets) to others that are not supported by
alternative solutions (Markdown, YAML, DOCx, Bibtex). Features include querying
Web APIs with high flexibility, parametrised queries, and chaining multiple
transformations into complex pipelines. In this paper, we describe the design
rationale and software architecture of the SPARQL Anything system. We provide
references to an extensive set of reusable, real-world scenarios from various
application domains. We report on the value-to-users of the founding
assumptions of its design, compared to alternative solutions through a
community survey and a field report from the industry.

</details>


### [127] [DataVinci: Learning Syntactic and Semantic String Repairs](https://arxiv.org/abs/2308.10922)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Gust Verbruggen*

Main category: cs.DB

TL;DR: DataVinci is a new unsupervised system that cleans string data by learning patterns and automatically correcting errors, even when strings have both syntactic and semantic parts, and it performs better than existing methods.


<details>
  <summary>Details</summary>
Motivation: Prior work has often been limited to error detection or requires user annotations, examples, or constraints to fix errors, and has focused independently on syntactic or semantic errors. There is a need for a system that can handle both types of errors and operate in a fully unsupervised manner.

Method: DataVinci learns regular-expression-based patterns that cover a majority of values in a column and reports values that do not satisfy such patterns as data errors. It can automatically derive edits to the data error based on the majority patterns and constraints learned over other columns without the need for further user interaction. To handle strings with both syntactic and semantic substrings, DataVinci uses an LLM to abstract and re-concretize portions of strings that are semantic prior to learning majority patterns and deriving edits. DataVinci leverages execution information from an existing program to identify and correct data repairs that would not otherwise be identified.

Result: DataVinci successfully detects and repairs errors in string data, outperforming 7 baseline methods on both tasks.

Conclusion: DataVinci outperforms 7 baselines on both error detection and repair when evaluated on 4 existing and new benchmarks.

Abstract: String data is common in real-world datasets: 67.6% of values in a sample of
1.8 million real Excel spreadsheets from the web were represented as text.
Systems that successfully clean such string data can have a significant impact
on real users. While prior work has explored errors in string data, proposed
approaches have often been limited to error detection or require that the user
provide annotations, examples, or constraints to fix the errors. Furthermore,
these systems have focused independently on syntactic errors or semantic errors
in strings, but ignore that strings often contain both syntactic and semantic
substrings. We introduce DataVinci, a fully unsupervised string data error
detection and repair system. DataVinci learns regular-expression-based patterns
that cover a majority of values in a column and reports values that do not
satisfy such patterns as data errors. DataVinci can automatically derive edits
to the data error based on the majority patterns and constraints learned over
other columns without the need for further user interaction. To handle strings
with both syntactic and semantic substrings, DataVinci uses an LLM to abstract
(and re-concretize) portions of strings that are semantic prior to learning
majority patterns and deriving edits. Because not all data can result in
majority patterns, DataVinci leverages execution information from an existing
program (which reads the target data) to identify and correct data repairs that
would not otherwise be identified. DataVinci outperforms 7 baselines on both
error detection and repair when evaluated on 4 existing and new benchmarks.

</details>


### [128] [Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples](https://arxiv.org/abs/2307.14565)
*Peng Li,Yeye He,Cong Yan,Yue Wang,Surajit Chaudhuri*

Main category: cs.DB

TL;DR: Auto-Tables 是一个能自动转换表格的数据准备工具，解决了现实世界中表格不符合关系型标准的问题，让数据分析更轻松。


<details>
  <summary>Details</summary>
Motivation: 在处理现实世界中的电子表格和网页表格时，超过 30% 的表格不符合关系型标准，这需要复杂的表格重构转换才能轻松查询。手动编程这些转换对于技术和非技术用户来说都很困难，并且在 StackOverflow 和 Excel/Power-BI/Tableau 等论坛上存在大量相关问题。

Method: 开发了一个名为 Auto-Tables 的系统，该系统可以自动合成包含多步转换（使用 Python 或其他语言）的管线，将非关系型表转换为标准关系型表，以实现下游分析。此外，还收集了 244 个来自用户电子表格和在线论坛的真实测试用例，以创建一个广泛的基准来评估该新任务。

Result: Auto-Tables 系统可以成功地为超过 70% 的测试用例合成转换，并且速度很快，无需用户输入。

Conclusion: Auto-Tables 系统能够自动合成转换管线，将非关系型表转换为标准关系型表，以实现下游分析。该系统可以处理 70% 以上的测试用例，并且速度很快，无需用户输入，非常适合技术和非技术用户的数据准备工作。

Abstract: Relational tables, where each row corresponds to an entity and each column
corresponds to an attribute, have been the standard for tables in relational
databases. However, such a standard cannot be taken for granted when dealing
with tables "in the wild". Our survey of real spreadsheet-tables and web-tables
shows that over 30% of such tables do not conform to the relational standard,
for which complex table-restructuring transformations are needed before these
tables can be queried easily using SQL-based analytics tools. Unfortunately,
the required transformations are non-trivial to program, which has become a
substantial pain point for technical and non-technical users alike, as
evidenced by large numbers of forum questions in places like StackOverflow and
Excel/Power-BI/Tableau forums.
  We develop an Auto-Tables system that can automatically synthesize pipelines
with multi-step transformations (in Python or other languages), to transform
non-relational tables into standard relational forms for downstream analytics,
obviating the need for users to manually program transformations. We compile an
extensive benchmark for this new task, by collecting 244 real test cases from
user spreadsheets and online forums. Our evaluation suggests that Auto-Tables
can successfully synthesize transformations for over 70% of test cases at
interactive speeds, without requiring any input from users, making this an
effective tool for both technical and non-technical users to prepare data for
analytics.

</details>


### [129] [Efficient and Compact Spreadsheet Formula Graphs](https://arxiv.org/abs/2302.05482)
*Dixin Tang,Fanchao Chen,Christopher De Leon,Tana Wattanawaroon,Jeaseok Yun,Srinivasan Seshadri,Aditya G. Parameswaran*

Main category: cs.DB

TL;DR: TACO通过利用表格局部性来压缩电子表格公式图，从而提高查询和维护效率。


<details>
  <summary>Details</summary>
Motivation: 电子表格是一种流行的数据分析工具，其公式图对于交互性至关重要。然而，公式图通常很大很复杂，导致查询和维护耗时，降低了交互性。

Method: TACO框架利用表格局部性（即相邻单元格具有相似的公式结构）的四个基于模式的算法来压缩公式图，并允许直接在压缩图上进行查询和增量维护。

Result: TACO框架能够显著减小公式图的大小，在查询方面比基线实现和商业电子表格系统分别快34,972倍和632倍。

Conclusion: TACO框架能够显著减小公式图的大小，在查询方面比基线实现和商业电子表格系统分别快34,972倍和632倍。

Abstract: Spreadsheets are one of the most popular data analysis tools, wherein users
can express computation as formulae alongside data. The ensuing dependencies
are tracked as formula graphs. Efficiently querying and maintaining these
formula graphs is critical for interactivity across multiple settings.
Unfortunately, formula graphs are often large and complex such that querying
and maintaining them is time-consuming, reducing interactivity. We propose
TACO, a framework for efficiently compressing formula graphs, thereby reducing
the time for querying and maintenance. The efficiency of TACO stems from a key
spreadsheet property: tabular locality, which means that cells close to each
other are likely to have similar formula structures. We leverage four such
tabular locality-based patterns and develop algorithms for compressing formula
graphs using these patterns, directly querying the compressed graph without
decompression, and incrementally maintaining the graph during updates. We
integrate TACO into an open-source spreadsheet system and show that TACO can
significantly reduce formula graph sizes. For querying formula graphs, the
speedups of TACO over a baseline implemented in our framework and a commercial
spreadsheet system are up to 34,972x and 632x, respectively.

</details>


### [130] [Consensus-Free Spreadsheet Integration](https://arxiv.org/abs/2209.14457)
*Brandon Baylor,Eric Daimler,James Hansen,Esteban Montero,Ryan Wisnesky*

Main category: cs.DB

TL;DR: 利用范畴论和代数理论，提出了一种无需共识即可合并工程电子表格的方法，并在油气行业的实际案例中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 旨在为工程模型合并提供一种无需作者达成共识的方法，因为该方法利用了保持语义的理论和模型态射。

Method: 通过将每个工作表（包括其公式和值）表示为代数（方程）理论及其模型，并使用范畴论中的图、提升和Kan扩展构造来集成它们，从而将多个电子表格合并为一个。

Result: 在一家主要能源公司的实际油气计算案例研究中，演示了将两个非交互工程师创建的独立井筒压力测试（MASP）计算电子表格集成在一起的方法。

Conclusion: 将此方法论应用于企业级工程的扩展，并讨论了与验证语义保持和集成表一致性相关的自动定理证明负担。

Abstract: We describe a method for merging multiple spreadsheets into one sheet, and/or
exchanging data among the sheets, by expressing each sheet's formulae as an
algebraic (equational) theory and each sheet's values as a model of its theory,
expressing the overlap between the sheets as theory and model morphisms, and
then performing colimit, lifting, and Kan-extension constructions from category
theory to compute a canonically universal integrated theory and model, which
can then be expressed as a spreadsheet. Our motivation is to find methods of
merging engineering models that do not require consensus (agreement) among the
authors of the models being merged, a condition fulfilled by our method because
theory and model morphisms are semantics-preserving. We describe a case study
of this methodology on a real-world oil and gas calculation at a major energy
company, describing the theories and models that arise when integrating two
different casing pressure test (MASP) calculation spreadsheets constructed by
two non-interacting engineers. We also describe the automated theorem proving
burden associated with both verifying the semantics preservation of the overlap
mappings as well as verifying the conservativity/consistency of the resulting
integrated sheet. We conclude with thoughts on how to apply the methodology to
scale engineering efforts across the enterprise.

</details>


### [131] [Sigma Workbook: A Spreadsheet for Cloud Data Warehouses](https://arxiv.org/abs/2204.03128)
*James Gale,Max Seiden,Deepanshu Utkarsh,Jason Frantz,Rob Woollen,Çağatay Demiralp*

Main category: cs.DB

TL;DR: Sigma Workbook is a new system with a spreadsheet-like interface that allows business users to easily analyze large datasets in Cloud Data Warehouses by automatically generating SQL queries.


<details>
  <summary>Details</summary>
Motivation: Existing tools for Cloud Data Warehouse analysis are either limited in ad-hoc transformations or difficult for business users. Sigma Workbook aims to address this by enabling business users to easily perform visual analysis at scale.

Method: Sigma Workbook dynamically constructs SQL queries from user interactions via a spreadsheet-like interface, executing them directly on Cloud Data Warehouses to leverage their scalability.

Result: The paper demonstrates Sigma Workbook's ease of use, scalability, and expressivity through 3 real-life use cases: cohort analysis, sessionization, and data augmentation.

Conclusion: Sigma Workbook facilitates business users in performing large-scale data analysis in Cloud Data Warehouses through an accessible, spreadsheet-like interface that dynamically generates SQL queries, demonstrating ease of use, scalability, and expressivity in real-life use cases.

Abstract: Cloud data warehouses (CDWs) bring large-scale data and compute power closer
to users in enterprises. However, existing tools for analyzing data in CDWs are
either limited in ad-hoc transformations or difficult to use for business
users. Here we introduce Sigma Workbook, a new interactive system that enables
business users to easily perform a visual analysis of data in CDWs at scale.
For this, Sigma Workbook provides an accessible spreadsheet-like interface for
analysis through direct manipulation. Sigma Workbook dynamically constructs
matching SQL queries from user interactions, building on the versatility and
expressivity of SQL. Constructed queries are directly executed on CDWs,
leveraging the superior characteristics of the new generation CDWs, including
scalability. We demonstrate Sigma Workbook through 3 real-life use cases --
cohort analysis, sessionization, and data augmentation -- and underline
Workbook's ease of use, scalability, and expressivity.

</details>


### [132] [Efficient Specialized Spreadsheet Parsing for Data Science](https://arxiv.org/abs/2202.13189)
*Felix Henze,Haralampos Gavriilidis,Eleni Tzirita Zacharatou,Volker Markl*

Main category: cs.DB

TL;DR: 该研究提出了一种新的电子表格解析方法，可显著减少内存使用和处理时间，从而在商用系统上实现更高效的数据探索。


<details>
  <summary>Details</summary>
Motivation: 为了在商用系统上实现实用的电子表格加载，解决了当前方法在运行时或内存使用方面存在的问题。

Method: 一种新颖的解析器，它将解压和解析紧密耦合，以最大限度地减少内存使用量。此外，为了减少运行时，我们引入了优化的特定于电子表格的解析例程并采用了并行处理。

Result: 与最先进的方法相比，该方法速度提高了 3 倍，内存使用量减少了 40 倍。

Conclusion: 该方法通过结合解压和解析、使用优化的特定于电子表格的解析例程以及并行处理，显著提高了电子表格加载到 R 环境的效率，内存占用量减少高达 40 倍，速度提高 3 倍。

Abstract: Spreadsheets are widely used for data exploration. Since spreadsheet systems
have limited capabilities, users often need to load spreadsheets to other data
science environments to perform advanced analytics. However, current approaches
for spreadsheet loading suffer from either high runtime or memory usage, which
hinders data exploration on commodity systems. To make spreasheet loading
practical on commodity systems, we introduce a novel parser that minimizes
memory usage by tightly coupling decompression and parsing. Furthermore, to
reduce the runtime, we introduce optimized spreadsheet-specific parsing
routines and employ parallelism. To evaluate our approach, we implement a
prototype for loading Excel spreadsheets into R environments. Our evaluation
shows that our novel approach is up to 3x faster while consuming up to 40x less
memory than state-of-the-art approaches.

</details>


### [133] [Spread2RML: Constructing Knowledge Graphs by Predicting RML Mappings on Messy Spreadsheets](https://arxiv.org/abs/2110.12829)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: Spread2RML是一个可以自动预测杂乱电子表格RML映射的工具，可以节省时间和精力。


<details>
  <summary>Details</summary>
Motivation: 由于电子表格具有复杂的数据模型并且可能相当杂乱，因此创建映射的过程非常耗时。为了减少这种工作量，本研究提出了Spread2RML。

Method: Spread2RML通过使用一组可扩展的RML对象映射模板，并基于启发式方法为每一列应用这些模板，来预测包含杂乱数据的电子表格的RML映射。

Result: 在评估中，使用了三个数据集，包括非常杂乱的合成数据以及来自data.gov的、不太杂乱的电子表格。该方法在处理杂乱数据方面表现良好，并且是全自动化的。

Conclusion: Spread2RML通过使用一组可扩展的RML对象映射模板，并基于启发式方法为每一列应用这些模板，成功地预测了包含杂乱数据的电子表格的RML映射，并取得了初步的、有希望的结果。

Abstract: The RDF Mapping Language (RML) allows to map semi-structured data to RDF
knowledge graphs. Besides CSV, JSON and XML, this also includes the mapping of
spreadsheet tables. Since spreadsheets have a complex data model and can become
rather messy, their mapping creation tends to be very time consuming. In order
to reduce such efforts, this paper presents Spread2RML which predicts RML
mappings on messy spreadsheets. This is done with an extensible set of RML
object map templates which are applied for each column based on heuristics. In
our evaluation, three datasets are used ranging from very messy synthetic data
to spreadsheets from data.gov which are less messy. We obtained first promising
results especially with regard to our approach being fully automatic and
dealing with rather messy data.

</details>


### [134] [Supercomputing Enabled Deployable Analytics for Disaster Response](https://arxiv.org/abs/2108.11525)
*Kaira Samuel,Jeremy Kepner,Michael Jones,Lauren Milechin,Vijay Gadepally,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Michael Houle,Anna Klein,Victor Lopez,Julie Mullen,Andrew Prout,Albert Reuther,Antonio Rosa,Sid Samsi,Charles Yee,Peter Michaleas*

Main category: cs.DB

TL;DR: 由于网络限制，急救人员无法使用云分析工具。本研究提出预处理分析数据为文件，以Excel和Google Earth格式提供人口普查数据，用于应急响应。


<details>
  <summary>Details</summary>
Motivation: 为了解决一线应急人员在网络接入受限和软件安全要求严格的情况下，无法使用基于云的微服务分析平台的问题，提出此方法。

Method: 该方法将地理空间人口普查数据作为文件（Google Earth的kml文件和Microsoft Excel的xlsx文件）进行预处理，这些文件可以与预装软件一起使用，无需网络连接或额外软件，并能在各种旧式硬件上运行。

Result: 该方法成功地处理了人口普查数据，生成了数千个Google Earth和Microsoft Excel文件，展示了人口普查数据（按人口普查区划分的总人口、15岁以下人口、65岁以上人口、年龄中位数）的快速映射。Excel文件支持坐标单位转换，Google Earth文件则利用不同颜色地图展示人口密度。

Conclusion: 该方法通过使用预先计算的分析文件，结合Google Earth和Microsoft Excel，为应急响应人员提供了一个强大的工具，以改善应急准备状况。

Abstract: First responders and other forward deployed essential workers can benefit
from advanced analytics. Limited network access and software security
requirements prevent the usage of standard cloud based microservice analytic
platforms that are typically used in industry. One solution is to precompute a
wide range of analytics as files that can be used with standard preinstalled
software that does not require network access or additional software and can
run on a wide range of legacy hardware. In response to the COVID-19 pandemic,
this approach was tested for providing geo-spatial census data to allow quick
analysis of demographic data for better responding to emergencies. These data
were processed using the MIT SuperCloud to create several thousand Google Earth
and Microsoft Excel files representative of many advanced analytics. The fast
mapping of census data using Google Earth and Microsoft Excel has the potential
to give emergency responders a powerful tool to improve emergency preparedness.
Our approach displays relevant census data (total population, population under
15, population over 65, median age) per census block, sorted by county, through
a Microsoft Excel spreadsheet (xlsx file) and Google Earth map (kml file). The
spreadsheet interface includes features that allow users to convert between
different longitude and latitude coordinate units. For the Google Earth files,
a variety of absolute and relative colors maps of population density have been
explored to provide an intuitive and meaningful interface. Using several
hundred cores on the MIT SuperCloud, new analytics can be generated in a few
minutes.

</details>


### [135] [Table2Charts: Recommending Charts by Learning Shared Table Representations](https://arxiv.org/abs/2008.11015)
*Mengyu Zhou,Qingtao Li,Xinyi He,Yuejiang Li,Yibo Liu,Wei Ji,Shi Han,Yining Chen,Daxin Jiang,Dongmei Zhang*

Main category: cs.DB

TL;DR: Table2Charts framework uses deep Q-learning and heuristic searching to recommend charts from tables, outperforming existing systems in multi-type tasks and human evaluations.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of efficiency, imbalanced data, and table context in recommending commonly composed charts from multi-dimensional datasets.

Method: Table2Charts uses deep Q-learning with a copying mechanism and heuristic searching to perform table-to-sequence generation, where each sequence follows a chart template.

Result: Table2Charts learns a shared representation of table fields, enabling recommendation tasks across different chart types to mutually enhance each other. It achieves significantly better performance compared to other chart recommendation systems.

Conclusion: Table2Charts outperforms other chart recommendation systems in both multi-type task (with doubled recall numbers R@3=0.61 and R@1=0.43) and human evaluations.

Abstract: It is common for people to create different types of charts to explore a
multi-dimensional dataset (table). However, to recommend commonly composed
charts in real world, one should take the challenges of efficiency, imbalanced
data and table context into consideration. In this paper, we propose
Table2Charts framework which learns common patterns from a large corpus of
(table, charts) pairs. Based on deep Q-learning with copying mechanism and
heuristic searching, Table2Charts does table-to-sequence generation, where each
sequence follows a chart template. On a large spreadsheet corpus with 165k
tables and 266k charts, we show that Table2Charts could learn a shared
representation of table fields so that recommendation tasks on different chart
types could mutually enhance each other. Table2Charts outperforms other chart
recommendation systems in both multi-type task (with doubled recall numbers
R@3=0.61 and R@1=0.43) and human evaluations.

</details>


### [136] [Sigma Worksheet: Interactive Construction of OLAP Queries](https://arxiv.org/abs/2012.00697)
*James Gale,Max Seiden,Gretchen Atwood,Jason Frantz,Rob Woollen,Çağatay Demiralp*

Main category: cs.DB

TL;DR: Sigma Worksheet 是一个交互式系统，通过类似电子表格的界面，让业务用户能够轻松地对云数据仓库进行 ad-hoc 可视化分析，并提高了用户的工作效率。


<details>
  <summary>Details</summary>
Motivation: 现有的云数据仓库分析工具在 ad-hoc 转换方面功能有限，或者对业务用户来说难以使用。

Method: Sigma Worksheet 通过提供类似电子表格的界面，允许用户通过直接操作来轻松地对云数据仓库中的数据进行 ad-hoc 可视化分析。它根据用户在熟悉界面上的交互动态地构建匹配的 SQL 查询，并直接在云数据仓库上执行这些查询，从而利用了云数据仓库的可扩展性等优势。

Result: Sigma Worksheet 生成的查询性能与 TPC-H 基准测试中的参考查询相当。用户反馈表明，Sigma Worksheet 更易于使用和学习，提高了用户的工作效率。

Conclusion: Sigma Worksheet 易于使用和学习，提高了用户的工作效率，但可通过提供指导来进一步改善用户体验。

Abstract: The new generation of cloud data warehouses (CDWs) brings large amounts of
data and compute power closer to users in enterprises. The ability to directly
access the warehouse data, interactively analyze and explore it at scale can
empower users to improve their decision making cycles. However, existing tools
for analyzing data in CDWs are either limited in ad-hoc transformations or
difficult to use for business users, the largest user segment in enterprises.
Here we introduce Sigma Worksheet, a new interactive system that enables users
to easily perform ad-hoc visual analysis of data in CDWs at scale. For this,
Sigma Worksheet provides an accessible spreadsheet-like interface for data
analysis through direct manipulation. Sigma Worksheet dynamically constructs
matching SQL queries from user interactions on this familiar interface,
building on the versatility and expressivity of SQL. Sigma Worksheet executes
constructed queries directly on CDWs, leveraging the superior characteristics
of the new generation CDWs, including scalability. To evaluate Sigma Worksheet,
we first demonstrate its expressivity through two real life use cases, cohort
analysis and sessionization. We then measure the performance of the Worksheet
generated queries with a set of experiments using the TPC-H benchmark. Results
show the performance of our compiled SQL queries is comparable to that of the
reference queries of the benchmark. Finally, to assess the usefulness of Sigma
Worksheet in deployment, we elicit feedback through a 100-person survey
followed by a semi-structured interview study with 70 participants. We find
that Sigma Worksheet is easier to use and learn, improving the productivity of
users. Our findings also suggest Sigma Worksheet can further improve user
experience by providing guidance to users at various steps of data analysis.

</details>


### [137] [Mapping Spreadsheets to RDF: Supporting Excel in RML](https://arxiv.org/abs/2104.13600)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: RML Mapper现已支持Excel文件，并提供在线演示。


<details>
  <summary>Details</summary>
Motivation: 解决RDF Mapping Language（RML）在处理电子表格数据方面的不足，因为该格式未被RML规范和现有工具充分支持。

Method: 通过扩展RML Mapper工具来支持Excel文件，并允许访问电子表格单元格的各种元数据。

Result: 实现了对Excel文件的支持，并提供了一个在线演示，允许用户访问电子表格单元格的元数据，并提供了一些实验性功能。

Conclusion: 该研究扩展了RML Mapper工具以支持Microsoft Excel电子表格文件，并提供了一个在线演示。

Abstract: The RDF Mapping Language (RML) enables, among other formats, the mapping of
tabular data as Comma-Separated Values (CSV) files to RDF graphs.
Unfortunately, the widely used spreadsheet format is currently neglected by its
specification and well-known implementations. Therefore, we extended one of the
tools which is RML Mapper to support Microsoft Excel spreadsheet files and
demonstrate its capabilities in an interactive online demo. Our approach allows
to access various meta data of spreadsheet cells in typical RML maps. Some
experimental features for more specific use cases are also provided. The
implementation code is publicly available in a GitHub fork.

</details>


### [138] [Dataset Generation Patterns for Evaluating Knowledge Graph Construction](https://arxiv.org/abs/2104.13576)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: Synthetic data generation using discovered spreadsheet patterns to overcome confidentiality issues in creating labeled datasets for knowledge graph construction.


<details>
  <summary>Details</summary>
Motivation: Confidentiality of personal and enterprise data prevents the publication of authentic labeled datasets, which are crucial for evaluating knowledge graph construction methods in industrial settings. The motivation is to overcome this limitation by synthetically generating realistic data.

Method: The study identifies 11 distinct patterns in real spreadsheets from industry and uses these patterns to develop a data generator named Data Sprout capable of reproducing them. The paper details the generator's process and the effects of its implemented patterns.

Result: The paper successfully derived 11 patterns from real spreadsheets and developed a generator, Data Sprout, that can reproduce these patterns to create synthetic datasets.

Conclusion: The paper demonstrates a synthetic data generation method called Data Sprout that can reproduce patterns found in real-world spreadsheets, addressing the challenge of data confidentiality in knowledge graph construction.

Abstract: Confidentiality hinders the publication of authentic, labeled datasets of
personal and enterprise data, although they could be useful for evaluating
knowledge graph construction approaches in industrial scenarios. Therefore, our
plan is to synthetically generate such data in a way that it appears as
authentic as possible. Based on our assumption that knowledge workers have
certain habits when they produce or manage data, generation patterns could be
discovered which can be utilized by data generators to imitate real datasets.
In this paper, we initially derived 11 distinct patterns found in real
spreadsheets from industry and demonstrate a suitable generator called Data
Sprout that is able to reproduce them. We describe how the generator produces
spreadsheets in general and what altering effects the implemented patterns
have.

</details>


### [139] [Interactively Constructing Knowledge Graphs from Messy User-Generated Spreadsheets](https://arxiv.org/abs/2103.03537)
*Markus Schröder,Christian Jilek,Michael Schulze,Andreas Dengel*

Main category: cs.DB

TL;DR: Spreadsheet data is often messy and hard for machines to read. This paper presents a tool that lets users easily label spreadsheet cells to build knowledge graphs. It works better than existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of interpreting and structuring unstructured data in spreadsheets, particularly for machine readability and knowledge graph construction, when data maintenance strategies are lacking and user-generated data becomes messy.

Method: An interactive approach involving a graphical user interface for bulk-annotation of spreadsheet cells to construct knowledge graphs.

Result: A 25k-triple knowledge graph was constructed using five spreadsheets from an industrial scenario. The evaluation demonstrated the superiority of the proposed method compared to the state-of-the-art RDF Mapping Language (RML).

Conclusion: The proposed interactive approach with a graphical user interface enables effective bulk-annotation of spreadsheet cells, facilitating the construction of knowledge graphs from unstructured data and outperforming the state-of-the-art RML in evaluations.

Abstract: When spreadsheets are filled freely by knowledge workers, they can contain
rather unstructured content. For humans and especially machines it becomes
difficult to interpret such data properly. Therefore, spreadsheets are often
converted to a more explicit, formal and structured form, for example, to a
knowledge graph. However, if a data maintenance strategy has been missing and
user-generated data becomes "messy", the construction of knowledge graphs will
be a challenging task. In this paper, we catalog several of those challenges
and propose an interactive approach to solve them. Our approach includes a
graphical user interface which enables knowledge engineers to bulk-annotate
spreadsheet cells with extracted information. Based on the cells' annotations a
knowledge graph is ultimately formed. Using five spreadsheets from an
industrial scenario, we built a 25k-triple graph during our evaluation. We
compared our method with the state-of-the-art RDF Mapping Language (RML)
attempt. The comparison highlights contributions of our approach.

</details>


### [140] [Leam: An Interactive System for In-situ Visual Text Analysis](https://arxiv.org/abs/2009.03520)
*Sajjadur Rahman,Peter Griggs,Çağatay Demiralp*

Main category: cs.DB

TL;DR: Leam是一个集计算笔记本、电子表格和可视化工具优点于一身的文本分析系统，解决了现有系统的局限性，并提供交互式用户界面、新的数据模型和表达性代数。


<details>
  <summary>Details</summary>
Motivation: 现有的文本分析系统通常只包含这些阶段的一部分，并且未能解决数据异质性、数据来源、工作流可重用性和可再现性以及与既定实践兼容性等方面的挑战。

Method: 提出了一种名为Leam的系统，该系统具有用于运行文本分析工作流的交互式用户界面、用于管理多种原子和复合数据类型的新数据模型以及捕获各种文本分析阶段的操作并实现数据、代码和可视化等不同系统组件之间协调的表达性代数。

Result: 报告了Leam开发进展，并通过使用示例展示了其有用性。

Conclusion: Leam是一个将文本分析过程视为一个连续过程的系统，通过结合计算笔记本、电子表格和可视化工具的优点来解决数据异质性、数据来源、工作流可重用性和可再现性以及与既定实践兼容性等挑战。

Abstract: With the increase in scale and availability of digital text generated on the
web, enterprises such as online retailers and aggregators often use text
analytics to mine and analyze the data to improve their services and products
alike. Text data analysis is an iterative, non-linear process with diverse
workflows spanning multiple stages, from data cleaning to visualization.
Existing text analytics systems usually accommodate a subset of these stages
and often fail to address challenges related to data heterogeneity, provenance,
workflow reusability and reproducibility, and compatibility with established
practices. Based on a set of design considerations we derive from these
challenges, we propose Leam, a system that treats the text analysis process as
a single continuum by combining advantages of computational notebooks,
spreadsheets, and visualization tools. Leam features an interactive user
interface for running text analysis workflows, a new data model for managing
multiple atomic and composite data types, and an expressive algebra that
captures diverse sets of operations representing various stages of text
analysis and enables coordination among different components of the system,
including data, code, and visualizations. We report our current progress in
Leam development while demonstrating its usefulness with usage examples.
Finally, we outline a number of enhancements to Leam and identify several
research directions for developing an interactive visual text analysis system.

</details>


### [141] [ObjTables: structured spreadsheets that promote data quality, reuse, and integration](https://arxiv.org/abs/2005.05227)
*Jonathan R. Karr,Wolfram Liebermeister,Arthur P. Goldberg,John A. P. Sekar,Bilal Shaikh*

Main category: cs.DB

TL;DR: ObjTables is a toolkit that helps researchers reuse and compose spreadsheets by making them human- and machine-readable using schemas and an object-relational mapping system.


<details>
  <summary>Details</summary>
Motivation: Spreadsheets are a key data source in science, but they are often difficult to reanalyze because they capture data ad hoc without schemas. This makes it challenging to aggregate, reuse, and integrate heterogeneous information to understand how system behaviors emerge from complex networks.

Method: ObjTables is a toolkit that makes spreadsheets human- and machine-readable by combining spreadsheets with schemas and an object-relational mapping system. It includes a format for schemas, markup for indicating the class and attribute represented by each spreadsheet and column, numerous data types for scientific information, and high-level software for using schemas to read, write, validate, compare, merge, revision, and analyze spreadsheets.

Result: ObjTables makes spreadsheets human- and machine-readable, enabling easier reuse and composition. This can lead to unprecedented secondary meta-analyses and accelerate emerging scientific fields.

Conclusion: ObjTables enables unprecedented secondary meta-analyses by making spreadsheets easier to reuse and can accelerate emerging scientific fields by making it easy to build new formats and associated software for new types of data.

Abstract: A central challenge in science is to understand how systems behaviors emerge
from complex networks. This often requires aggregating, reusing, and
integrating heterogeneous information. Supplementary spreadsheets to articles
are a key data source. Spreadsheets are popular because they are easy to read
and write. However, spreadsheets are often difficult to reanalyze because they
capture data ad hoc without schemas that define the objects, relationships, and
attributes that they represent. To help researchers reuse and compose
spreadsheets, we developed ObjTables, a toolkit that makes spreadsheets human-
and machine-readable by combining spreadsheets with schemas and an
object-relational mapping system. ObjTables includes a format for schemas;
markup for indicating the class and attribute represented by each spreadsheet
and column; numerous data types for scientific information; and high-level
software for using schemas to read, write, validate, compare, merge, revision,
and analyze spreadsheets. By making spreadsheets easier to reuse, ObjTables
could enable unprecedented secondary meta-analyses. By making it easy to build
new formats and associated software for new types of data, ObjTables can also
accelerate emerging scientific fields.

</details>


### [142] [Needles in the 'Sheet'stack: Augmented Analytics to get Insights from Spreadsheets](https://arxiv.org/abs/2006.08224)
*Medha Atre,Anand Deshpande,Reshma Godse,Pooja Deokar,Sandip Moharir,Dhruva Ray,Akshay Chitlangia,Trupti Phadnis,Yugansh Goyal*

Main category: cs.DB

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: 'NoneType' object has no attribute 'model_dump'

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Business intelligence (BI) tools for database analytics have come a long way
and nowadays also provide ready insights or visual query explorations, e.g.
QuickInsights by Microsoft Power BI, SpotIQ by ThoughtSpot, Zenvisage, etc. In
this demo, we focus on providing insights by examining periodic spreadsheets of
different reports (aka views), without prior knowledge of the schema of the
database or reports, or data information. Such a solution is targeted at users
without the familiarity with the database schema or resources to conduct
analytics in the contemporary way.

</details>


### [143] [A Billion Updates per Second Using 30,000 Hierarchical In-Memory D4M Databases](https://arxiv.org/abs/1902.00846)
*Jeremy Kepner,Vijay Gadepally,Lauren Milechin,Siddharth Samsi,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Micheal Houle,Micheal Jones,Anne Klein,Peter Michaleas,Julie Mullen,Andrew Prout,Antonio Rosa,Charles Yee,Albert Reuther*

Main category: cs.DB

TL;DR: D4M's hierarchical associative arrays on MIT SuperCloud achieve 1.9 billion updates/sec for large network analysis.


<details>
  <summary>Details</summary>
Motivation: Analyzing large-scale networks necessitates high-performance streaming updates for graph representations. Associative arrays, with their versatile properties, are suitable for this task, and the D4M library provides an implementation for them.

Method: The study implemented hierarchical Dynamic Distributed Dimensional Data Model (D4M) associative arrays across 1,100 server nodes on the MIT SuperCloud, running 34,000 instances to analyze their performance.

Result: The D4M library achieved a sustained update rate of 1,900,000,000 updates per second on the MIT SuperCloud, enabling the analysis of extremely large streaming network data sets.

Conclusion: The D4M library's hierarchical implementation of associative arrays allows for efficient analysis of large-scale streaming network data on the MIT SuperCloud, achieving a sustained update rate of 1.9 billion updates per second.

Abstract: Analyzing large scale networks requires high performance streaming updates of
graph representations of these data. Associative arrays are mathematical
objects combining properties of spreadsheets, databases, matrices, and graphs,
and are well-suited for representing and analyzing streaming network data. The
Dynamic Distributed Dimensional Data Model (D4M) library implements associative
arrays in a variety of languages (Python, Julia, and Matlab/Octave) and
provides a lightweight in-memory database. Associative arrays are designed for
block updates. Streaming updates to a large associative array requires a
hierarchical implementation to optimize the performance of the memory
hierarchy. Running 34,000 instances of a hierarchical D4M associative arrays on
1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of
1,900,000,000 updates per second. This capability allows the MIT SuperCloud to
analyze extremely large streaming network data sets.

</details>


### [144] [Towards Semantically Enhanced Data Understanding](https://arxiv.org/abs/1806.04952)
*Markus Schröder,Christian Jilek,Jörn Hees,Andreas Dengel*

Main category: cs.DB

TL;DR: 将数据和文档链接到一个语义模型中，简化数据科学工作流程。


<details>
  <summary>Details</summary>
Motivation: 解决数据科学任务中，数据文档与数据分离导致的查找开销大、非结构化数据难以利用的问题。

Method: 提出一种使用单一语义模型链接数据及其文档的方法。

Result: 展示了一个早期原型，证明了该方法的可行性，并支持搜索、比较、集成或可视化等多种数据处理应用。

Conclusion: 该方法能够将数据与其文档链接起来，方便数据科学家查找信息，并支持其他数据处理应用。

Abstract: In the field of machine learning, data understanding is the practice of
getting initial insights in unknown datasets. Such knowledge-intensive tasks
require a lot of documentation, which is necessary for data scientists to grasp
the meaning of the data. Usually, documentation is separate from the data in
various external documents, diagrams, spreadsheets and tools which causes
considerable look up overhead. Moreover, other supporting applications are not
able to consume and utilize such unstructured data. That is why we propose a
methodology that uses a single semantic model that interlinks data with its
documentation. Hence, data scientists are able to directly look up the
connected information about the data by simply following links. Equally, they
can browse the documentation which always refers to the data. Furthermore, the
model can be used by other approaches providing additional support, like
searching, comparing, integrating or visualizing data. To showcase our approach
we also demonstrate an early prototype.

</details>


### [145] [WebRelate: Integrating Web Data with Spreadsheets using Examples](https://arxiv.org/abs/1711.05787)
*Jeevana Priya Inala,Rishabh Singh*

Main category: cs.DB

TL;DR: WebRelate is a system that simplifies data integration between web sources and relational data by learning URL generation and data extraction programs from user examples, addressing the challenges faced by users without extensive programming expertise.


<details>
  <summary>Details</summary>
Motivation: Data integration between web sources and relational data is a key challenge faced by data scientists and spreadsheet users, who often lack the expertise to write complex scripts for data extraction.

Method: WebRelate decomposes the web data integration task into two sub-tasks of URL learning and input-dependent web extraction. It learns a string transformation program for URL generation and a program for data extraction using input-output examples.

Result: WebRelate was evaluated on 88 real-world web data integration tasks and showed successful learning within seconds using few examples.

Conclusion: WebRelate can learn the desired programs within few seconds using only 1 example for the majority of the tasks.

Abstract: Data integration between web sources and relational data is a key challenge
faced by data scientists and spreadsheet users. There are two main challenges
in programmatically joining web data with relational data. First, most websites
do not expose a direct interface to obtain tabular data, so the user needs to
formulate a logic to get to different webpages for each input row in the
relational table. Second, after reaching the desired webpage, the user needs to
write complex scripts to extract the relevant data, which is often conditioned
on the input data. Since many data scientists and end-users come from diverse
backgrounds, writing such complex regular-expression based logical scripts to
perform data integration tasks is unfortunately often beyond their programming
expertise.
  We present WebRelate, a system that allows users to join semi-structured web
data with relational data in spreadsheets using input-output examples.
WebRelate decomposes the web data integration task into two sub-tasks of i) URL
learning and ii) input-dependent web extraction. The first sub-task generates
the URLs for the webpages containing the desired data for all rows in the
relational table. WebRelate achieves this by learning a string transformation
program using a few example URLs. The second sub-task uses examples of desired
data to be extracted from the corresponding webpages and learns a program to
extract the data for the other rows. We design expressive domain-specific
languages for URL generation and web data extraction, and present efficient
synthesis algorithms for learning programs in these DSLs from few input-output
examples. We evaluate WebRelate on 88 real-world web data integration tasks
taken from online help forums and Excel product team, and show that WebRelate
can learn the desired programs within few seconds using only 1 example for the
majority of the tasks.

</details>


### [146] [Towards a Holistic Integration of Spreadsheets with Databases: A Scalable Storage Engine for Presentational Data Management](https://arxiv.org/abs/1708.06712)
*Mangesh Bendre,Vipul Venkataraman,Xinyan Zhou,Kevin Chang,Aditya Parameswaran*

Main category: cs.DB

TL;DR: 数据传播（DataSpread）项目通过将电子表格作为前端与数据库后端相结合，解决了电子表格的可扩展性和数据库的交互性问题。该研究开发了一种新的存储引擎，以高效地表示电子表格数据并支持位置访问，与现有方法相比，在存储和性能方面都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 电子表格软件虽然被广泛用于数据管理，但缺乏可扩展性；数据库系统虽然可扩展，但缺乏交互性。该研究旨在整合两者的优点。

Method: 开发了一个新的存储引擎，用于以灵活的方式在数据库中表示电子表格数据，并支持位置访问，同时解决了级联更新问题，实现了恒定的访问和修改性能。

Result: 通过对典型电子表格工作负载的评估，该方法在存储方面最多可减少 20%，在公式评估时间方面最多可减少 50%。

Conclusion: 这项工作为数据管理和电子表格系统之间的集成奠定了基础，展示了在存储效率和公式评估性能方面的显著改进。

Abstract: Spreadsheet software is the tool of choice for interactive ad-hoc data
management, with adoption by billions of users. However, spreadsheets are not
scalable, unlike database systems. On the other hand, database systems, while
highly scalable, do not support interactivity as a first-class primitive. We
are developing DataSpread, to holistically integrate spreadsheets as a
front-end interface with databases as a back-end datastore, providing
scalability to spreadsheets, and interactivity to databases, an integration we
term presentational data management (PDM). In this paper, we make a first step
towards this vision: developing a storage engine for PDM, studying how to
flexibly represent spreadsheet data within a database and how to support and
maintain access by position. We first conduct an extensive survey of
spreadsheet use to motivate our functional requirements for a storage engine
for PDM. We develop a natural set of mechanisms for flexibly representing
spreadsheet data and demonstrate that identifying the optimal representation is
NP-Hard; however, we develop an efficient approach to identify the optimal
representation from an important and intuitive subclass of representations. We
extend our mechanisms with positional access mechanisms that don't suffer from
cascading update issues, leading to constant time access and modification
performance. We evaluate these representations on a workload of typical
spreadsheets and spreadsheet operations, providing up to 20% reduction in
storage, and up to 50% reduction in formula evaluation time.

</details>


### [147] [Synthesizing Mapping Relationships Using Table Corpus](https://arxiv.org/abs/1705.09276)
*Yue Wang,Yeye He*

Main category: cs.DB

TL;DR: 提出了一种利用大规模表语料库合成映射关系的方法


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够支持智能应用（如自动更正、自动连接）的映射表存储库

Method: 利用基于共现统计的表兼容性和函数依赖等约束来合成映射关系

Result: 实验结果表明该方法可以生成高质量的映射关系

Conclusion: 该方法可以生成高质量的映射关系

Abstract: Mapping relationships, such as (country, country-code) or (company,
stock-ticker), are versatile data assets for an array of applications in data
cleaning and data integration like auto-correction and auto-join. However,
today there are no good repositories of mapping tables that can enable these
intelligent applications.
  Given a corpus of tables such as web tables or spreadsheet tables, we observe
that values of these mappings often exist in pairs of columns in same tables.
Motivated by their broad applicability, we study the problem of synthesizing
mapping relationships using a large table corpus. Our synthesis process
leverages compatibility of tables based on co-occurrence statistics, as well as
constraints such as functional dependency. Experiment results using web tables
and enterprise spreadsheets suggest that the proposed approach can produce high
quality mappings.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [148] [Dynamic Kernel Matching for Non-conforming Data: A Case Study of T-cell Receptor Datasets](https://arxiv.org/abs/2103.10472)
*Jared Ostmeyer,Scott Christley,Lindsay Cowell*

Main category: q-bio.QM

TL;DR: This paper presents dynamic kernel matching (DKM), a method to adapt statistical classifiers for non-conforming data, demonstrating its effectiveness on T-cell receptor sequence datasets for disease diagnosis and showing that the identified patterns align with experimental findings.


<details>
  <summary>Details</summary>
Motivation: Most statistical classifiers are designed to find patterns in data where numbers fit into rows and columns, like in a spreadsheet, but many kinds of data do not conform to this structure. To uncover patterns in non-conforming data, we describe an approach for modifying established statistical classifiers to handle non-conforming data.

Method: We describe an approach for modifying established statistical classifiers to handle non-conforming data, which we call dynamic kernel matching (DKM).

Result: We successfully fit statistical classifiers augmented with DKM to both datasets and report the performance on holdout data using standard metrics and metrics allowing for indeterminant diagnoses. Finally, we identify the patterns used by our statistical classifiers to generate predictions and show that these patterns agree with observations from experimental studies.

Conclusion: We successfully fit statistical classifiers augmented with DKM to both datasets and report the performance on holdout data using standard metrics and metrics allowing for indeterminant diagnoses. We identify the patterns used by our statistical classifiers to generate predictions and show that these patterns agree with observations from experimental studies.

Abstract: Most statistical classifiers are designed to find patterns in data where
numbers fit into rows and columns, like in a spreadsheet, but many kinds of
data do not conform to this structure. To uncover patterns in non-conforming
data, we describe an approach for modifying established statistical classifiers
to handle non-conforming data, which we call dynamic kernel matching (DKM). As
examples of non-conforming data, we consider (i) a dataset of T-cell receptor
(TCR) sequences labelled by disease antigen and (ii) a dataset of sequenced TCR
repertoires labelled by patient cytomegalovirus (CMV) serostatus, anticipating
that both datasets contain signatures for diagnosing disease. We successfully
fit statistical classifiers augmented with DKM to both datasets and report the
performance on holdout data using standard metrics and metrics allowing for
indeterminant diagnoses. Finally, we identify the patterns used by our
statistical classifiers to generate predictions and show that these patterns
agree with observations from experimental studies.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [149] [Trapped in Transformative Agreements? A Multifaceted Analysis of >1,000 Contracts](https://arxiv.org/abs/2409.20224)
*Laura Rothfritz,W. Benedikt Schmal,Ulrich Herb*

Main category: cs.DL

TL;DR: 转型协议虽然普遍，但却将学术界锁定在混合系统中，使传统出版商受益，并增加了大学的成本。


<details>
  <summary>Details</summary>
Motivation: 利用ESAC倡议提供的独特数据集，对其进行扩展并深入分析转型协议的特征和格局。

Method: 通过网络抓取ESAC数据库中超过1,000份合同的详细信息，并结合定性和定量方法对合同特征和转型协议格局进行深入分析。

Result: 研究发现，学术界被锁定在转型协议中，阻碍了向完全开放获取的过渡，并使传统出版商获得了相当大的市场权力，从而提高了图书馆和大学的准入门槛、降低了竞争并增加了成本。

Conclusion: 学术界似乎被锁定在转型协议中，阻碍了向完全开放获取的过渡，并使传统出版商获得了相当大的市场权力，从而提高了图书馆和大学的准入门槛、降低了竞争并增加了成本。

Abstract: Transformative agreements between academic publishers and research
institutions are ubiquitous. The 'Efficiency and Standards for Article Charges'
(ESAC) Initiative lists more than 1,000 contracts in its database. We make use
of this unique dataset by web-scraping the details of every contract to
substantially expand the overview spreadsheet provided by the ESAC Initiative.
Based on that hitherto unused data source, we combine qualitative and
quantitative methods to conduct an in-depth analysis of the contract
characteristics and the TA landscape. Our analysis demonstrates that research
institutions seem to be 'trapped' in transformative agreements. Instead of
being a bridge towards a fully Open Access world, academia is stuck in the
hybrid system. This endows the legacy (non-Open Access) publishing houses with
substantial market power. It raises entry barriers, lowers competition, and
increases costs for libraries and universities.

</details>


### [150] [Ensuring Adherence to Standards in Experiment-Related Metadata Entered Via Spreadsheets](https://arxiv.org/abs/2409.08897)
*Martin J. O'Connor,Josef Hardi,Marcos Martínez-Romero,Sowmya Somasundaram,Brendan Honick,Stephen A. Fisher,Ajay Pillai,Mark A. Musen*

Main category: cs.DL

TL;DR: This paper presents a system that uses spreadsheets for metadata entry but ensures compliance with standards through customizable templates, controlled vocabularies, and an interactive web tool, as demonstrated in the HuBMAP consortium.


<details>
  <summary>Details</summary>
Motivation: Despite the availability of sophisticated tools, investigators prefer using spreadsheets for metadata entry due to their limitations in ensuring consistency and compliance with formal specifications.

Method: The approach employs customizable templates that capture metadata standards and inform spreadsheets, controlled terminologies and ontologies for defining metadata values, and an interactive Web-based tool for error identification and correction.

Result: The approach is being deployed in the HuBMAP biomedical consortium to define and collect metadata about a wide range of biological assays, demonstrating its practical application and effectiveness.

Conclusion: The paper describes an end-to-end approach that supports spreadsheet-based entry of metadata, while ensuring rigorous adherence to community-based metadata standards and providing quality control.

Abstract: Scientists increasingly recognize the importance of providing rich,
standards-adherent metadata to describe their experimental results. Despite the
availability of sophisticated tools to assist in the process of data
annotation, investigators generally seem to prefer to use spreadsheets when
supplying metadata, despite the limitations of spreadsheets in ensuring
metadata consistency and compliance with formal specifications. In this paper,
we describe an end-to-end approach that supports spreadsheet-based entry of
metadata, while ensuring rigorous adherence to community-based metadata
standards and providing quality control. Our methods employ several key
components, including customizable templates that capture metadata standards
and that can inform the spreadsheets that investigators use to author metadata,
controlled terminologies and ontologies for defining metadata values that can
be accessed directly from a spreadsheet, and an interactive Web-based tool that
allows users to rapidly identify and fix errors in their spreadsheet-based
metadata. We demonstrate how this approach is being deployed in a biomedical
consortium known as HuBMAP to define and collect metadata about a wide range of
biological assays.

</details>


### [151] [A Comprehensive Approach to Ensuring Quality in Spreadsheet-Based Metadata](https://arxiv.org/abs/2312.09107)
*Martin J. O'Connor,Marcos Martínez-Romero,Mete Ugur Akdogan,Josef Hardi,Mark A. Musen*

Main category: cs.DL

TL;DR: 一项新的端到端方法通过使用可定制模板、受控术语和交互式 Web 工具，解决了电子表格在科学数据元数据管理中的局限性，提高了合规性和质量控制。


<details>
  <summary>Details</summary>
Motivation: 尽管科学家们越来越认识到元数据的重要性，但电子表格在提供这些信息方面仍然是首选工具，但它们在确保合规性和质量方面存在局限性。现有工具存在学习曲线陡峭和定制性有限等缺点。

Method: 本文介绍了一种端到端的方法，该方法支持基于电子表格的元数据输入，同时提供严格的合规性和质量控制。我们的方法采用了几项关键策略，包括用于定义元数据的可定制模板、为在定义这些模板时使用受控术语提供的集成支持，以及一个交互式 Web 工具，允许用户快速识别和修复他们提供的基于电子表格的元数据中的错误。

Result: 该方法通过可定制模板、集成受控术语支持和交互式 Web 工具，实现了基于电子表格的元数据输入的合规性和质量控制，并在生物医学联盟的实验元数据收集和定义中得到部署。

Conclusion: 科学家们越来越认识到元数据在描述数据方面的重要性，但电子表格在提供这些信息方面仍然是首选工具，尽管它们在确保合规性和质量方面存在局限性。已开发出各种工具来解决这些局限性，但它们存在学习曲线陡峭和定制性有限等缺点。本文介绍了一种端到端的方法，该方法支持基于电子表格的元数据输入，同时提供严格的合规性和质量控制。我们的方法采用了几项关键策略，包括用于定义元数据的可定制模板、为在定义这些模板时使用受控术语提供的集成支持，以及一个交互式 Web 工具，允许用户快速识别和修复他们提供的基于电子表格的元数据中的错误。我们演示了该方法如何在生物医学联盟中用于定义和收集有关科学实验的元数据。

Abstract: While scientists increasingly recognize the importance of metadata in
describing their data, spreadsheets remain the preferred tool for supplying
this information despite their limitations in ensuring compliance and quality.
Various tools have been developed to address these limitations, but they suffer
from their own shortcomings, such as steep learning curves and limited
customization. In this paper, we describe an end-to-end approach that supports
spreadsheet-based entry of metadata while providing rigorous compliance and
quality control. Our approach employs several key strategies, including
customizable templates for defining metadata, integral support for the use of
controlled terminologies when defining these templates, and an interactive
Web-based tool that allows users to rapidly identify and fix errors in the
spreadsheet-based metadata they supply. We demonstrate how this approach is
being deployed in a biomedical consortium to define and collect metadata about
scientific experiments.

</details>


### [152] [Towards Semantic Interoperability in Historical Research: Documenting Research Data and Knowledge with Synthesis](https://arxiv.org/abs/2107.13957)
*Pavlos Fafalios,Konstantina Konsolaki,Lida Charami,Kostas Petrakis,Manos Paterakis,Dimitris Angelakis,Yannis Tzitzikas,Chrysoula Bekiari,Martin Doerr*

Main category: cs.DL

TL;DR: Synthesis系统通过利用CIDOC-CRM和RDF等标准，解决了历史文物研究中数据组织、协作和互操作性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前历史科学研究中，文物和相关证据的记录和研究主要使用电子表格或简单的关系数据库，这在多用户协作、表示推断细节、扩展数据结构、整合多源数据以及数据重用方面存在困难。

Method: 介绍了一个名为Synthesis的基于Web的协作式文档系统，该系统利用CIDOC-CRM和RDF等现有标准，解决了大型项目中的数据组织、协作、细节表示、可扩展性和数据重用等问题。

Result: Synthesis系统已被大量艺术史学家用于一个持续的研究项目中，证明了其作为支持协作和互操作性数据记录的有效性。

Conclusion: 该系统支持历史学家应对在文物和相关证据的记录和研究方面遇到的挑战，利用现有的信息记录和发布标准（CIDOC-CRM、RDF），专注于语义互操作性和高质量、长期有效的数据。

Abstract: A vast area of research in historical science concerns the documentation and
study of artefacts and related evidence. Current practice mostly uses
spreadsheets or simple relational databases to organise the information as rows
with multiple columns of related attributes. This form offers itself for data
analysis and scholarly interpretation, however it also poses problems including
i) the difficulty for collaborative but controlled documentation by a large
number of users, ii) the lack of representation of the details from which the
documented relations are inferred, iii) the difficulty to extend the underlying
data structures as well as to combine and integrate data from multiple and
diverse information sources, and iv) the limitation to reuse the data beyond
the context of a particular research activity. To support historians to cope
with these problems, in this paper we describe the Synthesis documentation
system and its use by a large number of historians in the context of an ongoing
research project in the field of History of Art. The system is Web-based and
collaborative, and makes use of existing standards for information
documentation and publication (CIDOC-CRM, RDF), focusing on semantic
interoperability and the production of data of high value and long-term
validity.

</details>


### [153] [FAST CAT: Collaborative Data Entry and Curation for Semantic Interoperability in Digital Humanities](https://arxiv.org/abs/2105.13733)
*Pavlos Fafalios,Kostas Petrakis,Georgios Samaritakis,Korina Doerr,Athina Kritsotaki,Yannis Tzitzikas,Martin Doerr*

Main category: cs.DL

TL;DR: FAST CAT is a new system to help researchers in fields like history manage and enter data more effectively, overcoming issues with traditional tools like spreadsheets, and is being used in a project about Mediterranean maritime history.


<details>
  <summary>Details</summary>
Motivation: Current practices in descriptive and empirical sciences using spreadsheet software and relational databases have limitations, including high dependency on initial research hypothesis, lack of representation of details for inferred relations, and difficulty in verifying original data sources.

Method: The paper describes the challenges, methodology for supporting semantic interoperability, and discusses the use of FAST CAT in a Maritime History project (SeaLiT).

Result: FAST CAT is presented as a solution to cope with the limitations of current data management practices in empirical research, with a discussion of its application in the SeaLiT project.

Conclusion: The paper presents FAST CAT, a collaborative system for assistive data entry and curation in Digital Humanities and similar empirical research, addressing limitations of current practices like spreadsheet software and relational database management systems.

Abstract: Descriptive and empirical sciences, such as History, are the sciences that
collect, observe and describe phenomena in order to explain them and draw
interpretative conclusions about influences, driving forces and impacts under
given circumstances. Spreadsheet software and relational database management
systems are still the dominant tools for quantitative analysis and overall data
management in these these sciences, allowing researchers to directly analyse
the gathered data and perform scholarly interpretation. However, this current
practice has a set of limitations, including the high dependency of the
collected data on the initial research hypothesis, usually useless for other
research, the lack of representation of the details from which the registered
relations are inferred, and the difficulty to revisit the original data sources
for verification, corrections or improvements. To cope with these problems, in
this paper we present FAST CAT, a collaborative system for assistive data entry
and curation in Digital Humanities and similar forms of empirical research. We
describe the related challenges, the overall methodology we follow for
supporting semantic interoperability, and discuss the use of FAST CAT in the
context of a European (ERC) project of Maritime History, called SeaLiT, which
examines economic, social and demographic impacts of the introduction of
steamboats in the Mediterranean area between the 1850s and the 1920s.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [154] [Data Encryption Battlefield: A Deep Dive into the Dynamic Confrontations in Ransomware Attacks](https://arxiv.org/abs/2504.20681)
*Arash Mahboubi,Hamed Aboutorab,Seyit Camtepe,Hang Thanh Bui,Khanh Luong,Keyvan Ansari,Shenlu Wang,Bazara Barry*

Main category: cs.CR

TL;DR: 本研究通过在线增量机器学习方法，包括Hoeffding Tree和Random Forest，有效检测了复杂的勒索软件加密技术，为网络安全提供了新的解决方案。


<details>
  <summary>Details</summary>
Motivation: 为了应对日益复杂的勒索软件加密技术（如Base64编码导致的熵减少和部分/间歇性加密）对传统检测方法的规避，本研究旨在探索检测这些先进加密策略的机器学习方法。

Method: 利用在线增量机器学习算法（特别是Hoeffding Tree和带热启动功能的Random Forest）来预测文件加密活动，以应对不断演变的混淆技术。

Result: Hoeffding Tree算法在检测传统和AES-Base64加密方法方面表现出卓越的性能，而Random Forest分类器在识别间歇性加密方面效果显著。

Conclusion: Hoeffding Tree算法在检测传统和AES-Base64加密方面表现出优越的增量学习能力，而具有热启动功能的Random Forest分类器在识别间歇性加密方面表现出色，表明需要定制化的机器学习解决方案来应对复杂的勒索软件策略。

Abstract: In the rapidly evolving landscape of cybersecurity threats, ransomware
represents a significant challenge. Attackers increasingly employ sophisticated
encryption methods, such as entropy reduction through Base64 encoding, and
partial or intermittent encryption to evade traditional detection methods. This
study explores the dynamic battle between adversaries who continuously refine
encryption strategies and defenders developing advanced countermeasures to
protect vulnerable data. We investigate the application of online incremental
machine learning algorithms designed to predict file encryption activities
despite adversaries evolving obfuscation techniques. Our analysis utilizes an
extensive dataset of 32.6 GB, comprising 11,928 files across multiple formats,
including Microsoft Word documents (doc), PowerPoint presentations (ppt), Excel
spreadsheets (xlsx), image formats (jpg, jpeg, png, tif, gif), PDFs (pdf),
audio (mp3), and video (mp4) files. These files were encrypted by 75 distinct
ransomware families, facilitating a robust empirical evaluation of machine
learning classifiers effectiveness against diverse encryption tactics. Results
highlight the Hoeffding Tree algorithms superior incremental learning
capability, particularly effective in detecting traditional and AES-Base64
encryption methods employed to lower entropy. Conversely, the Random Forest
classifier with warm-start functionality excels at identifying intermittent
encryption methods, demonstrating the necessity of tailored machine learning
solutions to counter sophisticated ransomware strategies.

</details>


### [155] [JUBILEE: Secure Debt Relief and Forgiveness](https://arxiv.org/abs/2109.07267)
*David Cerezo Sánchez*

Main category: cs.CR

TL;DR: JUBILEE 是一种安全计算机制，可实现无摩擦的债务减免和豁免，比以往的方法更优，并为债务人带来更高利润和成功率。


<details>
  <summary>Details</summary>
Motivation: JUBILEE 旨在解决传统债务结算中效率低下、缺乏信任的问题，通过安全计算实现无摩擦的债务减免和豁免，并提高交易双方的满意度。

Method: JUBILEE 机制利用安全计算技术，激励各方真实披露私人信息，以实现个体理性、激励相容、公平/策略证明、事后有效和最优的债务减免和豁免。

Result: JUBILEE 在债务减免和豁免方面取得了显著的改进，其个体理性、激励相容、公平/策略证明、事后有效和最优的特性优于以往所有方法。此外，通过安全计算技术，JUBILEE 首次实现了“债务人恩惠”，即在不使用安全计算的情况下，债务结算能获得更高的预期利润和成功率。

Conclusion: JUBILEE 通过引入安全计算技术，实现了无需可信第三方即可进行债务减免和债务豁免，提高了债务结算的和谐度，并为债务人带来了更高的预期利润和成功率。

Abstract: JUBILEE is a securely computed mechanism for debt relief and forgiveness in a
frictionless manner without involving trusted third parties, leading to more
harmonious debt settlements by incentivising the parties to truthfully reveal
their private information. JUBILEE improves over all previous methods:
  - individually rational, incentive-compatible, truthful/strategy-proof,
ex-post efficient, optimal mechanism for debt relief and forgiveness with
private information
  - by the novel introduction of secure computation techniques to debt relief,
the "blessing of the debtor" is hereby granted for the first time: debt
settlements with higher expected profits and a higher probability of success
than without using secure computation
  A simple and practical implementation is included for "The Secure
Spreadsheet". Another implementation is realised using Raziel smart contracts
on a blockchain with Pravuil consensus.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [156] [FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining](https://arxiv.org/abs/2109.07323)
*Zhoujun Cheng,Haoyu Dong,Ran Jia,Pengfei Wu,Shi Han,Fan Cheng,Dongmei Zhang*

Main category: cs.IR

TL;DR: FORTAP利用电子表格公式进行表格预训练，以提高数值推理能力，并在下游任务中取得了领先成果。


<details>
  <summary>Details</summary>
Motivation: 电子表格公式是对表格中的数值进行计算，是数值推理的天然监督。网上有大量的包含专家制作公式的电子表格。

Method: FORTAP是第一个利用大型电子表格公式语料库进行数值推理感知表格预训练的方法。我们设计了两个公式预训练任务，以明确指导FORTAP学习半结构化表中的数值引用和计算。

Result: FORTAP在单元格类型分类和公式预测两个下游任务上均取得了最先进的结果。

Conclusion: FORTAP在两个代表性的下游任务（单元格类型分类和公式预测）上取得了最先进的结果，显示了数值推理感知预训练的巨大潜力。

Abstract: Tables store rich numerical data, but numerical reasoning over tables is
still a challenge. In this paper, we find that the spreadsheet formula, which
performs calculations on numerical values in tables, is naturally a strong
supervision of numerical reasoning. More importantly, large amounts of
spreadsheets with expert-made formulae are available on the web and can be
obtained easily. FORTAP is the first method for numerical-reasoning-aware table
pretraining by leveraging large corpus of spreadsheet formulae. We design two
formula pretraining tasks to explicitly guide FORTAP to learn numerical
reference and calculation in semi-structured tables. FORTAP achieves
state-of-the-art results on two representative downstream tasks, cell type
classification and formula prediction, showing great potential of
numerical-reasoning-aware pretraining.

</details>


### [157] [Detecting Layout Templates in Complex Multiregion Files](https://arxiv.org/abs/2109.06630)
*Gerardo Vitagliano,Lan Jiang,Felix Naumann*

Main category: cs.IR

TL;DR: Mondrian方法可自动识别跨多个文件的布局模板，并提取相应的区域，在检测可靠区域边界和识别重复布局方面表现优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 电子表格的广泛使用导致了大量数据的收集，但其灵活的画布结构使得在没有大量预处理的情况下难以进行自动化分析。一个常见的问题是单个电子表格中存在多个独立的区域，它们可能被重复的空单元格分隔。

Method: Mondrian方法包括三个阶段：1. 将电子表格渲染成图像并检查可能形成区域的元素；2. 使用聚类算法将已识别的元素分组形成区域；3. 将每个文件布局表示为图并进行比较以查找布局模板。

Result: 与最先进的表格识别算法相比，Mondrian方法在两个真实的企业电子表格语料库上表现出最佳性能。

Conclusion: Mondrian方法在检测文件内的可靠区域边界和跨文件识别重复布局方面表现出最佳性能。

Abstract: Spreadsheets are among the most commonly used file formats for data
management, distribution, and analysis. Their widespread employment makes it
easy to gather large collections of data, but their flexible canvas-based
structure makes automated analysis difficult without heavy preparation. One of
the common problems that practitioners face is the presence of multiple,
independent regions in a single spreadsheet, possibly separated by repeated
empty cells. We define such files as "multiregion" files. In collections of
various spreadsheets, we can observe that some share the same layout. We
present the Mondrian approach to automatically identify layout templates across
multiple files and systematically extract the corresponding regions. Our
approach is composed of three phases: first, each file is rendered as an image
and inspected for elements that could form regions; then, using a clustering
algorithm, the identified elements are grouped to form regions; finally, every
file layout is represented as a graph and compared with others to find layout
templates. We compare our method to state-of-the-art table recognition
algorithms on two corpora of real-world enterprise spreadsheets. Our approach
shows the best performances in detecting reliable region boundaries within each
file and can correctly identify recurring layouts across files.

</details>


### [158] [TUTA: Tree-based Transformers for Generally Structured Table Pre-training](https://arxiv.org/abs/2010.12537)
*Zhiruo Wang,Haoyu Dong,Ran Jia,Jia Li,Zhiyi Fu,Shi Han,Dongmei Zhang*

Main category: cs.IR

TL;DR: TUTA是一个统一的预训练框架，用于理解通用结构表格，通过引入新的结构感知机制和预训练目标，在表格理解任务上取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 现有表格理解方法主要关注关系表，忽略了其他常见表格结构。本文旨在提出一个统一的预训练框架（TUTA）来理解通用结构表格，因为它需要空间、层次和语义信息。

Method: TUTA通过引入一个统一的、基于树的结构（二维坐标树）来描述表格的空间和层次信息，并结合基于树的注意力和位置嵌入来捕捉这些信息。此外，还设计了三种渐进式的预训练目标，以实现令牌、单元格和表格级别的表示。

Result: TUTA在包括单元格类型分类和表格类型分类在内的表格结构理解任务上取得了最先进的成果，并在五个广泛的数据集上验证了其有效性。

Conclusion: TUTA在细胞类型分类和表格类型分类这两个表格结构理解的关键任务上取得了最先进的成果，在五个广泛研究的数据集上均表现出高效率。

Abstract: Tables are widely used with various structures to organize and present data.
Recent attempts on table understanding mainly focus on relational tables, yet
overlook to other common table structures. In this paper, we propose TUTA, a
unified pre-training architecture for understanding generally structured
tables. Noticing that understanding a table requires spatial, hierarchical, and
semantic information, we enhance transformers with three novel structure-aware
mechanisms. First, we devise a unified tree-based structure, called a
bi-dimensional coordinate tree, to describe both the spatial and hierarchical
information of generally structured tables. Upon this, we propose tree-based
attention and position embedding to better capture the spatial and hierarchical
information. Moreover, we devise three progressive pre-training objectives to
enable representations at the token, cell, and table levels. We pre-train TUTA
on a wide range of unlabeled web and spreadsheet tables and fine-tune it on two
critical tasks in the field of table structure understanding: cell type
classification and table type classification. Experiments show that TUTA is
highly effective, achieving state-of-the-art on five widely-studied datasets.

</details>


### [159] [TableSense: Spreadsheet Table Detection with Convolutional Neural Networks](https://arxiv.org/abs/2106.13500)
*Haoyu Dong,Shijie Liu,Shi Han,Zhouyu Fu,Dongmei Zhang*

Main category: cs.IR

TL;DR: TableSense, a novel framework using CNNs, enhances spreadsheet table detection by addressing structural diversity through improved cell featurization, a specialized CNN model, and active learning for dataset creation. It significantly outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: The task of automatic table detection in spreadsheets is challenged by the diversity of table structures and layouts. This paper aims to address this challenge by leveraging the success of Convolutional Neural Networks (CNNs) in computer vision.

Method: TableSense is an end-to-end framework for spreadsheet table detection that utilizes an effective cell featurization scheme, an enhanced convolutional neural network model for precise table boundary detection, and an uncertainty metric to guide an active learning based smart sampling algorithm for efficient dataset building.

Result: The proposed active learning strategy enabled the efficient build-up of a training dataset with 22,176 tables on 10,220 sheets, covering diverse table structures and layouts. The evaluation shows TableSense achieves 91.3% recall and 86.5% precision in EoB-2 metric.

Conclusion: TableSense is highly effective with 91.3% recall and 86.5% precision in EoB-2 metric, significantly improving over current detection algorithms and state-of-the-art convolutional neural networks in computer vision.

Abstract: Spreadsheet table detection is the task of detecting all tables on a given
sheet and locating their respective ranges. Automatic table detection is a key
enabling technique and an initial step in spreadsheet data intelligence.
However, the detection task is challenged by the diversity of table structures
and table layouts on the spreadsheet. Considering the analogy between a cell
matrix as spreadsheet and a pixel matrix as image, and encouraged by the
successful application of Convolutional Neural Networks (CNN) in computer
vision, we have developed TableSense, a novel end-to-end framework for
spreadsheet table detection. First, we devise an effective cell featurization
scheme to better leverage the rich information in each cell; second, we develop
an enhanced convolutional neural network model for table detection to meet the
domain-specific requirement on precise table boundary detection; third, we
propose an effective uncertainty metric to guide an active learning based smart
sampling algorithm, which enables the efficient build-up of a training dataset
with 22,176 tables on 10,220 sheets with broad coverage of diverse table
structures and layouts. Our evaluation shows that TableSense is highly
effective with 91.3\% recall and 86.5\% precision in EoB-2 metric, a
significant improvement over both the current detection algorithm that are used
in commodity spreadsheet tools and state-of-the-art convolutional neural
networks in computer vision.

</details>


### [160] [Recommending Related Tables](https://arxiv.org/abs/1907.03595)
*Shuo Zhang,Krisztian Balog*

Main category: cs.IR

TL;DR: 我们提出了一种新的表格匹配方法，通过将表格元素映射到多个语义空间并利用判别学习模型来组合相似性。该方法在维基百科表格测试集上取得了最先进的结果，并可用于向电子表格用户推荐相关表格。


<details>
  <summary>Details</summary>
Motivation: 在电子表格程序中，表格是一种非常强大、可视化和交互式的数据结构和操作工具，这使得电子表格程序成为最受欢迎的计算机应用程序之一。本次我们将介绍并处理一个任务，即推荐相关的表格：为用户提供一个输入的表格，并识别和返回一个相关的表格排序列表。这个任务的一个潜在应用场景是，在用户使用电子表格程序时，主动为用户推荐网上相关的结构化内容。

Method: 提出了一种理论上合理的表格匹配框架，该框架将表格元素映射到多个语义空间，并使用判别学习模型组合元素级相似性。

Result: 使用专门为此任务构建的维基百科表格测试集，我们证明了所提出的方法能够取得最先进的性能。

Conclusion: 与表格匹配相关的任务可以被精确地完成，并取得了最先进的性能。

Abstract: Tables are an extremely powerful visual and interactive tool for structuring
and manipulating data, making spreadsheet programs one of the most popular
computer applications. In this paper we introduce and address the task of
recommending related tables: given an input table, identifying and returning a
ranked list of relevant tables. One of the many possible application scenarios
for this task is to provide users of a spreadsheet program proactively with
recommendations for related structured content on the Web. At its core, the
related table recommendation task boils down to computing the similarity
between a pair of tables. We develop a theoretically sound framework for
performing table matching. Our approach hinges on the idea of representing
table elements in multiple semantic spaces, and then combining element-level
similarities using a discriminative learning model. Using a purpose-built test
collection from Wikipedia tables, we demonstrate that the proposed approach
delivers state-of-the-art performance.

</details>


### [161] [SmartTable: A Spreadsheet Program with Intelligent Assistance](https://arxiv.org/abs/1805.06353)
*Shuo Zhang,Vugar Abdul Zada,Krisztian Balog*

Main category: cs.IR

TL;DR: SmartTable is an open-source online spreadsheet application that assists users in populating tables with more data (rows) and adding new categories of data (columns).


<details>
  <summary>Details</summary>
Motivation: The motivation is to introduce SmartTable, an online spreadsheet application equipped with intelligent assistance capabilities for relational tables, aiming to help users populate tables with entities and extend them with attributes.

Method: The paper details the implementation of SmartTable, an online spreadsheet application with intelligent assistance capabilities focused on relational tables. Assistance is provided for populating tables with entities (rows) and extending them with attributes (columns).

Result: SmartTable is an online spreadsheet application with intelligent assistance for populating tables with entities and extending them with attributes. The implementation is open-source and available at http://smarttable.cc.

Conclusion: The paper introduces SmartTable, an online spreadsheet application with intelligent assistance for populating tables with entities and extending them with attributes. The implementation is open-source.

Abstract: We introduce SmartTable, an online spreadsheet application that is equipped
with intelligent assistance capabilities. With a focus on relational tables,
describing entities along with their attributes, we offer assistance in two
flavors: (i) for populating the table with additional entities (rows) and (ii)
for extending it with additional entity attributes (columns). We provide
details of our implementation, which is also released as open source. The
application is available at http://smarttable.cc.

</details>


### [162] [EntiTables: Smart Assistance for Entity-Focused Tables](https://arxiv.org/abs/1708.08721)
*Shuo Zhang,Krisztian Balog*

Main category: cs.IR

TL;DR: 本研究旨在增强电子表格程序的功能，通过开发用于填充表格行和列的生成概率模型。


<details>
  <summary>Details</summary>
Motivation: 我们的动机是为电子表格程序配备智能辅助功能。

Method: 我们开发了两种任务的生成概率模型，并考虑了知识库和大型表格语料库来估计这些模型的组件。

Result: 我们的实验评估模拟了用户在实际表格中输入内容的各种阶段，结果表明所提出的模型是互补的，并且我们的方法优于现有方法。

Conclusion: 我们的方法优于文献中现有的方法。

Abstract: Tables are among the most powerful and practical tools for organizing and
working with data. Our motivation is to equip spreadsheet programs with smart
assistance capabilities. We concentrate on one particular family of tables,
namely, tables with an entity focus. We introduce and focus on two specific
tasks: populating rows with additional instances (entities) and populating
columns with new headings. We develop generative probabilistic models for both
tasks. For estimating the components of these models, we consider a knowledge
base as well as a large table corpus. Our experimental evaluation simulates the
various stages of the user entering content into an actual table. A detailed
analysis of the results shows that the models' components are complimentary and
that our methods outperform existing approaches from the literature.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [163] [An Email Attachment is Worth a Thousand Words, or Is It?](https://arxiv.org/abs/1709.00362)
*Gregory Tsipenyuk,Jon Crowcroft*

Main category: cs.SI

TL;DR: 本研究提出了一种新的社交网络分析方法，通过分析电子邮件中的共享附件来构建网络，而非传统的通信记录。研究发现，附件分析更能反映关系的亲密度，并能提供比通信分析更深入的社交结构洞察。对Enron邮件数据的分析结果验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统邮件挖掘方法忽略了附件，而附件占用了大部分存储空间（约80-90%），并且可以作为关系强度的“亲密度”指标。因此，本研究旨在探索基于附件的网络分析能否提供更深入的社交结构洞察。

Method: 提出了一种新的基于附件的网络提取方法。与传统的从电子邮件头或正文提取信息不同，该方法将用户之间共享的附件作为网络中的边。然后，使用度、介数、接近度和特征向量中心性等网络分析技术来分析通信网络和共享附件网络。最后，使用最近邻算法对Enron电子邮件语料库中的员工进行相似性分组，以验证该方法。

Result: 基于附件的网络分析结果与基于通信的网络分析结果存在显著差异。附件网络分析提供的见解与Enron的组织结构图高度一致，验证了该方法的有效性。附件作为关系亲密度的指标，能够捕捉到通信网络中未体现的社交信息。

Conclusion: 通过使用共享附件作为节点之间的边，可以提供对电子邮件档案社交结构的更深入的了解。将附件共享视为关系的“亲密度”表现，可以捕捉到通信网络中未体现的细微差别。对Enron电子邮件语料库的分析和基于最近邻算法的相似性分组证实了该方法的有效性，并与组织的结构相吻合。

Abstract: There is an extensive body of research on Social Network Analysis (SNA) based
on the email archive. The network used in the analysis is generally extracted
either by capturing the email communication in From, To, Cc and Bcc email
header fields or by the entities contained in the email message. In the latter
case, the entities could be, for instance, the bag of words, url's, names,
phones, etc. It could also include the textual content of attachments, for
instance Microsoft Word documents, excel spreadsheets, or Adobe pdfs. The nodes
in this network represent users and entities. The edges represent communication
between users and relations to the entities. We suggest taking a different
approach to the network extraction and use attachments shared between users as
the edges. The motivation for this is two-fold. First, attachments represent
the "intimacy" manifestation of the relation's strength. Second, the
statistical analysis of private email archives that we collected and Enron
email corpus shows that the attachments contribute in average around 80-90% to
the archive's disk-space usage, which means that most of the data is presently
ignored in the SNA of email archives. Consequently, we hypothesize that this
approach might provide more insight into the social structure of the email
archive. We extract the communication and shared attachments networks from
Enron email corpus. We further analyze degree, betweenness, closeness, and
eigenvector centrality measures in both networks and review the differences and
what can be learned from them. We use nearest neighbor algorithm to generate
similarity groups for five Enron employees. The groups are consistent with
Enron's organizational chart, which validates our approach.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [164] [Mathematics of Digital Hyperspace](https://arxiv.org/abs/2103.15203)
*Jeremy Kepner,Timothy Davis,Vijay Gadepally,Hayden Jananthan,Lauren Milechin*

Main category: cs.MS

TL;DR: 本文介绍了一种名为“半链”的新数学概念，该概念利用半环来处理海量非结构化数据，并提出 GraphBLAS 可作为支持这些运算的更强大的关联数组代数。


<details>
  <summary>Details</summary>
Motivation: 数字高维度空间（包括社交媒体、电子商务、流媒体视频、电子邮件、云文档、网页、流量和网络数据包）中的海量非结构化数据可以通过超图、超稀疏矩阵和关联数组代数的数学方法进行优雅地表示、遍历和转换。

Method: 本文探索了一种新的数学概念——半链，它结合了成对的半环，为图分析、数据库操作和机器学习提供基本运算。

Result: 半链结合了成对的半环，为图分析、数据库操作和机器学习提供了基本运算。通过加入基于键的索引（例如指向字符串的指针）和半链，GraphBLAS 可以成为更丰富的关联数组代数，并可作为电子表格、数据库表和以数据为中心的操作系统的即插即用替代品，从而增强对数字超空间中非结构化数据的导航。GraphBLAS 标准目前支持超图、超稀疏矩阵、半链所需的数学以及无缝执行图、网络和矩阵操作。  

Conclusion: GraphBLAS 通过支持超图、超稀疏矩阵、半链所需的数学以及无缝执行图、网络和矩阵操作，可以成为更丰富的关联数组代数，并可作为电子表格、数据库表和以数据为中心的操作系统的即插即用替代品，从而增强对数字超空间中非结构化数据的导航。

Abstract: Social media, e-commerce, streaming video, e-mail, cloud documents, web
pages, traffic flows, and network packets fill vast digital lakes, rivers, and
oceans that we each navigate daily. This digital hyperspace is an amorphous
flow of data supported by continuous streams that stretch standard concepts of
type and dimension. The unstructured data of digital hyperspace can be
elegantly represented, traversed, and transformed via the mathematics of
hypergraphs, hypersparse matrices, and associative array algebra. This paper
explores a novel mathematical concept, the semilink, that combines pairs of
semirings to provide the essential operations for graph analytics, database
operations, and machine learning. The GraphBLAS standard currently supports
hypergraphs, hypersparse matrices, the mathematics required for semilinks, and
seamlessly performs graph, network, and matrix operations. With the addition of
key based indices (such as pointers to strings) and semilinks, GraphBLAS can
become a richer associative array algebra and be a plug-in replacement for
spreadsheets, database tables, and data centric operating systems, enhancing
the navigation of unstructured data found in digital hyperspace.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [165] [Excel: Automated Ledger or Analytics IDE?](https://arxiv.org/abs/2409.12976)
*Andrew Kumiega*

Main category: cs.CY

TL;DR: Excel现在是一个分析IDE，需要新的风险管理方法。


<details>
  <summary>Details</summary>
Motivation: 随着Excel从简单的账簿自动化工具转变为功能齐全的分析IDE，认识到管理其作为开发环境所带来的风险的必要性。

Method: 分析了Excel作为分析IDE的功能，并评估了当前风险框架的充分性。

Result: Excel的功能已大大扩展，包括数据库、OLAP引擎、统计编程语言、第三方库、动态图表和实时数据连接器，这些都可通过低代码框架进行访问。

Conclusion: Excel已从桌面应用程序演变为分析的集成开发环境（IDE），因此需要扩展现有的电子表格风险框架来管理由此产生的日益增长的风险。

Abstract: Since the inception of VisiCalc over four decades ago, spreadsheets have
undergone a gradual transformation, evolving from simple ledger automation
tools to the current state of Excel, which can be described as an Integrated
Development Environment (IDE) for analytics. The slow evolution of Excel from
an automation tool for ledgers to an IDE for analytics explains why many people
have not noticed that Excel includes a fully functional database, an OLAP
Engine, multiple statistical programming languages, multiple third-party
software libraries, dynamic charts, and real time data connectors. The
simplicity of accessing these multiple tools is a low-code framework controlled
from the Excel tool that is effectively an IDE. Once we acknowledge Excel's
shift from a desk top application to an IDE for analytics, the importance of
establishing a comprehensive risk framework for managing this distinctive
development environment becomes clear. In this paper we will explain how the
current risk framework for spreadsheets needs to be expanded to manage the
growing risks of using Excel as an IDE for analytics.

</details>


### [166] [Improving Feedback from Automated Reviews of Student Spreadsheets](https://arxiv.org/abs/2311.10728)
*Sören Aguirre Reid,Frank Kammer,Jonas-Ian Kuche,Pia-Doreen Ritzke,Markus Siepermann,Max Stephan,Armin Wagenknecht*

Main category: cs.CY

TL;DR: An Intelligent Tutoring System (ITS) was developed to automatically assess student Excel assignments, providing individualized feedback that improves submission accuracy and is well-received by students.


<details>
  <summary>Details</summary>
Motivation: The scarcity of digital solutions for assessing spreadsheet assignments in educational contexts, despite the widespread use of spreadsheets like Excel in curricula.

Method: Developed an Intelligent Tutoring System (ITS) that automatically analyzes student Excel submissions through value matching, detailed formula analysis, and quality assessment, incorporating feedback levels to cater to different learning stages.

Result: The ITS successfully analyzes student submissions in multiple ways and provides feedback levels. Higher feedback levels resulted in a greater percentage of correct submissions and were found to be understandable and helpful by students.

Conclusion: The developed ITS effectively analyzes student Excel submissions and provides automated, individualized feedback, with higher feedback levels leading to more correct submissions and positive student perception.

Abstract: Spreadsheets are one of the most widely used tools for end users. As a
result, spreadsheets such as Excel are now included in many curricula. However,
digital solutions for assessing spreadsheet assignments are still scarce in the
teaching context. Therefore, we have developed an Intelligent Tutoring System
(ITS) to review students' Excel submissions and provide individualized feedback
automatically. Although the lecturer only needs to provide one reference
solution, the students' submissions are analyzed automatically in several ways:
value matching, detailed analysis of the formulas, and quality assessment of
the solution. To take the students' learning level into account, we have
developed feedback levels for an ITS that provide gradually more information
about the error by using one of the different analyses. Feedback at a higher
level has been shown to lead to a higher percentage of correct submissions and
was also perceived as well understandable and helpful by the students.

</details>


### [167] [How Beaufort, Neumann and Gates met? Subject integration with spreadsheeting](https://arxiv.org/abs/2309.12353)
*Maria Csernoch,Julia Csernoch*

Main category: cs.CY

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: 'NoneType' object has no attribute 'model_dump'

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Computational thinking should be the fourth fundamental skill, along with
reading, writing, and arithmetic (3R). To reach the level where computational
thinking skills, especially digital problem solving have their own schemata,
there is a long way to go. In the present paper, a novel approach is detailed
to support subject integration and building digital schemata, on the well-known
Beaufort scale. The conversion of a traditional, paper-based problem and a data
retrieval process are presented within the frame of a Grade 8 action research
study. It is found that both students content knowledge and their digital
skills developed more efficiently than in traditional course book and
decontextualized digital environments. Furthermore, the method presented here
can be adapted to any paper-based problems whose solutions would be more
effective in a digital environment and which offer various forms for building
schemata both in the subject matter and informatics.

</details>


### [168] [A Simpler Method for Understanding Emergency Shelter Access Patterns](https://arxiv.org/abs/2210.13619)
*Geoffrey G. Messier*

Main category: cs.CY

TL;DR: "SAM is a new metric for analyzing shelter access patterns, providing an intuitive way for non-technical staff to understand client vulnerability. It yields similar results to traditional cluster analysis but requires less data and can provide real-time insights into external factors affecting access. SAM's output can be directly used as a measure of vulnerability, moving beyond traditional client labels."


<details>
  <summary>Details</summary>
Motivation: "The goal of SAM is to provide shelter operators with an intuitive way to understand access patterns that can be implemented by non-technical staff using spreadsheet operations."

Method: "The Simplified Access Metric (SAM) is a new approach for characterizing emergency shelter access patterns as a measure of shelter client vulnerability. Client data from a large North American shelter will be used to demonstrate that SAM produces similar results to traditional transitional, episodic and chronic client cluster analysis."

Result: "Timelines generated from nine years of shelter client data using SAM demonstrate the impact of Housing First programming and the COVID-19 lockdown on how people access shelter. Since SAM requires less data than cluster analysis, it is also able to generate a real time picture of how shelter access patterns are affected by external factors."

Conclusion: "SAM allows shelter staff to move beyond assigning transitional, episodic and chronic labels and instead use the "soft" output of SAM directly as a measure of vulnerability. It also generates a real time picture of how shelter access patterns are affected by external factors."

Abstract: The Simplified Access Metric (SAM) is a new approach for characterizing
emergency shelter access patterns as a measure of shelter client vulnerability.
The goal of SAM is to provide shelter operators with an intuitive way to
understand access patterns that can be implemented by non-technical staff using
spreadsheet operations. Client data from a large North American shelter will be
used to demonstrate that SAM produces similar results to traditional
transitional, episodic and chronic client cluster analysis. Since SAM requires
less data than cluster analysis, it is also able to generate a real time
picture of how shelter access patterns are affected by external factors.
Timelines generated from nine years of shelter client data using SAM
demonstrate the impact of Housing First programming and the COVID-19 lockdown
on how people access shelter. Finally, SAM allows shelter staff to move beyond
assigning transitional, episodic and chronic labels and instead use the "soft"
output of SAM directly as a measure of vulnerability.

</details>


### [169] [Long-Term Mentoring for Computer Science Researchers](https://arxiv.org/abs/2208.04738)
*Emily Ruppel,Sihang Liu,Elba Garza,Sukyoung Ryu,Alexandra Silva,Talia Ringer*

Main category: cs.CY

TL;DR: Two long-term mentoring programs, SIGPLAN-M and CALM, were established in the PL and CA research communities, respectively, to address the difficulty of forming new connections. Both programs have shown positive impacts and received good feedback, with the goal of inspiring similar efforts across computer science.


<details>
  <summary>Details</summary>
Motivation: The problem identified was that forming new, lasting connections within the programming languages (PL) and computer architecture (CA) research communities was difficult without pre-existing connections, necessitating long-term mentoring programs beyond existing short-term options.

Method: Established SIGPLAN-M, an official cross-institutional long-term mentoring program, and CALM, a pilot long-term mentoring program within computer architecture. SIGPLAN-M has 328 mentees and 234 mentors across 41 countries, with mentees reporting life-changing and career-saving impacts. CALM, in its pilot phase, has 13 mentors and 21 mentees across 7 countries and has received positive feedback.

Result: SIGPLAN-M has reached 328 mentees and 234 mentors across 41 countries, with mentees describing it as "life changing" and "a career saver." CALM, in its pilot phase, has 13 mentors and 21 mentees across 7 countries and has received very positive feedback.

Conclusion: We hope to inspire a broader long-term mentoring initiative across computer science by sharing the designs, impacts, and challenges of SIGPLAN-M and CALM.

Abstract: Early in the pandemic, we -- leaders in the research areas of programming
languages (PL) and computer architecture (CA) -- realized that we had a
problem: the only way to form new lasting connections in the community was to
already have lasting connections in the community. Both of our academic
communities had wonderful short-term mentoring programs to address this
problem, but it was clear that we needed long-term mentoring programs.
  Those of us in CA approached this scientifically, making an evidence-backed
case for community-wide long-term mentoring. In the meantime, one of us in PL
had impulsively launched an unofficial long-term mentoring program, founded on
chaos and spreadsheets. In January 2021, the latter grew to an official
cross-institutional long-term mentoring program called SIGPLAN-M; in January
2022, the former grew to Computer Architecture Long-term Mentoring (CALM).
  The impacts have been strong: SIGPLAN-M reaches 328 mentees and 234 mentors
across 41 countries, and mentees have described it as "life changing" and "a
career saver." And while CALM is in its pilot phase -- with 13 mentors and 21
mentees across 7 countries -- it has received very positive feedback. The
leaders of SIGPLAN-M and CALM shared our designs, impacts, and challenges along
the way. Now, we wish to share those with you. We hope this will kick-start a
larger long-term mentoring effort across all of computer science.

</details>


### [170] [Optimization Helps Scheduling Nursing Staff at the Long-Term Care Homes of the City of Toronto](https://arxiv.org/abs/2102.09461)
*Manion Anderson,Merve Bodur,Scott Rathwell,Vahid Sarhangian*

Main category: cs.CY

TL;DR: 多伦多市长期护理服务部使用了一个基于分层优化模型的电子表格工具来自动生成护理人员排班表，该工具考虑了员工的偏好和资历要求，并成功地减少了排班所需的时间。


<details>
  <summary>Details</summary>
Motivation: 为了解决多伦多市长期护理服务部在安排护理人员和高缺勤率方面遇到的挑战。

Method: 提出一个基于电子表格的调度工具，核心是一个分层优化模型，该模型在满足优先日期需求的同时，最大化总偏好得分。

Result: 该工具在多伦多一家 391 张床位的养老院实施后，将生成可行排班的时间从数小时缩短到几分之一小时，并且平均有 94% 的排班任务被排为最优先的。

Conclusion: 该工具成功地将排班时间从数小时缩短至几分之一小时，并满足了 94% 的优先排班。

Abstract: The City of Toronto Long Term Care Homes & Services (LTCH&S) division is one
of the largest providers of long-term care in the Canadian province of Ontario,
providing care to 2,640 residents at 10 homes across Toronto. Our collaboration
with LTCH&S was initiated to facilitate the increasingly challenging task of
scheduling nursing staff and reduce high absenteeism rate observed among the
part-time nurses. We developed a spreadsheet-based scheduling tool to automate
the generation of schedules and incorporate nurses' preferences for different
shifts into the schedules. At the core of the scheduling tool is a hierarchical
optimization model that generates a feasible schedule with the highest total
preference score while satisfying the maximum possible demand. Feasible
schedules had to abide by a set of complex seniority requirements which
prioritized more senior nurses when allocating the available shifts. Our
scheduling tool was implemented in a 391-bed home in Toronto. The tool allowed
nursing managers to generate feasible schedules within a fraction of an hour,
in contrast to the status-quo manual approach which could took up to tens of
hours. In addition, the schedules successfully accounted for preferences with
on average above 94% of the allocated shifts ranked as most preferred.

</details>


### [171] [Developing Excel Thought Leadership](https://arxiv.org/abs/2006.04793)
*David Lyford-Smith*

Main category: cs.CY

TL;DR: ICAEW developed three papers on spreadsheet best practices over five years, offering valuable lessons and solidifying their expertise in the area.


<details>
  <summary>Details</summary>
Motivation: To examine the evolution of ICAEW's thought leadership in spreadsheet best practices over five years.

Method: Reviewing the history and key lessons of three ICAEW thought leadership papers on spreadsheet best practices.

Result: The development of three ICAEW thought leadership papers has provided key lessons and strengthened ICAEW's standing in the field of spreadsheet best practices.

Conclusion: ICAEW has established a strong position in the field of spreadsheet best practices through the development of its thought leadership papers.

Abstract: Over a period of five years, the Institute of Chartered Accountants in
England and Wales (ICAEW) has developed a suite of three 'thought leadership'
papers surrounding good practice in spreadsheet use and spreadsheet work
environments. We will review the history of these three papers, the key lessons
which each has to teach, and discuss how the process of making them has helped
ICAEW to develop its position in the field.

</details>


### [172] [A Case Study of Spreadsheet Use within the Finance and Academic Registry units within a Higher Education Institution](https://arxiv.org/abs/1909.07462)
*Simon Thorne,Jamie Hancock*

Main category: cs.CY

TL;DR: 本研究调查了英国一所高等教育机构中电子表格的使用情况，发现需要制定明确的指南以确保数据完整性和效率。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨高等教育机构中电子表格的使用情况及其对数据准确性、完整性和效率的影响。

Method: 本研究采用案例研究的方法，考察了英国一所高等教育机构的电子表格使用情况，重点关注了学术注册和财务两个部门。研究内容涵盖了电子表格的使用重要性、培训、经验、目的、部署的技术、创建的电子表格的大小以及电子表格的共享。

Result: 研究结果显示，该机构创建和使用的电子表格数量庞大，电子表格开发者的特征与其他研究中的情况类似。

Conclusion: 该研究结果表明，需要为该组织制定明确的电子表格模型开发原则和指南，以确保数据完整性，减少重复工作，并优化电子表格的使用以满足机构的目标。

Abstract: This paper presents the findings of a case study of spreadsheet use in a
higher education institution in the UK. The paper considers the use of
spreadsheets in two units of the organisation, academic registry and finance.
Spreadsheet use is explored in terms of importance, training, experience,
purpose, techniques deployed, size of spreadsheets created and sharing of
spreadsheets. The implications of the results are then considered in terms of
accurate reporting to external funding bodies such the funding councils,
internal data integrity and internal data efficiencies. The results show a
large volume of spreadsheets being created and used, that the profile of
spreadsheet developers is typical of other studies of spreadsheet use and the
need for the organisation to have clear principles and guidelines for the
development of spreadsheet models in the organisation to ensure data integrity,
reduce duplication of effort and to optimise the use of spreadsheets to meet
the institutions goals.

</details>


### [173] [Computer Simulation of Neural Networks Using Spreadsheets: The Dawn of the Age of Camelot](https://arxiv.org/abs/1807.00018)
*Serhiy O. Semerikov,Illia O. Teplytskyi,Yuliia V. Yechkalo,Arnold E. Kiv*

Main category: cs.CY

TL;DR: 论证了在电子表格环境中开发神经网络计算机模拟训练方法的必要性，对现有方法进行了回顾，并确定了有潜力的模型以获得神经模拟能力。


<details>
  <summary>Details</summary>
Motivation: 论证了在电子表格环境中开发神经网络计算机模拟训练方法的必要性。

Method: 对电子表格模拟神经网络应用的系统性回顾，区分了解决网络计算机模拟训练问题的基本方法，包括电子表格和神经网络模拟工具的联合应用、第三方加载项的应用、使用嵌入式语言开发宏、使用标准电子表格加载项进行非线性优化、以及在没有加载项和宏的情况下在电子表格环境中创建神经网络。此外，还分析了 1890-1950 年的文献，以确定“数学生物物理公报”及其创始人 Nicolas Rashevsky 和围绕该期刊的科学界在创建和开发计算神经科学模型和方法中的作用。

Result: 确定了神经网络模拟能力可以通过掌握基于历史和遗传方法论的模型来获得，并指出了三组有潜力的模型：Rashevsky 的连续双因子模型、McCulloch 和 Pitts 的离散模型以及 Householder 和 Landahl 的离散-连续模型。

Conclusion: 该研究确定了在电子表格环境中获得神经模拟能力应掌握基于历史和遗传方法论的模型。它指出了三组有潜力的模型，可以发展相应的方法——Rashevsky 的连续双因子模型、McCulloch 和 Pitts 的离散模型以及 Householder 和 Landahl 的离散-连续模型。

Abstract: The article substantiates the necessity to develop training methods of
computer simulation of neural networks in the spreadsheet environment. The
systematic review of their application to simulating artificial neural networks
is performed. The authors distinguish basic approaches to solving the problem
of network computer simulation training in the spreadsheet environment, joint
application of spreadsheets and tools of neural network simulation, application
of third-party add-ins to spreadsheets, development of macros using the
embedded languages of spreadsheets; use of standard spreadsheet add-ins for
non-linear optimization, creation of neural networks in the spreadsheet
environment without add-ins and macros. After analyzing a collection of
writings of 1890-1950, the research determines the role of the scientific
journal "Bulletin of Mathematical Biophysics", its founder Nicolas Rashevsky
and the scientific community around the journal in creating and developing
models and methods of computational neuroscience. There are identified
psychophysical basics of creating neural networks, mathematical foundations of
neural computing and methods of neuroengineering (image recognition, in
particular). The role of Walter Pitts in combining the descriptive and
quantitative theories of training is discussed. It is shown that to acquire
neural simulation competences in the spreadsheet environment, one should master
the models based on the historical and genetic approach. It is indicated that
there are three groups of models, which are promising in terms of developing
corresponding methods - the continuous two-factor model of Rashevsky, the
discrete model of McCulloch and Pitts, and the discrete-continuous models of
Householder and Landahl.

</details>


### [174] [Edu-Edition Spreadsheet Competency Framework](https://arxiv.org/abs/1802.00496)
*Maria Csernoch,Piroska Biró*

Main category: cs.CY

TL;DR: A new framework (E2SCF) is proposed to teach spreadsheet skills early and effectively, focusing on real-world problem-solving and programming aspects.


<details>
  <summary>Details</summary>
Motivation: To emphasize that building spreadsheet competences should start early in education and be supported by expert teachers.

Method: Introduce the Edu-Edition of the Spreadsheet Competency Framework (E2SCF), based on the Spreadsheet Competency Framework for finance professionals. Highlight key features like mathability, computer-supported real-world problem-solving, two-directional knowledge transfer, data/error analysis, and the programming aspect of spreadsheets.

Result: The E2SCF framework, with its focus on practical application and programming, aims to equip users with firm spreadsheet knowledge and transferable problem-solving skills.

Conclusion: E2SCF is designed for basic and general users to build solid spreadsheet knowledge and develop transferable problem-solving skills.

Abstract: Based on the Spreadsheet Competency Framework for finance professionals, in
the present paper we introduce the Edu-Edition of the Spreadsheet Competency
Framework (E2SCF). We claim that building spreadsheet competences should start
in education, as early as possible, and this process is a lot more effective if
support arrives from expert teachers. The main feature of E2SCF is high
mathability computer-supported real world problem solving. This approach is
based on - from the very beginning of training - a two-directional knowledge
transfer, data and error analysis and handling, and the programming aspect of
spreadsheets. Based on these features, E2SCF is set up for basic and general
users to build up firm spreadsheet knowledge and to develop transferable
problem solving skills and competences.

</details>


### [175] [The Future of Spreadsheets in the Big Data Era](https://arxiv.org/abs/1801.10231)
*David Birch,David Lyford-Smith,Yike Guo*

Main category: cs.CY

TL;DR: Spreadsheets, despite being unchanged for 30 years, are evolving due to Big Data, end-user computing, and mobile computing. A workshop identified key trends, research directions, and user implications for the future of spreadsheets.


<details>
  <summary>Details</summary>
Motivation: To explore the future of spreadsheet technology and its consequences for users, given its enduring ubiquity and fundamental lack of change over the past three decades despite emerging technological drivers.

Method: A workshop was convened with academia and industry participants to discuss the success factors, trends, and future directions of spreadsheet technology.

Result: The paper records participants' views on the reasons for spreadsheet success, trends driving change, likely future directions, and sets out key research directions and implications for end-users.

Conclusion: The paper discusses the future evolution of spreadsheet technology, driven by Big Data, end-user computing, and mobile computing, and outlines key research directions and implications for end-users.

Abstract: The humble spreadsheet is the most widely used data storage, manipulation and
modelling tool. Its ubiquity over the past 30 years has seen its successful
application in every area of life. Surprisingly the spreadsheet has remained
fundamentally unchanged over the past three decades. As spreadsheet technology
enters its 4th decade a number of drivers of change are beginning to impact
upon the spreadsheet. The rise of Big Data, increased end-user computing and
mobile computing will undoubtedly increasingly shape the evolution and use of
spreadsheet technology.
  To explore the future of spreadsheet technology a workshop was convened with
the aim of "bringing together academia and industry to examine the future
direction of spreadsheet technology and the consequences for users". This paper
records the views of the participants on the reasons for the success of the
spreadsheet, the trends driving change and the likely directions of change for
the spreadsheet. We then set out key directions for further research in the
evolution and use of spreadsheets. Finally we look at the implications of these
trends for the end users who after all are the reason for the remarkable
success of the spreadsheet.

</details>


### [176] [The Role of Spreadsheets in Clinical Decision Support: A Survey of the Medical Algorithms Company User Community](https://arxiv.org/abs/1801.07782)
*Simon Thorne*

Main category: cs.CY

TL;DR: This paper surveys users of Clinical Decision Support Systems (CDSS) to understand their impact on clinical practice. Results show CDSS are used in diverse settings for operations, research, and reference, with logic often developed on spreadsheets before database implementation.


<details>
  <summary>Details</summary>
Motivation: To contribute to the wider understanding of how CDSS impact on clinical practice by analysing, discussing, and comparing survey results with other similar studies.

Method: A small scoping survey of Clinical Decision Support System (CDSS) users from the Medical Algorithms Company website.

Result: CDSS are being used by clinical professionals in various settings as both operational and research/reference tools. The process of developing CDSS logic on spreadsheets before database implementation was described.

Conclusion: CDSS provided by Medal are being used by clinical professionals in a variety of settings, both as an operational tool and as a research and reference tool. The initial logic is worked out on a spreadsheet, and then the tools are implemented and executed in a database.

Abstract: This paper presents and discusses the results of a small scoping survey of
Clinical Decision Support System (CDSS) users from the Medical Algorithms
Company website which hosts 24,000 different CDSS. These results are analysed,
discussed, and compared with other similar studies and contribute to the wider
understanding of how CDSS impact on clinical practice. The results show that
CDSS provided by Medal are being used by clinical professionals in a variety of
settings, both as an operational tool and as a research and reference tool.
Whilst these tools are implemented and executed in a database, the initial
logic is worked out on a spreadsheet. The paper describes that process and
examines some of the results of the survey.

</details>


<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [177] [Real-time stock analysis for blending recipes in industrial plants](https://arxiv.org/abs/1909.02960)
*Florin Zamfir,Nicolae Paraschiv,Emil Pricop*

Main category: cs.OH

TL;DR: 该研究开发了一个C#应用程序，用于实时库存分析和配料过程的优化，解决了Excel表格管理数据不便的问题，并能通过规划算法确定最优成品数量。


<details>
  <summary>Details</summary>
Motivation: 现有Excel电子表格在管理产品库存和过程数据时存在难以理解、跟踪困难以及易被意外修改公式等问题，因此需要一个集中的应用程序来辅助操作员进行决策。

Method: 开发了一个实时库存分析应用程序，该应用程序使用C#语言，集成了规划算法，能够处理来自生产线的实时数据。

Result: 该应用程序能够识别配料所需的成分、根据现有成分确定可生产的成品数量，并优化成品数量。

Conclusion: 该应用程序使用C#开发，并利用规划算法，能够逐步指导用户完成配料过程，最终确定最优成品数量。

Abstract: Many companies use Excel spreadsheets to keep stock records and to calculate
process-specific data. These spreadsheets are often hard to understand and
track. And if the user does not protect them, there is a risk that the user
randomly changes or erase formulas. The paper focuses on the stocks of products
used in a blending process with a known recipe. Developing an application that
can bring this data in a centralized form and that can assist the operator in
decide is a necessity. When a programmer implements an application that uses
data from plants he needs to consider one fundamental aspect as reading
real-time data from the process. The real-time stock analysis application takes
into account all the above elements. The application is easy to use by an
operator in the command room of installation because of the planning algorithms
integrated into it. The algorithms proposed and implemented in this paper have
well-defined goals: identifying the ingredients needed to achieve the blending
process for required quantities, determine the quantities of the finished
product that can be made with the existing ingredients and determine the
optimum quantities of the finished product. The application implemented in C#
intensively uses these algorithms and gives the user the ability to build the
result step by step.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [178] [Hillview: A trillion-cell spreadsheet for big data](https://arxiv.org/abs/1907.04827)
*Mihai Budiu,Parikshit Gopalan,Lalith Suresh,Udi Wieder,Han Kruiger,Marcos K. Aguilera*

Main category: cs.DC

TL;DR: Hillview是一个分布式电子表格系统，通过名为vizketches的紧凑可视化技术，实现了对超大数据集的交互式浏览，性能优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 为解决单机无法处理的大型数据集浏览问题，提供一种高交互性、允许数据分析师快速探索信息并随意切换可视化方式的分布式电子表格系统。

Method: Hillview利用vizketches（一种数据摘要算法和图形渲染技术相结合的紧凑数据可视化方法）来扩展电子表格功能，以实现对无法在单机上处理的大型数据集的快速交互式探索。

Result: 在八台服务器上运行的Hillview能够浏览和可视化包含数十亿行、数万亿单元格的数据集，其性能超出了同类系统的公开能力。

Conclusion: Hillview通过vizketches实现了对超大数据集的交互式浏览和可视化，在大规模数据处理方面超越了现有系统。

Abstract: Hillview is a distributed spreadsheet for browsing very large datasets that
cannot be handled by a single machine. As a spreadsheet, Hillview provides a
high degree of interactivity that permits data analysts to explore information
quickly along many dimensions while switching visualizations on a whim. To
provide the required responsiveness, Hillview introduces visualization
sketches, or vizketches, as a simple idea to produce compact data
visualizations. Vizketches combine algorithmic techniques for data
summarization with computer graphics principles for efficient rendering. While
simple, vizketches are effective at scaling the spreadsheet by parallelizing
computation, reducing communication, providing progressive visualizations, and
offering precise accuracy guarantees. Using Hillview running on eight servers,
we can navigate and visualize datasets of tens of billions of rows and
trillions of cells, much beyond the published capabilities of competing
systems.

</details>


### [179] [Streaming 1.9 Billion Hypersparse Network Updates per Second with D4M](https://arxiv.org/abs/1907.04217)
*Jeremy Kepner,Vijay Gadepally,Lauren Milechin,Siddharth Samsi,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Michael Houle,Michael Jones,Anne Klein,Peter Michaleas,Julie Mullen,Andrew Prout,Antonio Rosa,Charles Yee,Albert Reuther*

Main category: cs.DC

TL;DR: D4M 库通过分层关联数组优化了网络数据分析中的内存和更新性能。


<details>
  <summary>Details</summary>
Motivation: 解决 D4M 关联数组在处理流式更新时面临的内存压力和更新速率瓶颈问题。

Method: 提出并实现了分层关联数组，通过控制层级间的条目数量来优化内存使用和更新性能。

Result: 实现了比传统 D4M 关联数组更优化的内存管理和更新性能，单实例更新速率达 40,000 次/秒，大规模集群可达 1,900,000,000 次/秒。

Conclusion: D4M 库实现了分布式关联数组，适用于网络数据分析。通过引入分层关联数组，该实现有效降低了内存压力并显著提高了更新速率，单实例最高可达每秒 40,000 次更新，在 1,100 个服务器节点上可扩展至每秒 1,900,000,000 次更新，能够分析极大规模的流式网络数据集。

Abstract: The Dynamic Distributed Dimensional Data Model (D4M) library implements
associative arrays in a variety of languages (Python, Julia, and Matlab/Octave)
and provides a lightweight in-memory database implementation of hypersparse
arrays that are ideal for analyzing many types of network data. D4M relies on
associative arrays which combine properties of spreadsheets, databases,
matrices, graphs, and networks, while providing rigorous mathematical
guarantees, such as linearity. Streaming updates of D4M associative arrays put
enormous pressure on the memory hierarchy. This work describes the design and
performance optimization of an implementation of hierarchical associative
arrays that reduces memory pressure and dramatically increases the update rate
into an associative array. The parameters of hierarchical associative arrays
rely on controlling the number of entries in each level in the hierarchy before
an update is cascaded. The parameters are easily tunable to achieve optimal
performance for a variety of applications. Hierarchical arrays achieve over
40,000 updates per second in a single instance. Scaling to 34,000 instances of
hierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud
achieved a sustained update rate of 1,900,000,000 updates per second. This
capability allows the MIT SuperCloud to analyze extremely large streaming
network data sets.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [180] [Billion-files File Systems (BfFS): A Comparison](https://arxiv.org/abs/2408.01805)
*Sohail Shaikh*

Main category: cs.PF

TL;DR: 研究人员分析了EXT4、XFS、Btrfs、ZFS和F2FS这五种流行的Linux文件系统，以了解它们在处理海量数据时的性能和局限性。他们创建、存储和读取了10亿个文件，并评估了读/写吞吐量、存储使用情况和性能下降等指标。


<details>
  <summary>Details</summary>
Motivation: 随着数据量的爆炸式增长，需要快速处理数据，因此数据需要靠近计算设备以减少传输延迟，这引起了对本地文件系统工作原理、性能及其局限性的关注。

Method: 通过创建、存储和读取十亿个文件来分析EXT4, XFS, BtrFS, ZFS和F2FS这几个流行的Linux文件系统，并捕获和分析了读/写吞吐量、存储块使用情况、磁盘空间利用率和开销等指标。

Result: 该研究捕获和分析了读/写吞吐量、存储块使用情况、磁盘空间利用率和开销等指标，并探讨了在创建大量文件和文件夹期间及之后文件系统性能下降等副作用。

Conclusion: 该研究分析了EXT4, XFS, BtrFS, ZFS和F2FS这几个流行的Linux文件系统，通过创建、存储和读取十亿个文件来评估它们在处理海量数据时的性能。

Abstract: As the volume of data being produced is increasing at an exponential rate
that needs to be processed quickly, it is reasonable that the data needs to be
available very close to the compute devices to reduce transfer latency. Due to
this need, local filesystems are getting close attention to understand their
inner workings, performance, and more importantly their limitations. This study
analyzes few popular Linux filesystems: EXT4, XFS, BtrFS, ZFS, and F2FS by
creating, storing, and then reading back one billion files from the local
filesystem. The study also captured and analyzed read/write throughput, storage
blocks usage, disk space utilization and overheads, and other metrics useful
for system designers and integrators. Furthermore, the study explored other
side effects such as filesystem performance degradation during and after these
large numbers of files and folders are created.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [181] [Reducing Aleatoric and Epistemic Uncertainty through Multi-modal Data Acquisition](https://arxiv.org/abs/2501.18268)
*Arthur Hoarau,Benjamin Quost,Sébastien Destercke,Willem Waegeman*

Main category: cs.LG

TL;DR: 该研究提出了一个多模态数据采集框架，通过增加数据模态来减少随机不确定性，通过增加观测数量来减少情境不确定性。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统需要结合来自文本、图像、音频、电子表格和时间序列等多种模态的数据来生成准确可靠的预测。然而，多模态数据给解耦不确定性带来了新的机遇和挑战，因为现有的假设（例如，情境不确定性可以通过收集更多数据来减少，而随机不确定性是不可约减的）在多模态数据中受到挑战。

Method: 提出了一种创新的数据采集框架，该框架结合了主动学习、主动特征采集和不确定性量化等思想，并允许在样本数量和数据模态两个方向上进行采样，以解耦不确定性。

Result: 通过在两个多模态数据集上提供概念验证实现，展示了所提出的数据采集框架，证明了增加模态数量可以减少随机不确定性，而增加观测数量可以减少情境不确定性。

Conclusion: 该研究提出了一个创新的数据采集框架，该框架通过样本数量和数据模态两个方向的采样来解耦不确定性，并将其应用于多模态数据，以提高AI系统的预测准确性和可靠性。

Abstract: To generate accurate and reliable predictions, modern AI systems need to
combine data from multiple modalities, such as text, images, audio,
spreadsheets, and time series. Multi-modal data introduces new opportunities
and challenges for disentangling uncertainty: it is commonly assumed in the
machine learning community that epistemic uncertainty can be reduced by
collecting more data, while aleatoric uncertainty is irreducible. However, this
assumption is challenged in modern AI systems when information is obtained from
different modalities. This paper introduces an innovative data acquisition
framework where uncertainty disentanglement leads to actionable decisions,
allowing sampling in two directions: sample size and data modality. The main
hypothesis is that aleatoric uncertainty decreases as the number of modalities
increases, while epistemic uncertainty decreases by collecting more
observations. We provide proof-of-concept implementations on two multi-modal
datasets to showcase our data acquisition framework, which combines ideas from
active learning, active feature acquisition and uncertainty quantification.

</details>


### [182] [Large Scale Transfer Learning for Tabular Data via Language Modeling](https://arxiv.org/abs/2406.12031)
*Josh Gardner,Juan C. Perdomo,Ludwig Schmidt*

Main category: cs.LG

TL;DR: 本研究提出了 TabuLa-8B，一个用于表格预测的语言模型。该模型在零样本和少样本场景下均优于现有模型，解决了表格数据迁移学习的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在语言和视觉领域取得了显著进展，但在表格数据领域，迁移学习范式尚未产生类似的影响。本研究旨在缩小这一差距，为表格预测开发语言模型。

Method: 该研究提出了一种名为 TabuLa-8B 的表格预测语言模型。研究人员从 TabLib 语料库中提取了一个大型、高质量的训练数据集，并采用了表格数据过滤和质量控制方法。他们使用了一个包含超过 21 亿行、来自 400 万个独立表格的数据集，对 Llama 3-8B LLM 进行了微调，并引入了一种新颖的表格数据打包和注意力机制。

Result: 在 329 个数据集的测试中，TabuLa-8B 在零样本设置下比随机猜测高出 15 个百分点以上，这优于现有的表格预测模型。在少样本设置下（1-32 个样本），TabuLa-8B 的准确性比经过显式训练的 XGBoost 和 TabPFN 模型高出 5-15 个百分点。

Conclusion: TabuLa-8B 在表格预测任务上展现出优越的性能，即使在零样本和少样本设置下，其准确性也显著优于现有模型，如 XGBoost 和 TabPFN。

Abstract: Tabular data -- structured, heterogeneous, spreadsheet-style data with rows
and columns -- is widely used in practice across many domains. However, while
recent foundation models have reduced the need for developing task-specific
datasets and predictors in domains such as language modeling and computer
vision, this transfer learning paradigm has not had similar impact in the
tabular domain. In this work, we seek to narrow this gap and present TabuLa-8B,
a language model for tabular prediction. We define a process for extracting a
large, high-quality training dataset from the TabLib corpus, proposing methods
for tabular data filtering and quality control. Using the resulting dataset,
which comprises over 2.1B rows from over 4M unique tables, we fine-tune a Llama
3-8B large language model (LLM) for tabular data prediction (classification and
binned regression) using a novel packing and attention scheme for tabular
prediction. Through evaluation across a test suite of 329 datasets, we find
that TabuLa-8B has zero-shot accuracy on unseen tables that is over 15
percentage points (pp) higher than random guessing, a feat that is not possible
with existing state-of-the-art tabular prediction models (e.g. XGBoost,
TabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the
target datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN
models that are explicitly trained on equal, or even up to 16x more data. We
release our model, code, and data along with the publication of this paper.

</details>


### [183] [Generating tabular datasets under differential privacy](https://arxiv.org/abs/2308.14784)
*Gianluca Truda*

Main category: cs.LG

TL;DR: Sensitive tabular data is crucial for ML, but privacy concerns hinder its use. Existing methods like DP-GANs struggle with quality-privacy trade-offs and training instability. This paper introduces TableDiffusion, a novel differentially-private diffusion model for tabular data. TableDiffusion achieves better quality and privacy by using attention mechanisms and a smoother training process, outperforming DP-GANs and avoiding mode collapse.


<details>
  <summary>Details</summary>
Motivation: Machine learning (ML) relies on accessible and high-quality training data, especially in sensitive biomedical and financial domains. While synthetic data generation can unlock sensitive data, existing generative models tend to memorize and regurgitate training data, undermining privacy. Although Differential Privacy (DP) has been incorporated into deep neural network training, it creates a trade-off between data quality and privacy. Generative Adversarial Networks (GANs), the dominant paradigm for synthesizing tabular data under DP, suffer from unstable adversarial training and mode collapse, which are exacerbated by privacy constraints and the challenging tabular data modality.

Method: This work optimizes the quality-privacy trade-off of generative models, producing higher quality tabular datasets with the same privacy guarantees. We implement novel end-to-end models that leverage attention mechanisms to learn reversible tabular representations. We also introduce TableDiffusion, the first differentially-private diffusion model for tabular data synthesis.

Result: TableDiffusion produces higher-fidelity synthetic datasets, avoids the mode collapse problem, and achieves state-of-the-art performance on privatised tabular data synthesis. It also bypasses the challenges of reconstructing mixed-type tabular data by predicting the added noise.

Conclusion: ", "Overall, the diffusion paradigm proves vastly more data and privacy efficient than the adversarial paradigm, due to augmented re-use of each data batch and a smoother iterative training process. By implementing TableDiffusion to predict the added noise, we enabled it to bypass the challenges of reconstructing mixed-type tabular data. TableDiffusion produces higher-fidelity synthetic datasets, avoids the mode collapse problem, and achieves state-of-the-art performance on privatised tabular data synthesis."

Abstract: Machine Learning (ML) is accelerating progress across fields and industries,
but relies on accessible and high-quality training data. Some of the most
important datasets are found in biomedical and financial domains in the form of
spreadsheets and relational databases. But this tabular data is often sensitive
in nature. Synthetic data generation offers the potential to unlock sensitive
data, but generative models tend to memorise and regurgitate training data,
which undermines the privacy goal. To remedy this, researchers have
incorporated the mathematical framework of Differential Privacy (DP) into the
training process of deep neural networks. But this creates a trade-off between
the quality and privacy of the resulting data. Generative Adversarial Networks
(GANs) are the dominant paradigm for synthesising tabular data under DP, but
suffer from unstable adversarial training and mode collapse, which are
exacerbated by the privacy constraints and challenging tabular data modality.
This work optimises the quality-privacy trade-off of generative models,
producing higher quality tabular datasets with the same privacy guarantees. We
implement novel end-to-end models that leverage attention mechanisms to learn
reversible tabular representations. We also introduce TableDiffusion, the first
differentially-private diffusion model for tabular data synthesis. Our
experiments show that TableDiffusion produces higher-fidelity synthetic
datasets, avoids the mode collapse problem, and achieves state-of-the-art
performance on privatised tabular data synthesis. By implementing
TableDiffusion to predict the added noise, we enabled it to bypass the
challenges of reconstructing mixed-type tabular data. Overall, the diffusion
paradigm proves vastly more data and privacy efficient than the adversarial
paradigm, due to augmented re-use of each data batch and a smoother iterative
training process.

</details>


### [184] [Smart Metro: Deep Learning Approaches to Forecasting the MRT Line 3 Ridership](https://arxiv.org/abs/2304.07303)
*Jayrald Empino,Jean Allyson Junsay,Mary Grace Verzon,Mideth Abisado,Shekinah Lor Huyo-a,Gabriel Avelino Sampedro*

Main category: cs.LG

TL;DR: This study proposes a time series model to predict daily ridership for Metro Manila's MRT3 to aid commuters and transportation management.


<details>
  <summary>Details</summary>
Motivation: The daily ridership of MRT3 fluctuates due to various factors, making it challenging for commuters to plan and for the DOTr to manage. Current reliance on spreadsheets with historical data is inefficient.

Method: Time series prediction

Result: A time series prediction model for anticipating future daily traffic and attendance at specific MRT3 stations on specific days is proposed. This will help address the challenges faced by commuters and the DOTr.

Conclusion: The study presents a time series prediction model for forecasting daily MRT3 ridership, aiming to help commuters plan their itineraries and assist the DOTr in managing the system.

Abstract: Since its establishment in 1999, the Metro Rail Transit Line 3 (MRT3) has
served as a transportation option for numerous passengers in Metro Manila,
Philippines. The Philippine government's transportation department records more
than a thousand people using the MRT3 daily and forecasting the daily passenger
count may be rather challenging. The MRT3's daily ridership fluctuates owing to
variables such as holidays, working days, and other unexpected issues.
Commuters do not know how many other commuters are on their route on a given
day, which may hinder their ability to plan an efficient itinerary. Currently,
the DOTr depends on spreadsheets containing historical data, which might be
challenging to examine. This study presents a time series prediction of daily
traffic to anticipate future attendance at a particular station on specific
days.

</details>


### [185] [Adversarial Networks and Machine Learning for File Classification](https://arxiv.org/abs/2301.11964)
*Ken St. Germain,Josh Angichiodo*

Main category: cs.LG

TL;DR: 本研究提出了一种基于SGAN的文件分类器，能够精确识别被混淆的文件类型，准确率高达97.6%。


<details>
  <summary>Details</summary>
Motivation: 为了在文件扩展名或文件头被混淆的情况下，准确识别文件的真实类型，以协助取证调查。

Method: 提出了一种半监督生成对抗网络（SGAN）来识别文件类型。

Result: 所提出的SGAN在11种不同文件类型上的分类准确率达到了97.6%，并且在监督样本较少的情况下，其精确度优于传统的独立神经网络和其他三种机器学习算法。

Conclusion: 该研究提出了一种使用对抗性训练的机器学习神经网络来识别文件类型的方法，即使在文件扩展名或文件头被混淆的情况下也能有效识别。

Abstract: Correctly identifying the type of file under examination is a critical part
of a forensic investigation. The file type alone suggests the embedded content,
such as a picture, video, manuscript, spreadsheet, etc. In cases where a system
owner might desire to keep their files inaccessible or file type concealed, we
propose using an adversarially-trained machine learning neural network to
determine a file's true type even if the extension or file header is obfuscated
to complicate its discovery. Our semi-supervised generative adversarial network
(SGAN) achieved 97.6% accuracy in classifying files across 11 different types.
We also compared our network against a traditional standalone neural network
and three other machine learning algorithms. The adversarially-trained network
proved to be the most precise file classifier especially in scenarios with few
supervised samples available. Our implementation of a file classifier using an
SGAN is implemented on GitHub (https://ksaintg.github.io/SGAN-File-Classier).

</details>


### [186] [TabularNet: A Neural Network Architecture for Understanding Semantic Structures of Tabular Data](https://arxiv.org/abs/2106.03096)
*Lun Du,Fei Gao,Xu Chen,Ran Jia,Junshan Wang,Jiang Zhang,Shi Han,Dongmei Zhang*

Main category: cs.LG

TL;DR: A new neural network, TabularNet, is proposed to understand tabular data by extracting both spatial and relational information using CNN and GCN components, showing improved performance on classification tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing studies that focus only on spatial information of tabular structures and ignore diverse relational information (hierarchical, paratactic) between cells.

Method: TabularNet utilizes a spatial encoder with row/column-level Pooling and Bi-GRU, and a GCN-based encoder incorporating a novel graph construction method based on the WordNet tree to capture hierarchical and paratactic relationships between cells.

Result: Extensive experiments on three classification tasks with two real-world spreadsheet datasets show that TabularNet is effective and outperforms existing methods.

Conclusion: The proposed TabularNet, which effectively extracts both spatial and relational information, demonstrates superiority over state-of-the-art baselines on three classification tasks with two real-world spreadsheet datasets.

Abstract: Tabular data are ubiquitous for the widespread applications of tables and
hence have attracted the attention of researchers to extract underlying
information. One of the critical problems in mining tabular data is how to
understand their inherent semantic structures automatically. Existing studies
typically adopt Convolutional Neural Network (CNN) to model the spatial
information of tabular structures yet ignore more diverse relational
information between cells, such as the hierarchical and paratactic
relationships. To simultaneously extract spatial and relational information
from tables, we propose a novel neural network architecture, TabularNet. The
spatial encoder of TabularNet utilizes the row/column-level Pooling and the
Bidirectional Gated Recurrent Unit (Bi-GRU) to capture statistical information
and local positional correlation, respectively. For relational information, we
design a new graph construction method based on the WordNet tree and adopt a
Graph Convolutional Network (GCN) based encoder that focuses on the
hierarchical and paratactic relationships between cells. Our neural network
architecture can be a unified neural backbone for different understanding tasks
and utilized in a multitask scenario. We conduct extensive experiments on three
classification tasks with two real-world spreadsheet data sets, and the results
demonstrate the effectiveness of our proposed TabularNet over state-of-the-art
baselines.

</details>


### [187] [Visual Backpropagation](https://arxiv.org/abs/1906.04011)
*Roy S. Freedman*

Main category: cs.LG

TL;DR: Backpropagation implemented visually in spreadsheets using formulas, no macros or external code needed.


<details>
  <summary>Details</summary>
Motivation: To create a visual and transparent implementation of backpropagation within spreadsheets.

Method: Visual Backpropagation, which utilizes array worksheet formulas, manual calculation, and a sequential order of computation akin to systolic arrays.

Result: A visual and transparent implementation of backpropagation using spreadsheet formulas, compared with a Tensorflow solution on a regression problem.

Conclusion: Our method, Visual Backpropagation, offers a transparent and visual implementation of backpropagation using spreadsheet formulas, avoiding macros and external code.

Abstract: We show how a declarative functional programming specification of
backpropagation yields a visual and transparent implementation within
spreadsheets. We call our method Visual Backpropagation. This backpropagation
implementation exploits array worksheet formulas, manual calculation, and has a
sequential order of computation similar to the processing of a systolic array.
The implementation uses no hidden macros nor user-defined functions; there are
no loops, assignment statements, or links to any procedural programs written in
conventional languages. As an illustration, we compare a Visual Backpropagation
solution to a Tensorflow (Python) solution on a standard regression problem.

</details>


### [188] [MOLTE: a Modular Optimal Learning Testing Environment](https://arxiv.org/abs/1709.04553)
*Yingfei Wang,Warren Powell*

Main category: cs.LG

TL;DR: MOLTE is a new Matlab-based testing environment for learning algorithms (Bayesian ranking, bandits, sequential design) that makes it easy to add new algorithms/problems, supports parallel computing, and aims to promote more comprehensive empirical testing in the research community.


<details>
  <summary>Details</summary>
Motivation: To address the lack of empirical testing for learning algorithms by providing a flexible and easy-to-use testing environment.

Method: Introduction of MOLTE, a Matlab-based simulator allowing comparison of learning policies (.m modules) across various problems (.m modules). Features include a spreadsheet-based interface, graphical metrics, and compatibility with parallel computing.

Result: Demonstrated MOLTE's capabilities through policy comparisons on test problems and addressed the often-overlooked issues of tuning and constructing priors in optimal learning.

Conclusion: MOLTE is a new public-domain, modular, optimal learning testing environment designed to facilitate comprehensive empirical testing of learning algorithms for Bayesian ranking and selection, stochastic bandits, and sequential experimental design problems. It simplifies the addition of new algorithms and problems, supports parallel computing, and includes various graphical metrics and a spreadsheet-based interface. MOLTE aims to encourage more extensive research in optimal learning by providing an easy-to-use tool for the research community.

Abstract: We address the relative paucity of empirical testing of learning algorithms
(of any type) by introducing a new public-domain, Modular, Optimal Learning
Testing Environment (MOLTE) for Bayesian ranking and selection problem,
stochastic bandits or sequential experimental design problems. The Matlab-based
simulator allows the comparison of a number of learning policies (represented
as a series of .m modules) in the context of a wide range of problems (each
represented in its own .m module) which makes it easy to add new algorithms and
new test problems. State-of-the-art policies and various problem classes are
provided in the package. The choice of problems and policies is guided through
a spreadsheet-based interface. Different graphical metrics are included. MOLTE
is designed to be compatible with parallel computing to scale up from local
desktop to clusters and clouds. We offer MOLTE as an easy-to-use tool for the
research community that will make it possible to perform much more
comprehensive testing, spanning a broader selection of algorithms and test
problems. We demonstrate the capabilities of MOLTE through a series of
comparisons of policies on a starter library of test problems. We also address
the problem of tuning and constructing priors that have been largely overlooked
in optimal learning literature. We envision MOLTE as a modest spur to provide
researchers an easy environment to study interesting questions involved in
optimal learning.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [189] [Is spreadsheet syntax better than numeric indexing for cell selection?](https://arxiv.org/abs/2505.23296)
*Philip Heltweg,Dirk Riehle,Georg-Daniel Schwarz*

Main category: cs.PL

TL;DR: Spreadsheet-style syntax is better for cell selection in data engineering than numeric syntax.


<details>
  <summary>Details</summary>
Motivation: The paper aims to compare different approaches for cell selection in data engineering, specifically numeric indexing versus spreadsheet-style syntax, to determine which is more efficient and less error-prone for practitioners.

Method: A large-scale controlled experiment was conducted with student participants to compare numeric indexing and spreadsheet-style syntax for cell selection in terms of speed and correctness of code reading and writing.

Result: Participants made fewer mistakes when reading code using spreadsheet-style syntax. When writing code, participants were faster and made fewer mistakes using spreadsheet syntax compared to numeric syntax.

Conclusion: When writing and reading code, spreadsheet-style syntax results in fewer mistakes and faster performance compared to numeric syntax.

Abstract: Selecting a subset of cells is a common task in data engineering, for
example, to remove errors or select only specific parts of a table. Multiple
approaches to express this selection exist. One option is numeric indexing,
commonly found in general programming languages, where a tuple of numbers
identifies the cell. Alternatively, the separate dimensions can be referred to
using different enumeration schemes like "A1" for the first cell, commonly
found in software such as spreadsheet systems.
  In a large-scale controlled experiment with student participants as proxy for
data practitioners, we compare the two options with respect to speed and
correctness of reading and writing code.
  The results show that, when reading code, participants make less mistakes
using spreadsheet-style syntax. Additionally, when writing code, they make
fewer mistakes and are faster when using spreadsheet syntax compared to numeric
syntax.
  From this, a domain-specific syntax, such as spreadsheet syntax for data
engineering, appears to be a promising alternative to explore in future tools
to support practitioners without a software engineering background.

</details>


### [190] [Solving Data-centric Tasks using Large Language Models](https://arxiv.org/abs/2402.11734)
*Shraddha Barke,Christian Poelitz,Carina Suzana Negreanu,Benjamin Zorn,José Cambronero,Andrew D. Gordon,Vu Le,Elnaz Nouri,Nadia Polikarpova,Advait Sarkar,Brian Slininger,Neil Toronto,Jack Williams*

Main category: cs.PL

TL;DR: LLM在处理数据任务时，通过选择代表性数据行加入提示，可以提高性能，尤其是在数据多样性较高的情况下。


<details>
  <summary>Details</summary>
Motivation: 为了解决在向LLM提供自然语言到代码任务的指令时，如何有效选择和提供数据的问题，以提升LLM在数据中心任务（如电子表格操作和数据整理）上的表现。

Method: 通过聚类然后选择（cluster-then-select）的提示技术，将最具代表性的行添加到LLM提示中，以优化LLM在数据密集型任务中的表现。

Result: LLM在提示中加入的数据量对性能有显著影响，所提出的聚类然后选择方法在处理具有句法变异的表格数据时优于随机选择基线。

Conclusion: LLM在处理数据密集型任务时，通过聚类然后选择的方法，能够比随机选择的方法表现出更好的性能，尤其是在处理具有大量句法变异的表格数据时。

Abstract: Large language models (LLMs) are rapidly replacing help forums like
StackOverflow, and are especially helpful for non-professional programmers and
end users. These users are often interested in data-centric tasks, such as
spreadsheet manipulation and data wrangling, which are hard to solve if the
intent is only communicated using a natural-language description, without
including the data. But how do we decide how much data and which data to
include in the prompt? This paper makes two contributions towards answering
this question. First, we create a dataset of real-world NL-to-code tasks
manipulating tabular data, mined from StackOverflow posts. Second, we introduce
a cluster-then-select prompting technique, which adds the most representative
rows from the input data to the LLM prompt. Our experiments show that LLM
performance is indeed sensitive to the amount of data passed in the prompt, and
that for tasks with a lot of syntactic variation in the input table, our
cluster-then-select technique outperforms a random selection baseline.

</details>


### [191] [FLAME: A small language model for spreadsheet formulas](https://arxiv.org/abs/2301.13779)
*Harshit Joshi,Abishai Ebenezer,José Cambronero,Sumit Gulwani,Aditya Kanade,Vu Le,Ivan Radiček,Gust Verbruggen*

Main category: cs.PL

TL;DR: FLAME是一个小巧高效的AI模型，专门用于帮助用户编写Excel公式，即使在处理复杂公式时也能提供出色的辅助，并且在性能上超越了许多更大的模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决在电子表格环境中使用大型语言模型进行公式创作辅助的困难，这些模型因其庞大的参数量（高达数十亿）而训练成本高昂且部署困难。

Method: FLAME是一个基于Transformer的模型，专门为Excel公式设计。它使用了领域洞察，模型参数量为60M，训练数据量少。该模型通过草图去重技术进行了训练数据集的策划，并采用了Excel特定的公式分词器。预训练目标包括掩码跨度预测和噪声自动编码的特定领域版本。

Result: FLAME模型在公式修复、公式补全和基于相似度的公式检索任务中，在10个评估设置中优于Davinci（175B）和Cushman（12B）等更大的模型，在公式检索方面也优于CodeT5、CodeBERT和GraphCodeBERT。

Conclusion: FLAME可以在公式修复、公式补全和基于相似度的公式检索方面表现出色，在14个评估设置中的10个中优于更大的模型，并在公式检索方面优于CodeT5、CodeBERT和GraphCodeBERT。

Abstract: Spreadsheets are a vital tool for end-user data management. Using large
language models for formula authoring assistance in these environments can be
difficult, as these models are expensive to train and challenging to deploy due
to their size (up to billions of parameters). We present FLAME, a
transformer-based model trained exclusively on Excel formulas that leverages
domain insights to achieve competitive performance while being substantially
smaller (60M parameters) and training on two orders of magnitude less data. We
curate a training dataset using sketch deduplication, introduce an
Excel-specific formula tokenizer, and use domain-specific versions of masked
span prediction and noisy auto-encoding as pre-training objectives. We evaluate
FLAME on formula repair, formula completion, and similarity-based formula
retrieval. FLAME can outperform much larger models, such as the Davinci (175B)
and Cushman (12B) variants of Codex and CodeT5 (220M), in 10 of 14 evaluation
settings for the repair and completion tasks. For formula retrieval, FLAME
outperforms CodeT5, CodeBERT, and GraphCodeBERT.

</details>


### [192] [Enhanced Spreadsheet Computing with Finite-Domain Constraint Satisfaction](https://arxiv.org/abs/2203.16346)
*Ezana N. Beyenne,Hai-Feng Guo*

Main category: cs.PL

TL;DR: Spreadsheets are extended to solve constraint satisfaction problems using a visual interface and a new constraint language.


<details>
  <summary>Details</summary>
Motivation: To break the limitation of spreadsheets' mono-directional data flow for bookkeeping-like applications and enable them to solve constraint satisfaction problems.

Method: Extension of the spreadsheet computing paradigm to support finite-domain constraint solving in a visual environment, with a spreadsheet-specific constraint language for users to specify constraints.

Result: The new spreadsheet system significantly simplifies the development of many constraint-based applications using a visual tabular interface.

Conclusion: The enhanced spreadsheet system simplifies the development of constraint-based applications using a visual tabular interface, demonstrating usability and usefulness.

Abstract: The spreadsheet application is among the most widely used computing tools in
modern society. It provides excellent usability and usefulness, and it easily
enables a non-programmer to perform programming-like tasks in a visual tabular
"pen and paper" approach. However, spreadsheets are mostly limited to
bookkeeping-like applications due to their mono-directional data flow. This
paper shows how the spreadsheet computing paradigm is extended to break this
limitation for solving constraint satisfaction problems. We present an enhanced
spreadsheet system where finite-domain constraint solving is well supported in
a visual environment. Furthermore, a spreadsheet-specific constraint language
is constructed for general users to specify constraints among data cells in a
declarative and scalable way. The new spreadsheet system significantly
simplifies the development of many constraint-based applications using a visual
tabular interface. Examples are given to illustrate the usability and
usefulness of the extended spreadsheet paradigm.
  KEYWORDS: Spreadsheet computing, Finite-domain constraint satisfaction,
Constraint logic programming

</details>


### [193] [Typed Image-based Programming with Structure Editing](https://arxiv.org/abs/2110.08993)
*Jonathan Edwards,Tomas Petricek*

Main category: cs.PL

TL;DR: Image-based programming systems like Smalltalk and LISP are great for exploration but lack collaboration features. This paper proposes using static types and structure editing to enable collaboration by tackling schema changes in persistent data. The main contribution is a theory for version control of structure editing, which could be extended to the entire programming experience.


<details>
  <summary>Details</summary>
Motivation: Image-based programming avoids much of the complexity of modern programming technology stacks and encourages more casual and exploratory programming. However conventional file-based programming has better support for collaboration and deployment. These problems have been blamed for the limited commercial success of Smalltalk. We propose to enable collaboration in image-based programming via types and structure editing.

Method: We focus on the problem of schema change on persistent data. We turn to static types, which paradoxically require more schema change but also provide a mechanism to express and execute those changes. To determine those changes we turn to structure editing, so that we can capture changes in type definitions with sufficient fidelity to automatically adapt the data to suit.

Result: We conjecture that typical schema changes can be handled through structure editing of static types.

Conclusion: We present a theory realizing this idea, which is our main technical contribution. While we focus here on editing types, if we can extend the approach to cover the entire programming experience then it would offer a new way to collaborate in image-based programming.

Abstract: Many beloved programming systems are image-based: self-contained worlds that
persist both code and data in a single file. Examples include Smalltalk, LISP,
HyperCard, Flash, and spreadsheets. Image-based programming avoids much of the
complexity of modern programming technology stacks and encourages more casual
and exploratory programming. However conventional file-based programming has
better support for collaboration and deployment. These problems have been
blamed for the limited commercial success of Smalltalk. We propose to enable
collaboration in image-based programming via types and structure editing.
  We focus on the problem of schema change on persistent data. We turn to
static types, which paradoxically require more schema change but also provide a
mechanism to express and execute those changes. To determine those changes we
turn to structure editing, so that we can capture changes in type definitions
with sufficient fidelity to automatically adapt the data to suit. We conjecture
that typical schema changes can be handled through structure editing of static
types.
  That positions us to tackle collaboration with what could be called version
control for structure editing. We present a theory realizing this idea, which
is our main technical contribution. While we focus here on editing types, if we
can extend the approach to cover the entire programming experience then it
would offer a new way to collaborate in image-based programming.

</details>


### [194] [ExceLint: Automatically Finding Spreadsheet Formula Errors](https://arxiv.org/abs/1901.11100)
*Daniel W. Barowy,Emery D. Berger,Benjamin Zorn*

Main category: cs.PL

TL;DR: ExceLint 是一种用于 Microsoft Excel 的静态分析工具，可以快速有效地查找电子表格公式错误，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 电子表格被广泛使用，尤其是在金融等错误可能导致灾难性后果的领域，因此需要一种有效的错误查找方法。

Method: 使用信息论方法，专门为查找电子表格公式错误而设计的静态分析，利用电子表格的矩形特性，识别对附近矩形区域造成特别令人惊讶的干扰的公式。

Result: ExceLint 能够快速有效地发现电子表格公式错误，性能优于现有技术。

Conclusion: ExceLint 速度快且有效，在 70 个电子表格的语料库中，ExceLint 处理每个电子表格的中位数为 5 秒，并且其性能显著优于最先进的分析。

Abstract: Spreadsheets are one of the most widely used programming environments, and
are widely deployed in domains like finance where errors can have catastrophic
consequences. We present a static analysis specifically designed to find
spreadsheet formula errors. Our analysis directly leverages the rectangular
character of spreadsheets. It uses an information-theoretic approach to
identify formulas that are especially surprising disruptions to nearby
rectangular regions. We present ExceLint, an implementation of our static
analysis for Microsoft Excel. We demonstrate that ExceLint is fast and
effective: across a corpus of 70 spreadsheets, ExceLint takes a median of 5
seconds per spreadsheet, and it significantly outperforms the state of the art
analysis.

</details>


### [195] [Synthesizing Bijective Lenses](https://arxiv.org/abs/1710.03248)
*Anders Miltner,Kathleen Fisher,Benjamin C. Pierce,David Walker,Steve Zdancewic*

Main category: cs.PL

TL;DR: Optician通过类型导向合成技术，使用正则表达式和示例自动生成双向字符串转换程序，解决了手动编写转换函数的繁琐和易错问题，并将合成效率大幅提升。


<details>
  <summary>Details</summary>
Motivation: 手动构建双向数据转换（如序列化/反序列化、数据库视图和视图更新）既繁琐又容易出错。现有的特定领域语言（DSL）虽然允许用单个表达式编写双向转换，但编程起来可能很复杂，需要程序员处理细节和复杂的类型系统。

Method: Optician通过接收两种数据格式的正则表达式和一些具体的示例，利用类型导向合成技术生成Boomerang语言（一种基于镜头理论的双向语言）的程序。其关键技术挑战在于高效地导航庞大的程序搜索空间，利用正则表达式类型的丰富等价关系来合成等价语言的项，并将其转换为镜头语言。

Result: Optician成功地将合成问题从无解变为高效可解，并在39个示例的基准套件上进行了评估，这些示例包括微基准测试和来自Flash Fill、Augeas等数据管理系统的实际应用，证明了其在实际应用中的有效性。

Conclusion: Optician是一个用于类型导向合成双射字符串变换器的工具，它通过将程序搜索空间中的一个难题转化为一个高效的问题来解决现有方法的不足。该工具在包含39个示例的基准套件上进行了评估，证明了其有效性。

Abstract: Bidirectional transformations between different data representations occur
frequently in modern software systems. They appear as serializers and
deserializers, as database views and view updaters, and more. Manually building
bidirectional transformations---by writing two separate functions that are
intended to be inverses---is tedious and error prone. A better approach is to
use a domain-specific language in which both directions can be written as a
single expression. However, these domain-specific languages can be difficult to
program in, requiring programmers to manage fiddly details while working in a
complex type system.
  To solve this, we present Optician, a tool for type-directed synthesis of
bijective string transformers. The inputs to Optician are two ordinary regular
expressions representing two data formats and a few concrete examples for
disambiguation. The output is a well-typed program in Boomerang (a
bidirectional language based on the theory of lenses). The main technical
challenge involves navigating the vast program search space efficiently enough.
Unlike most prior work on type-directed synthesis, our system operates in the
context of a language with a rich equivalence relation on types (the theory of
regular expressions). We synthesize terms of a equivalent language and convert
those generated terms into our lens language. We prove the correctness of our
synthesis algorithm. We also demonstrate empirically that our new language
changes the synthesis problem from one that admits intractable solutions to one
that admits highly efficient solutions. We evaluate Optician on a benchmark
suite of 39 examples including both microbenchmarks and realistic examples
derived from other data management systems including Flash Fill, a tool for
synthesizing string transformations in spreadsheets, and Augeas, a tool for
bidirectional processing of Linux system configuration files.

</details>


### [196] [Active Learning of Input Grammars](https://arxiv.org/abs/1708.08731)
*Matthias Höschele,Alexander Kampmann,Andreas Zeller*

Main category: cs.PL

TL;DR: AUTOGRAM creates input grammars for testing using data flow analysis and membership queries, requiring few samples for accurate and readable results.


<details>
  <summary>Details</summary>
Motivation: Precise knowledge of a program's input format is essential for systematic testing.

Method: The method involves tracking data flow to aggregate input fragments into lexical and syntactic entities, assigning meaningful names to these entities, and generalizing production rules using membership queries.

Result: Human-readable context-free grammars that accurately reflect valid input structure are obtained with minimal sample inputs, as demonstrated on inputs like URLs, spreadsheets, and configuration files.

Conclusion: Our AUTOGRAM prototype generates accurate and readable context-free grammars from a minimal set of sample inputs, which can be directly used for automated testing.

Abstract: Knowing the precise format of a program's input is a necessary prerequisite
for systematic testing. Given a program and a small set of sample inputs, we
(1) track the data flow of inputs to aggregate input fragments that share the
same data flow through program execution into lexical and syntactic entities;
(2) assign these entities names that are based on the associated variable and
function identifiers; and (3) systematically generalize production rules by
means of membership queries. As a result, we need only a minimal set of sample
inputs to obtain human-readable context-free grammars that reflect valid input
structure. In our evaluation on inputs like URLs, spreadsheets, or
configuration files, our AUTOGRAM prototype obtains input grammars that are
both accurate and very readable - and that can be directly fed into test
generators for comprehensive automated testing.

</details>


### [197] [Synthesis of Data Completion Scripts using Finite Tree Automata](https://arxiv.org/abs/1707.01469)
*Xinyu Wang,Isil Dillig,Rishabh Singh*

Main category: cs.PL

TL;DR: 通过PBE和DSL自动完成表格数据补全任务，使用FTA学习算法，并在DACE工具上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 为了解决数据补全任务（如缺失值填充和派生数据计算）需要编程专业知识的痛点，提出了一种自动化数据补全的技术。

Method: 提出了一种结合空间和关系推理的DSL，并设计了一种新的综合算法，该算法能够生成与给定输入输出示例一致的DSL程序。其关键技术创新在于基于FTA的新版本空间学习算法。

Result: 在84个基准测试上对名为DACE的工具进行了评估，并与PROSE和SKETCH两种现有合成器进行了比较，证明了该方法的优势。

Conclusion: 该技术通过结合编程by-example（PBE）和轻量级草图方法，利用领域特定语言（DSL）和基于有限树自动机（FTA）的版本空间学习算法，实现了数据补全任务的自动化。

Abstract: In application domains that store data in a tabular format, a common task is
to fill the values of some cells using values stored in other cells. For
instance, such data completion tasks arise in the context of missing value
imputation in data science and derived data computation in spreadsheets and
relational databases. Unfortunately, end-users and data scientists typically
struggle with many data completion tasks that require non-trivial programming
expertise. This paper presents a synthesis technique for automating data
completion tasks using programming-by-example (PBE) and a very lightweight
sketching approach. Given a formula sketch (e.g., AVG($?_1$, $?_2$)) and a few
input-output examples for each hole, our technique synthesizes a program to
automate the desired data completion task. Towards this goal, we propose a
domain-specific language (DSL) that combines spatial and relational reasoning
over tabular data and a novel synthesis algorithm that can generate DSL
programs that are consistent with the input-output examples. The key technical
novelty of our approach is a new version space learning algorithm that is based
on finite tree automata (FTA). The use of FTAs in the learning algorithm leads
to a more compact representation that allows more sharing between programs that
are consistent with the examples. We have implemented the proposed approach in
a tool called DACE and evaluate it on 84 benchmarks taken from online help
forums. We also illustrate the advantages of our approach by comparing our
technique against two existing synthesizers, namely PROSE and SKETCH.

</details>
