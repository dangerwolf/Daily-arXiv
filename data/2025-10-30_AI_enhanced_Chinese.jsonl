{"id": "2510.25726", "title": "The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution", "url": "https://arxiv.org/abs/2510.25726", "pdf": "https://arxiv.org/pdf/2510.25726", "abs": "https://arxiv.org/abs/2510.25726", "authors": ["Junlong Li", "Wenshuo Zhao", "Jian Zhao", "Weihao Zeng", "Haoze Wu", "Xiaochen Wang", "Rui Ge", "Yuxuan Cao", "Yuzhen Huang", "Wei Liu", "Junteng Liu", "Zhaochen Su", "Yiyang Guo", "Fan Zhou", "Lueyang Zhang", "Juan Michelini", "Xingyao Wang", "Xiang Yue", "Shuyan Zhou", "Graham Neubig", "Junxian He"], "categories": ["cs.CL", "cs.AI"], "comment": "Website: https://toolathlon.xyz/", "summary": "Real-world language agents must handle complex, multi-step workflows across\ndiverse Apps. For instance, an agent may manage emails by coordinating with\ncalendars and file systems, or monitor a production database to detect\nanomalies and generate reports following an operating manual. However, existing\nlanguage agent benchmarks often focus on narrow domains or simplified tasks\nthat lack the diversity, realism, and long-horizon complexity required to\nevaluate agents' real-world performance. To address this gap, we introduce the\nTool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering\ndiverse Apps and tools, realistic environment setup, and reliable\nexecution-based evaluation. Toolathlon spans 32 software applications and 604\ntools, ranging from everyday platforms such as Google Calendar and Notion to\nprofessional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools\nare based on a high-quality set of Model Context Protocol (MCP) servers that we\nmay have revised or implemented ourselves. Unlike prior works, which primarily\nensure functional realism but offer limited environment state diversity, we\nprovide realistic initial environment states from real software, such as Canvas\ncourses with dozens of students or real financial spreadsheets. This benchmark\nincludes 108 manually sourced or crafted tasks in total, requiring interacting\nwith multiple Apps over around 20 turns on average to complete. Each task is\nstrictly verifiable through dedicated evaluation scripts. Comprehensive\nevaluation of SOTA models highlights their significant shortcomings: the\nbest-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate\nwith 20.2 tool calling turns on average, while the top open-weights model\nDeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development\nof more capable language agents for real-world, long-horizon task execution.", "AI": {"tldr": "Toolathlon\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u4e16\u754c\u3001\u591a\u5e94\u7528\u3001\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\u4e2d\u7684\u8868\u73b0\u3002\u5b83\u63d0\u4f9b\u4e86\u591a\u6837\u5316\u7684\u5e94\u7528\u7a0b\u5e8f\u548c\u5de5\u5177\u3001\u771f\u5b9e\u7684\u521d\u59cb\u73af\u5883\u72b6\u6001\u4ee5\u53ca\u53ef\u9760\u7684\u6267\u884c\u8bc4\u4f30\u3002\u76ee\u524d\u7684SOTA\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u4fa7\u91cd\u4e8e\u72ed\u7a84\u9886\u57df\u6216\u7b80\u5316\u4efb\u52a1\uff0c\u7f3a\u4e4f\u8bc4\u4f30\u667a\u80fd\u4f53\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u6240\u9700\u7684\u4efb\u52a1\u591a\u6837\u6027\u3001\u771f\u5b9e\u6027\u548c\u957f\u5468\u671f\u590d\u6742\u6027\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5f15\u5165\u4e86Tool Decathlon\uff08Toolathlon\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u9488\u5bf9\u8bed\u8a00\u667a\u80fd\u4f53\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u5b83\u6db5\u76d6\u4e8632\u4e2a\u8f6f\u4ef6\u5e94\u7528\u548c604\u4e2a\u5de5\u5177\uff0c\u63d0\u4f9b\u4e86\u771f\u5b9e\u7684\u521d\u59cb\u73af\u5883\u72b6\u6001\uff08\u800c\u975e\u4ec5\u529f\u80fd\u4e0a\u7684\u771f\u5b9e\u6027\uff09\uff0c\u5e76\u5305\u542b108\u4e2a\u9700\u8981\u4e0e\u591a\u4e2a\u5e94\u7528\u4ea4\u4e92\u7684\u590d\u6742\u4efb\u52a1\u3002\u6bcf\u4e2a\u4efb\u52a1\u90fd\u901a\u8fc7\u4e13\u7528\u8bc4\u4f30\u811a\u672c\u4e25\u683c\u9a8c\u8bc1\u3002", "result": "\u5bf9SOTA\u6a21\u578b\u7684\u7efc\u5408\u8bc4\u4f30\u663e\u793a\u5b83\u4eec\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff1a\u8868\u73b0\u6700\u597d\u7684\u6a21\u578bClaude-4.5-Sonnet\u7684\u6210\u529f\u7387\u4ec5\u4e3a38.6%\uff0c\u800c\u9876\u7ea7\u7684\u5f00\u6e90\u6a21\u578bDeepSeek-V3.2-Exp\u4ec5\u8fbe\u523020.1%\u3002", "conclusion": "Toolathlon\u6709\u671b\u63a8\u52a8\u5f00\u53d1\u51fa\u66f4\u5f3a\u5927\u7684\u8bed\u8a00\u667a\u80fd\u4f53\uff0c\u4ee5\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u3001\u957f\u5468\u671f\u4efb\u52a1\u7684\u6267\u884c\u9700\u6c42\u3002"}}
{"id": "2508.11715", "title": "Benchmark Dataset Generation and Evaluation for Excel Formula Repair with LLMs", "url": "https://arxiv.org/abs/2508.11715", "pdf": "https://arxiv.org/pdf/2508.11715", "abs": "https://arxiv.org/abs/2508.11715", "authors": ["Ananya Singha", "Harshita Sahijwani", "Walt Williams", "Emmanuel Aboah Boateng", "Nick Hausman", "Miguel Di Luca", "Keegan Choudhury", "Chaya Binet", "Vu Le", "Tianwei Chen", "Oryan Rokeah Chen", "Sulaiman Vesal", "Sadid Hasan"], "categories": ["cs.SE", "cs.AI"], "comment": "Accepted at the KDD workshop on Evaluation and Trustworthiness of\n  Agentic and Generative AI Models", "summary": "Excel is a pervasive yet often complex tool, particularly for novice users,\nwhere runtime errors arising from logical mistakes or misinterpretations of\nfunctions pose a significant challenge. While large language models (LLMs)\noffer promising assistance by explaining formula errors, the automated\ncorrection of these semantic runtime errors remains an open problem. A primary\nchallenge to advancing models for such scenarios is the severe lack of\nhigh-quality, comprehensive datasets for training and rigorous evaluation. This\npaper addresses this gap by introducing a novel approach for constructing a\nbenchmark dataset specifically designed for Excel formula repair. We propose a\ndata generation pipeline, which leverages a small set of curated seed samples\nfrom online forums to synthetically expand the dataset. Our pipeline integrates\nfew-shot prompting with LLMs and employs a robust \\textit{LLM-as-a-Judge}\nvalidation framework, combined with execution-based checks to ensure the\ncorrectness and semantic fidelity of the generated data. This process produced\na benchmark dataset of 618 high-quality samples, covering common runtime\nerrors. Furthermore, we propose a context-aware baseline technique for Excel\nformula repair that utilizes LLMs to leverage both the faulty formula, and\nrelevant spreadsheet context. We evaluate the performance of various LLMs\n(GPT-4o, GPT-4.1, Phi-3, Mistral) on our newly generated benchmark using\nexecution-based metrics. Our analysis demonstrates the dataset's quality\nthrough manual annotation and provides insights into error and function\ndistributions. The proposed generation methodology is highly scalable and can\nbe readily adapted to create evaluation benchmarks for similar code repair\ntasks in other low-resource programming languages.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210Excel\u516c\u5f0f\u4fee\u590d\u57fa\u51c6\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u57fa\u7ebf\u4fee\u590d\u6280\u672f\uff0c\u5bf9\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "motivation": "Excel\u65b0\u624b\u7528\u6237\u5e38\u9047\u5230\u903b\u8f91\u9519\u8bef\u5bfc\u81f4\u7684\u8fd0\u884c\u65f6\u9519\u8bef\uff0c\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u89e3\u91ca\u9519\u8bef\u4f46\u7f3a\u4e4f\u81ea\u52a8\u5316\u8bed\u4e49\u9519\u8bef\u4fee\u590d\u6240\u9700\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u5229\u7528\u5c11\u91cf\u7cbe\u9009\u79cd\u5b50\u6837\u672c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5c11\u6837\u672c\u63d0\u793a\u548c\u201cLLM-as-a-Judge\u201d\u9a8c\u8bc1\u6846\u67b6\uff0c\u7ed3\u5408\u6267\u884c\u68c0\u67e5\u6765\u6784\u5efa\u57fa\u51c6\u6570\u636e\u96c6\u3002\u540c\u65f6\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u76f8\u5173\u7535\u5b50\u8868\u683c\u4e0a\u4e0b\u6587\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u57fa\u7ebf\u4fee\u590d\u6280\u672f\u3002", "result": "\u751f\u6210\u4e86\u4e00\u4e2a\u5305\u542b618\u4e2a\u9ad8\u8d28\u91cf\u6837\u672c\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5e38\u89c1\u8fd0\u884c\u65f6\u9519\u8bef\u3002\u5728\u65b0\u751f\u6210\u7684\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e86\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08GPT-4o, GPT-4.1, Phi-3, Mistral\uff09\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u8bc1\u660e\u4e86\u6570\u636e\u96c6\u7684\u8d28\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\u5177\u6709\u9ad8\u5ea6\u53ef\u6269\u5c55\u6027\uff0c\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6\u4f4e\u8d44\u6e90\u7f16\u7a0b\u8bed\u8a00\u4e2d\u7c7b\u4f3c\u7684\u7a0b\u5e8f\u4fee\u590d\u4efb\u52a1\u7684\u8bc4\u4f30\u57fa\u51c6\u521b\u5efa\u3002"}}
