<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.CL](#cs.CL) [Total: 14]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.DB](#cs.DB) [Total: 20]
- [cs.CY](#cs.CY) [Total: 9]
- [cs.OH](#cs.OH) [Total: 1]
- [cs.LG](#cs.LG) [Total: 7]
- [eess.IV](#eess.IV) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [math.HO](#math.HO) [Total: 2]
- [cs.SE](#cs.SE) [Total: 35]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.MS](#cs.MS) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 13]
- [stat.OT](#stat.OT) [Total: 1]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.PL](#cs.PL) [Total: 5]
- [cs.DL](#cs.DL) [Total: 5]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 25]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Image deidentification in the XNAT ecosystem: use cases and solutions](https://arxiv.org/abs/2504.20657)
*Alex Michie,Simon J Doran*

Main category: cs.CV

TL;DR: XNAT平台结合外部工具，通过基于规则和机器学习的方法对DICOM数据进行去标识化处理，并在MIDI-B挑战赛中取得了99.61%的准确率，未来将着重改进地址识别和图像像素内标识信息的移除。


<details>
  <summary>Details</summary>
Motivation: 在学术界广泛使用XNAT平台进行DICOM图像数据库的构建和研究，需要一个详细的去标识化工作流程来处理这些数据。

Method: 结合XNAT平台及其生态系统中的独立工具，采用基于规则的方法和机器学习模型来移除DICOM数据中的姓名、地址等个人信息。

Result: 基于规则的方法能够移除所有姓名相关信息，但在处理地址数据时存在不足。机器学习模型在移除地址方面取得部分成功，但对其他自由文本数据存在“过度移除”的问题。最终在MIDI-B挑战赛的测试阶段，经过改进的去标识化准确率达到99.61%。

Conclusion: 去标识化在处理DICOM数据方面是可行的，但仍需在地址识别和图像像素内信息移除方面进行改进。基于规则的方法和机器学习模型各有优劣，未来需要结合两者的优点并进一步优化。

Abstract: XNAT is a server-based data management platform widely used in academia for curating large databases of DICOM images for research projects. We describe in detail a deidentification workflow for DICOM data using facilities in XNAT, together with independent tools in the XNAT "ecosystem". We list different contexts in which deidentification might be needed, based on our prior experience. The starting point for participation in the Medical Image De-Identification Benchmark (MIDI-B) challenge was a set of pre-existing local methodologies, which were adapted during the validation phase of the challenge. Our result in the test phase was 97.91\%, considerably lower than our peers, due largely to an arcane technical incompatibility of our methodology with the challenge's Synapse platform, which prevented us receiving feedback during the validation phase. Post-submission, additional discrepancy reports from the organisers and via the MIDI-B Continuous Benchmarking facility, enabled us to improve this score significantly to 99.61\%. An entirely rule-based approach was shown to be capable of removing all name-related information in the test corpus, but exhibited failures in dealing fully with address data. Initial experiments using published machine-learning models to remove addresses were partially successful but showed the models to be "over-aggressive" on other types of free-text data, leading to a slight overall degradation in performance to 99.54\%. Future development will therefore focus on improving address-recognition capabilities, but also on better removal of identifiable data burned into the image pixels. Several technical aspects relating to the "answer key" are still under discussion with the challenge organisers, but we estimate that our percentage of genuine deidentification failures on the MIDI-B test corpus currently stands at 0.19\%. (Abridged from original for arXiv submission)

</details>


### [2] [Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language](https://arxiv.org/abs/2204.00598)
*Andy Zeng,Maria Attarian,Brian Ichter,Krzysztof Choromanski,Adrian Wong,Stefan Welker,Federico Tombari,Aveek Purohit,Michael Ryoo,Vikas Sindhwani,Johnny Lee,Vincent Vanhoucke,Pete Florence*

Main category: cs.CV

TL;DR: 通过Socratic Models（SMs）框架，可以零样本组合多个预训练模型，无需微调即可实现新的多模态能力，并在图像字幕、视频-文本检索、自由形式的视频问答、多模态辅助对话和机器人感知与规划等应用中展现出竞争力。


<details>
  <summary>Details</summary>
Motivation: 不同的预训练模型（如视觉-语言模型和语言模型）在不同领域的数据上训练，存储了不同的常识知识，这种多样性可以被利用来捕获新的多模态能力。

Method: 提出Socratic Models（SMs）框架，一个模块化框架，允许通过多模态信息提示将多个预训练模型组合起来，进行信息交换，无需微调。

Result: SMs在零样本图像字幕和视频-文本检索方面具有竞争力，并实现了新的应用，如自由形式的视频问答、与外部API和数据库接口的多模态辅助对话，以及机器人感知与规划。

Conclusion: Socratic Models（SMs）框架通过组合现有预训练模型，在无需微调的情况下，能够有效地利用不同模型之间的知识多样性，实现新的多模态能力，并在多种下游任务中取得成功。

Abstract: Large pretrained (e.g., "foundation") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.

</details>


### [3] [TableParser: Automatic Table Parsing with Weak Supervision from Spreadsheets](https://arxiv.org/abs/2201.01654)
*Susie Xi Rao,Johannes Rausch,Peter Egger,Ce Zhang*

Main category: cs.CV

TL;DR: TableParser是一个能解析原生PDF和扫描图像中表格的系统，并能有效利用领域自适应技术；同时提出了TableAnnotator和ExcelAnnotator，用于表格解析。


<details>
  <summary>Details</summary>
Motivation: 解析表格结构和提取表格内容在许多应用中都非常重要。

Method: 提出了TableParser系统，能够解析原生PDF和扫描图像中的表格；通过实验证明了领域自适应在开发此类工具中的有效性；创建了TableAnnotator和ExcelAnnotator，构成了基于电子表格的弱监督机制和表格解析流程。

Result: TableParser在解析表格结构和提取内容方面表现出高精度。

Conclusion: TableParser系统能够高精度地解析表格，并且通过TableAnnotator和ExcelAnnotator可以实现表格解析的弱监督和自动化。

Abstract: Tables have been an ever-existing structure to store data. There exist now different approaches to store tabular data physically. PDFs, images, spreadsheets, and CSVs are leading examples. Being able to parse table structures and extract content bounded by these structures is of high importance in many applications. In this paper, we devise TableParser, a system capable of parsing tables in both native PDFs and scanned images with high precision. We have conducted extensive experiments to show the efficacy of domain adaptation in developing such a tool. Moreover, we create TableAnnotator and ExcelAnnotator, which constitute a spreadsheet-based weak supervision mechanism and a pipeline to enable table parsing. We share these resources with the research community to facilitate further research in this interesting direction.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [4] [From Rows to Reasoning: A Retrieval-Augmented Multimodal Framework for Spreadsheet Understanding](https://arxiv.org/abs/2601.08741)
*Anmol Gulati,Sahil Sen,Waqar Sarguroh,Kevin Paul*

Main category: cs.CL

TL;DR: LLMs在处理大型企业电子表格时存在困难，本研究提出了FRTR-Bench基准和FRTR框架，显著提高了多模态电子表格推理的准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在处理包含大量数据、多个关联工作表和视觉元素（如图表、收据）的企业电子表格时能力有限，现有的方法在可扩展性和真实用户交互模拟方面存在不足。

Method: 提出FRTR-Bench，一个包含30个企业级Excel工作簿、近400万个单元格和50多个嵌入图像的大规模多模态电子表格推理基准。同时，提出FRTR框架，一种多模态检索增强生成框架，通过分解电子表格为行、列和块嵌入，使用混合词汇-密集检索（RRF）以及整合多模态嵌入来处理数值和视觉信息。

Result: FRTR框架在FRTR-Bench基准上使用Claude Sonnet 4.5模型达到了74%的答案准确率，远超现有技术（24%）。在SpreadsheetLLM基准上，使用GPT-5模型达到了87%的准确率，并将令牌使用量减少了约50%。

Conclusion: FRTR框架通过其创新的多模态处理和检索方法，有效解决了大型企业电子表格的推理挑战，显著提高了准确性和效率，为相关领域的研究和应用提供了新的可能性。

Abstract: Large Language Models (LLMs) struggle to reason over large-scale enterprise spreadsheets containing thousands of numeric rows, multiple linked sheets, and embedded visual content such as charts and receipts. Prior state-of-the-art spreadsheet reasoning approaches typically rely on single-sheet compression or full-context encoding, which limits scalability and fails to reflect how real users interact with complex, multimodal workbooks. We introduce FRTR-Bench, the first large-scale benchmark for multimodal spreadsheet reasoning, comprising 30 enterprise-grade Excel workbooks spanning nearly four million cells and more than 50 embedded images. To address these challenges, we present From Rows to Reasoning (FRTR), an advanced, multimodal retrieval-augmented generation framework that decomposes Excel workbooks into granular row, column, and block embeddings, employs hybrid lexical-dense retrieval with Reciprocal Rank Fusion (RRF), and integrates multimodal embeddings to reason over both numerical and visual information. We tested FRTR on six LLMs, achieving 74% answer accuracy on FRTR-Bench with Claude Sonnet 4.5, a substantial improvement over prior state-of-the-art approaches that reached only 24%. On the SpreadsheetLLM benchmark, FRTR achieved 87% accuracy with GPT-5 while reducing token usage by roughly 50% compared to context-compression methods.

</details>


### [5] [SQuARE: Structured Query & Adaptive Retrieval Engine For Tabular Formats](https://arxiv.org/abs/2512.04292)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Main category: cs.CL

TL;DR: SQuARE是一个混合检索框架，用于在包含多行表头、合并单元格和单位注释的电子表格上进行准确的问题回答。它通过一个计算表头深度和合并密度的分数来路由查询，可以选择保留结构的块检索或SQL查询。一个轻量级代理在低置信度时监督检索、精炼或组合结果。SQuARE在准确性和延迟方面优于现有方法，并能与未来的表格基础模型兼容。


<details>
  <summary>Details</summary>
Motivation: 现有的电子表格问答方法难以处理多行表头、合并单元格和单位注释等复杂情况，而固定的SQL视图在模式不一致的文件上表现不佳。因此，需要一种能够处理这些复杂性并提供准确答案的框架。

Method: SQuARE是一个混合检索框架，它计算一个基于表头深度和合并密度的连续分数，然后将查询路由到：1. 保留结构的块检索；2. 在自动构建的关系表示上执行SQL查询。一个轻量级代理在置信度低时监督检索、精炼或组合来自两个路径的结果。

Result: 在多表头公司资产负债表、合并的银行工作簿和多个公共数据集上，SQuARE在检索精度和端到端答案准确性方面持续优于单一策略基线和ChatGPT-4o，同时保持可预测的延迟。

Conclusion: SQuARE通过解耦检索和模型选择，提供了一个结合了块检索和SQL查询的混合框架，能够准确地处理复杂的电子表格，并为未来的表格理解提供了一个实用的桥梁。

Abstract: Accurate question answering over real spreadsheets remains difficult due to multirow headers, merged cells, and unit annotations that disrupt naive chunking, while rigid SQL views fail on files lacking consistent schemas. We present SQuARE, a hybrid retrieval framework with sheet-level, complexity-aware routing. It computes a continuous score based on header depth and merge density, then routes queries either through structure-preserving chunk retrieval or SQL over an automatically constructed relational representation. A lightweight agent supervises retrieval, refinement, or combination of results across both paths when confidence is low. This design maintains header hierarchies, time labels, and units, ensuring that returned values are faithful to the original cells and straightforward to verify. Evaluated on multi-header corporate balance sheets, a heavily merged World Bank workbook, and diverse public datasets, SQuARE consistently surpasses single-strategy baselines and ChatGPT-4o on both retrieval precision and end-to-end answer accuracy while keeping latency predictable. By decoupling retrieval from model choice, the system is compatible with emerging tabular foundation models and offers a practical bridge toward a more robust table understanding.

</details>


### [6] [An Empirical Study of Validating Synthetic Data for Formula Generation](https://arxiv.org/abs/2407.10657)
*Usneek Singh,José Cambronero,Sumit Gulwani,Aditya Kanade,Anirudh Khatry,Vu Le,Mukul Singh,Gust Verbruggen*

Main category: cs.CL

TL;DR: LLM可用于帮助编写电子表格公式，但相关资源稀缺。本文提出了一种通过验证模型生成合成训练样本的方法，并进行了实证研究，证明验证可提高模型性能，并能解决更复杂的问题。


<details>
  <summary>Details</summary>
Motivation: 电子表格公式相关的LLM资源稀缺，影响预训练模型性能和微调能力。需要验证LLM生成的自然语言（NL）文本的准确性，以确保其对微调有益。

Method: 利用模型为电子表格公式生成合成的自然语言（NL）文本，并使用代理目标来评估合成注释的准确性，对这些合成训练样本进行验证，并实证研究验证的影响。

Result: 在四种模型（两种开源，两种闭源）上，与使用原始数据相比，验证提高了性能。尽管验证倾向于剔除更具挑战性的样本，但它增加了模型在微调后能够解决的问题的复杂度。

Conclusion: 验证LLM生成的合成训练样本可以提高模型的性能，并且能够让模型解决更复杂的问题。

Abstract: Large language models (LLMs) can be leveraged to help with writing formulas in spreadsheets, but resources on these formulas are scarce, impacting both the base performance of pre-trained models and limiting the ability to fine-tune them. Given a corpus of formulas, we can use a(nother) model to generate synthetic natural language utterances for fine-tuning. However, it is important to validate whether the NL generated by the LLM is indeed accurate to be beneficial for fine-tuning. In this paper, we provide empirical results on the impact of validating these synthetic training examples with surrogate objectives that evaluate the accuracy of the synthetic annotations. We demonstrate that validation improves performance over raw data across four models (2 open and 2 closed weight). Interestingly, we show that although validation tends to prune more challenging examples, it increases the complexity of problems that models can solve after being fine-tuned on validated data.

</details>


### [7] [TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios](https://arxiv.org/abs/2403.19318)
*Xiaokang Zhang,Sijia Luo,Bohan Zhang,Zeyao Ma,Jing Zhang,Yang Li,Guanlin Li,Zijun Yao,Kangli Xu,Jinchang Zhou,Daniel Zhang-Li,Jifan Yu,Shu Zhao,Juanzi Li,Jie Tang*

Main category: cs.CL

TL;DR: TableLLM是一个80亿参数的大型语言模型，专为处理文档和电子表格中的表格数据操作而设计，并提出了远程监督训练方法，通过推理过程扩展和交叉验证策略来提高模型性能。评估结果表明TableLLM优于现有模型，并已公开模型、代码和基准。 


<details>
  <summary>Details</summary>
Motivation: 旨在创建一个能够熟练处理文档或电子表格中表格数据操作的大型语言模型，以满足实际办公场景的需求。

Method: 提出了一种远程监督训练方法，该方法包括一个推理过程扩展策略，用于增强LLM对推理模式的理解，以及一个交叉验证策略，用于保证自动生成数据的质量。

Result: 在针对文档和电子表格格式定制的基准上进行了评估，结果显示TableLLM相比于现有的通用和专注于表格数据的LLM具有优势。

Conclusion: TableLLM在表格数据处理方面表现出色，并已公开相关资源以供使用和进一步研究。

Abstract: We introduce TableLLM, a robust large language model (LLM) with 8 billion parameters, purpose-built for proficiently handling tabular data manipulation tasks, whether they are embedded within documents or spreadsheets, catering to real-world office scenarios. We propose a distant supervision method for training, which comprises a reasoning process extension strategy, aiding in training LLMs to understand reasoning patterns more effectively as well as a cross-way validation strategy, ensuring the quality of the automatically generated data. To evaluate the performance of TableLLM, we have crafted benchmarks tailored to address both document and spreadsheet formats as well as constructed a well-organized evaluation pipeline capable of handling both scenarios. Thorough evaluations underscore the advantages of TableLLM when compared to various existing general-purpose and tabular data-focused LLMs. We have publicly released the model checkpoint, source code, benchmarks, and a web application for user interaction. Our codes and data are publicly available at https://github.com/TableLLM/TableLLM.

</details>


### [8] [GiesKaNe: Bridging Past and Present in Grammatical Theory and Practical Application](https://arxiv.org/abs/2502.05113)
*Volker Emmrich*

Main category: cs.CL

TL;DR: GiesKaNe项目旨在创建一个历史性的、句法标注的参考语料库，通过结合人工与机器辅助的方法，并提出了一种新的文本分类方法，同时展示了如何利用现有基础设施和简单工具来管理复杂的语料库构建过程。


<details>
  <summary>Details</summary>
Motivation: GiesKaNe项目旨在建立一个参考语料库、历史语料库和句法标注树库，以连接历史和当代语料库，满足研究界的需求。

Method: 该项目结合了人工专业知识和机器辅助流程，处理了分词、规范化、句子定义、词性标注、句法分析和标注员间一致性等基础问题，并进行了语法模型、标注模式和实际标注标准的比较。提出了一种新的机器学习辅助方法来对文本的口语-书面语连续体进行分类，并开发了一种从现有标注中提取实际标注标准的方法。

Result: 通过对现有研究基础设施的战略性使用，包括简单的电子表格，GiesKaNe项目证明了即使是宏伟的项目也可以在没有专门标注工具的情况下有效实施。

Conclusion: GiesKaNe项目展示了如何有效地构建一个复杂的、历史性的、句法标注的树库，强调了结合人工和机器的方法、灵活的标注策略以及利用现有基础设施的重要性。

Abstract: This article explores the requirements for corpus compilation within the GiesKaNe project (University of Giessen and Kassel, Syntactic Basic Structures of New High German). The project is defined by three central characteristics: it is a reference corpus, a historical corpus, and a syntactically deeply annotated treebank. As a historical corpus, GiesKaNe aims to establish connections with both historical and contemporary corpora, ensuring its relevance across temporal and linguistic contexts. The compilation process strikes the balance between innovation and adherence to standards, addressing both internal project goals and the broader interests of the research community. The methodological complexity of such a project is managed through a complementary interplay of human expertise and machine-assisted processes. The article discusses foundational topics such as tokenization, normalization, sentence definition, tagging, parsing, and inter-annotator agreement, alongside advanced considerations. These include comparisons between grammatical models, annotation schemas, and established de facto annotation standards as well as the integration of human and machine collaboration. Notably, a novel method for machine-assisted classification of texts along the continuum of conceptual orality and literacy is proposed, offering new perspectives on text selection. Furthermore, the article introduces an approach to deriving de facto standard annotations from existing ones, mediating between standardization and innovation. In the course of describing the workflow the article demonstrates that even ambitious projects like GiesKaNe can be effectively implemented using existing research infrastructure, requiring no specialized annotation tools. Instead, it is shown that the workflow can be based on the strategic use of a simple spreadsheet and integrates the capabilities of the existing infrastructure.

</details>


### [9] [MiMoTable: A Multi-scale Spreadsheet Benchmark with Meta Operations for Table Reasoning](https://arxiv.org/abs/2412.11711)
*Zheng Li,Yang Du,Mao Zheng,Mingyang Song*

Main category: cs.CL

TL;DR: MiMoTable是一个包含真实世界电子表格的多尺度表格推理基准，具有元操作作为难度指标，可以评估LLM在表格推理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有表格推理基准与真实世界场景中的复杂表格和用户问题之间存在差距。

Method: 提出MiMoTable基准，包含真实世界的电子表格和用于衡量问题难度的六类元操作。对现有基准进行分类以评估LLM的性能。

Result: MiMoTable在Claude-3.5-Sonnet上的准确率为77.4%，表明LLM仍有改进空间。LLM在难度不断增加的基准上的性能下降，证明了所提出难度评估标准的有效性。

Conclusion: MiMoTable基准为评估LLM在表格推理方面的能力提供了一个更现实的评估，并且所提出的元操作可以作为衡量表格推理任务难度的有效指标。

Abstract: Extensive research has been conducted to explore the capability of Large Language Models (LLMs) for table reasoning and has significantly improved the performance on existing benchmarks. However, tables and user questions in real-world applications are more complex and diverse, presenting an unignorable gap compared to the existing benchmarks. To fill the gap, we propose a \textbf{M}ult\textbf{i}-scale spreadsheet benchmark with \textbf{M}eta \textbf{o}perations for \textbf{Table} reasoning, named as MiMoTable. Specifically, MiMoTable incorporates two key features. First, the tables in MiMoTable are all spreadsheets used in real-world scenarios, which cover seven domains and contain different types. Second, we define a new criterion with six categories of meta operations for measuring the difficulty of each question in MiMoTable, simultaneously as a new perspective for measuring the difficulty of the existing benchmarks. Experimental results show that Claude-3.5-Sonnet achieves the best performance with 77.4\% accuracy, indicating that there is still significant room to improve for LLMs on MiMoTable. Furthermore, we grade the difficulty of existing benchmarks according to our new criteria. Experiments have shown that the performance of LLMs decreases as the difficulty of benchmarks increases, thereby proving the effectiveness of our proposed new criterion.

</details>


### [10] [SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation](https://arxiv.org/abs/2406.14991)
*Zeyao Ma,Bohan Zhang,Jing Zhang,Jifan Yu,Xiaokang Zhang,Xiaohan Zhang,Sijia Luo,Xi Wang,Jie Tang*

Main category: cs.CL

TL;DR: SpreadsheetBench是一个基于真实世界场景的电子表格处理基准测试，包含912个来自Excel论坛的真实问题和相应的电子表格，旨在评估大型语言模型（LLMs）在实际电子表格操作中的能力。该基准测试采用了类似在线评判平台的评估方法，并发现当前SOTA模型与人类表现之间存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有的电子表格处理基准测试多依赖于合成数据和简化的电子表格文件，未能充分反映真实世界用户在电子表格操作中遇到的复杂需求。因此，需要一个能够模拟真实用户工作流程、包含真实数据和复杂表格结构的基准测试来评估大型语言模型（LLMs）的能力。

Method: 构建了一个包含912个真实用户问题和相应电子表格的基准测试集SpreadsheetBench。这些数据来源于Excel在线论坛，模拟了真实的用户场景。在评估方法上，借鉴了在线评判平台的机制，为每个指令创建多个包含不同数据的电子表格文件作为测试用例，以评估模型处理各种数据变体的鲁棒性。最后，在单轮和多轮推理设置下，对多种LLMs进行了全面的评估。

Result: 通过在SpreadsheetBench基准测试上对多种LLMs进行评估，发现在单轮和多轮推理设置下，当前最先进（SOTA）的模型与人类的表现之间存在显著的性能差距。这表明SpreadsheetBench具有相当的挑战性，并且当前LLMs在处理真实世界的电子表格任务方面仍有很大的提升空间。

Conclusion: SpreadsheetBench基准测试的评估结果表明，尽管大型语言模型（LLMs）在电子表格处理方面取得了进展，但与人类的表现相比仍有较大差距。这强调了该基准测试的有效性，并为未来研究提供了方向，旨在缩小这一差距，使LLMs能够更好地满足真实世界用户的电子表格操作需求。

Abstract: We introduce SpreadsheetBench, a challenging spreadsheet manipulation benchmark exclusively derived from real-world scenarios, designed to immerse current large language models (LLMs) in the actual workflow of spreadsheet users. Unlike existing benchmarks that rely on synthesized queries and simplified spreadsheet files, SpreadsheetBench is built from 912 real questions gathered from online Excel forums, which reflect the intricate needs of users. The associated spreadsheets from the forums contain a variety of tabular data such as multiple tables, non-standard relational tables, and abundant non-textual elements. Furthermore, we propose a more reliable evaluation metric akin to online judge platforms, where multiple spreadsheet files are created as test cases for each instruction, ensuring the evaluation of robust solutions capable of handling spreadsheets with varying values. Our comprehensive evaluation of various LLMs under both single-round and multi-round inference settings reveals a substantial gap between the state-of-the-art (SOTA) models and human performance, highlighting the benchmark's difficulty.

</details>


### [11] [NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries](https://arxiv.org/abs/2402.14853)
*Wei Zhao,Zhitao Hou,Siyuan Wu,Yan Gao,Haoyu Dong,Yao Wan,Hongyu Zhang,Yulei Sui,Haidong Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一个名为NL2Formula的新基准任务，旨在根据自然语言查询生成电子表格公式，并提供了一个名为fCoder的基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有的电子表格公式编写方式对于复杂操作来说既繁琐又容易出错，需要一种更便捷的方法。

Method: 构建了一个包含70,799个查询-公式对的数据集，并实现了一个名为fCoder的序列到序列模型来解决NL2Formula任务。

Result: fCoder模型在NL2Formula任务上表现优于基线模型，并且与GPT-3.5（text-davinci-003）进行了比较。

Conclusion: NL2Formula任务具有挑战性，需要进一步研究，fCoder模型为该任务提供了一个有效的解决方案。

Abstract: Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets, is a widespread practice among users performing data analysis. However, crafting formulas on spreadsheets remains a tedious and error-prone task for many end-users, particularly when dealing with complex operations. To alleviate the burden associated with writing spreadsheet formulas, this paper introduces a novel benchmark task called NL2Formula, with the aim to generate executable formulas that are grounded on a spreadsheet table, given a Natural Language (NL) query as input. To accomplish this, we construct a comprehensive dataset consisting of 70,799 paired NL queries and corresponding spreadsheet formulas, covering 21,670 tables and 37 types of formula functions. We realize the NL2Formula task by providing a sequence-to-sequence baseline implementation called fCoder. Experimental results validate the effectiveness of fCoder, demonstrating its superior performance compared to the baseline models. Furthermore, we also compare fCoder with an initial GPT-3.5 model (i.e., text-davinci-003). Lastly, through in-depth error analysis, we identify potential challenges in the NL2Formula task and advocate for further investigation.

</details>


### [12] [InstructExcel: A Benchmark for Natural Language Instruction in Excel](https://arxiv.org/abs/2310.14495)
*Justin Payan,Swaroop Mishra,Mukul Singh,Carina Negreanu,Christian Poelitz,Chitta Baral,Subhro Roy,Rasika Chakravarthy,Benjamin Van Durme,Elnaz Nouri*

Main category: cs.CL

TL;DR: LLMs 可以在 Excel 中生成解决特定任务的代码 (OfficeScripts)，但 InstructExcel 仍然是一个具有挑战性的基准。


<details>
  <summary>Details</summary>
Motivation: 评估 LLMs 生成 Excel OfficeScripts 的能力，以响应自然语言指令，并创建一个新的基准来衡量这一能力。

Method: 使用 Excel 的 'Automate' 功能从用户操作中自动生成 OfficeScripts，创建了一个包含 10k 多个样本、涵盖 170 多个 Excel 操作的大型基准 InstructExcel。

Result: 在 InstructExcel 基准上，GPT-4 相较于 GPT-3.5 表现更好，并且增加上下文示例和动态提示可以进一步提高性能。

Conclusion: InstructExcel 是一个具有挑战性的基准，表明 LLMs 在生成 Excel 特定代码方面仍有改进空间，但 GPT-4 和一些提示技术可以提高性能。

Abstract: With the evolution of Large Language Models (LLMs) we can solve increasingly more complex NLP tasks across various domains, including spreadsheets. This work investigates whether LLMs can generate code (Excel OfficeScripts, a TypeScript API for executing many tasks in Excel) that solves Excel specific tasks provided via natural language user instructions. To do so we introduce a new large-scale benchmark, InstructExcel, created by leveraging the 'Automate' feature in Excel to automatically generate OfficeScripts from users' actions. Our benchmark includes over 10k samples covering 170+ Excel operations across 2,000 publicly available Excel spreadsheets. Experiments across various zero-shot and few-shot settings show that InstructExcel is a hard benchmark for state of the art models like GPT-4. We observe that (1) using GPT-4 over GPT-3.5, (2) providing more in-context examples, and (3) dynamic prompting can help improve performance on this benchmark.

</details>


### [13] [Active Learning with Tabular Language Models](https://arxiv.org/abs/2211.04128)
*Martin Ringsquandl,Aneta Koleva*

Main category: cs.CL

TL;DR: 尽管在表格语言模型研究方面取得了最新进展，但实际应用仍然充满挑战。在工业界，电子表格中存在大量表格，但获取大量标签的成本很高，因为只有专家才能注释通常技术性强且特定于领域的表格。主动学习有可能降低标注成本，但是，到目前为止，还没有与表格语言模型结合使用主动学习的相关工作。在本文中，我们针对单元格内命名实体识别的一个实际工业表格语言模型用例，研究了不同的采集函数。我们的结果表明，内置了多样性的单元格级采集函数可以显着减少标注工作量，而强制性的表格多样性则是有害的。我们还看到了有关计算效率和人类注释者视角的开放性基本问题。


<details>
  <summary>Details</summary>
Motivation: 工业界存在大量表格，但获取专业知识标注的成本高昂，因此需要主动学习来降低标注成本。

Method: 针对单元格内命名实体识别的实际工业表格语言模型用例，研究了不同的采集函数。

Result: 内置了多样性的单元格级采集函数可以显着减少标注工作量，而强制性的表格多样性则是有害的。

Conclusion: 主动学习，特别是单元格级采集函数，在降低表格语言模型标注成本方面显示出潜力，但仍需解决计算效率和注释者体验等问题。

Abstract: Despite recent advancements in tabular language model research, real-world applications are still challenging. In industry, there is an abundance of tables found in spreadsheets, but acquisition of substantial amounts of labels is expensive, since only experts can annotate the often highly technical and domain-specific tables. Active learning could potentially reduce labeling costs, however, so far there are no works related to active learning in conjunction with tabular language models. In this paper we investigate different acquisition functions in a real-world industrial tabular language model use case for sub-cell named entity recognition. Our results show that cell-level acquisition functions with built-in diversity can significantly reduce the labeling effort, while enforced table diversity is detrimental. We further see open fundamental questions concerning computational efficiency and the perspective of human annotators.

</details>


### [14] [Table-To-Text generation and pre-training with TabT5](https://arxiv.org/abs/2210.09162)
*Ewa Andrejczuk,Julian Martin Eisenschlos,Francesco Piccinno,Syrine Krichene,Yasemin Altun*

Main category: cs.CL

TL;DR: TABT5是一个新的编码器-解码器模型，在表格理解任务中取得了最先进的成果，包括公式预测、问答和数据到文本生成。


<details>
  <summary>Details</summary>
Motivation: 编码器-仅Transformer模型在表格理解任务中存在局限性，仅限于类似分类的任务。TABT5旨在通过引入解码器来克服这一限制。

Method: TABT5是一个编码器-解码器模型，它结合了文本和表格输入，并使用了表格特定的嵌入和预训练方法。

Result: TABT5在电子表格公式预测（序列准确率提高15%）、问答（序列准确率提高2.5%）和数据到文本生成（BLEU提高2.5%）等任务上取得了新的最先进成果。

Conclusion: TABT5通过结合解码器组件和利用表格结构，克服了现有编码器-仅模型的局限性，并在多项表格理解任务上取得了显著的性能提升。

Abstract: Encoder-only transformer models have been successfully applied to different table understanding tasks, as in TAPAS (Herzig et al., 2020). A major limitation of these architectures is that they are constrained to classification-like tasks such as cell selection or entailment detection. We present TABT5, an encoder-decoder model that generates natural language text based on tables and textual inputs. TABT5 overcomes the encoder-only limitation by incorporating a decoder component and leverages the input structure with table specific embeddings and pre-training. TABT5 achieves new state-of-the-art results on several domains, including spreadsheet formula prediction with a 15% increase in sequence accuracy, QA with a 2.5% increase in sequence accuracy and data-to-text generation with a 2.5% increase in BLEU.

</details>


### [15] [Table Pre-training: A Survey on Model Architectures, Pre-training Objectives, and Downstream Tasks](https://arxiv.org/abs/2201.09745)
*Haoyu Dong,Zhoujun Cheng,Xinyi He,Mengyu Zhou,Anda Zhou,Fan Zhou,Ao Liu,Shi Han,Dongmei Zhang*

Main category: cs.CL

TL;DR: 基于文本和图像的预训练框架的成功，表格预训练框架应运而生，并在表格问答、表格类型识别、列关系分类、表格搜索、公式预测等任务上取得了新的进展。为了充分利用无标签表格中的监督信号，研究者们设计了多种预训练目标，例如，对单元格值进行去噪、预测数值关系以及隐式执行SQL。同时，为了最好地利用（半）结构化表格的特性，研究者们探索了各种表格语言模型，特别是具有特殊设计的注意力机制。由于表格通常与自由形式的文本一起出现并进行交互，因此表格预训练通常采用表格-文本联合预训练的形式，这引起了多个领域的研究兴趣。本调查旨在全面回顾表格预训练的不同模型设计、预训练目标和下游任务，并进一步分享我们对现有挑战和未来机遇的思考和愿景。


<details>
  <summary>Details</summary>
Motivation: 表格在网页、电子表格、PDF等各种文档类型中大量存在，为了充分利用这些数据，表格预训练框架被提出并应用于多种下游任务。

Method: 通过设计多种预训练目标（如单元格去噪、数值关系预测、SQL执行）和表格语言模型（特别是具有特殊注意力机制的模型），并采用表格-文本联合预训练的方式进行。

Result: 表格预训练框架在表格问答、表格类型识别、列关系分类、表格搜索、公式预测等任务上取得了新的进展。

Conclusion: 本调查全面回顾了表格预训练的模型设计、预训练目标和下游任务，并对现有挑战和未来机遇进行了探讨。

Abstract: Since a vast number of tables can be easily collected from web pages, spreadsheets, PDFs, and various other document types, a flurry of table pre-training frameworks have been proposed following the success of text and images, and they have achieved new state-of-the-arts on various tasks such as table question answering, table type recognition, column relation classification, table search, formula prediction, etc. To fully use the supervision signals in unlabeled tables, a variety of pre-training objectives have been designed and evaluated, for example, denoising cell values, predicting numerical relationships, and implicitly executing SQLs. And to best leverage the characteristics of (semi-)structured tables, various tabular language models, particularly with specially-designed attention mechanisms, have been explored. Since tables usually appear and interact with free-form text, table pre-training usually takes the form of table-text joint pre-training, which attracts significant research interests from multiple domains. This survey aims to provide a comprehensive review of different model designs, pre-training objectives, and downstream tasks for table pre-training, and we further share our thoughts and vision on existing challenges and future opportunities.

</details>


### [16] [TableQuery: Querying tabular data with natural language](https://arxiv.org/abs/2202.00454)
*Abhijith Neil Abraham,Fariz Rahman,Damanpreet Kaur*

Main category: cs.CL

TL;DR: TableQuery 使用预训练的深度学习模型将自然语言查询转换为结构化查询，以查询表格数据，解决了现有方法需要将整个表加载到内存的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有表格数据问答的深度学习方法存在局限性，如需要将整个表作为输入，不适用于现实世界中可能包含数百万行且实时更新的数据库。

Method: 使用预训练的、用于自由文本问答的深度学习模型，将自然语言查询转换为可执行的结构化查询，无需将整个数据加载到内存或序列化数据库。

Result: TableQuery 能够高效查询大型表格数据，并且可以方便地替换和更新底层模型，而无需重新训练。

Conclusion: TableQuery 是一种创新的表格数据查询工具，通过利用预训练的语言模型，克服了现有方法的局限性，提高了效率和实用性。

Abstract: This paper presents TableQuery, a novel tool for querying tabular data using deep learning models pre-trained to answer questions on free text. Existing deep learning methods for question answering on tabular data have various limitations, such as having to feed the entire table as input into a neural network model, making them unsuitable for most real-world applications. Since real-world data might contain millions of rows, it may not entirely fit into the memory. Moreover, data could be stored in live databases, which are updated in real-time, and it is impractical to serialize an entire database to a neural network-friendly format each time it is updated. In TableQuery, we use deep learning models pre-trained for question answering on free text to convert natural language queries to structured queries, which can be run against a database or a spreadsheet. This method eliminates the need for fitting the entire data into memory as well as serializing databases. Furthermore, deep learning models pre-trained for question answering on free text are readily available on platforms such as HuggingFace Model Hub (7). TableQuery does not require re-training; when a newly trained model for question answering with better performance is available, it can replace the existing model in TableQuery.

</details>


### [17] [The Impact of Text Presentation on Translator Performance](https://arxiv.org/abs/2011.05978)
*Samuel Läubli,Patrick Simianer,Joern Wuebker,Geza Kovacs,Rico Sennrich,Spence Green*

Main category: cs.CL

TL;DR: CAT工具的常用设计，如句子分割和左右视图，对翻译人员的速度和准确性有显著影响。句子分割提高了文本复制速度和句子内错误识别，而上下视图比左右视图更快。然而，在修订任务中，未分割的文本在准确性和时间效率方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 评估计算机辅助翻译（CAT）工具的设计选择（如句子分割和视图布局）对翻译人员速度和准确性的影响。

Method: 进行了三项实验性文本处理任务，以测量翻译人员在不同分割和视图配置下的速度和准确性。

Result: 句子分割比未分割文本能更快地进行文本复制，并能更快地识别句子内的错误。上下视图比左右视图能更快地进行文本复制。在修订任务中，未分割文本的准确性和时间效率最高。

Conclusion: CAT工具的设计选择，如句子分割和视图布局，会对翻译人员的表现产生重要影响。句子分割和上下视图在某些任务中可以提高效率，但在修订任务中，未分割文本可能更优。

Abstract: Widely used computer-aided translation (CAT) tools divide documents into segments such as sentences and arrange them in a side-by-side, spreadsheet-like view. We present the first controlled evaluation of these design choices on translator performance, measuring speed and accuracy in three experimental text processing tasks. We find significant evidence that sentence-by-sentence presentation enables faster text reproduction and within-sentence error identification compared to unsegmented text, and that a top-and-bottom arrangement of source and target sentences enables faster text reproduction compared to a side-by-side arrangement. For revision, on the other hand, our results suggest that presenting unsegmented text results in the highest accuracy and time efficiency. Our findings have direct implications for best practices in designing CAT tools.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [18] [Using the Abstract Computer Architecture Description Language to Model AI Hardware Accelerators](https://arxiv.org/abs/2402.00069)
*Mika Markus Müller,Alexander Richard Manfred Borst,Konstantin Lübeck,Alexander Louis-Ferdinand Jung,Oliver Bringmann*

Main category: cs.AR

TL;DR: AI硬件加速器需要根据产品需求进行选择和配置，但现有评估方法效率低下。本文提出使用ACADL来建模AI硬件加速器，将DNN映射到加速器上，并通过模拟获取性能。


<details>
  <summary>Details</summary>
Motivation: 制造商在选择和配置AI硬件加速器时面临挑战，需要一种更有效的方法来评估不同设计方案的性能。

Method: 使用ACADL建模AI硬件加速器，将DNN映射到加速器，并解释模拟语义以收集性能数据。

Result: 通过ACADL模型和模拟，能够更有效地评估AI硬件加速器的性能。

Conclusion: ACADL是一种有前景的方法，可以简化AI硬件加速器的选择和配置过程，从而更好地满足产品性能需求。

Abstract: Artificial Intelligence (AI) has witnessed remarkable growth, particularly through the proliferation of Deep Neural Networks (DNNs). These powerful models drive technological advancements across various domains. However, to harness their potential in real-world applications, specialized hardware accelerators are essential. This demand has sparked a market for parameterizable AI hardware accelerators offered by different vendors.
  Manufacturers of AI-integrated products face a critical challenge: selecting an accelerator that aligns with their product's performance requirements. The decision involves choosing the right hardware and configuring a suitable set of parameters. However, comparing different accelerator design alternatives remains a complex task. Often, engineers rely on data sheets, spreadsheet calculations, or slow black-box simulators, which only offer a coarse understanding of the performance characteristics.
  The Abstract Computer Architecture Description Language (ACADL) is a concise formalization of computer architecture block diagrams, which helps to communicate computer architecture on different abstraction levels and allows for inferring performance characteristics. In this paper, we demonstrate how to use the ACADL to model AI hardware accelerators, use their ACADL description to map DNNs onto them, and explain the timing simulation semantics to gather performance results.

</details>


### [19] [Online Model Swapping in Architectural Simulation](https://arxiv.org/abs/2012.01571)
*Patrick Lavin,Jeffrey Young,Rich Vuduc,Jonathan Beard*

Main category: cs.AR

TL;DR: 详细模拟耗时过长，导致设计迭代缓慢，迫使架构师使用简单的模型（如电子表格）。然而，从简单模拟迁移到详细模拟需要多次执行来确定简单模型的有效性，这可能比直接运行详细模型更昂贵。此外，架构师必须依靠直觉来选择简单的模型，这使得问题更加复杂。本研究提出了一种在线监控模拟行为并自动用更简单的统计模型替换详细模型的方法，以弥合简单模拟和详细模拟之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当系统和应用程序变得越来越复杂时，详细的模拟需要越来越长的时间。模拟时间增加可能导致设计迭代变慢，迫使架构师使用更简单的模型（例如电子表格）来快速迭代设计。然而，将简单模拟迁移到更详细的模拟的任务通常需要多次执行才能找到简单模型有效的区域，而这可能比直接运行详细模型本身更昂贵。此外，架构师通常必须依靠直觉来选择这些简单的模型，这使得问题更加复杂。

Method: 本研究提出了一种通过在线监控模拟行为并自动用更简单的统计近似替换详细模型来弥合简单模拟和详细模拟之间差距的方法。

Result: 通过在开源模拟器SVE-Cachesim中实现该方法，并用统计模型替换一级数据缓存（L1D），验证了该方法的潜力。该概念验证表明，该技术不仅可以处理局部时间不变统计数据的近似，还可以处理随时间变化的统计数据（例如，L1D是时间序列函数的一种形式）以及下游的副作用（例如，L1D过滤掉对二级缓存的访问）。我们的模拟用近似缓存模型替换了内置缓存模型，在模拟周期计数方面仅有8%的误差，并且在超过90%的模拟中使用了近似缓存模型，而我们更简单的模型每次“执行”模型所需的计算量要少两到八倍。

Conclusion: 本研究提出了一种通过在线监控模拟行为并自动用更简单的统计近似替换详细模型来弥合简单模拟和详细模拟之间差距的方法。该方法在SVE-Cachesim中得到了验证，并在近似缓存模型方面取得了良好的效果，同时大大减少了计算量。

Abstract: As systems and applications grow more complex, detailed simulation takes an ever increasing amount of time. The prospect of increased simulation time resulting in slower design iteration forces architects to use simpler models, such as spreadsheets, when they want to iterate quickly on a design. However, the task of migrating from a simple simulation to one with more detail often requires multiple executions to find where simple models could be effective, which could be more expensive than running the detailed model in the first place. Also, architects must often rely on intuition to choose these simpler models, further complicating the problem.
  In this work, we present a method of bridging the gap between simple and detailed simulation by monitoring simulation behavior online and automatically swapping out detailed models with simpler statistical approximations. We demonstrate the potential of our methodology by implementing it in the open-source simulator SVE-Cachesim to swap out the level one data cache (L1D) within a memory hierarchy. This proof of concept demonstrates that our technique can handle a non-trivial use-case in not just approximation of local time-invariant statistics, but also those that vary with time (e.g., the L1D is a form of a time-series function), and downstream side-effects (e.g., the L1D filters accesses for the level two cache). Our simulation swaps out the built-in cache model with only an 8% error in the simulated cycle count while using the approximated cache models for over 90% of the simulation, and our simpler models require two to eight times less computation per "execution" of the model

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [20] [Hillview: A trillion-cell spreadsheet for big data](https://arxiv.org/abs/1907.04827)
*Mihai Budiu,Parikshit Gopalan,Lalith Suresh,Udi Wieder,Han Kruiger,Marcos K. Aguilera*

Main category: cs.DC

TL;DR: Hillview 是一个分布式电子表格系统，用于浏览无法由单台机器处理的大型数据集。它引入了 vizketches（可视化草图）技术，通过算法数据摘要和高效渲染相结合，实现了对海量数据的快速交互式探索和可视化，性能远超现有系统。


<details>
  <summary>Details</summary>
Motivation: 提供一个能够处理单机无法处理的大型数据集的分布式电子表格系统，并保持高交互性，以支持数据分析师快速探索信息。

Method: 引入 vizketches（可视化草图），这是一种结合了数据摘要算法和计算机图形学原理的紧凑数据可视化技术。Vizketches 通过并行计算、减少通信、提供渐进式可视化和精确的准确性保证来扩展电子表格的功能。

Result: 使用 Hillview 在八台服务器上，能够导航和可视化包含数十亿行和数万亿单元格的数据集，其能力超越了现有竞争系统。

Conclusion: Vizketches 是一种简单但有效的技术，能够通过多种方式（并行计算、减少通信、渐进式可视化、精确的准确性保证）扩展电子表格的功能，使得 Hillview 能够处理和可视化比现有系统更大规模的数据集。

Abstract: Hillview is a distributed spreadsheet for browsing very large datasets that cannot be handled by a single machine. As a spreadsheet, Hillview provides a high degree of interactivity that permits data analysts to explore information quickly along many dimensions while switching visualizations on a whim. To provide the required responsiveness, Hillview introduces visualization sketches, or vizketches, as a simple idea to produce compact data visualizations. Vizketches combine algorithmic techniques for data summarization with computer graphics principles for efficient rendering. While simple, vizketches are effective at scaling the spreadsheet by parallelizing computation, reducing communication, providing progressive visualizations, and offering precise accuracy guarantees. Using Hillview running on eight servers, we can navigate and visualize datasets of tens of billions of rows and trillions of cells, much beyond the published capabilities of competing systems.

</details>


### [21] [Streaming 1.9 Billion Hypersparse Network Updates per Second with D4M](https://arxiv.org/abs/1907.04217)
*Jeremy Kepner,Vijay Gadepally,Lauren Milechin,Siddharth Samsi,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Michael Houle,Michael Jones,Anne Klein,Peter Michaleas,Julie Mullen,Andrew Prout,Antonio Rosa,Charles Yee,Albert Reuther*

Main category: cs.DC

TL;DR: D4M库实现了支持多种语言（Python、Julia和Matlab/Octave）的关联数组，并提供了一个轻量级的内存数据库，用于分析网络数据。


<details>
  <summary>Details</summary>
Motivation: D4M库在处理网络数据时，其关联数组的流式更新对内存层次结构造成巨大压力，因此需要对实现进行设计和性能优化。

Method: 通过实现分层关联数组来减少内存压力，并显著提高更新速率。分层关联数组的参数可以通过调整每层中的条目数来控制，以实现最优性能。

Result: 单实例分层关联数组的更新速率超过每秒40,000次。在MIT SuperCloud的1,100个服务器节点上扩展到34,000个分层D4M关联数组实例，实现了每秒1,900,000,000次的持续更新速率。

Conclusion: 分层关联数组的实现显著提高了D4M处理大规模流式网络数据的能力，使MIT SuperCloud能够分析极大的流式网络数据集。

Abstract: The Dynamic Distributed Dimensional Data Model (D4M) library implements associative arrays in a variety of languages (Python, Julia, and Matlab/Octave) and provides a lightweight in-memory database implementation of hypersparse arrays that are ideal for analyzing many types of network data. D4M relies on associative arrays which combine properties of spreadsheets, databases, matrices, graphs, and networks, while providing rigorous mathematical guarantees, such as linearity. Streaming updates of D4M associative arrays put enormous pressure on the memory hierarchy. This work describes the design and performance optimization of an implementation of hierarchical associative arrays that reduces memory pressure and dramatically increases the update rate into an associative array. The parameters of hierarchical associative arrays rely on controlling the number of entries in each level in the hierarchy before an update is cascaded. The parameters are easily tunable to achieve optimal performance for a variety of applications. Hierarchical arrays achieve over 40,000 updates per second in a single instance. Scaling to 34,000 instances of hierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of 1,900,000,000 updates per second. This capability allows the MIT SuperCloud to analyze extremely large streaming network data sets.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [22] [Data-Semantics-Aware Recommendation of Diverse Pivot Tables](https://arxiv.org/abs/2507.06171)
*Whanhee Cho,Anna Fariha*

Main category: cs.DB

TL;DR: SAGE是一个能推荐k个不同且有用的数据透视表的系统，克服了现有方法在处理高维数据时的不足。


<details>
  <summary>Details</summary>
Motivation: 识别有用的数据透视表组合是一个挑战，尤其是在高维数据集上，现有的方法在解决表多样性问题时存在不足。

Method: SAGE使用一个数据语义感知模型来评估单个数据透视表的效用和数据透视表集合的多样性，并采用一个可扩展的贪婪算法来选择高效用且多样化的数据透视表。

Result: SAGE在真实数据集上的实验表明，它比替代方法表现更好，并且能够有效地处理高维数据集。案例研究也证明了SAGE在定性方面的优势。

Conclusion: SAGE通过其数据语义感知模型和可扩展的贪婪算法，能够有效地推荐多样化且有用的数据透视表，克服了现有方法的局限性，并在处理高维数据集方面表现出色。

Abstract: Data summarization is essential to discover insights from large datasets. In a spreadsheets, pivot tables offer a convenient way to summarize tabular data by computing aggregates over some attributes, grouped by others. However, identifying attribute combinations that will result in useful pivot tables remains a challenge, especially for high-dimensional datasets. We formalize the problem of automatically recommending insightful and interpretable pivot tables, eliminating the tedious manual process. A crucial aspect of recommending a set of pivot tables is to diversify them. Traditional works inadequately address the table-diversification problem, which leads us to consider the problem of pivot table diversification.
  We present SAGE, a data-semantics-aware system for recommending k-budgeted diverse pivot tables, overcoming the shortcomings of prior work for top-k recommendations that cause redundancy. SAGE ensures that each pivot table is insightful, interpretable, and adaptive to the user's actions and preferences, while also guaranteeing that the set of pivot tables are different from each other, offering a diverse recommendation. We make two key technical contributions: (1) a data-semantics-aware model to measure the utility of a single pivot table and the diversity of a set of pivot tables, and (2) a scalable greedy algorithm that can efficiently select a set of diverse pivot tables of high utility, by leveraging data semantics to significantly reduce the combinatorial search space. Our extensive experiments on three real-world datasets show that SAGE outperforms alternative approaches, and efficiently scales to accommodate high-dimensional datasets. Additionally, we present several case studies to highlight SAGE's qualitative effectiveness over commercial software and Large Language Models (LLMs).

</details>


### [23] [Facilitating Digital Agriculture with Simple Databases](https://arxiv.org/abs/2312.06517)
*Dennis Buckmaster,Sami Basir,Hanae Sakata*

Main category: cs.DB

TL;DR: 本研究提供开源的数据库模板，帮助农业工作者（尤其是电子表格技能有限者）更好地收集和管理农业数据。


<details>
  <summary>Details</summary>
Motivation: 为农业工作者提供易于使用的数据库解决方案，以收集、管理和分析运营数据，促进数字农业的推广。

Method: 提供结构化的、开源的、易于使用的Airtable数据库模板，具备数据验证功能和类似定制应用程序的界面，以便收集、整理、可读、可编辑和可导出的运营数据。同时提供一个解释如何构建活动记录数据库的研讨会录像。

Result: 农业工作者可以获得整洁、机器和人类可读、可编辑且可导出的运营数据，从而促进物流、提供上下文元数据和改进企业分析。

Conclusion: 这些资源可以帮助推广和结构化教育项目，从而促进数字农业原理的融入。

Abstract: As an on-ramp to databases, we offer several well-structured private database templates as open source resources for agriculturalists, particularly those with modest spreadsheet skills. These farmer-oriented Air table databases use simple data-validated forms, with the look and feel of a customized app, to yield operational data that is tidy, machine- and human-readable, editable, and exportable for analysis in other software. Such data can facilitate logistics, provide contextual metadata, and improve enterprise analysis. A recorded workshop explaining how to build a database for activity records is presented. These resources may facilitate infusion of digital agriculture principles through Extension and structured educational programming.

</details>


### [24] [Streamlining Knowledge Graph Construction with a façade: The SPARQL Anything project](https://arxiv.org/abs/2310.16700)
*Luigi Asprino,Enrico Daga,Justin Dowdy,Paul Mulholland,Aldo Gangemi,Marco Ratta*

Main category: cs.DB

TL;DR: SPARQL Anything是一个用于查询异构资源的框架，通过重载SERVICE子句支持多种文件格式和API查询，并提供设计原理、软件架构和用户价值的分析。


<details>
  <summary>Details</summary>
Motivation: 探索用于知识工程师的数据集成框架的设计，特别是借鉴面向对象软件工程中的'外观'概念来构建知识图谱。

Method: 描述SPARQL Anything系统的设计原理和软件架构，该系统通过重载SERVICE子句，允许使用SPARQL 1.1查询非RDF资源。支持多种文件格式（CSV, JSON, XML, Markdown, YAML, DOCx, Bibtex等）、Web API查询、参数化查询和复杂管道。

Result: 展示了SPARQL Anything在各种应用领域中的实际应用场景，并通过社区调查和行业现场报告，评估了其设计基础假设相对于替代解决方案的价值。

Conclusion: SPARQL Anything系统通过其灵活的特性和广泛的格式支持，为知识工程师提供了一个强大的数据集成解决方案，并在实际应用中得到了验证。

Abstract: What should a data integration framework for knowledge engineers look like? Recent research on Knowledge Graph construction proposes the design of a façade, a notion borrowed from object-oriented software engineering. This idea is applied to SPARQL Anything, a system that allows querying heterogeneous resources as-if they were in RDF, in plain SPARQL 1.1, by overloading the SERVICE clause. SPARQL Anything supports a wide variety of file formats, from popular ones (CSV, JSON, XML, Spreadsheets) to others that are not supported by alternative solutions (Markdown, YAML, DOCx, Bibtex). Features include querying Web APIs with high flexibility, parametrised queries, and chaining multiple transformations into complex pipelines. In this paper, we describe the design rationale and software architecture of the SPARQL Anything system. We provide references to an extensive set of reusable, real-world scenarios from various application domains. We report on the value-to-users of the founding assumptions of its design, compared to alternative solutions through a community survey and a field report from the industry.

</details>


### [25] [DataVinci: Learning Syntactic and Semantic String Repairs](https://arxiv.org/abs/2308.10922)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Gust Verbruggen*

Main category: cs.DB

TL;DR: DataVinci是一个无监督的字符串数据错误检测和修复系统，它学习基于正则表达式的模式来识别和修复错误，并利用LLM处理具有语义的字符串，同时结合程序执行信息来提高修复效果，在评估中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有字符串数据清理方法在处理语法和语义错误方面存在不足，且通常需要用户交互，而本文旨在开发一个全自动的无监督系统来解决这些问题。

Method: DataVinci首先学习列值中的基于正则表达式的模式，将不符合模式的值识别为错误。然后，它利用其他列的约束自动推导编辑。为了处理包含语义部分的字符串，DataVinci使用LLM来抽象和具体化这些部分。此外，它还利用现有程序的执行信息来识别和修复无法通过模式学习识别的错误。

Result: DataVinci在错误检测和修复方面优于7个基线方法，并在4个现有和新的基准测试中得到了验证。

Conclusion: DataVinci是一个有效的全无监督字符串数据错误检测和修复系统，通过结合正则表达式模式学习、LLM和程序执行信息，能够处理复杂的字符串错误，并且无需用户交互，在实际应用中具有重要价值。

Abstract: String data is common in real-world datasets: 67.6% of values in a sample of 1.8 million real Excel spreadsheets from the web were represented as text. Systems that successfully clean such string data can have a significant impact on real users. While prior work has explored errors in string data, proposed approaches have often been limited to error detection or require that the user provide annotations, examples, or constraints to fix the errors. Furthermore, these systems have focused independently on syntactic errors or semantic errors in strings, but ignore that strings often contain both syntactic and semantic substrings. We introduce DataVinci, a fully unsupervised string data error detection and repair system. DataVinci learns regular-expression-based patterns that cover a majority of values in a column and reports values that do not satisfy such patterns as data errors. DataVinci can automatically derive edits to the data error based on the majority patterns and constraints learned over other columns without the need for further user interaction. To handle strings with both syntactic and semantic substrings, DataVinci uses an LLM to abstract (and re-concretize) portions of strings that are semantic prior to learning majority patterns and deriving edits. Because not all data can result in majority patterns, DataVinci leverages execution information from an existing program (which reads the target data) to identify and correct data repairs that would not otherwise be identified. DataVinci outperforms 7 baselines on both error detection and repair when evaluated on 4 existing and new benchmarks.

</details>


### [26] [Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples](https://arxiv.org/abs/2307.14565)
*Peng Li,Yeye He,Cong Yan,Yue Wang,Surajit Chaudhuri*

Main category: cs.DB

TL;DR: 超过30%的真实表格不符合关系标准，需要进行复杂的重构。我们开发了Auto-Tables系统，可以自动合成转换流程，将非关系表转换为标准关系表，成功率超过70%。


<details>
  <summary>Details</summary>
Motivation: 处理“野生”表格时，超过30%的表格不符合关系标准，需要进行复杂的重构，这给技术和非技术用户带来了困难。

Method: 开发了Auto-Tables系统，可以自动合成多步转换流程（Python或其他语言），将非关系表转换为标准关系表，无需用户手动编程。

Result: 在包含244个真实案例的基准测试中，Auto-Tables成功合成了超过70%的转换流程，并且处理速度快。

Conclusion: Auto-Tables是一个有效的工具，可以帮助技术和非技术用户准备数据进行分析，它能自动合成转换流程，将非关系表转换为标准关系表。

Abstract: Relational tables, where each row corresponds to an entity and each column corresponds to an attribute, have been the standard for tables in relational databases. However, such a standard cannot be taken for granted when dealing with tables "in the wild". Our survey of real spreadsheet-tables and web-tables shows that over 30% of such tables do not conform to the relational standard, for which complex table-restructuring transformations are needed before these tables can be queried easily using SQL-based analytics tools. Unfortunately, the required transformations are non-trivial to program, which has become a substantial pain point for technical and non-technical users alike, as evidenced by large numbers of forum questions in places like StackOverflow and Excel/Power-BI/Tableau forums.
  We develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations (in Python or other languages), to transform non-relational tables into standard relational forms for downstream analytics, obviating the need for users to manually program transformations. We compile an extensive benchmark for this new task, by collecting 244 real test cases from user spreadsheets and online forums. Our evaluation suggests that Auto-Tables can successfully synthesize transformations for over 70% of test cases at interactive speeds, without requiring any input from users, making this an effective tool for both technical and non-technical users to prepare data for analytics.

</details>


### [27] [Efficient and Compact Spreadsheet Formula Graphs](https://arxiv.org/abs/2302.05482)
*Dixin Tang,Fanchao Chen,Christopher De Leon,Tana Wattanawaroon,Jeaseok Yun,Srinivasan Seshadri,Aditya G. Parameswaran*

Main category: cs.DB

TL;DR: TACO框架通过利用表格局部性来压缩公式图，显著提高了电子表格的查询和维护效率。


<details>
  <summary>Details</summary>
Motivation: 电子表格中的公式图通常庞大而复杂，导致查询和维护耗时，影响交互性。

Method: TACO框架利用表格局部性（相邻单元格具有相似的公式结构）的特性，识别四种基于表格局部性的模式，并开发了相应的压缩算法。这些算法可以直接在压缩后的图上进行查询，无需解压，并能增量式地维护图的更新。

Result: TACO框架显著减小了公式图的大小。在查询公式图方面，与基线实现和商业电子表格系统相比，TACO的速度分别提高了34,972倍和632倍。

Conclusion: TACO框架通过压缩公式图，有效解决了电子表格中查询和维护效率低下的问题，显著提升了用户体验。

Abstract: Spreadsheets are one of the most popular data analysis tools, wherein users can express computation as formulae alongside data. The ensuing dependencies are tracked as formula graphs. Efficiently querying and maintaining these formula graphs is critical for interactivity across multiple settings. Unfortunately, formula graphs are often large and complex such that querying and maintaining them is time-consuming, reducing interactivity. We propose TACO, a framework for efficiently compressing formula graphs, thereby reducing the time for querying and maintenance. The efficiency of TACO stems from a key spreadsheet property: tabular locality, which means that cells close to each other are likely to have similar formula structures. We leverage four such tabular locality-based patterns and develop algorithms for compressing formula graphs using these patterns, directly querying the compressed graph without decompression, and incrementally maintaining the graph during updates. We integrate TACO into an open-source spreadsheet system and show that TACO can significantly reduce formula graph sizes. For querying formula graphs, the speedups of TACO over a baseline implemented in our framework and a commercial spreadsheet system are up to 34,972x and 632x, respectively.

</details>


### [28] [Sigma Workbook: A Spreadsheet for Cloud Data Warehouses](https://arxiv.org/abs/2204.03128)
*James Gale,Max Seiden,Deepanshu Utkarsh,Jason Frantz,Rob Woollen,Çağatay Demiralp*

Main category: cs.DB

TL;DR: Sigma Workbook是一个交互式系统，让业务用户能够轻松地对云数据仓库中的大规模数据进行可视化分析，它提供类似电子表格的界面，通过直接操作动态构建SQL查询，并在云数据仓库上执行。


<details>
  <summary>Details</summary>
Motivation: 现有用于分析云数据仓库（CDW）中数据的工具，要么在即席转换方面功能有限，要么对业务用户来说难以使用。

Method: Sigma Workbook提供了一个易于访问的、类似电子表格的界面，用于通过直接操作进行分析。它根据用户交互动态地构建匹配的SQL查询，并直接在CDW上执行这些查询，从而利用了新一代CDW的可扩展性等优点。

Result: 通过三个实际用例（队列分析、会话化和数据增强）展示了Sigma Workbook的易用性、可扩展性和表现力。

Conclusion: Sigma Workbook能够让业务用户轻松地对云数据仓库中的大规模数据进行可视化分析。

Abstract: Cloud data warehouses (CDWs) bring large-scale data and compute power closer to users in enterprises. However, existing tools for analyzing data in CDWs are either limited in ad-hoc transformations or difficult to use for business users. Here we introduce Sigma Workbook, a new interactive system that enables business users to easily perform a visual analysis of data in CDWs at scale. For this, Sigma Workbook provides an accessible spreadsheet-like interface for analysis through direct manipulation. Sigma Workbook dynamically constructs matching SQL queries from user interactions, building on the versatility and expressivity of SQL. Constructed queries are directly executed on CDWs, leveraging the superior characteristics of the new generation CDWs, including scalability. We demonstrate Sigma Workbook through 3 real-life use cases -- cohort analysis, sessionization, and data augmentation -- and underline Workbook's ease of use, scalability, and expressivity.

</details>


### [29] [Efficient Specialized Spreadsheet Parsing for Data Science](https://arxiv.org/abs/2202.13189)
*Felix Henze,Haralampos Gavriilidis,Eleni Tzirita Zacharatou,Volker Markl*

Main category: cs.DB

TL;DR: 通过将解压与解析相结合并使用并行化来优化电子表格加载，显著减少了内存和时间消耗。


<details>
  <summary>Details</summary>
Motivation: 当前的电子表格加载方法在运行时和内存使用方面效率低下，阻碍了在普通计算机上进行数据探索。

Method: 开发了一种新的解析器，通过将解压和解析紧密耦合来最小化内存使用，并引入了针对电子表格优化的解析例程和并行处理技术。

Result: 实现的用于将 Excel 电子表格加载到 R 环境中的原型，与现有方法相比，速度提高了 3 倍，内存消耗减少了 40 倍。

Conclusion: 所提出的方法通过有效的内存管理和并行处理，使得在普通计算机上进行电子表格数据探索更加实用高效。

Abstract: Spreadsheets are widely used for data exploration. Since spreadsheet systems have limited capabilities, users often need to load spreadsheets to other data science environments to perform advanced analytics. However, current approaches for spreadsheet loading suffer from either high runtime or memory usage, which hinders data exploration on commodity systems. To make spreasheet loading practical on commodity systems, we introduce a novel parser that minimizes memory usage by tightly coupling decompression and parsing. Furthermore, to reduce the runtime, we introduce optimized spreadsheet-specific parsing routines and employ parallelism. To evaluate our approach, we implement a prototype for loading Excel spreadsheets into R environments. Our evaluation shows that our novel approach is up to 3x faster while consuming up to 40x less memory than state-of-the-art approaches.

</details>


### [30] [Spread2RML: Constructing Knowledge Graphs by Predicting RML Mappings on Messy Spreadsheets](https://arxiv.org/abs/2110.12829)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: Spread2RML can automatically predict RML mappings for messy spreadsheets, reducing manual effort in transforming semi-structured data to RDF knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: Mapping semi-structured data, especially complex and messy spreadsheet tables, to RDF knowledge graphs using RML is time-consuming. This paper aims to reduce this effort by automatically predicting RML mappings for spreadsheets.

Method: Spread2RML predicts RML mappings by applying an extensible set of RML object map templates to each column of a spreadsheet based on heuristics.

Result: The evaluation using three datasets (messy synthetic data to less messy data.gov spreadsheets) showed promising results, particularly in the system's ability to be fully automatic and handle messy data.

Conclusion: Spread2RML offers a promising automatic approach to generating RML mappings from messy spreadsheets, significantly reducing the effort required for data transformation.

Abstract: The RDF Mapping Language (RML) allows to map semi-structured data to RDF knowledge graphs. Besides CSV, JSON and XML, this also includes the mapping of spreadsheet tables. Since spreadsheets have a complex data model and can become rather messy, their mapping creation tends to be very time consuming. In order to reduce such efforts, this paper presents Spread2RML which predicts RML mappings on messy spreadsheets. This is done with an extensible set of RML object map templates which are applied for each column based on heuristics. In our evaluation, three datasets are used ranging from very messy synthetic data to spreadsheets from data.gov which are less messy. We obtained first promising results especially with regard to our approach being fully automatic and dealing with rather messy data.

</details>


### [31] [Supercomputing Enabled Deployable Analytics for Disaster Response](https://arxiv.org/abs/2108.11525)
*Kaira Samuel,Jeremy Kepner,Michael Jones,Lauren Milechin,Vijay Gadepally,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Michael Houle,Anna Klein,Victor Lopez,Julie Mullen,Andrew Prout,Albert Reuther,Antonio Rosa,Sid Samsi,Charles Yee,Peter Michaleas*

Main category: cs.DB

TL;DR: 向一线工人提供预先计算的分析，以应对网络限制。


<details>
  <summary>Details</summary>
Motivation: 需要为网络访问受限和安全要求高的一线工人提供高级分析。

Method: 将大量分析预先计算为文件，以便在不访问网络或安装额定软件的情况下使用，并利用MIT SuperCloud处理数据，生成Google Earth和Microsoft Excel文件。

Result: 生成了代表多种高级分析的数千个Google Earth和Microsoft Excel文件，能够快速映射人口普查数据。

Conclusion: 所提出的方法有可能通过提供人口普查数据分析工具来增强应急响应人员的能力，从而改善应急准备工作。

Abstract: First responders and other forward deployed essential workers can benefit from advanced analytics. Limited network access and software security requirements prevent the usage of standard cloud based microservice analytic platforms that are typically used in industry. One solution is to precompute a wide range of analytics as files that can be used with standard preinstalled software that does not require network access or additional software and can run on a wide range of legacy hardware. In response to the COVID-19 pandemic, this approach was tested for providing geo-spatial census data to allow quick analysis of demographic data for better responding to emergencies. These data were processed using the MIT SuperCloud to create several thousand Google Earth and Microsoft Excel files representative of many advanced analytics. The fast mapping of census data using Google Earth and Microsoft Excel has the potential to give emergency responders a powerful tool to improve emergency preparedness. Our approach displays relevant census data (total population, population under 15, population over 65, median age) per census block, sorted by county, through a Microsoft Excel spreadsheet (xlsx file) and Google Earth map (kml file). The spreadsheet interface includes features that allow users to convert between different longitude and latitude coordinate units. For the Google Earth files, a variety of absolute and relative colors maps of population density have been explored to provide an intuitive and meaningful interface. Using several hundred cores on the MIT SuperCloud, new analytics can be generated in a few minutes.

</details>


### [32] [Table2Charts: Recommending Charts by Learning Shared Table Representations](https://arxiv.org/abs/2008.11015)
*Mengyu Zhou,Qingtao Li,Xinyi He,Yuejiang Li,Yibo Liu,Wei Ji,Shi Han,Yining Chen,Daxin Jiang,Dongmei Zhang*

Main category: cs.DB

TL;DR: Table2Charts是一个学习图表推荐的框架，考虑了效率、数据不平衡和表格上下文等挑战。


<details>
  <summary>Details</summary>
Motivation: 推荐常用的图表组合，需要解决效率、数据不平衡和表格上下文等问题。

Method: Table2Charts框架通过深度Q学习和复制机制，以及启发式搜索，从（表格，图表）对语料库中学习常见模式，将表格转换为遵循图表模板的序列。

Result: 在包含165k表格和266k图表的大型电子表格语料库上，Table2Charts能够学习表格字段的共享表示，实现多图表推荐任务的相互促进。其在多类型任务上的表现优于其他图表推荐系统（R@3=0.61, R@1=0.43），并在人工评估中表现良好。

Conclusion: Table2Charts在图表推荐方面取得了显著的成果，优于现有系统，并能实现多图表推荐任务的相互促进。

Abstract: It is common for people to create different types of charts to explore a multi-dimensional dataset (table). However, to recommend commonly composed charts in real world, one should take the challenges of efficiency, imbalanced data and table context into consideration. In this paper, we propose Table2Charts framework which learns common patterns from a large corpus of (table, charts) pairs. Based on deep Q-learning with copying mechanism and heuristic searching, Table2Charts does table-to-sequence generation, where each sequence follows a chart template. On a large spreadsheet corpus with 165k tables and 266k charts, we show that Table2Charts could learn a shared representation of table fields so that recommendation tasks on different chart types could mutually enhance each other. Table2Charts outperforms other chart recommendation systems in both multi-type task (with doubled recall numbers R@3=0.61 and R@1=0.43) and human evaluations.

</details>


### [33] [Sigma Worksheet: Interactive Construction of OLAP Queries](https://arxiv.org/abs/2012.00697)
*James Gale,Max Seiden,Gretchen Atwood,Jason Frantz,Rob Woollen,Çağatay Demiralp*

Main category: cs.DB

TL;DR: Sigma Worksheet是一个交互式系统，它提供类似电子表格的界面，使业务用户能够轻松地对云数据仓库中的数据进行即席可视化分析。


<details>
  <summary>Details</summary>
Motivation: 现有工具难以满足业务用户对云数据仓库进行即席转换和交互式分析的需求。

Method: Sigma Worksheet通过用户在熟悉界面上的直接操作动态构建SQL查询，并在云数据仓库上执行这些查询，以利用其可扩展性。

Result: Sigma Worksheet的查询性能与TPC-H基准测试中的参考查询相当；用户调查和访谈表明，该系统易于使用，提高了用户生产力，并有改进用户体验的潜力。

Conclusion: Sigma Worksheet通过提供易于使用的类似电子表格的界面，有效地解决了云数据仓库分析工具的可用性和效率问题，并有望通过提供指导来进一步增强用户体验。

Abstract: The new generation of cloud data warehouses (CDWs) brings large amounts of data and compute power closer to users in enterprises. The ability to directly access the warehouse data, interactively analyze and explore it at scale can empower users to improve their decision making cycles. However, existing tools for analyzing data in CDWs are either limited in ad-hoc transformations or difficult to use for business users, the largest user segment in enterprises. Here we introduce Sigma Worksheet, a new interactive system that enables users to easily perform ad-hoc visual analysis of data in CDWs at scale. For this, Sigma Worksheet provides an accessible spreadsheet-like interface for data analysis through direct manipulation. Sigma Worksheet dynamically constructs matching SQL queries from user interactions on this familiar interface, building on the versatility and expressivity of SQL. Sigma Worksheet executes constructed queries directly on CDWs, leveraging the superior characteristics of the new generation CDWs, including scalability. To evaluate Sigma Worksheet, we first demonstrate its expressivity through two real life use cases, cohort analysis and sessionization. We then measure the performance of the Worksheet generated queries with a set of experiments using the TPC-H benchmark. Results show the performance of our compiled SQL queries is comparable to that of the reference queries of the benchmark. Finally, to assess the usefulness of Sigma Worksheet in deployment, we elicit feedback through a 100-person survey followed by a semi-structured interview study with 70 participants. We find that Sigma Worksheet is easier to use and learn, improving the productivity of users. Our findings also suggest Sigma Worksheet can further improve user experience by providing guidance to users at various steps of data analysis.

</details>


### [34] [Mapping Spreadsheets to RDF: Supporting Excel in RML](https://arxiv.org/abs/2104.13600)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: RML Mappe r被扩展以支持Excel文件，允许将电子表格映射到RDF图，并提供了一个在线演示。


<details>
  <summary>Details</summary>
Motivation: RML规范和现有实现忽略了Excel电子表格格式，而Excel被广泛使用。

Method: 扩展了RML Mapper工具以支持Excel文件，并允许访问电子表格元数据。

Result: 成功实现了对Excel文件的支持，并在在线演示中展示了其功能。

Conclusion: 该扩展增强了RML在处理电子表格数据方面的能力，使其能够处理更广泛的数据源。

Abstract: The RDF Mapping Language (RML) enables, among other formats, the mapping of tabular data as Comma-Separated Values (CSV) files to RDF graphs. Unfortunately, the widely used spreadsheet format is currently neglected by its specification and well-known implementations. Therefore, we extended one of the tools which is RML Mapper to support Microsoft Excel spreadsheet files and demonstrate its capabilities in an interactive online demo. Our approach allows to access various meta data of spreadsheet cells in typical RML maps. Some experimental features for more specific use cases are also provided. The implementation code is publicly available in a GitHub fork.

</details>


### [35] [Dataset Generation Patterns for Evaluating Knowledge Graph Construction](https://arxiv.org/abs/2104.13576)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: The paper proposes a synthetic data generation method called Data Sprout to create realistic datasets for knowledge graph construction, addressing the confidentiality issues that prevent the use of real-world data.


<details>
  <summary>Details</summary>
Motivation: Confidentiality issues prevent the publication of authentic personal and enterprise data, which are valuable for evaluating knowledge graph construction methods in industrial settings. This paper aims to overcome this limitation by generating synthetic data that mimics real datasets.

Method: The paper identifies 11 distinct patterns in real-world spreadsheets from industry. It then develops a data generator, Data Sprout, capable of reproducing these patterns to create synthetic spreadsheets.

Result: Data Sprout has been demonstrated to effectively reproduce the identified patterns found in real spreadsheets, generating synthetic data that appears authentic.

Conclusion: The paper presents Data Sprout as a viable solution for generating realistic synthetic datasets for knowledge graph construction, thereby enabling the evaluation of these methods in industrial scenarios despite data confidentiality concerns. The effectiveness of the generator is shown through its ability to replicate real-world data patterns.

Abstract: Confidentiality hinders the publication of authentic, labeled datasets of personal and enterprise data, although they could be useful for evaluating knowledge graph construction approaches in industrial scenarios. Therefore, our plan is to synthetically generate such data in a way that it appears as authentic as possible. Based on our assumption that knowledge workers have certain habits when they produce or manage data, generation patterns could be discovered which can be utilized by data generators to imitate real datasets. In this paper, we initially derived 11 distinct patterns found in real spreadsheets from industry and demonstrate a suitable generator called Data Sprout that is able to reproduce them. We describe how the generator produces spreadsheets in general and what altering effects the implemented patterns have.

</details>


### [36] [Interactively Constructing Knowledge Graphs from Messy User-Generated Spreadsheets](https://arxiv.org/abs/2103.03537)
*Markus Schröder,Christian Jilek,Michael Schulze,Andreas Dengel*

Main category: cs.DB

TL;DR: 知识工作者输入的表格数据常包含非结构化内容，难以被机器理解。本文提出一种交互式方法，通过图形化界面辅助知识工程师标注表格单元格，并基于标注构建知识图谱，以应对数据维护缺失导致的“脏”数据问题。


<details>
  <summary>Details</summary>
Motivation: 解决知识工作者输入的表格数据因缺乏数据维护策略而变得混乱，导致难以构建知识图谱的挑战。

Method: 提出一种交互式方法，包含一个图形化用户界面，允许知识工程师批量标注表格单元格中的提取信息，并基于这些标注构建知识图谱。

Result: 在五个工业场景的电子表格上进行了评估，构建了一个包含25,000个三元组的知识图谱，并与现有的RDF映射语言（RML）方法进行了比较，证明了该方法的优越性。

Conclusion: 所提出的交互式方法能够有效解决表格数据混乱问题，并成功构建知识图谱，其效果优于现有的RML方法。

Abstract: When spreadsheets are filled freely by knowledge workers, they can contain rather unstructured content. For humans and especially machines it becomes difficult to interpret such data properly. Therefore, spreadsheets are often converted to a more explicit, formal and structured form, for example, to a knowledge graph. However, if a data maintenance strategy has been missing and user-generated data becomes "messy", the construction of knowledge graphs will be a challenging task. In this paper, we catalog several of those challenges and propose an interactive approach to solve them. Our approach includes a graphical user interface which enables knowledge engineers to bulk-annotate spreadsheet cells with extracted information. Based on the cells' annotations a knowledge graph is ultimately formed. Using five spreadsheets from an industrial scenario, we built a 25k-triple graph during our evaluation. We compared our method with the state-of-the-art RDF Mapping Language (RML) attempt. The comparison highlights contributions of our approach.

</details>


### [37] [Leam: An Interactive System for In-situ Visual Text Analysis](https://arxiv.org/abs/2009.03520)
*Sajjadur Rahman,Peter Griggs,Çağatay Demiralp*

Main category: cs.DB

TL;DR: Leam是一个结合了计算笔记本、电子表格和可视化工具优点的文本分析系统，解决了现有系统在数据异构性、可重复性等方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有文本分析系统在处理数据异构性、可重复性、工作流重用性和兼容性方面存在不足，需要一个能将文本分析过程视为一个连续过程的系统。

Method: 提出Leam系统，结合计算笔记本、电子表格和可视化工具的优点，通过交互式用户界面、新的数据模型和支持数据、代码和可视化之间协调的表达式代数来实现文本分析。

Result: 展示了Leam系统的开发进展和使用示例，证明了其有效性。

Conclusion: Leam系统在解决现有文本分析挑战方面取得了进展，并指出了未来增强和研究方向。

Abstract: With the increase in scale and availability of digital text generated on the web, enterprises such as online retailers and aggregators often use text analytics to mine and analyze the data to improve their services and products alike. Text data analysis is an iterative, non-linear process with diverse workflows spanning multiple stages, from data cleaning to visualization. Existing text analytics systems usually accommodate a subset of these stages and often fail to address challenges related to data heterogeneity, provenance, workflow reusability and reproducibility, and compatibility with established practices. Based on a set of design considerations we derive from these challenges, we propose Leam, a system that treats the text analysis process as a single continuum by combining advantages of computational notebooks, spreadsheets, and visualization tools. Leam features an interactive user interface for running text analysis workflows, a new data model for managing multiple atomic and composite data types, and an expressive algebra that captures diverse sets of operations representing various stages of text analysis and enables coordination among different components of the system, including data, code, and visualizations. We report our current progress in Leam development while demonstrating its usefulness with usage examples. Finally, we outline a number of enhancements to Leam and identify several research directions for developing an interactive visual text analysis system.

</details>


### [38] [ObjTables: structured spreadsheets that promote data quality, reuse, and integration](https://arxiv.org/abs/2005.05227)
*Jonathan R. Karr,Wolfram Liebermeister,Arthur P. Goldberg,John A. P. Sekar,Bilal Shaikh*

Main category: cs.DB

TL;DR: ObjTables工具包通过结合模式和对象关系映射系统，使电子表格具有人类可读和机器可读的特性，从而便于科学家重用和整合异构信息。


<details>
  <summary>Details</summary>
Motivation: 科学中的一个核心挑战是从复杂的网络中理解系统行为的出现。这通常需要聚合、重用和整合异构信息。文章的补充电子表格是一个关键的数据源。电子表格很受欢迎，因为它们易于阅读和编写。然而，电子表格通常难以重新分析，因为它们是临时捕获数据的，没有定义它们所表示的对象、关系和属性的模式。

Method: ObjTables工具包通过结合模式和对象关系映射系统，使电子表格具有人类可读和机器可读的特性。ObjTables包括一种用于模式的格式；用于指示每个电子表格和列所代表的类和属性的标记；多种科学信息的数据类型；以及用于使用模式读取、写入、验证、比较、合并、修订和分析电子表格的高级软件。

Result: 通过使电子表格更易于重用，ObjTables有可能实现前所未有的二次元分析。通过轻松构建新格式和相关软件以适应新类型的数据，ObjTables还可以加速新兴的科学领域。

Conclusion: ObjTables工具包通过使电子表格更易于重用和整合异构信息，为科学研究和新兴科学领域的发展提供了支持。

Abstract: A central challenge in science is to understand how systems behaviors emerge from complex networks. This often requires aggregating, reusing, and integrating heterogeneous information. Supplementary spreadsheets to articles are a key data source. Spreadsheets are popular because they are easy to read and write. However, spreadsheets are often difficult to reanalyze because they capture data ad hoc without schemas that define the objects, relationships, and attributes that they represent. To help researchers reuse and compose spreadsheets, we developed ObjTables, a toolkit that makes spreadsheets human- and machine-readable by combining spreadsheets with schemas and an object-relational mapping system. ObjTables includes a format for schemas; markup for indicating the class and attribute represented by each spreadsheet and column; numerous data types for scientific information; and high-level software for using schemas to read, write, validate, compare, merge, revision, and analyze spreadsheets. By making spreadsheets easier to reuse, ObjTables could enable unprecedented secondary meta-analyses. By making it easy to build new formats and associated software for new types of data, ObjTables can also accelerate emerging scientific fields.

</details>


### [39] [Needles in the 'Sheet'stack: Augmented Analytics to get Insights from Spreadsheets](https://arxiv.org/abs/2006.08224)
*Medha Atre,Anand Deshpande,Reshma Godse,Pooja Deokar,Sandip Moharir,Dhruva Ray,Akshay Chitlangia,Trupti Phadnis,Yugansh Goyal*

Main category: cs.DB

TL;DR: 该演示通过检查不同报告（也称为视图）的周期性电子表格来提供见解，而无需预先了解数据库或报告的模式或数据信息。


<details>
  <summary>Details</summary>
Motivation: 该解决方案的目标用户是对数据库模式不熟悉或没有资源以传统方式进行分析的用户。

Method: 该演示通过检查不同报告（也称为视图）的周期性电子表格来提供见解。

Result: 该演示侧重于提供见解。

Conclusion: 该演示提供了一个解决方案，允许用户在不熟悉数据库模式或资源的情况下进行分析。

Abstract: Business intelligence (BI) tools for database analytics have come a long way and nowadays also provide ready insights or visual query explorations, e.g. QuickInsights by Microsoft Power BI, SpotIQ by ThoughtSpot, Zenvisage, etc. In this demo, we focus on providing insights by examining periodic spreadsheets of different reports (aka views), without prior knowledge of the schema of the database or reports, or data information. Such a solution is targeted at users without the familiarity with the database schema or resources to conduct analytics in the contemporary way.

</details>


### [40] [A Billion Updates per Second Using 30,000 Hierarchical In-Memory D4M Databases](https://arxiv.org/abs/1902.00846)
*Jeremy Kepner,Vijay Gadepally,Lauren Milechin,Siddharth Samsi,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Micheal Houle,Micheal Jones,Anne Klein,Peter Michaleas,Julie Mullen,Andrew Prout,Antonio Rosa,Charles Yee,Albert Reuther*

Main category: cs.DB

TL;DR: 该论文提出了一种使用D4M库和分层关联数组来优化大规模流网络数据分析的更新性能的方法，并在MIT SuperCloud上实现了每秒19亿次的更新速率。


<details>
  <summary>Details</summary>
Motivation: 大规模网络分析需要高效的流式更新图表示。

Method: 实现了分层D4M关联数组，并在MIT SuperCloud上进行了大规模分布式测试。

Result: 在1100个服务器节点上实现了每秒19亿次的流式更新速率。

Conclusion: 这种能力使得MIT SuperCloud能够分析极其庞大的流网络数据集。

Abstract: Analyzing large scale networks requires high performance streaming updates of graph representations of these data. Associative arrays are mathematical objects combining properties of spreadsheets, databases, matrices, and graphs, and are well-suited for representing and analyzing streaming network data. The Dynamic Distributed Dimensional Data Model (D4M) library implements associative arrays in a variety of languages (Python, Julia, and Matlab/Octave) and provides a lightweight in-memory database. Associative arrays are designed for block updates. Streaming updates to a large associative array requires a hierarchical implementation to optimize the performance of the memory hierarchy. Running 34,000 instances of a hierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of 1,900,000,000 updates per second. This capability allows the MIT SuperCloud to analyze extremely large streaming network data sets.

</details>


### [41] [Towards Semantically Enhanced Data Understanding](https://arxiv.org/abs/1806.04952)
*Markus Schröder,Christian Jilek,Jörn Hees,Andreas Dengel*

Main category: cs.DB

TL;DR: 通过单一语义模型整合数据及其文档，简化数据科学家的查找过程，并支持其他数据处理应用。


<details>
  <summary>Details</summary>
Motivation: 当前数据文档分散且查找不便，阻碍了数据科学家对数据的理解和支持性应用的使用。

Method: 提出一种使用单一语义模型的方法，将数据与其文档链接起来。

Result: 数据科学家可以直接查找链接信息，也可浏览相关文档；支持搜索、比较、集成和可视化等数据处理应用。已演示早期原型。

Conclusion: 所提出的方法通过单一语义模型有效解决了数据查找开销大、非结构化数据难以利用的问题，并支持多种数据处理应用。

Abstract: In the field of machine learning, data understanding is the practice of getting initial insights in unknown datasets. Such knowledge-intensive tasks require a lot of documentation, which is necessary for data scientists to grasp the meaning of the data. Usually, documentation is separate from the data in various external documents, diagrams, spreadsheets and tools which causes considerable look up overhead. Moreover, other supporting applications are not able to consume and utilize such unstructured data. That is why we propose a methodology that uses a single semantic model that interlinks data with its documentation. Hence, data scientists are able to directly look up the connected information about the data by simply following links. Equally, they can browse the documentation which always refers to the data. Furthermore, the model can be used by other approaches providing additional support, like searching, comparing, integrating or visualizing data. To showcase our approach we also demonstrate an early prototype.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [42] [Improving Feedback from Automated Reviews of Student Spreadsheets](https://arxiv.org/abs/2311.10728)
*Sören Aguirre Reid,Frank Kammer,Jonas-Ian Kuche,Pia-Doreen Ritzke,Markus Siepermann,Max Stephan,Armin Wagenknecht*

Main category: cs.CY

TL;DR: 该文开发了一套用于自动评估学生Excel作业的智能辅导系统（ITS），实现了对学生提交的Excel文件进行值匹配、公式分析和质量评估，并能根据学生的学习水平提供不同层次的反馈，以提高作业正确率和学生理解度。


<details>
  <summary>Details</summary>
Motivation: 目前在教学环境中，用于评估学生电子表格作业的数字解决方案仍然很少，因此需要开发一个智能辅导系统来自动审查学生的Excel作业并提供个性化反馈。

Method: 开发了一个智能辅导系统（ITS），能够自动分析学生提交的Excel文件，包括值匹配、详细的公式分析和解决方案的质量评估。系统还设计了不同层次的反馈机制，以适应学生的学习水平，并提供逐步增加的错误信息。

Result: 研究结果表明，提供更高层次反馈的ITS能够显著提高学生作业的正确率，并且学生认为这种反馈是易于理解且有帮助的。

Conclusion: 所开发的智能辅导系统（ITS）能够有效自动评估学生的Excel作业，并提供个性化反馈，有助于提高学生的学习效果和满意度。

Abstract: Spreadsheets are one of the most widely used tools for end users. As a result, spreadsheets such as Excel are now included in many curricula. However, digital solutions for assessing spreadsheet assignments are still scarce in the teaching context. Therefore, we have developed an Intelligent Tutoring System (ITS) to review students' Excel submissions and provide individualized feedback automatically. Although the lecturer only needs to provide one reference solution, the students' submissions are analyzed automatically in several ways: value matching, detailed analysis of the formulas, and quality assessment of the solution. To take the students' learning level into account, we have developed feedback levels for an ITS that provide gradually more information about the error by using one of the different analyses. Feedback at a higher level has been shown to lead to a higher percentage of correct submissions and was also perceived as well understandable and helpful by the students.

</details>


### [43] [How Beaufort, Neumann and Gates met? Subject integration with spreadsheeting](https://arxiv.org/abs/2309.12353)
*Maria Csernoch,Julia Csernoch*

Main category: cs.CY

TL;DR: 计算思维应作为继读写算之后的第四项基本技能。本文提出了一种结合 Beaufort 风级表的新方法，以支持学科整合和数字图式构建，并在八年级行动研究中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 为了使计算思维技能（尤其是数字问题解决方法）达到独立图式的水平，还有很长的路要走。

Method: 提出了一种结合 Beaufort 风级表的新颖方法，支持学科整合和数字图式构建。将传统的纸质问题和数据检索过程数字化，并在八年级行动研究中进行了展示。

Result: 研究发现，与传统的教科书和去情境化的数字环境相比，学生的内容知识和数字技能都得到了更有效的提升。

Conclusion: 所提出的方法可以应用于任何纸质问题，将其解决方案置于数字环境中将更有效，并且可以为学科内容和信息学提供多种图式构建形式。

Abstract: Computational thinking should be the fourth fundamental skill, along with reading, writing, and arithmetic (3R). To reach the level where computational thinking skills, especially digital problem solving have their own schemata, there is a long way to go. In the present paper, a novel approach is detailed to support subject integration and building digital schemata, on the well-known Beaufort scale. The conversion of a traditional, paper-based problem and a data retrieval process are presented within the frame of a Grade 8 action research study. It is found that both students content knowledge and their digital skills developed more efficiently than in traditional course book and decontextualized digital environments. Furthermore, the method presented here can be adapted to any paper-based problems whose solutions would be more effective in a digital environment and which offer various forms for building schemata both in the subject matter and informatics.

</details>


### [44] [A Simpler Method for Understanding Emergency Shelter Access Patterns](https://arxiv.org/abs/2210.13619)
*Geoffrey G. Messier*

Main category: cs.CY

TL;DR: SAM是一种新的紧急避难所可访问性指标，可量化客户的脆弱性，并提供比传统方法更简化的方法。


<details>
  <summary>Details</summary>
Motivation: SAM的目的是为避难所提供一个直观的、可由非技术人员使用的可访问性模式的度量，并能比传统的聚类分析方法产生更实时的客户脆弱性图景。

Method: 使用来自北美一个大型避难所的客户数据，通过SAM指标来分析客户的访问模式，并将其与传统的聚类分析方法进行比较。

Result: SAM在表征客户访问模式和客户脆弱性方面，能产生与传统聚类分析相似的结果，但需要的数据更少，并且能够生成实时数据，从而能够显示外部因素对访问模式的影响，例如住房优先计划和COVID-19封锁。

Conclusion: SAM是一种有效的、简化的方法，用于量化紧急避难所客户的脆弱性，使避难所工作人员能够超越传统的客户分类，并直接利用SAM的输出作为脆弱性度量的依据。

Abstract: The Simplified Access Metric (SAM) is a new approach for characterizing emergency shelter access patterns as a measure of shelter client vulnerability. The goal of SAM is to provide shelter operators with an intuitive way to understand access patterns that can be implemented by non-technical staff using spreadsheet operations. Client data from a large North American shelter will be used to demonstrate that SAM produces similar results to traditional transitional, episodic and chronic client cluster analysis. Since SAM requires less data than cluster analysis, it is also able to generate a real time picture of how shelter access patterns are affected by external factors. Timelines generated from nine years of shelter client data using SAM demonstrate the impact of Housing First programming and the COVID-19 lockdown on how people access shelter. Finally, SAM allows shelter staff to move beyond assigning transitional, episodic and chronic labels and instead use the "soft" output of SAM directly as a measure of vulnerability.

</details>


### [45] [Long-Term Mentoring for Computer Science Researchers](https://arxiv.org/abs/2208.04738)
*Emily Ruppel,Sihang Liu,Elba Garza,Sukyoung Ryu,Alexandra Silva,Talia Ringer*

Main category: cs.CY

TL;DR: PL和CA社区都推出了长期指导计划，以解决社区成员之间缺乏长期联系的问题。


<details>
  <summary>Details</summary>
Motivation: 解决PL和CA社区中新成员难以建立长期联系的问题，因为只有在社区内已有联系的人才能建立新的长期联系。

Method: CA社区通过科学论证的方式建立了一个社区范围的长期指导计划；PL社区则是一个由个人发起的、基于电子表格的非官方长期指导计划，后来发展为SIGPLAN-M；CALM则是在CA社区的倡议下建立的。

Result: SIGPLAN-M计划覆盖了328名受助者和234名导师，遍布41个国家，受助者称其“改变人生”和“挽救职业生涯”。CALM计划目前处于试点阶段，有13名导师和21名受助者，遍布7个国家，收到了非常积极的反馈。

Conclusion: SIGPLAN-M和CALM计划的领导者们分享了他们的经验、影响和挑战，希望能够推动整个计算机科学领域建立更广泛的长期指导计划。

Abstract: Early in the pandemic, we -- leaders in the research areas of programming languages (PL) and computer architecture (CA) -- realized that we had a problem: the only way to form new lasting connections in the community was to already have lasting connections in the community. Both of our academic communities had wonderful short-term mentoring programs to address this problem, but it was clear that we needed long-term mentoring programs.
  Those of us in CA approached this scientifically, making an evidence-backed case for community-wide long-term mentoring. In the meantime, one of us in PL had impulsively launched an unofficial long-term mentoring program, founded on chaos and spreadsheets. In January 2021, the latter grew to an official cross-institutional long-term mentoring program called SIGPLAN-M; in January 2022, the former grew to Computer Architecture Long-term Mentoring (CALM).
  The impacts have been strong: SIGPLAN-M reaches 328 mentees and 234 mentors across 41 countries, and mentees have described it as "life changing" and "a career saver." And while CALM is in its pilot phase -- with 13 mentors and 21 mentees across 7 countries -- it has received very positive feedback. The leaders of SIGPLAN-M and CALM shared our designs, impacts, and challenges along the way. Now, we wish to share those with you. We hope this will kick-start a larger long-term mentoring effort across all of computer science.

</details>


### [46] [Optimization Helps Scheduling Nursing Staff at the Long-Term Care Homes of the City of Toronto](https://arxiv.org/abs/2102.09461)
*Manion Anderson,Merve Bodur,Scott Rathwell,Vahid Sarhangian*

Main category: cs.CY

TL;DR: 多伦多长期护理机构利用分层优化模型开发了一个电子表格工具，以自动化生成满足复杂优先顺序和护士偏好的排班表，显著减少了排班时间并提高了护士满意度。


<details>
  <summary>Details</summary>
Motivation: 为了应对多伦多长期护理机构日益严峻的护士排班挑战和部分工时护士的高缺勤率，需要一个更高效、更能满足护士偏好的排班解决方案。

Method: 开发了一个基于电子表格的排班工具，其核心是一个分层优化模型，该模型在满足复杂资历要求的前提下，生成最高总偏好得分且尽可能满足需求的可用班次排班表。

Result: 该排班工具在多伦多一家391张床位的护理院实施后，将生成可行排班表的时间从手动方式的数小时缩短至一小时以内。此外，排班表成功考虑了护士偏好，平均超过94%的分配班次均被列为最偏好的班次。

Conclusion: 该排班工具通过自动化排班流程，并有效整合护士偏好和资历要求，显著提高了排班效率和护士满意度，为多伦多长期护理机构提供了一个成功的解决方案。

Abstract: The City of Toronto Long Term Care Homes & Services (LTCH&S) division is one of the largest providers of long-term care in the Canadian province of Ontario, providing care to 2,640 residents at 10 homes across Toronto. Our collaboration with LTCH&S was initiated to facilitate the increasingly challenging task of scheduling nursing staff and reduce high absenteeism rate observed among the part-time nurses. We developed a spreadsheet-based scheduling tool to automate the generation of schedules and incorporate nurses' preferences for different shifts into the schedules. At the core of the scheduling tool is a hierarchical optimization model that generates a feasible schedule with the highest total preference score while satisfying the maximum possible demand. Feasible schedules had to abide by a set of complex seniority requirements which prioritized more senior nurses when allocating the available shifts. Our scheduling tool was implemented in a 391-bed home in Toronto. The tool allowed nursing managers to generate feasible schedules within a fraction of an hour, in contrast to the status-quo manual approach which could took up to tens of hours. In addition, the schedules successfully accounted for preferences with on average above 94% of the allocated shifts ranked as most preferred.

</details>


### [47] [Developing Excel Thought Leadership](https://arxiv.org/abs/2006.04793)
*David Lyford-Smith*

Main category: cs.CY

TL;DR: ICAEW发布了三篇关于电子表格使用和工作环境的'思想领导力'报告，总结了五年来的经验教训，并巩固了其在该领域的地位。


<details>
  <summary>Details</summary>
Motivation: 介绍ICAEW在五年间开发的关于电子表格使用和工作环境的“思想领导力”报告。

Method: 回顾了三篇报告的历史，总结了每篇报告的关键经验，并讨论了报告的编写过程如何帮助ICAEW巩固其在该领域的地位。

Result: ICAEW发布了三篇关于电子表格使用和工作环境的“思想领导力”报告。

Conclusion: ICAEW通过发布一系列关于电子表格使用和工作环境的“思想领导力”报告，不仅分享了宝贵的经验教训，还成功地确立了其在该领域的专业地位。

Abstract: Over a period of five years, the Institute of Chartered Accountants in England and Wales (ICAEW) has developed a suite of three 'thought leadership' papers surrounding good practice in spreadsheet use and spreadsheet work environments. We will review the history of these three papers, the key lessons which each has to teach, and discuss how the process of making them has helped ICAEW to develop its position in the field.

</details>


### [48] [A Case Study of Spreadsheet Use within the Finance and Academic Registry units within a Higher Education Institution](https://arxiv.org/abs/1909.07462)
*Simon Thorne,Jamie Hancock*

Main category: cs.CY

TL;DR: 该研究调查了英国一所高等教育机构中电子表格的使用情况，重点关注学术注册和财务部门。研究发现，电子表格被大量创建和使用，其开发者具有典型特征。为了确保数据完整性、减少重复工作并优化电子表格的使用以实现机构目标，该机构需要制定明确的电子表格开发原则和指南。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨英国一所高等教育机构中电子表格的使用情况，并分析其在准确报告、数据完整性和效率方面的影响。

Method: 通过案例研究，考察了学术注册和财务部门的电子表格使用情况，内容涵盖其重要性、培训、经验、目的、所用技术、创建的电子表格大小以及共享情况。

Result: 研究结果显示，该机构创建和使用的电子表格数量巨大，电子表格开发者的构成与其他研究中的情况类似。此外，研究强调了制定明确的电子表格开发原则和指南的必要性。

Conclusion: 为了确保数据完整性、减少重复工作并优化电子表格的使用以实现机构目标，该机构需要制定明确的电子表格开发原则和指南。

Abstract: This paper presents the findings of a case study of spreadsheet use in a higher education institution in the UK. The paper considers the use of spreadsheets in two units of the organisation, academic registry and finance. Spreadsheet use is explored in terms of importance, training, experience, purpose, techniques deployed, size of spreadsheets created and sharing of spreadsheets. The implications of the results are then considered in terms of accurate reporting to external funding bodies such the funding councils, internal data integrity and internal data efficiencies. The results show a large volume of spreadsheets being created and used, that the profile of spreadsheet developers is typical of other studies of spreadsheet use and the need for the organisation to have clear principles and guidelines for the development of spreadsheet models in the organisation to ensure data integrity, reduce duplication of effort and to optimise the use of spreadsheets to meet the institutions goals.

</details>


### [49] [Computer Simulation of Neural Networks Using Spreadsheets: The Dawn of the Age of Camelot](https://arxiv.org/abs/1807.00018)
*Serhiy O. Semerikov,Illia O. Teplytskyi,Yuliia V. Yechkalo,Arnold E. Kiv*

Main category: cs.CY

TL;DR: 该文章论证了在电子表格环境中开发神经网络计算机模拟培训方法的必要性。


<details>
  <summary>Details</summary>
Motivation: 文章旨在论证在电子表格环境中开发神经网络计算机模拟培训方法的必要性，并对现有方法进行了系统性回顾。

Method: 通过对1890-1950年间的文献进行分析，特别是围绕 "Bulletin of Mathematical Biophysics" 杂志及其创始人Nicolas Rashevsky的科学社群，研究了神经网络模型和计算神经科学方法的创建与发展。文章还探讨了神经网络创建的心理物理学基础、计算神经科学的数学基础以及神经工程学方法（特别是图像识别）。

Result: 文章识别出在电子表格环境中进行神经网络模拟的几种基本方法，包括结合电子表格和神经网络模拟工具、使用第三方插件、开发嵌入式语言宏、利用标准插件进行非线性优化、以及不使用插件和宏直接在电子表格中创建神经网络。研究还确定了三种有前景的模型，有助于开发相应的培训方法：Rashevsky的连续双因子模型、McCulloch和Pitts的离散模型以及Householder和Landahl的离散-连续模型。文中还讨论了Walter Pitts在结合描述性理论和定量理论方面的作用。

Conclusion: 为了在电子表格环境中获得神经网络模拟能力，应掌握基于历史和遗传方法的模型。文章指出了三种有前景的模型组，可用于开发相关方法。

Abstract: The article substantiates the necessity to develop training methods of computer simulation of neural networks in the spreadsheet environment. The systematic review of their application to simulating artificial neural networks is performed. The authors distinguish basic approaches to solving the problem of network computer simulation training in the spreadsheet environment, joint application of spreadsheets and tools of neural network simulation, application of third-party add-ins to spreadsheets, development of macros using the embedded languages of spreadsheets; use of standard spreadsheet add-ins for non-linear optimization, creation of neural networks in the spreadsheet environment without add-ins and macros. After analyzing a collection of writings of 1890-1950, the research determines the role of the scientific journal "Bulletin of Mathematical Biophysics", its founder Nicolas Rashevsky and the scientific community around the journal in creating and developing models and methods of computational neuroscience. There are identified psychophysical basics of creating neural networks, mathematical foundations of neural computing and methods of neuroengineering (image recognition, in particular). The role of Walter Pitts in combining the descriptive and quantitative theories of training is discussed. It is shown that to acquire neural simulation competences in the spreadsheet environment, one should master the models based on the historical and genetic approach. It is indicated that there are three groups of models, which are promising in terms of developing corresponding methods - the continuous two-factor model of Rashevsky, the discrete model of McCulloch and Pitts, and the discrete-continuous models of Householder and Landahl.

</details>


### [50] [Edu-Edition Spreadsheet Competency Framework](https://arxiv.org/abs/1802.00496)
*Maria Csernoch,Piroska Biró*

Main category: cs.CY

TL;DR: 该论文提出了电子表格能力框架的教育版（E2SCF），强调在教育早期通过专家教师的指导，利用高数学能力的计算机支持的现实世界问题解决能力来培养电子表格能力。


<details>
  <summary>Details</summary>
Motivation: 在教育早期开始培养电子表格能力，并由专家教师提供支持，可以更有效地提高其效果。

Method: 通过高数学能力的计算机支持的现实世界问题解决、双向知识转移、数据和错误分析处理以及电子表格的编程方面来构建电子表格能力。

Result: E2SCF 旨在为基础用户和普通用户奠定扎实的电子表格知识基础，并培养可转移的问题解决技能和能力。

Conclusion: E2SCF 是一种有效的教育工具，可以通过强调实际应用和关键电子表格技能来培养学生的问题解决能力。

Abstract: Based on the Spreadsheet Competency Framework for finance professionals, in the present paper we introduce the Edu-Edition of the Spreadsheet Competency Framework (E2SCF). We claim that building spreadsheet competences should start in education, as early as possible, and this process is a lot more effective if support arrives from expert teachers. The main feature of E2SCF is high mathability computer-supported real world problem solving. This approach is based on - from the very beginning of training - a two-directional knowledge transfer, data and error analysis and handling, and the programming aspect of spreadsheets. Based on these features, E2SCF is set up for basic and general users to build up firm spreadsheet knowledge and to develop transferable problem solving skills and competences.

</details>


<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [51] [Real-time stock analysis for blending recipes in industrial plants](https://arxiv.org/abs/1909.02960)
*Florin Zamfir,Nicolae Paraschiv,Emil Pricop*

Main category: cs.OH

TL;DR: 该论文开发了一款实时库存分析应用程序，以解决Excel电子表格在库存管理和工艺数据计算中存在的问题。


<details>
  <summary>Details</summary>
Motivation: Excel电子表格在库存管理方面存在难以理解、追踪困难以及公式被随意更改或删除的风险。为解决这些问题，开发一款能集中管理数据并辅助操作员决策的应用程序是必要的。

Method: 开发一款实时库存分析应用程序，使用C#语言，集成规划算法来识别所需配料、确定可用成品量以及优化成品量。

Result: 实现了易于操作员使用的应用程序，能够实时分析库存，并辅助用户逐步完成生产目标。

Conclusion: 该应用程序通过集成规划算法，有效解决了传统库存管理方式的弊端，提高了生产效率和决策能力。

Abstract: Many companies use Excel spreadsheets to keep stock records and to calculate process-specific data. These spreadsheets are often hard to understand and track. And if the user does not protect them, there is a risk that the user randomly changes or erase formulas. The paper focuses on the stocks of products used in a blending process with a known recipe. Developing an application that can bring this data in a centralized form and that can assist the operator in decide is a necessity. When a programmer implements an application that uses data from plants he needs to consider one fundamental aspect as reading real-time data from the process. The real-time stock analysis application takes into account all the above elements. The application is easy to use by an operator in the command room of installation because of the planning algorithms integrated into it. The algorithms proposed and implemented in this paper have well-defined goals: identifying the ingredients needed to achieve the blending process for required quantities, determine the quantities of the finished product that can be made with the existing ingredients and determine the optimum quantities of the finished product. The application implemented in C# intensively uses these algorithms and gives the user the ability to build the result step by step.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [Reducing Aleatoric and Epistemic Uncertainty through Multi-modal Data Acquisition](https://arxiv.org/abs/2501.18268)
*Arthur Hoarau,Benjamin Quost,Sébastien Destercke,Willem Waegeman*

Main category: cs.LG

TL;DR: 多模态学习中，不确定性消歧可以通过增加数据模态数量或增加观测数量来减少。通过选择性地收集数据，可以实现可操作的决策。


<details>
  <summary>Details</summary>
Motivation: 多模态数据为机器学习带来了新的机遇和挑战，挑战了关于如何减少认知不确定性和随机不确定性的传统假设。

Method: 提出了一种新颖的数据采集框架，该框架允许在样本量和数据模态两个方向上进行采样，以实现不确定性消歧。

Result: 在两个多模态数据集上提供了概念验证实现，以展示所提出的数据采集框架。

Conclusion: 不确定性消歧可以通过增加模态数量或增加观测数量来减少，这为多模态学习提供了新的见解。

Abstract: To generate accurate and reliable predictions, modern AI systems need to combine data from multiple modalities, such as text, images, audio, spreadsheets, and time series. Multi-modal data introduces new opportunities and challenges for disentangling uncertainty: it is commonly assumed in the machine learning community that epistemic uncertainty can be reduced by collecting more data, while aleatoric uncertainty is irreducible. However, this assumption is challenged in modern AI systems when information is obtained from different modalities. This paper introduces an innovative data acquisition framework where uncertainty disentanglement leads to actionable decisions, allowing sampling in two directions: sample size and data modality. The main hypothesis is that aleatoric uncertainty decreases as the number of modalities increases, while epistemic uncertainty decreases by collecting more observations. We provide proof-of-concept implementations on two multi-modal datasets to showcase our data acquisition framework, which combines ideas from active learning, active feature acquisition and uncertainty quantification.

</details>


### [53] [Large Scale Transfer Learning for Tabular Data via Language Modeling](https://arxiv.org/abs/2406.12031)
*Josh Gardner,Juan C. Perdomo,Ludwig Schmidt*

Main category: cs.LG

TL;DR: TabuLa-8B是一个针对表格预测任务的语言模型，在零样本和少样本设置下均优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 尽管在其他领域已有预训练模型，但表格数据领域缺乏类似模型，因此需要开发适用于表格预测的语言模型。

Method: 从TabLib语料库中提取数据，进行过滤和质量控制，构建了包含21亿行、400万个表格的数据集。然后，使用该数据集和一种新颖的打包和注意力机制对Llama 3-8B模型进行微调，以进行表格预测。

Result: 在329个数据集的测试中，TabuLa-8B在零样本设置下的准确率比随机猜测高15个百分点以上，优于XGBoost和TabPFN。在少样本设置下，TabuLa-8B的准确率也比XGBoost和TabPFN高5-15个百分点。

Conclusion: TabuLa-8B在表格预测任务上展现出强大的性能，尤其是在零样本和少样本场景下，为表格数据处理提供了一种新的方法。

Abstract: Tabular data -- structured, heterogeneous, spreadsheet-style data with rows and columns -- is widely used in practice across many domains. However, while recent foundation models have reduced the need for developing task-specific datasets and predictors in domains such as language modeling and computer vision, this transfer learning paradigm has not had similar impact in the tabular domain. In this work, we seek to narrow this gap and present TabuLa-8B, a language model for tabular prediction. We define a process for extracting a large, high-quality training dataset from the TabLib corpus, proposing methods for tabular data filtering and quality control. Using the resulting dataset, which comprises over 2.1B rows from over 4M unique tables, we fine-tune a Llama 3-8B large language model (LLM) for tabular data prediction (classification and binned regression) using a novel packing and attention scheme for tabular prediction. Through evaluation across a test suite of 329 datasets, we find that TabuLa-8B has zero-shot accuracy on unseen tables that is over 15 percentage points (pp) higher than random guessing, a feat that is not possible with existing state-of-the-art tabular prediction models (e.g. XGBoost, TabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the target datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN models that are explicitly trained on equal, or even up to 16x more data. We release our model, code, and data along with the publication of this paper.

</details>


### [54] [Generating tabular datasets under differential privacy](https://arxiv.org/abs/2308.14784)
*Gianluca Truda*

Main category: cs.LG

TL;DR: DP-GANs 在处理敏感表格数据时存在隐私和质量之间的权衡。本文提出了 TableDiffusion，一种新的 DP 扩散模型，用于生成更高质量的合成表格数据，并解决了 GANs 的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型在合成敏感表格数据时，往往在数据质量和隐私保护之间存在权衡。特别是 DP-GANs 在处理表格数据时，会遇到训练不稳定和模式崩溃的问题，而隐私约束会加剧这些问题。

Method: 本文提出了一种新的端到端模型，利用注意力机制学习可逆的表格表示。此外，还引入了 TableDiffusion，这是第一个用于表格数据合成的差分隐私扩散模型。该模型通过预测添加的噪声来绕过混合类型表格数据的重建挑战。

Result: TableDiffusion 能够生成更高保真度的合成数据集，避免模式崩溃问题，并在私有表格数据合成方面达到最先进的性能。扩散模型比对抗模型在数据利用率和隐私效率方面更优。

Conclusion: 扩散模型在表格数据合成方面比对抗模型更具优势，能够以更高的效率实现更好的数据质量和隐私保护。

Abstract: Machine Learning (ML) is accelerating progress across fields and industries, but relies on accessible and high-quality training data. Some of the most important datasets are found in biomedical and financial domains in the form of spreadsheets and relational databases. But this tabular data is often sensitive in nature. Synthetic data generation offers the potential to unlock sensitive data, but generative models tend to memorise and regurgitate training data, which undermines the privacy goal. To remedy this, researchers have incorporated the mathematical framework of Differential Privacy (DP) into the training process of deep neural networks. But this creates a trade-off between the quality and privacy of the resulting data. Generative Adversarial Networks (GANs) are the dominant paradigm for synthesising tabular data under DP, but suffer from unstable adversarial training and mode collapse, which are exacerbated by the privacy constraints and challenging tabular data modality. This work optimises the quality-privacy trade-off of generative models, producing higher quality tabular datasets with the same privacy guarantees. We implement novel end-to-end models that leverage attention mechanisms to learn reversible tabular representations. We also introduce TableDiffusion, the first differentially-private diffusion model for tabular data synthesis. Our experiments show that TableDiffusion produces higher-fidelity synthetic datasets, avoids the mode collapse problem, and achieves state-of-the-art performance on privatised tabular data synthesis. By implementing TableDiffusion to predict the added noise, we enabled it to bypass the challenges of reconstructing mixed-type tabular data. Overall, the diffusion paradigm proves vastly more data and privacy efficient than the adversarial paradigm, due to augmented re-use of each data batch and a smoother iterative training process.

</details>


### [55] [Smart Metro: Deep Learning Approaches to Forecasting the MRT Line 3 Ridership](https://arxiv.org/abs/2304.07303)
*Jayrald Empino,Jean Allyson Junsay,Mary Grace Verzon,Mideth Abisado,Shekinah Lor Huyo-a,Gabriel Avelino Sampedro*

Main category: cs.LG

TL;DR: 该研究提出了一种时间序列预测方法，用于预测菲律宾马尼拉地铁3号线（MRT3）的每日客流量，以帮助通勤者规划行程和交通部门进行数据分析。


<details>
  <summary>Details</summary>
Motivation: 目前，交通部门依赖电子表格处理历史数据，难以进行有效分析，且通勤者无法准确预测每日客流量，影响行程规划。

Method: 该研究提出了一种时间序列预测方法，用于预测MRT3特定车站特定日期的每日客流量。

Result: 该研究旨在通过时间序列预测，提供对MRT3未来客流量的预测。

Conclusion: 该研究通过时间序列预测，为MRT3的客流量预测提供了一种解决方案。

Abstract: Since its establishment in 1999, the Metro Rail Transit Line 3 (MRT3) has served as a transportation option for numerous passengers in Metro Manila, Philippines. The Philippine government's transportation department records more than a thousand people using the MRT3 daily and forecasting the daily passenger count may be rather challenging. The MRT3's daily ridership fluctuates owing to variables such as holidays, working days, and other unexpected issues. Commuters do not know how many other commuters are on their route on a given day, which may hinder their ability to plan an efficient itinerary. Currently, the DOTr depends on spreadsheets containing historical data, which might be challenging to examine. This study presents a time series prediction of daily traffic to anticipate future attendance at a particular station on specific days.

</details>


### [56] [Adversarial Networks and Machine Learning for File Classification](https://arxiv.org/abs/2301.11964)
*Ken St. Germain,Josh Angichiodo*

Main category: cs.LG

TL;DR: 使用对抗训练的机器学习神经网络来识别被混淆的文件类型


<details>
  <summary>Details</summary>
Motivation: 在法证调查中，正确识别文件类型至关重要，即使文件扩展名或文件头被混淆。

Method: 提出了一种半监督生成对抗网络（SGAN）来对抗混淆的文件类型。

Result: SGAN 在 11 种不同文件类型上的分类准确率达到了 97.6%，并且在只有少量监督样本的情况下，其精确度优于传统神经网络和其他机器学习算法。

Conclusion: 使用 SGAN 实现的文件分类器在混淆的文件类型识别方面非常精确，尤其是在监督样本有限的情况下。

Abstract: Correctly identifying the type of file under examination is a critical part of a forensic investigation. The file type alone suggests the embedded content, such as a picture, video, manuscript, spreadsheet, etc. In cases where a system owner might desire to keep their files inaccessible or file type concealed, we propose using an adversarially-trained machine learning neural network to determine a file's true type even if the extension or file header is obfuscated to complicate its discovery. Our semi-supervised generative adversarial network (SGAN) achieved 97.6% accuracy in classifying files across 11 different types. We also compared our network against a traditional standalone neural network and three other machine learning algorithms. The adversarially-trained network proved to be the most precise file classifier especially in scenarios with few supervised samples available. Our implementation of a file classifier using an SGAN is implemented on GitHub (https://ksaintg.github.io/SGAN-File-Classier).

</details>


### [57] [TabularNet: A Neural Network Architecture for Understanding Semantic Structures of Tabular Data](https://arxiv.org/abs/2106.03096)
*Lun Du,Fei Gao,Xu Chen,Ran Jia,Junshan Wang,Jiang Zhang,Shi Han,Dongmei Zhang*

Main category: cs.LG

TL;DR: 本研究提出了一种名为TabularNet的新型神经网络架构，用于同时提取表格数据的空间和关系信息，以解决现有方法忽略表格中单元格之间多样化关系的问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究在理解表格数据时，虽然利用卷积神经网络（CNN）处理空间信息，但忽略了单元格之间如层级和并列关系等多样化的关系信息。本研究旨在弥补这一不足。

Method: 提出了一种名为TabularNet的新型神经网络架构。其空间编码器结合了行/列池化和双向门控循环单元（Bi-GRU）来捕捉统计信息和局部位置相关性。关系信息方面，设计了一种基于WordNet树的新图构建方法，并采用基于图卷积网络（GCN）的编码器来处理单元格之间的层级和并列关系。

Result: 在两个真实世界的电子表格数据集上的三个分类任务上进行了广泛的实验，结果表明TabularNet在多任务学习场景下，作为统一的神经骨干，优于现有的基线方法。

Conclusion: TabularNet能够有效地同时提取表格数据的空间和关系信息，是一种可用于不同理解任务的多任务学习的统一神经骨干。

Abstract: Tabular data are ubiquitous for the widespread applications of tables and hence have attracted the attention of researchers to extract underlying information. One of the critical problems in mining tabular data is how to understand their inherent semantic structures automatically. Existing studies typically adopt Convolutional Neural Network (CNN) to model the spatial information of tabular structures yet ignore more diverse relational information between cells, such as the hierarchical and paratactic relationships. To simultaneously extract spatial and relational information from tables, we propose a novel neural network architecture, TabularNet. The spatial encoder of TabularNet utilizes the row/column-level Pooling and the Bidirectional Gated Recurrent Unit (Bi-GRU) to capture statistical information and local positional correlation, respectively. For relational information, we design a new graph construction method based on the WordNet tree and adopt a Graph Convolutional Network (GCN) based encoder that focuses on the hierarchical and paratactic relationships between cells. Our neural network architecture can be a unified neural backbone for different understanding tasks and utilized in a multitask scenario. We conduct extensive experiments on three classification tasks with two real-world spreadsheet data sets, and the results demonstrate the effectiveness of our proposed TabularNet over state-of-the-art baselines.

</details>


### [58] [Visual Backpropagation](https://arxiv.org/abs/1906.04011)
*Roy S. Freedman*

Main category: cs.LG

TL;DR: 使用电子表格实现可视化反向传播


<details>
  <summary>Details</summary>
Motivation: 提出一种新的反向传播实现方法

Method: 利用数组工作表公式、手动计算和类似脉动阵列的计算顺序，在电子表格中声明式地实现反向传播。

Result: 实现了不使用宏、用户定义函数、循环、赋值语句或传统过程语言程序的反向传播。

Conclusion: 与 Tensorflow 解决方案相比，在标准回归问题上展示了 Visual Backpropagation 的有效性。

Abstract: We show how a declarative functional programming specification of backpropagation yields a visual and transparent implementation within spreadsheets. We call our method Visual Backpropagation. This backpropagation implementation exploits array worksheet formulas, manual calculation, and has a sequential order of computation similar to the processing of a systolic array. The implementation uses no hidden macros nor user-defined functions; there are no loops, assignment statements, or links to any procedural programs written in conventional languages. As an illustration, we compare a Visual Backpropagation solution to a Tensorflow (Python) solution on a standard regression problem.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [59] [A CNN toolbox for skin cancer classification](https://arxiv.org/abs/1908.08187)
*Fabrizio Nunnari,Daniel Sonntag*

Main category: eess.IV

TL;DR: 一个用于皮肤癌分类的深度神经网络配置软件工具箱，允许技术和非技术用户快速配置和探索模型设置。


<details>
  <summary>Details</summary>
Motivation: 开发一个软件工具箱，以简化皮肤癌分类领域深度神经网络的配置过程，同时支持技术用户和非技术用户。

Method: 实现了一个软件架构，允许快速设置新的CNN架构和超参数配置。用户界面设计得像一个简单的电子表格，方便非技术用户探索配置设置。

Result: 在皮肤镜图像黑色素瘤检测的背景下，使用两个CNN进行了初步测试，量化了图像增强、图像分辨率和重缩放滤波器对整体检测性能和训练时间的影响。

Conclusion: 该软件工具箱能够有效地支持皮肤癌分类任务中深度神经网络的配置和探索，并为未来集成元学习和 AutoML 系统奠定了基础。

Abstract: We describe a software toolbox for the configuration of deep neural networks in the domain of skin cancer classification. The implemented software architecture allows developers to quickly set up new convolutional neural network (CNN) architectures and hyper-parameter configurations. At the same time, the user interface, manageable as a simple spreadsheet, allows non-technical users to explore different configuration settings that need to be explored when switching to different data sets. In future versions, meta leaning frameworks can be added, or AutoML systems that continuously improve over time. Preliminary results, conducted with two CNNs in the context melanoma detection on dermoscopic images, quantify the impact of image augmentation, image resolution, and rescaling filter on the overall detection performance and training time.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [60] [LEI2JSON: Schema-based Validation and Conversion of Livestock Event Information](https://arxiv.org/abs/2310.17414)
*Mahir Habib,Muhammad Ashad Kabir,Lihong Zheng*

Main category: eess.SY

TL;DR: LEI2JSON是一款用于Google表格的插件，能帮助畜牧业生产者将畜牧事件数据标准化为JSON格式，从而节省时间和资源。


<details>
  <summary>Details</summary>
Motivation: 畜牧业生产者在标准化（转换和验证）畜牧事件数据时，常常需要帮助。

Method: LEI2JSON是一款Google表格的插件，它通过构建包含适当的列标题、注释和验证规则的电子表格模板，将电子表格数据转换为JSON格式，并根据LEI模式验证输出。

Result: LEI2JSON能有效地将畜牧事件信息转换为JSON格式，并进行验证，方便本地或Google云端存储。对该工具进行了广泛的实验评估。

Conclusion: LEI2JSON为畜牧业生产者提供了一种有效的方法来标准化他们的畜牧事件数据，从而节省大量时间和资源。

Abstract: Livestock producers often need help in standardising (i.e., converting and validating) their livestock event data. This article introduces a novel solution, LEI2JSON (Livestock Event Information To JSON). The tool is an add-on for Google Sheets, adhering to the livestock event information (LEI) schema. The core objective of LEI2JSON is to provide livestock producers with an efficient mechanism to standardise their data, leading to substantial savings in time and resources. This is achieved by building the spreadsheet template with the appropriate column headers, notes, and validation rules, converting the spreadsheet data into JSON format, and validating the output against the schema. LEI2JSON facilitates the seamless storage of livestock event information locally or on Google Drive in JSON. Additionally, we have conducted an extensive experimental evaluation to assess the effectiveness of the tool.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [61] [Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!](https://arxiv.org/abs/2309.02110)
*James P. Dilger*

Main category: math.HO

TL;DR: Wordle玩家可以通过分析他们的猜测行为来推断出欺骗、个人喜好和易受影响的证据。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是利用信息论来分析Wordle玩家的猜测行为，以量化地揭示欺骗、个人偏好和易受外界影响等方面的问题，超越了传统的社交媒体和调查方法。

Method: 通过收集和分析2023年5月至8月期间的Wordle玩家的首次猜测数据，利用信息论来推断玩家行为。

Result: A) 每天有0.2-0.5%的玩家在一次尝试中就解决了谜题，这表明大量玩家可能通过作弊获得了目标单词，因为随机猜测的成功率仅为0.043%。B) 至少有1/3的玩家有他们喜欢的起始单词，并且即使在目标单词重复出现后，他们仍然忠于这个单词。C) 在2023年8月15日，约有30,000名玩家突然改变了他们的起始单词，这表明玩家容易受到外部线索（例如填字游戏）的影响。

Conclusion: Wordle玩家的行为可以提供关于欺骗、个人偏好和易受影响的量化证据，这些证据可以通过分析游戏数据来获得。

Abstract: Wordle is a popular, online word game offered by the New York Times (nytimes.com). Currently there are some 2 million players of the English version worldwide. Players have 6 attempts to guess the daily word (target word) and after each attempt, the player receives color-coded information about the correctness and position of each letter in the guess. After either a successful completion of the puzzle or the final unsuccessful attempt, software can assess the player's luck and skill using Information Theory and can display data for the first, second, ..., sixth guesses of a random sample of all players. Recently, I discovered that the latter data is presented in a format that can easily be copied and pasted into a spreadsheet. I compiled data on Wordle players' first guesses from May 2023 - August 2023 and inferred some interesting information about Wordle players. A) Every day, about 0.2-0.5% of players solve the puzzle in one attempt. Because the odds of guessing the one of 2,315 possible target words at random is 0.043%, this implies that 4,000 - 10,000 players cheat by obtaining the target word outside of playing the game! B) At least 1/3 of the players have a favorite starting word, or cycle through several. And even though players should be aware that target words are never repeated, most players appear to remain loyal to their starting word even after its appearance as a target word. C) On August 15, 2023, about 30,000 players abruptly changed their starting word, presumably based on a crossword puzzle clue! Wordle players can be influenced! This study goes beyond social media postings, surveys, and Google Trends to provide solid, quantitative evidence about cheating in Wordle.

</details>


### [62] [GeoGebra e situações que envolvem modelação numa abordagem STEAM](https://arxiv.org/abs/1907.02099)
*J. M. D. S. Dos Santos,A. P. Silveira,A. E. S. Trocado*

Main category: math.HO

TL;DR: 该论文旨在探索在数学教学中应用STEAM理念，并结合GeoGebra交互式软件，为葡萄牙语国家教师提供培训材料，促进学生在二维和三维几何问题建模及分析中的应用。


<details>
  <summary>Details</summary>
Motivation: 为了在数学课程中实施STEAM教学方法，并引入交互式数学软件GeoGebra，特别是在葡萄牙语地区，因此构思了本材料。

Method: 本研究使用GeoGebra软件的2D、3D、CAS窗口、电子表格和额外的二维窗口，通过脚本指导用户完成二维和三维几何问题的建模和分析，旨在展示软件的功能。

Result: 该论文提供了一套完整的任务材料，能够支持不同水平的用户学习GeoGebra在几何问题分析中的应用，并促进与其他学科的联系，同时支持STEAM教学方法。

Conclusion: 该论文提出的GeoGebra教学任务适用于数学教学，支持STEAM方法，促进跨学科联系和项目式学习，并计划将其改编成西班牙语和英语，以扩大其国际影响力。

Abstract: In order to implement a STEAM approach including the use of technology, namely the use of interactive mathematics software GeoGebra, in mathematics classes, in the lusophone space, the materials presented here were conceived, to be implemented in a first phase among teachers. Later, with the necessary adaptations, these tasks will be applied to the students. The tasks deal with modeling situations, in two- and three-dimensional geometric problems, in order to apply GeoGebra software in its analysis to illustrate its capabilities. The different windows of this software are used, namely the 2D and 3D windows, CAS window, spreadsheet and extra two dimensional windows in order to study cutting planes in solids and some surfaces. The tasks are presented so that any user, regardless of the degree of knowledge they have of the software, can follow them, being supported in scripts with some indications of the tools and commands to use. Designed for the teaching and learning of Mathematics, from a STEAM approach, these tasks allow connections with other Sciences and the Arts, and allow the development of projects using and consolidating relevant mathematical contents. These tasks are part of the proposals of activities of the participants of the Training Courses for Trainers in GeoGebra for Portuguese Speaking Countries, which from 2019 have an impact on the STEAM approach. These courses are carried out with the high sponsorship of the Organization of Ibero-American States for Education, Science and Culture (OEI). Given the interest that the tasks have for the users of the Iberian space, as well as their dissemination at a global level, the materials initially developed in Portuguese language will be adapted for Spanish and English speakers.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [63] [Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE](https://arxiv.org/abs/2506.17330)
*Simon Thorne*

Main category: cs.SE

TL;DR: LLMs在处理电子表格任务时表现不一，简单任务效果好，但复杂任务容易出错，需要结合符号推理能力。为此，我们提出了FLARE基准来评估LLM在电子表格逻辑、审计和推理方面的表现。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在电子表格相关任务中的表现，并为未来的研究提供基准。

Method: 设计了一个包含公式生成、数据操作和复杂场景的基准测试框架，并提出了FLARE基准。

Result: LLMs在简单电子表格任务中表现良好，但在需要精确逻辑推理的复杂多步操作中表现不佳，常生成看似合理但错误的输出。

Conclusion: 当前LLM在处理需要精确逻辑推理的电子表格任务方面存在局限性，需要集成符号推理能力。FLARE基准旨在解决这一问题。

Abstract: Large Language Models (LLMs) have demonstrated some significant capabilities across various domains; however, their effectiveness in spreadsheet related tasks remains underexplored. This study introduces a foundation for a comprehensive benchmark framework to evaluate the performance of leading LLMs in executing spreadsheet functions, formula generation and data manipulation tasks. The benchmark encompasses tasks ranging from basic formula creation to complex, real world spreadsheet scenarios. Our findings reveal that while LLMs exhibit proficiency in straightforward tasks, they often falter in complex, multi step operations, frequently producing plausible yet incorrect outputs. These results underscore the limitations of current LLMs in handling spreadsheet tasks that require precise logical reasoning and highlight the need for integrating symbolic reasoning capabilities into LLM architectures. To support this, we introduce FLARE (Formula Logic, Auditing, Reasoning and Evaluation) a new benchmark for evaluating LLM performance on real-world spreadsheet logic, auditing, and reasoning tasks.

</details>


### [64] [Overcoming Vision Language Model Challenges in Diagram Understanding: A Proof-of-Concept with XML-Driven Large Language Models Solutions](https://arxiv.org/abs/2502.04389)
*Shue Shiinoki,Ryo Koshihara,Hayato Motegi,Masumi Morishige*

Main category: cs.SE

TL;DR: 通过分析包含图表的可编辑源文件（如xlsx, pptx, docx），利用LLM提取图表结构和关系，相比基于VLM的方法，在需要详细理解图表结构的问题上，准确度更高，从而提高了工作流程效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有VLMs在理解业务文档图表结构和关系方面的挑战，提出一种不依赖视觉识别的文本驱动方法。

Method: 提取xlsx格式系统设计文档中的图表信息，将形状数据转换为文本输入，供LLM分析关系并回答业务问题。

Result: 实验证明，该文本驱动框架比基于VLM的方法能更准确地回答需要详细理解图表结构的问题，且该方法可扩展到pptx和docx等其他格式。

Conclusion: 所提出的方法通过直接从源文件进行文本提取，可以有效规避VLM的限制，实现LLM对图表的强大理解能力，为提高业务场景中的工作流程效率和信息分析提供了有前景的途径。

Abstract: Diagrams play a crucial role in visually conveying complex relationships and processes within business documentation. Despite recent advances in Vision-Language Models (VLMs) for various image understanding tasks, accurately identifying and extracting the structures and relationships depicted in diagrams continues to pose significant challenges. This study addresses these challenges by proposing a text-driven approach that bypasses reliance on VLMs' visual recognition capabilities. Instead, it utilizes the editable source files--such as xlsx, pptx or docx--where diagram elements (e.g., shapes, lines, annotations) are preserved as textual metadata. In our proof-of-concept, we extracted diagram information from xlsx-based system design documents and transformed the extracted shape data into textual input for Large Language Models (LLMs). This approach allowed the LLM to analyze relationships and generate responses to business-oriented questions without the bottleneck of image-based processing. Experimental comparisons with a VLM-based method demonstrated that the proposed text-driven framework yielded more accurate answers for questions requiring detailed comprehension of diagram structures.The results obtained in this study are not limited to the tested .xlsx files but can also be extended to diagrams in other documents with source files, such as Office pptx and docx formats. These findings highlight the feasibility of circumventing VLM constraints through direct textual extraction from original source files. By enabling robust diagram understanding through LLMs, our method offers a promising path toward enhanced workflow efficiency and information analysis in real-world business scenarios.

</details>


### [65] [On the Workflows and Smells of Leaderboard Operations (LBOps): An Exploratory Study of Foundation Model Leaderboards](https://arxiv.org/abs/2407.04065)
*Zhimin Zhao,Abdul Ali Bangash,Filipe Roseiro Côgo,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: FM Leaderboards在软件工程（SE）领域至关重要，但缺乏标准化评估方法。本研究通过分析1,045个FM Leaderboard，识别出五种工作流程模式和八种“Leaderboard Smells”，旨在提高FM评估的透明度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有FM Leaderboard缺乏标准化评估和比较指南，威胁其透明度，并限制了利益相关者有效选择FM的能力。本研究旨在解决这一挑战，理解FM Leaderboard的实际运作方式，并识别潜在的陷阱和改进领域。

Method: 收集了来自GitHub、Hugging Face Spaces、Papers With Code、电子表格和独立平台等五个来源的1,045个FM Leaderboard。通过卡片分类和协商一致，识别了五种不同的工作流程模式，并开发了一个捕获这些工作流程中关键组件及其交互的领域模型。随后，识别出八种独特的Leaderboard Smells。

Result: 识别出五种不同的工作流程模式和八种独特的Leaderboard Smells。这些Smells的存在会影响Leaderboard的透明度、问责制和协作。

Conclusion: 通过缓解这些Smells，SE团队可以提高当前LBOps实践的透明度、问责制和协作，从而促进一个更强大、更负责任的FM比较和选择生态系统。

Abstract: Foundation models (FM), such as large language models (LLMs), which are large-scale machine learning (ML) models, have demonstrated remarkable adaptability in various downstream software engineering (SE) tasks, such as code completion, code understanding, and software development. As a result, FM leaderboards have become essential tools for SE teams to compare and select the best third-party FMs for their specific products and purposes. However, the lack of standardized guidelines for FM evaluation and comparison threatens the transparency of FM leaderboards and limits stakeholders' ability to perform effective FM selection. As a first step towards addressing this challenge, our research focuses on understanding how these FM leaderboards operate in real-world scenarios ("leaderboard operations") and identifying potential pitfalls and areas for improvement ("leaderboard smells"). In this regard, we collect up to 1,045 FM leaderboards from five different sources: GitHub, Hugging Face Spaces, Papers With Code, spreadsheet and independent platform, to examine their documentation and engage in direct communication with leaderboard operators to understand their workflows. Through card sorting and negotiated agreement, we identify five distinct workflow patterns and develop a domain model that captures the key components and their interactions within these workflows. We then identify eight unique types of leaderboard smells in LBOps. By mitigating these smells, SE teams can improve transparency, accountability, and collaboration in current LBOps practices, fostering a more robust and responsible ecosystem for FM comparison and selection.

</details>


### [66] [Will Dynamic Arrays finally change the way Models are built?](https://arxiv.org/abs/2006.14706)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: 电子表格虽然直观易用，但极易出错，不适合严肃的分析或建模任务。本文探讨了动态数组在专业开发环境中的应用，认为它们可以替代传统技术，提高解决方案的完整性，并减少错误和风险。


<details>
  <summary>Details</summary>
Motivation: 电子表格因其直观和 ad-hoc 的特性在商业和工程领域广泛使用，但这些特性也导致其极易出错，限制了其在严肃分析或建模任务中的应用。

Method: 探讨了动态数组在专业开发环境中的应用，认为它们可以替代传统技术，提高解决方案的完整性。

Result: 动态数组的引入使得电子表格能够作为一种更专业的开发环境，用于需要高解决方案完整性的任务。与手动更新相比，全动态模型需要更少的手动干预，从而有潜力减少相关的错误和风险。

Conclusion: 动态数组的引入为提高电子表格解决方案的完整性提供了新的可能性，尤其是在需要减少手动干预和降低风险的专业开发环境中。

Abstract: Spreadsheets offer a supremely successful and intuitive means of processing and exchanging numerical content. Its intuitive ad-hoc nature makes it hugely popular for use in diverse areas including business and engineering, yet these very same characteristics make it extraordinarily error-prone; many would question whether it is suitable for serious analysis or modelling tasks. A previous EuSpRIG paper examined the role of Names in increasing solution transparency and providing a readable notation to forge links with the problem domain. Extensive use was made of CSE array formulas, but it is acknowledged that their use makes spreadsheet development a distinctly cumbersome task. Since that time, the new dynamic arrays have been introduced and array calculation is now the default mode of operation for Excel. This paper examines the thesis that their adoption within a more professional development environment could replace traditional techniques where solution integrity is important. A major advantage of fully dynamic models is that they require less manual intervention to keep them updated and so have the potential to reduce the attendant errors and risk.

</details>


### [67] [A Structured Approach to the development of Solutions in Excel](https://arxiv.org/abs/1704.01142)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: Spreadsheets lack structure beyond single-cell formulas, hindering scalability and error management. This paper explores using unconventional techniques to create structured, programmable-like solutions in Excel.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of inherent structure in standard Excel solutions, which limits scalability and increases the potential for errors, by applying unconventional techniques to mimic programming language structures.

Method: This paper proposes using controversial or lesser-used techniques within Excel to build a sequence of formulas that mirror the steps of a programmed language, thereby creating a more coherent and scalable solution.

Result: The paper aims to demonstrate how unconventional spreadsheet techniques can be used to implement a structured, step-by-step approach similar to programming, enhancing the problem-solving capabilities of Excel.

Conclusion: By employing less common Excel features, it's possible to create structured, scalable, and maintainable solutions that overcome the limitations of traditional spreadsheet development, offering a more robust alternative for complex problems.

Abstract: Spreadsheets offer a supremely successful democratisation platform, placing the manipulation and presentation of numbers within the grasp of users that have little or no mathematical expertise or IT experience. What appears to be almost completely lacking within a "normal" solution built using Excel default settings is the deployment of any structure that extends beyond a single-cell formula. The structural elements that allow conventional code to scale without escalating errors appear to be absent. This paper considers the use of controversial or lesser-used techniques to create a coherent solution strategy in which the problem is solved by a sequence of formulas resembling the steps of a programmed language.

</details>


### [68] [Spreadsheet-based Configuration of Families of Real-Time Specifications](https://arxiv.org/abs/2310.20395)
*José Proença,David Pereira,Giann Spilere Nandi,Sina Borrami,Jonas Melchert*

Main category: cs.SE

TL;DR: 该研究提出了一种利用形式化模型和检查需求中的可变性来促进实时规范变体模型检查的方法。


<details>
  <summary>Details</summary>
Motivation: 在模型检查实时系统时，需要在模型的详细程度和避免状态爆炸之间进行仔细权衡。本研究旨在解决这一复杂性。

Method: 通过利用形式化模型和待检查需求中的可变性来促进实时规范变体的模型检查。使用具有特定结构的MS Excel电子表格来配置形式化规范的可变性，并自动处理这些电子表格以生成实例并运行模型检查器。研究还将先前工作进行了扩展，探索了在保持基于电子表格的简单界面的同时，对有效特征组合进行分析。

Result: 提出了一种自动化的方法，可以通过MS Excel电子表格配置和生成实时系统变体的模型实例，并进行模型检查。

Conclusion: 该方法使得在保持简单易用的电子表格界面的同时，能够更有效地进行实时系统变体的模型检查，从而在实际应用中（如与Alstom公司的铁路应用场景）具有潜在价值。

Abstract: Model checking real-time systems is complex, and requires a careful trade-off between including enough detail to be useful and not too much detail to avoid state explosion. This work exploits variability of the formal model being analysed and the requirements being checked, to facilitate the model-checking of variations of real-time specifications.  This work results from the collaboration between academics and Alstom, a railway company with a concrete use-case, in the context of the VALU3S European project. The configuration of the variability of the formal specifications is described in MS Excel spreadsheets with a particular structure, making it easy to use also by developers. These spreadsheets are processed automatically by our prototype tool that generates instances and runs the model checker.  We propose the extension of our previous work by exploiting analysis over valid combination of features, while preserving the simplicity of a spreadsheet-based interface with the model checker.

</details>


### [69] [SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models](https://arxiv.org/abs/2305.19308)
*Hongxin Li,Jingran Su,Yuntao Chen,Qing Li,Zhaoxiang Zhang*

Main category: cs.SE

TL;DR: SheetCopilot 是一个利用大语言模型（LLMs）通过自然语言指令控制电子表格以完成重复性、易错性任务的代理。


<details>
  <summary>Details</summary>
Motivation: 现有的电子表格任务自动化工具缺乏，而大语言模型的发展使得自然语言指导软件成为可能。

Method: 提出了一套原子动作作为电子表格功能的抽象，并设计了一个基于状态机的任务规划框架，让大语言模型能够与电子表格进行交互。

Result: SheetCopilot 在自动完成电子表格任务方面表现出色，一次生成即可完成 44.3% 的任务，显著优于代码生成基线。

Conclusion: SheetCopilot 在处理电子表格任务方面展现了强大的能力，为用户提供了通过自然语言自动化复杂任务的可能性。

Abstract: Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill to automate these burdensome works. With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal. In this work, we propose a SheetCopilot agent that takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for LLMs to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of LLMs in software control tasks. Our SheetCopilot correctly completes 44.3\% of tasks for a single generation, outperforming the strong code generation baseline by a wide margin. Our project page:https://sheetcopilot.github.io/.

</details>


### [70] [Excel as a Turing-complete Functional Programming Environment](https://arxiv.org/abs/2309.00115)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: Excel 的动态数组功能带来了根本性的变革，使得传统电子表格的开发方式向更规范的编程范式转变，但其对业务和工程领域的影响以及潜在风险仍有待观察。


<details>
  <summary>Details</summary>
Motivation: Excel 在 2018 年引入动态数组功能后，其计算引擎进行了重大升级，这引发了电子表格构建方式的重大变革。

Method: 本文探讨了如何将传统电子表格的临时性用户实践，转变为更接近于形式化编程的、具有更强可预测性的方法。

Result: 虽然现在预测这种新功能被商业和工程界采纳的程度以及其对风险的影响还为时过早，但 Excel 社区中的一些初步探索已经显示出一些新兴趋势。

Conclusion: Excel 的动态数组功能正在推动电子表格开发范式向更规范的编程方式转变，尽管其广泛影响和潜在风险尚不明确，但已观察到一些初步的积极趋势。

Abstract: Since the calculation engine of Excel was the subject of a major upgrade to accommodate Dynamic Arrays in 2018 there has been a series of seismic changes to the art of building spreadsheet solutions. This paper will show the ad-hoc end user practices of traditional spreadsheets can be replaced by radically different approaches that have far more in common with formal programming. It is too early to guess the extent to which the new functionality will be adopted by the business and engineering communities and the impact that may have upon risk. Nevertheless, some trends are emerging from pioneering work within the Excel community which we will discuss here.

</details>


### [71] [A Use Case-Engineering Resources Taxonomy for Analytical Spreadsheet Models](https://arxiv.org/abs/2309.00104)
*Thomas A. Grossman,Vijay Mehrotra*

Main category: cs.SE

TL;DR: 本文提出了一个分析型电子表格模型分类法，考虑了其用例和开发所需的工程资源，将先前三类型分类法扩展到九种类型，以涵盖文献中的各种分析型电子表格模型。


<details>
  <summary>Details</summary>
Motivation: 为了对分析型电子表格模型进行分类，考虑其用例和开发资源，并扩展现有分类法以更好地涵盖文献中的模型。

Method: 通过结合用例和工程资源两个维度，将先前三类型分类法扩展为九种类型，并对这些类型进行界定、关联研究文献，并探讨其产生的原因。

Result: 提出了一个包含九种类型分析型电子表格模型的分类法，区分了“分析型解决方案”和“工业级分析型电子表格模型”，并阐述了各类型模型的特点。

Conclusion: 该分类法有助于识别不同电子表格开发指南的适用性，提供审视电子表格错误和风险的视角，并为理解电子表格的演变提供结构，同时也为未来的研究开辟了道路。

Abstract: This paper presents a taxonomy for analytical spreadsheet models. It considers both the use case that a spreadsheet is meant to serve, and the engineering resources devoted to its development. We extend a previous three-type taxonomy, to identify nine types of spreadsheet models, that encompass the many analytical spreadsheet models seen in the literature. We connect disparate research literature to distinguish between an "analytical solution" and an "industrial-quality analytical spreadsheet model". We explore the nature of each of the nine types, propose definitions for some, relate them to the literature, and hypothesize on how they might arise. The taxonomy aids in identifying where various spreadsheet development guidelines are most useful, provides a lens for viewing spreadsheet errors and risk, and offers a structure for understanding how spreadsheets change over time. This taxonomy opens the door to many interesting research questions, including refinements to itself.

</details>


### [72] [Experimenting with ChatGPT for Spreadsheet Formula Generation: Evidence of Risk in AI Generated Spreadsheets](https://arxiv.org/abs/2309.00095)
*Simon Thorne*

Main category: cs.SE

TL;DR: ChatGPT在特定情况下可以生成正确的电子表格公式，但在信息有限、不确定或问题过于复杂时，其准确性会下降，并可能产生虚假陈述。


<details>
  <summary>Details</summary>
Motivation: 探索ChatGPT在根据自然语言描述生成电子表格公式和计算输出方面的能力，特别是在需要推理、推断和解决问题的情况下。

Method: 进行一系列实验，使用ChatGPT来测试其根据自然语言描述生成电子表格公式和相关计算输出的能力，并分析其在信息有限、不确定或问题复杂时的表现。

Result: 在某些情况下，ChatGPT能够生成正确的电子表格公式，并伴随正确的推理、推断和分析。然而，当信息有限、不确定或问题过于复杂时，其准确性、推理、推断和分析能力会下降，并可能出现虚假陈述和“幻觉”。

Conclusion: ChatGPT在生成电子表格公式方面有潜力，但在处理复杂或不确定的问题时需要谨慎，并且需要注意其可能出现的错误和“幻觉”。

Abstract: Large Language Models (LLM) have become sophisticated enough that complex computer programs can be created through interpretation of plain English sentences and implemented in a variety of modern languages such as Python, Java Script, C++ and Spreadsheets. These tools are powerful and relatively accurate and therefore provide broad access to computer programming regardless of the background or knowledge of the individual using them. This paper presents a series of experiments with ChatGPT to explore the tool's ability to produce valid spreadsheet formulae and related computational outputs in situations where ChatGPT has to deduce, infer and problem solve the answer. The results show that in certain circumstances, ChatGPT can produce correct spreadsheet formulae with correct reasoning, deduction and inference. However, when information is limited, uncertain or the problem is too complex, the accuracy of ChatGPT breaks down as does its ability to reason, infer and deduce. This can also result in false statements and "hallucinations" that all subvert the process of creating spreadsheet formulae.

</details>


### [73] [Demonstration of CORNET: A System For Learning Spreadsheet Formatting Rules By Example](https://arxiv.org/abs/2308.07357)
*Mukul Singh,Jose Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Gust Verbruggen*

Main category: cs.SE

TL;DR: CORNET系统能自动学习用户在Excel中定义的条件格式规则。


<details>
  <summary>Details</summary>
Motivation: 用户需要手动编写条件格式规则，效率低下，CORNET旨在解决此问题。

Method: CORNET结合了归纳程序合成、基于半监督聚类和迭代决策树学习的符号规则枚举，以及用于生成准确条件格式规则的神经排名器。

Result: CORNET能够根据用户提供的少量示例，自动生成条件格式规则的建议。

Conclusion: CORNET作为Excel插件，通过学习用户示例，简化了条件格式规则的创建过程。

Abstract: Data management and analysis tasks are often carried out using spreadsheet software. A popular feature in most spreadsheet platforms is the ability to define data-dependent formatting rules. These rules can express actions such as "color red all entries in a column that are negative" or "bold all rows not containing error or failure." Unfortunately, users who want to exercise this functionality need to manually write these conditional formatting (CF) rules. We introduce CORNET, a system that automatically learns such conditional formatting rules from user examples. CORNET takes inspiration from inductive program synthesis and combines symbolic rule enumeration, based on semi-supervised clustering and iterative decision tree learning, with a neural ranker to produce accurate conditional formatting rules. In this demonstration, we show CORNET in action as a simple add-in to Microsoft Excel. After the user provides one or two formatted cells as examples, CORNET generates formatting rule suggestions for the user to apply to the spreadsheet.

</details>


### [74] [Excel Spreadsheet Analyzer](https://arxiv.org/abs/2211.06333)
*Amir Nassereldine,Patrick Chen,Jinjun Xiong*

Main category: cs.SE

TL;DR: 该研究提出了一种工具，用于将电子表格转换为抽象中间表示（AIR），以在迁移到Python等科学编程语言时保留单元格之间的依赖关系，并提供了一个Python库来进行数据分析。


<details>
  <summary>Details</summary>
Motivation: 数据科学家倾向于使用Python等科学编程语言进行数据分析，但从电子表格迁移到Python时会丢失公式和单元格依赖关系等信息。现有方法需要一种能够保留这些依赖关系的数据传输机制。

Method: 提出一种创建电子表格抽象中间表示（AIR）的工具，该表示可以保留数据之间的相互依赖关系。在此工具之上构建了一个Python库，用于在Python中执行数据分析。

Result: 该工具和Python库能够促进从电子表格到科学编程语言的数据迁移，同时保留单元格之间的依赖关系。

Conclusion: 所提出的AIR表示和Python库能够有效地解决在从电子表格迁移到Python进行数据分析时丢失依赖关系的问题。

Abstract: Spreadsheets are widely used in various fields to do large numerical analysis. While several companies have relied on spreadsheets for decades, data scientists are going in the direction of using scientific programming languages such as python to do their data analysis due to the support, community, and vast amount of libraries. While using python to analyze a company's spreadsheets, some information such as the formulas and dependencies of a cell are lost. We propose a tool that creates an abstract intermediate representation (AIR) of a spreadsheet. This representation facilitates the transfer from spreadsheets into scientific programming languages while preserving inter-dependency information about data. In addition to that, we build a python library on top of our tool to perform some data analysis in python.

</details>


### [75] [Exploring Spreadsheet Use and Practices in a Technologically Constrained Setting](https://arxiv.org/abs/2201.07696)
*Khwima Mckinley Mkamanga,Simon Thorne*

Main category: cs.SE

TL;DR: 本研究探讨了电子表格在马拉维一家水务国有企业中的应用及其对业务运营的影响，该企业是技术不发达国家中半政府机构的典型代表。研究结果将通过影响管理电子表格相关风险的新方法来帮助定义未来电子表格的使用。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨电子表格在马拉维水务国有企业中的应用及其对业务运营的影响，并为该组织未来电子表格的使用提供指导，以管理相关风险。

Method: 研究聚焦于电子表格的使用范围和生命周期，以及相关的组织政策和治理。

Result: 研究结果表明，电子表格在该组织的广泛使用促进了业务自动化，但也突显了管理、技术和人为因素导致的高风险。

Conclusion: 研究结论指出，在制定和采纳全面的电子表格开发流程管理政策和法规方面，仍有很大的改进空间。

Abstract: This paper explores the impacts of spreadsheets on business operations in a water utility parastatal in Malawi, Sub-Saharan Africa. The organisation is a typical example of a semi-government body operating in a technologically underdeveloped country. The study focused on spreadsheet scope of use and life cycle as well as organisational policy and governance. The results will help define future spreadsheet usage by influencing new approaches for managing potential risks associated with spreadsheets in the organization. Generally, findings indicate that the proliferation of spreadsheets in the organization has provided an enabling environment for business automation. The paper also highlights management, technological and human factor issues contributing to high risks associated with the pervasive spreadsheet use. The conclusions drawn from the research confirms that there is ample room for improvement in many areas such as implementation of comprehensive policies and regulations governing spreadsheet development processes and adoption.

</details>


### [76] [Methodology for Assessing the State of the Practice for Domain X](https://arxiv.org/abs/2110.11575)
*Spencer Smith,Jacques Carette,Peter Michalski,Ao Dong,Olu Owojaiye*

Main category: cs.SE

TL;DR: 本研究提出了一种评估研究软件开发实践状况的方法论，以识别伪影、工具、流程、痛点和改进措施。


<details>
  <summary>Details</summary>
Motivation: 为了改进研究软件的软件开发方法和工具，首先需要了解其实际应用情况。

Method: 该方法论包括确定领域、筛选软件包、收集代码和存储库数据、使用包含108个问题的模板进行测量、进行20个问题的开发者访谈、使用层次分析法（AHP）进行排名，以及分析数据。建议在整个过程中聘请领域专家。

Result: 该方法论为评估研究软件开发实践提供了一个结构化的方法，包括确定要评估的领域、收集和分析代码、存储库和开发者反馈数据，并使用AHP进行排名。

Conclusion: 本研究提出的方法论能够系统地评估研究软件开发实践的状况，为改进软件开发方法和工具提供了基础。 整个评估过程估计需要173个人时。

Abstract: To improve software development methods and tools for research software, we first need to understand the current state of the practice. Therefore, we have developed a methodology for assessing the state of the software development practices for a given research software domain. For each domain we wish to answer questions such as: i) What artifacts (documents, code, test cases, etc.) are present? ii) What tools are used? iii) What principles, process and methodologies are used? iv) What are the pain points for developers? v) What actions are used to improve qualities like maintainability and reproducibility? To answer these questions, our methodology prescribes the following steps: i) Identify the domain; ii) Identify a list of candidate software packages; iii) Filter the list to a length of about 30 packages; iv) Gather source code and documentation for each package; v) Collect repository related data on each software package, like number of stars, number of open issues, number of lines of code; vi) Fill in the measurement template (the template consists of 108 questions to assess 9 qualities (including the qualities of installability, usability and visibility)); vii) Interview developers (the interview consists of 20 questions and takes about an hour); viii) Rank the software using the Analytic Hierarchy Process (AHP); and, ix) Analyze the data to answer the questions posed above. A domain expert should be engaged throughout the process, to ensure that implicit information about the domain is properly represented and to assist with conducting an analysis of the commonalities and variabilities between the 30 selected packages. Using our methodology, spreadsheet templates and AHP tool, we estimate (based on our experience with using the process) the time to complete an assessment for a given domain at 173 person hours.

</details>


### [77] [Agile Elicitation of Scalability Requirements for Open Systems: A Case Study](https://arxiv.org/abs/2108.00567)
*Gunnar Brataas,Antonio Martini,Geir Kjetil Hanssen,Georg Ræder*

Main category: cs.SE

TL;DR: ScrumScale模型是一个用于敏捷开发的可扩展性需求获取的工具，通过案例研究证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 软件开发中，特别是在敏捷开发中，获取可扩展性需求是一个复杂且研究不足的领域。

Method: 本文提出并应用了ScrumScale模型，一个基于电子表格的轻量级工具，并结合了协调理论进行研究设计。

Result: 在开放银行案例研究中，使用ScrumScale模型成功获取了遗留银行系统开放所带来的可扩展性需求，该过程耗时55小时，并被TietoEVRY认为系统化且有益于与其他利益相关者的沟通。

Conclusion: ScrumScale模型为敏捷软件开发中的可扩展性需求获取提供了一个系统化的方法，并通过实际案例证明了其在处理复杂系统（如开放银行系统）时的有效性和优势。

Abstract: Eliciting scalability requirements during agile software development is complicated and poorly described in previous research. This article presents a lightweight artifact for eliciting scalability requirements during agile software development: the ScrumScale model. The ScrumScale model is a simple spreadsheet. The scalability concepts underlying the ScrumScale model are clarified in this design science research, which also utilizes coordination theory. This paper describes the open banking case study, where a legacy banking system becomes open. This challenges the scalability of this legacy system. The first step in understanding this challenge is to elicit the new scalability requirements. In the open banking case study, key stakeholders from TietoEVRY spent 55 hours eliciting TietoEVRY's open banking project's scalability requirements. According to TietoEVRY, the ScrumScale model provided a systematic way of producing scalability requirements. For TietoEVRY, the scalability concepts behind the ScrumScale model also offered significant advantages in dialogues with other stakeholders.

</details>


### [78] [SpreadsheetCoder: Formula Prediction from Semi-structured Context](https://arxiv.org/abs/2106.15339)
*Xinyun Chen,Petros Maniatis,Rishabh Singh,Charles Sutton,Hanjun Dai,Max Lin,Denny Zhou*

Main category: cs.SE

TL;DR: SpreadsheetCoder利用表格上下文（包括表头和半结构化数据）来预测电子表格公式，显著优于仅使用输入输出示例的方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅将输入输出示例作为电子表格公式合成的规范，未能充分利用真实电子表格中的丰富表格上下文（如行、列依赖关系和表头）。

Method: 提出了一种名为SpreadsheetCoder的基于BERT的模型架构，该模型能够同时处理基于行和基于列的表格上下文表示，并使用大型电子表格数据集进行训练。

Result: SpreadsheetCoder 在预测准确性方面达到了 42.51% 的 top-1 准确率，显著优于不使用丰富表格上下文的基线方法。在实际应用中，SpreadsheetCoder 比基于规则的系统能帮助多 82% 的用户在 Google Sheets 上编写公式。

Conclusion: SpreadsheetCoder 是第一个从包含表头和半结构化表格数据的表格上下文中合成电子表格公式的方法，在准确性和用户辅助方面均取得了显著的改进。

Abstract: Spreadsheet formula prediction has been an important program synthesis problem with many real-world applications. Previous works typically utilize input-output examples as the specification for spreadsheet formula synthesis, where each input-output pair simulates a separate row in the spreadsheet. However, this formulation does not fully capture the rich context in real-world spreadsheets. First, spreadsheet data entries are organized as tables, thus rows and columns are not necessarily independent from each other. In addition, many spreadsheet tables include headers, which provide high-level descriptions of the cell data. However, previous synthesis approaches do not consider headers as part of the specification. In this work, we present the first approach for synthesizing spreadsheet formulas from tabular context, which includes both headers and semi-structured tabular data. In particular, we propose SpreadsheetCoder, a BERT-based model architecture to represent the tabular context in both row-based and column-based formats. We train our model on a large dataset of spreadsheets, and demonstrate that SpreadsheetCoder achieves top-1 prediction accuracy of 42.51%, which is a considerable improvement over baselines that do not employ rich tabular context. Compared to the rule-based system, SpreadsheetCoder assists 82% more users in composing formulas on Google Sheets.

</details>


### [79] [From webtables to datatables](https://arxiv.org/abs/2006.14694)
*Mária Csernoch*

Main category: cs.SE

TL;DR: 网页表格是教授电子表格技能的宝贵资源，可用于商业和专业组织，用于开发计算思维技能。


<details>
  <summary>Details</summary>
Motivation: 网页表格，即网页上的表格和类似表格的结构，是教授电子表格技能的绝佳来源，可在商业和专业组织中用于利用和开发知识转移项目，提出和处理各种实际问题和解决方案，进行讨论和调试，以及普遍地开发和利用计算思维技能。

Method: 本文详细介绍了其中一个LOL排行榜（英雄联盟，Riot Games Inc. 2019）的转换过程。在给出转换算法后，提供了两种解决方案——一种在文字处理器中，另一种完全在电子表格应用程序中——为讨论、发明其他解决方案以及组合它们留下了空间。

Result: 本文介绍了一个将LOL排行榜数据转换为可用于电子表格的格式的算法，并提供了在文字处理器和电子表格应用程序中实现该算法的两种解决方案。

Conclusion: 通过将网页表格转换为电子表格，可以有效地教授和应用计算思维技能，并为解决实际问题提供灵活的解决方案。

Abstract: Webtables -- tables and table-like structures on webpages -- are excellent sources for teaching spreadsheeting, in commercial and professional organisations by utilizing and developing knowledge-transfer items, presenting and handling various real-world problems and solutions, discussing and debugging, and in general, developing and utilizing computational thinking skills. In the present paper the conversion process of one of the LOL Boards (League of Legends, Riot Games Inc. 2019) is detailed. After presenting the algorithm of the conversion, two solutions are offered -- one in a word processor, the other purely in a spreadsheet application -- leaving space for discussions, inventing other solutions and combining them.

</details>


### [80] [Implementation Strategies for Multidimensional Spreadsheets](https://arxiv.org/abs/2006.05814)
*Paul Mireault*

Main category: cs.SE

TL;DR: Excel开发者在处理多维变量时，主要采用两种策略：将多维变量投影到二维平面，或使用数据库方法。


<details>
  <summary>Details</summary>
Motivation: 分析Excel开发者在处理多维变量时的不同实现策略。

Method: 邀请Excel开发者实现包含多维变量的电子表格，并分析其实现策略。

Result: 识别出两种主要策略：二维投影和数据库方法。数据库方法可以简化公式。

Conclusion: Excel开发者在处理多维变量时，可以采用二维投影或数据库方法，其中数据库方法在简化公式方面更具优势。

Abstract: Seasoned Excel developers were invited to participate in a challenge to implement a spreadsheet with multi-dimensional variables. We analyzed their spreadsheet to see the different implement strategies employed. We identified two strategies: most participants used a projection of three or four-dimensional variables on the two-dimensional plane used by Excel. A few participants used a database approach where the multi-dimensional variables are presented in the form of a dataset table with the appropriate primary key. This approach leads to simpler formulas.

</details>


### [81] [Abstracting spreadsheet data flow through hypergraph redrawing](https://arxiv.org/abs/2006.04794)
*David Birch,Nicolai Stawinoga,Jack Binks,Bruno Nicoletti,Paul Kelly*

Main category: cs.SE

TL;DR: 传统电子表格因其低抽象级别而易出错。本文提出通过“重新绘制单元格边界”来提高电子表格的抽象级别，将电子表格转换为细粒度图，将“单元格”表示为围绕一组操作符/数据节点的超图边。通过识别常见子表达式和应用子树同构来检测向量（数组）运算。


<details>
  <summary>Details</summary>
Motivation: 传统电子表格的易错性源于其低抽象级别，用户被迫使用低级的单元格来构建数据模型，而单元格内容有限且链接隐藏。

Method: 通过将电子表格转换为细粒度图，将操作符和值作为节点，并将“单元格”表示为超图边来提高抽象级别。提出通过重新绘制单元格边界来创建更高级别的单元格，并利用常见子表达式识别和子树同构来检测向量运算。

Result: 提出了一种新的电子表格模型，其中单元格表示为围绕一组操作符/数据节点的超图边，从而提高了抽象级别并暴露了隐藏的链接结构。

Conclusion: 通过提高电子表格的抽象级别，可以创建更符合用户心智模型的电子表格，从而减少错误。所提出的方法通过细粒度图和超图边来表示电子表格，并通过识别常见子表达式和子树同构来检测向量运算。

Abstract: We believe the error prone nature of traditional spreadsheets is due to their low level of abstraction. End user programmers are forced to construct their data models from low level cells which we define as "a data container or manipulator linked by user-intent to model their world and positioned to reflect its structure". Spreadsheet cells are limited in what they may contain (scalar values) and the links between them are inherently hidden. This paper proposes a method of raising the level of abstraction of spreadsheets by "redrawing the boundary" of the cell. To expose the hidden linkage structure we transform spreadsheets into fine-grained graphs with operators and values as nodes. "cells" are then represented as hypergraph edges by drawing a boundary "wall" around a set of operator/data nodes. To extend what cells may contain and to create a higher level model of the spreadsheet we propose that researchers should seek techniques to redraw these boundaries to create higher level "cells" which will more faithfully represent the end-user's real world/mental model. We illustrate this approach via common sub-expression identification and the application of sub-tree isomorphisms for the detection of vector (array) operations.

</details>


### [82] [Comprehensive review for common types of errors using spreadsheets](https://arxiv.org/abs/1912.09209)
*Ali Aburas*

Main category: cs.SE

TL;DR: 电子表格因其灵活性和组织数据的能力而被广泛使用，但其中包含大量错误。本研究全面回顾和分类了查找和修复电子表格中错误的方法，讨论了现有研究，包括它们的定义、工作原理以及可以发现的错误类型，并探讨了用户常犯的错误。


<details>
  <summary>Details</summary>
Motivation: 鉴于电子表格在组织中的广泛使用及其普遍存在的错误，需要对现有的错误预防、检测和纠正方法进行全面审查。

Method: 对查找和修复电子表格中错误的研究工作进行全面的审查和分类，讨论其定义、工作原理和所能发现的错误类型。

Result: 总结了当前研究中用于查找和修复电子表格中错误的不同方法，并讨论了用户常犯的错误类型。

Conclusion: 本综述提供了电子表格错误检测和修复研究的全面概述，有助于用户和研究人员更好地理解和解决这些问题。

Abstract: Thanks to their flexibility and capability to perform different tasks and organize data in the best form and format, spreadsheets are widely used in different organizations and by different end users. Many business organizations rely on spreadsheets to fulfill their various tasks. On the other hand, the number of spreadsheets that contain errors are very high, thus researchers have developed different tools aimed at the prevention, detection, and correction of errors in spreadsheets. This research work is a comprehensive review that describes and classifies approaches on finding and fixing errors in spreadsheets. The paper discusses up-to-date research work approaches in terms of definition, how they work, and kinds of errors they can find in spreadsheets. The paper looks also for the kinds of errors that end users commonly make in spreadsheets.

</details>


### [83] [A Coding-free Software Framework of Developing Web Data Management Systems](https://arxiv.org/abs/1910.05685)
*Can Yang,Shiying Pan,Runmin Li,Yu Liu,Lizhang Peng*

Main category: cs.SE

TL;DR: 本论文提出了一种无需编程即可构建数据管理系统的理论和方法，利用SaaS架构，通过解析电子表格中的需求表来生成定制化的数据管理系统。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的企业将数据管理系统部署到云端，但由于软件开发的专业性，即使是小型系统，对于非程序员来说仍然难以开发。SaaS架构的出现为无代码开发提供了可行性。

Method: 基于SaaS架构，通过抽象数据管理系统的共性特征，设计了一个通用的Web平台，并提出了一种使用电子表格中的特定需求表来开发数据管理系统的方法。该平台通过解析需求表模型并在运行阶段实现目标系统来映射需求表。

Result: 实现所提出的框架并将其部署在Web上，并通过实证结果证明了开发Web数据管理系统中无需编程的方法的可行性和可用性。

Conclusion: 本研究提出的无需编程即可构建数据管理系统的理论和方法是可行且可用的，能够帮助非程序员快速开发定制化的数据管理系统。

Abstract: More and more enterprises recently intend to deploy data management systems in the cloud. Due to the professionalism of software development, it has still been difficult for non-programmers to develop this kind of systems, even a small one. However, the development of SaaS brings forth the more feasibility of coding-free software development than before. Based on the SaaS architecture, this paper presents a set of theory and method for coding-free construction of a data management system, on which our contributions involve in a practical application platform, a set of construction method and a set of interface on data exchange. By abstracting the common features of data management systems, we design a universal web platform to quickly generate and publish customized system instances. Moreover, we propose a kind of method to develop a data management system using a specific requirements table in spreadsheet. The corresponding platform maps the requirements table into a system instance through parsing the table model and implementing the objective system in the running stage. Finally, we implement the proposed framework and deploy it on web. The empirical result demonstrates the feasibility and availability of the coding-free method in developing web data management systems.

</details>


### [84] [Structured Spreadsheet Modelling and Implementation with Multiple Dimensions -- Part 2: Implementation](https://arxiv.org/abs/1909.00891)
*Paul Mireault*

Main category: cs.SE

TL;DR: 该论文展示了如何在多维变量（如产品、区域、行业和月份）的问题中实现一个多维模型，以生成易于维护的电子表格。


<details>
  <summary>Details</summary>
Motivation: 介绍如何实现一个多维模型，以生成易于维护的电子表格。

Method: 展示了实现多维问题的精确步骤，这些步骤将生成一个易于维护的电子表格。

Result: 实现了一个易于维护的电子表格。

Conclusion: 介绍如何实现一个多维模型，以生成易于维护的电子表格。

Abstract: In Part 1, we showed how to develop a conceptual model of a problem involving variables of multiple dimensions, like Products, Regions, Sectors and Months. The conceptual model is presented as a Formula Diagram, giving a global view of the interaction between all the variables, and a Formula List, giving a precise view of the interaction between the variables. In this paper, we present precise steps to implement a multi-dimensional problem in a way that will produce a spreadsheet that is easy to maintain

</details>


### [85] [On the Refinement of Spreadsheet Smells by means of Structure Information](https://arxiv.org/abs/1810.04542)
*Patrick Koch,Birgit Hofer,Franz Wotawa*

Main category: cs.SE

TL;DR: 通过利用推断的结构信息来改进电子表格的异味检测，以减少不正确和冗余的报告，并发现新的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的电子表格异味检测技术存在缺陷，会导致不正确或冗余的报告，使问题复杂化。

Method: 提出一种静态分析方法来推断相关单元格的簇和块，并利用这些结构信息来改进现有的异味检测技术并提出三种新的检测技术。

Result: 实验评估表明，改进后的技术成功减少了不正确和冗余的异味报告数量，并且新引入的异味揭示了新的缺陷。

Conclusion: 通过整合推断的结构信息来改进电子表格的异味检测，可以显著提高其准确性和有效性，并发现新的问题。

Abstract: Spreadsheet users are often unaware of the risks imposed by poorly designed spreadsheets. One way to assess spreadsheet quality is to detect smells which attempt to identify parts of spreadsheets that are hard to comprehend or maintain and which are more likely to be the root source of bugs. Unfortunately, current spreadsheet smell detection techniques suffer from a number of drawbacks that lead to incorrect or redundant smell reports. For example, the same quality issue is often reported for every copy of a cell, which may overwhelm users. To deal with these issues, we propose to refine spreadsheet smells by exploiting inferred structural information for smell detection. We therefore first provide a detailed description of our static analysis approach to infer clusters and blocks of related cells. We then elaborate on how to improve existing smells by providing three example refinements of existing smells that incorporate information about cell groups and computation blocks. Furthermore, we propose three novel smell detection techniques that make use of the inferred spreadsheet structures. Empirical evaluation of the proposed techniques suggests that the refinements successfully reduce the number of incorrectly and redundantly reported smells, and novel deficits are revealed by the newly introduced smells.

</details>


### [86] [Now You're Thinking With Structures: A Concept for Structure-based Interactions with Spreadsheets](https://arxiv.org/abs/1809.03435)
*Patrick Koch*

Main category: cs.SE

TL;DR: Spreadsheets are complex and hard to understand. This paper proposes a structure-aware system to enhance spreadsheet comprehension and interaction by enriching visualizations and providing proactive tools, aiming to improve user productivity and spreadsheet quality.


<details>
  <summary>Details</summary>
Motivation: Cognition of complex systems is improved by higher-order mental models. Existing spreadsheets are hard to comprehend and adapt beyond a certain complexity.

Method: The paper presents a concept for structure-aware understanding and interaction with spreadsheets, extending previous work on structure inference. This concept enriches visualizations, reactively enhances user actions, and provides tools to proactively alter spreadsheet structure, implemented as a tool for structure inference and visualization.

Result: A tool for structure inference and visualization has been implemented, forming a framework for introducing proactive and reactive interaction mechanics and structure-aware functionality as an add-in.

Conclusion: Providing tools for structure-aware thinking and interaction will benefit users by improving both productivity and the overall quality of spreadsheets.

Abstract: Spreadsheets are the go-to tool for computerized calculation and modelling, but are hard to comprehend and adapt after reaching a certain complexity. In general, cognition of complex systems is facilitated by having a higher order mental model of the system in question to work with. We therefore present a concept for structure-aware understanding of and interaction with spreadsheets that extends previous work on structure inference in the domain. Following this concept, structural information is used to enrich visualizations, reactively enhance traditional user actions, and provide tools to proactively alter the overall spreadsheet makeup instead of individual cells The intended systems should, in first approximation, not replace common spreadsheet tools, but provide an additional layer of functionality alongside the established interface. In ongoing work, we therefore implemented a tool for structure inference and visualization along the common spreadsheet layout. Based on this framework, we plan to introduce the envisioned proactive and reactive interaction mechanics, and finally provide structure-aware unctionality as an add-in for common spreadsheet processors. We believe that providing the tools for thinking about and interacting with spreadsheets in this manner will benefit users both in terms of productivity and overall spreadsheet quality.

</details>


### [87] [Typed Table Transformations](https://arxiv.org/abs/1809.02746)
*Martin Erwig*

Main category: cs.SE

TL;DR: 电子表格表格的标签定义了其中数据的类型，表格结构由行列类型的定义决定。本文提出了一种基于行列类型定义的表格转换新方法，并阐述了未来研究中应解决的一系列研究问题。


<details>
  <summary>Details</summary>
Motivation: 在电子表格表格中，标签定义了其中数据的类型，表格结构由行列类型的定义决定。本文提出了一种基于行列类型定义的表格转换新方法。

Method: 提出了一种基于行列类型定义的表格转换新方法。

Result: 本文阐述了表格构建和转换的基本思想。

Conclusion: 未来研究中应解决一系列关于表格构建和转换的研究问题。

Abstract: Spreadsheet tables are often labeled, and these labels effectively constitute types for the data in the table. In such cases tables can be considered to be built from typed data where the placement of values within the table is controlled by the types used for rows and columns. We present a new approach to the transformations of spreadsheet tables that is based on transformations of row and column types. We illustrate the basic idea of type-based table construction and transformation and lay out a series of research questions that should be addressed in future work.

</details>


### [88] [Implementing WHERE and ORDER BY as spreadsheet formulas](https://arxiv.org/abs/1809.00025)
*Paul Mireault*

Main category: cs.SE

TL;DR: We develop spreadsheet formulas that implement SQL's WHERE and ORDER BY clauses, which automatically react to changes in spreadsheet values.


<details>
  <summary>Details</summary>
Motivation: Spreadsheet users often need to filter and sort data, similar to SQL's WHERE and ORDER BY clauses. Existing tools like Excel's filter/sort buttons, Query, or Pivot Tables do not automatically update when underlying data changes.

Method: Develop spreadsheet formulas to replicate the functionality of SQL's WHERE and ORDER BY clauses.

Result: The developed formulas allow for dynamic filtering and sorting of spreadsheet data, automatically updating with changes in calculated values.

Conclusion: Spreadsheet formulas can effectively implement SQL's WHERE and ORDER BY clauses, providing a more dynamic data manipulation solution for users compared to existing spreadsheet tools.

Abstract: The WHERE and ORDER BY clauses of the SQL SELECT statement select a subset of rows in the result of a database query and present the result in the specified order. In a spreadsheet program like Microsoft Excel, one could use the filter and sort buttons, or use its Query or its Pivot Table tools to achieve a similar effect. The disadvantage of using those tools is that they don't react automatically to changes in the calculated values of the spreadsheet. In this paper, we develop spreadsheet formulas that implement SQL's WHERE and ORDER BY clauses.

</details>


### [89] [The use of Charts, Pivot Tables, and Array Formulas in two Popular Spreadsheet Corpora](https://arxiv.org/abs/1808.10642)
*Bas Jansen,Felienne Hermans*

Main category: cs.SE

TL;DR: 电子表格被广泛用于工业界，但它们容易出错，可能导致错误的决策。虽然已有研究关注公式的错误，但对于图表、数据透视表和数组公式等其他构造的研究却很少。为了提高电子表格的质量，理解这些构造的使用方式至关重要，因此本研究分析了 Enron 和 EUSES 两个流行的电子表格语料库中这些构造的使用情况。


<details>
  <summary>Details</summary>
Motivation: 电子表格在工业界被广泛使用，但它们容易出错，可能导致错误的决策和经济损失。目前的研究主要集中在公式的错误，而忽视了图表、数据透视表和数组公式等其他构造。为了提高电子表格的质量，理解这些构造的使用方式至关重要。

Method: 分析 Enron 和 EUSES 两个流行的电子表格语料库，研究图表、数据透视表和数组公式的使用情况。

Result: （由于原文未提供具体结果，此部分留空）

Conclusion: 为了提高电子表格的质量，需要将图表、数据透视表和数组公式等构造的使用纳入研究范围，以获得对电子表格使用的完整理解。

Abstract: The use of spreadsheets in industry is widespread. Companies base decisions on information coming from spreadsheets. Unfortunately, spreadsheets are error-prone and this increases the risk that companies base their decisions on inaccurate information, which can lead to incorrect decisions and loss of money. In general, spreadsheet research is aimed to reduce the error-proneness of spreadsheets. Most research is concentrated on the use of formulas. However, there are other constructions in spreadsheets, like charts, pivot tables, and array formulas, that are also used to present decision support information to the user. There is almost no research about how these constructions are used. To improve spreadsheet quality it is important to understand how spreadsheets are used and to obtain a complete understanding, the use of charts, pivot tables, and array formulas should be included in research. In this paper, we analyze two popular spreadsheet corpora: Enron and EUSES on the use of the aforementioned constructions.

</details>


### [90] [Asheetoxy: A Taxonomy for Classifying Negative Spreadsheet-related Phenomena](https://arxiv.org/abs/1808.10231)
*Daniel Kulesz,Stefan Wagner*

Main category: cs.SE

TL;DR: Asheetoxy是一种新的、面向现象的电子表格错误分类法，它避免了使用“错误”这个词，并且易于非专业人士使用。


<details>
  <summary>Details</summary>
Motivation: 现有电子表格错误分类法存在歧义和应用门槛高的问题，阻碍了研究者和专业人士的讨论。

Method: 提出了一种名为Asheetoxy的简单、面向现象的分类法，并进行了一项包含7名参与者的初步研究。

Result: 研究表明，即使是没有电子表格研究背景的参与者，也能使用Asheetoxy对现实世界中的电子表格现象进行分类。

Conclusion: Asheetoxy提供了一种更简单、更易于理解和应用的电子表格错误分类方法，有助于弥合研究者和专业人士之间的差距。

Abstract: Spreadsheets (sometimes also called Excel programs) are powerful tools which play a business-critical role in many organizations. However, due to faulty spreadsheets many bad decisions have been taken in recent years. Since then, a number of researchers have been studying spreadsheet errors. However, one issue that hinders discussion among researchers and professionals is the lack of a commonly accepted taxonomy.
  Albeit a number of taxonomies for spreadsheet errors have been proposed in previous work, a major issue is that they use the term error that itself is already ambiguous. Furthermore, to apply most existing taxonomies, detailed knowledge about the underlying process and knowledge about the "brain state" of the acting spreadsheet users is required. Due to these limitations, known error-like phenomena in freely available spreadsheet corpora cannot be classified with these taxonomies.
  We propose Asheetoxy, a simple and phenomenon-oriented taxonomy that avoids the problematic term error altogether. An initial study with 7 participants indicates that even non-spreadsheet researchers similarly classify real-world spreadsheet phenomena using Asheetoxy.

</details>


### [91] [Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (SEMS'18)](https://arxiv.org/abs/1808.09174)
*Birgit Hofer,Jorge Mendes*

Main category: cs.SE

TL;DR: 该论文集是关于电子表格软件工程方法的第五届国际研讨会（SEMS'18）的论文集。


<details>
  <summary>Details</summary>
Motivation: 研讨会的动机是聚集该领域的专家，讨论电子表格软件工程的最新进展。

Method: 该论文集包含由专家提交和审查的论文，涵盖了电子表格软件工程的各个方面。

Result: 论文集展示了电子表格软件工程领域的最新研究成果和最佳实践。

Conclusion: 该论文集为电子表格软件工程领域的研究人员和从业人员提供了宝贵的见解和资源。

Abstract: Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (SEMS'18), held on October 1st, 2018, in Lisbon, Portugal, and co-located with the 2018 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC).

</details>


### [92] [Combining Spreadsheet Smells for Improved Fault Prediction](https://arxiv.org/abs/1805.10493)
*Patrick Koch,Konstantin Schekotihin,Dietmar Jannach,Birgit Hofer,Franz Wotawa*

Main category: cs.SE

TL;DR: 通过使用 AdaBoost 集成学习器，可以提高电子表格代码异味的故障预测能力。


<details>
  <summary>Details</summary>
Motivation: 电子表格被广泛用作商业计算和决策工具，但其中的错误可能导致严重的业务后果。现有的电子表格异味分析表明，单个异味的预测能力有限。

Method: 提出了一种基于机器学习的方法，该方法结合了单个电子表格异味的预测能力，并使用 AdaBoost 集成学习器。

Result: 在包含真实世界电子表格错误的两个公共数据集上进行的实验表明，故障预测准确性得到了显著提高。

Conclusion: 所提出的基于 AdaBoost 集成学习器的方法可以有效提高电子表格代码异味的故障预测准确性。

Abstract: Spreadsheets are commonly used in organizations as a programming tool for business-related calculations and decision making. Since faults in spreadsheets can have severe business impacts, a number of approaches from general software engineering have been applied to spreadsheets in recent years, among them the concept of code smells. Smells can in particular be used for the task of fault prediction. An analysis of existing spreadsheet smells, however, revealed that the predictive power of individual smells can be limited. In this work we therefore propose a machine learning based approach which combines the predictions of individual smells by using an AdaBoost ensemble classifier. Experiments on two public datasets containing real-world spreadsheet faults show significant improvements in terms of fault prediction accuracy.

</details>


### [93] [An Easy & Collaborative RDF Data Entry Method using the Spreadsheet Metaphor](https://arxiv.org/abs/1804.04175)
*Markus Schröder,Christian Jilek,Jörn Hees,Sven Hertling,Andreas Dengel*

Main category: cs.SE

TL;DR: 本文提出了一种易于使用的、零配置的、基于Web的电子表格编辑器，可将电子表格条目同时转换为RDF语句，以简化知识工作者（尤其是工业领域的知识工作者）创建语义数据。


<details>
  <summary>Details</summary>
Motivation: 知识工作者（尤其是在工业领域）广泛使用电子表格，因为它们提供了易于理解、简单快捷的数据输入方法。然而，与创建RDF语句相比，填写电子表格对普通知识工作者来说更容易。因此，需要一种工具来简化语义数据的创建过程。

Method: 提出了一种易于使用的、零配置的、基于Web的电子表格编辑器，可将电子表格条目同时转换为RDF语句。该工具支持各种用户（无论是否为RDF专家）轻松创建语义数据，特别适用于从空知识库开始逐步填充实例数据。

Result: 用户研究表明，与其它方法相比，使用该编辑器的参与者能够在更短的时间内创建更多语句，并且质量相当或显著更高。

Conclusion: 所提出的电子表格编辑器能够有效地简化语义数据的创建，提高了效率和质量，尤其适用于知识工作者在没有RDF专业知识的情况下创建实例数据。

Abstract: Spreadsheets are widely used by knowledge workers, especially in the industrial sector. Their methodology enables a well understood, easy and fast possibility to enter data. As filling out a spreadsheet is more accessible to common knowledge workers than defining RDF statements, in this paper, we propose an easy-to-use, zero-configuration, web-based spreadsheet editor that simultaneously transfers spreadsheet entries into RDF statements. It enables various kinds of users to easily create semantic data whether they are RDF experts or novices. The typical scenario we address focuses on creating instance data starting with an empty knowledge base that is filled incrementally. In a user study, participants were able to create more statements in shorter time, having similar or even significantly outperforming quality, compared to other approaches.

</details>


### [94] [Structured Spreadsheet Modelling and Implementation with Multiple Dimensions - Part 1: Modelling](https://arxiv.org/abs/1801.09777)
*Paul Mireault*

Main category: cs.SE

TL;DR: The paper discusses the common use of dimensions, particularly time, in everyday models like spreadsheets, and highlights the inefficiencies in representing a second dimension.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the common practice of using dimensions, especially time, in everyday models and the associated inefficiencies in representing a second dimension.

Method: The paper describes the common methods used to represent a second dimension in spreadsheets, such as repeating formula blocks or creating multiple worksheets with identical structures.

Result: The paper identifies that repeating formula blocks or creating multiple worksheets are common, albeit inefficient, methods for representing a second dimension.

Conclusion: The paper implies that current methods for handling multiple dimensions in spreadsheets are not optimal and suggests a need for better approaches.

Abstract: Dimensions are an integral part of many models we use every day. Without thinking about it, we frequently use the time dimension: many financial and accounting spreadsheets have columns representing months or years. Representing a second dimension is often done by repeating blocs of formulas in a worksheet or creating multiple worksheets with the same structure.

</details>


### [95] [Mitigating Spreadsheet Risk in Complex Multi-Dimensional Models in Excel](https://arxiv.org/abs/1802.01640)
*Steve Litt*

Main category: cs.SE

TL;DR: Excel is widely used but risky due to manual processes. PivotModel aims to mitigate this risk for complex, multi-dimensional models by leveraging Excel's capabilities, similar to how PivotTables work.


<details>
  <summary>Details</summary>
Motivation: Spreadsheets like Excel are powerful and flexible but prone to manual errors, leading to significant spreadsheet risk for companies. This is especially true for 'complex multi-dimensional models' used for tasks like planning and forecasting.

Method: The proposed solution, 'PivotModel', is designed to function like a PivotTable within Microsoft Excel. It aims to mitigate spreadsheet risk by leveraging Excel's robust capabilities for complex, multi-dimensional models.

Result: The paper proposes a solution called 'PivotModel' to address spreadsheet risk in complex, multi-dimensional models.

Conclusion: PivotModel is presented as a solution to mitigate spreadsheet risk by enhancing the functionality of Excel for complex analytical tasks.

Abstract: Microsoft Excel is the most ubiquitous analytical tool ever built. Companies around the world leverage it for its power, flexibility and ease of use. However, spreadsheets are manually intensive and prone to error, making it difficult for companies to control spreadsheet risk. The following solution is designed to mitigate spreadsheet risk for a set of problems commonly addressed in a spreadsheet defined as "complex multi-dimensional models". "Complex" referring to certain types of applications that require functionality such as sophisticated algorithms, challenging hierarchies and database write-back (i.e. planning, forecasting, etc.) and "multi-dimensional" referring to providing capabilities such as reporting, data input forms and ad hoc analysis on the different attributes associated with the resulting model. The solution is defined as a "PivotModel" because it works similarly to a PivotTable but is designed to leverage the robust capabilities of the Microsoft Excel platform.

</details>


### [96] [Proposed Spreadsheet Transparency Definition and Measures](https://arxiv.org/abs/1802.01628)
*Craig Hatmaker*

Main category: cs.SE

TL;DR: 该论文提出了一个关于电子表格模型透明度的定义，旨在解决当前缺乏明确定义和量化标准的问题。


<details>
  <summary>Details</summary>
Motivation: 当前金融建模领域缺乏对“模型透明度”的明确定义和量化标准，导致无法客观评估模型和选择更优的建模方法。

Method: 提出一个具体的电子表格模型透明度定义，该定义可用于创建度量和自动化工具，以便审计师评估模型透明度，并允许建模者客观比较不同的建模方法。

Result: 提出一个具体的电子表格模型透明度定义，该定义可用于创建度量和自动化工具，以便审计师评估模型透明度，并允许建模者客观比较不同的建模方法。

Conclusion: 该论文提出的透明度定义能够为审计师提供量化工具，为建模者提供客观的比较方法，从而解决当前金融建模透明度方面存在的争议和不足。

Abstract: Auditors demand financial models be transparent yet no consensus exists on what that means precisely. Without a clear modeling transparency definition we cannot know when our models are "transparent". The financial modeling community debates which methods are more or less transparent as though transparency is a quantifiable entity yet no measures exist. Without a transparency measure modelers cannot objectively evaluate methods and know which improves model transparency.
  This paper proposes a definition for spreadsheet modeling transparency that is specific enough to create measures and automation tools for auditors to determine if a model meets transparency requirements. The definition also provides modelers the ability to objectively compare spreadsheet modeling methods to select which best meets their goals.

</details>


### [97] [Alternative Spreadsheet Model Designs for an Operations Management Model Embedded in a Periodic Business Process](https://arxiv.org/abs/1802.00484)
*Thomas A. Grossman,Vijay Mehrotra,Mouwafac Sidaoui*

Main category: cs.SE

TL;DR: 本文提出了一种用于供应链和分销规划的运筹学管理模型，并探讨了三种不同的电子表格实现方式：数据驱动设计、典型设计和一种新颖的表格驱动技术设计。


<details>
  <summary>Details</summary>
Motivation: 在周期性业务流程中，运筹学模型需要经常修改和重用，因此需要评估不同实现方式的适用性。

Method: 评估了三种电子表格实现方式（数据驱动、典型、表格驱动）在准确性、可修改性、可分析性和可迁移性方面的优劣，并考虑了使用它们所需的技术培训和复杂性。

Result: 数据驱动设计揭示了初学者在电子表格建模中存在的问题。典型设计易于理解但修改困难。表格驱动技术设计允许在不手动编辑公式的情况下修改模型以适应新数据和结构，从而提高了修改速度并降低了错误风险。

Conclusion: 表格驱动技术设计在模型修改和错误风险控制方面表现出色，并具有应用于其他模型类的潜力。同时，本文也指出了未来研究的方向。

Abstract: We present a widely-used operations management model used in supply and distribution planning, that is typically embedded in a periodic business process that necessitates model modification and reuse. We consider three alternative spreadsheet implementations, a data-driven design, a canonical (textbook) design, and a novel (table-driven) technical design. We evaluate each regarding suitability for accuracy, modification, analysis, and transfer. We consider the degree of training and technical sophistication required to utilize each design. The data-driven design provides insight into poor spreadsheet practices by naïve modelers. The technical design can be modified for new data and new structural elements without manual writing or editing of cell formulas, thus speeding modification and reducing risk of error. The technical design has potential for use with other classes of models. We identify opportunities for future research.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [98] [Radif Corpus: A Symbolic Dataset for Non-Metric Iranian Classical Music](https://arxiv.org/abs/2507.10456)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.SD

TL;DR: 该研究介绍了一个包含伊朗古典音乐完整非公制radif曲目的数字语料库，并提供MIDI文件和数据电子表格，可用于计算音乐学研究。


<details>
  <summary>Details</summary>
Motivation: 介绍伊朗古典音乐的核心——radif，并提出构建一个数字语料库以支持相关计算研究。

Method: 构建了一个包含228首乐曲的数字语料库，提供MIDI文件和详细数据，涵盖所有13个radif组成部分，并忠实呈现了包括四分音和非公制在内的音乐特征，同时提供统计数据、复杂性和相似性度量。

Result: 创建了一个数字语料库，包含281分钟的MIDI文件和数据电子表格，详细描述了音乐的各个方面，并进行了初步的统计分析。

Conclusion: 该语料库为伊朗古典音乐的计算研究提供了一个平台，可用于研究旋律模式、即兴风格以及音乐信息检索、音乐理论和计算（民族）音乐学等领域的其他任务。

Abstract: Non-metric music forms the core of the repertoire in Iranian classical music. Dastgahi music serves as the underlying theoretical system for both Iranian art music and certain folk traditions. At the heart of Iranian classical music lies the radif, a foundational repertoire that organizes melodic material central to performance and pedagogy.
  In this study, we introduce a digital corpus representing the complete non-metrical radif repertoire, covering all 13 existing components of this repertoire. We provide MIDI files (about 281 minutes in total) and data spreadsheets describing notes, note durations, intervals, and hierarchical structures for 228 pieces of music. We faithfully represent the tonality including quarter-tones, and the non-metric aspect. Furthermore, we provide supporting basic statistics, and measures of complexity and similarity over the corpus.
  Our corpus provides a platform for computational studies of Iranian classical music. Researchers might employ it in studying melodic patterns, investigating improvisational styles, or for other tasks in music information retrieval, music theory, and computational (ethno)musicology.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [99] [Mathematics of Digital Hyperspace](https://arxiv.org/abs/2103.15203)
*Jeremy Kepner,Timothy Davis,Vijay Gadepally,Hayden Jananthan,Lauren Milechin*

Main category: cs.MS

TL;DR: 数字高维空间由各种数据流组成，可以使用超图、超稀疏矩阵和关联数组代数来处理。本文提出了一种新的数学概念“semilink”，它结合了两个半环，为图分析、数据库操作和机器学习提供了基本操作。GraphBLAS标准支持这些数学概念，并可以通过添加键索引（如指向字符串的指针）和semilinks来增强，从而成为更丰富的关联数组代数，并可替代电子表格、数据库表和面向数据的操作系统，从而增强对数字高维空间中非结构化数据的导航。


<details>
  <summary>Details</summary>
Motivation: 数字高维空间中的非结构化数据需要新的数学方法来表示、遍历和转换。

Method: 提出了一种名为“semilink”的新数学概念，它结合了两个半环，并利用GraphBLAS标准来支持超图、超稀疏矩阵和semilinks，以实现图分析、数据库操作和机器学习。

Result: GraphBLAS可以通过添加键索引和semilinks来增强，成为更丰富的关联数组代数，并可替代电子表格、数据库表和面向数据的操作系统，从而增强对数字高维空间中非结构化数据的导航。

Conclusion: 通过引入semilinks和键索引，GraphBLAS可以成为一种更强大的工具，用于处理和导航数字高维空间中的非结构化数据。

Abstract: Social media, e-commerce, streaming video, e-mail, cloud documents, web pages, traffic flows, and network packets fill vast digital lakes, rivers, and oceans that we each navigate daily. This digital hyperspace is an amorphous flow of data supported by continuous streams that stretch standard concepts of type and dimension. The unstructured data of digital hyperspace can be elegantly represented, traversed, and transformed via the mathematics of hypergraphs, hypersparse matrices, and associative array algebra. This paper explores a novel mathematical concept, the semilink, that combines pairs of semirings to provide the essential operations for graph analytics, database operations, and machine learning. The GraphBLAS standard currently supports hypergraphs, hypersparse matrices, the mathematics required for semilinks, and seamlessly performs graph, network, and matrix operations. With the addition of key based indices (such as pointers to strings) and semilinks, GraphBLAS can become a richer associative array algebra and be a plug-in replacement for spreadsheets, database tables, and data centric operating systems, enhancing the navigation of unstructured data found in digital hyperspace.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [100] [Dynamic Kernel Matching for Non-conforming Data: A Case Study of T-cell Receptor Datasets](https://arxiv.org/abs/2103.10472)
*Jared Ostmeyer,Scott Christley,Lindsay Cowell*

Main category: q-bio.QM

TL;DR: 本研究提出动态核匹配（DKM）方法来处理非结构化数据，并成功应用于TCR序列和患者TCR库数据集，用于疾病诊断，且识别出的模式与实验观察一致。


<details>
  <summary>Details</summary>
Motivation: 现有的统计分类器主要处理结构化数据（如电子表格），而许多类型的数据不符合这种格式。为了在非结构化数据中发现模式，需要一种能够处理此类数据的方法。

Method: 提出并应用动态核匹配（DKM）方法来修改和增强现有的统计分类器，使其能够处理非结构化数据。将DKM应用于两个具体案例：(i) 带有疾病抗原标签的T细胞受体（TCR）序列数据集；(ii) 带有患者巨细胞病毒（CMV）血清状态标签的TCR库数据集。

Result: 成功将增强了DKM的统计分类器应用于这两个数据集，并使用标准指标和允许不确定诊断的指标报告了在测试数据上的性能。同时，识别出分类器用于生成预测的模式，并验证了这些模式与实验研究的观察结果一致。

Conclusion: DKM是一种有效的方法，可以扩展统计分类器的能力，以处理非结构化数据，并在生物医学领域（如TCR序列分析）具有实际应用价值，其识别的模式具有生物学意义。

Abstract: Most statistical classifiers are designed to find patterns in data where numbers fit into rows and columns, like in a spreadsheet, but many kinds of data do not conform to this structure. To uncover patterns in non-conforming data, we describe an approach for modifying established statistical classifiers to handle non-conforming data, which we call dynamic kernel matching (DKM). As examples of non-conforming data, we consider (i) a dataset of T-cell receptor (TCR) sequences labelled by disease antigen and (ii) a dataset of sequenced TCR repertoires labelled by patient cytomegalovirus (CMV) serostatus, anticipating that both datasets contain signatures for diagnosing disease. We successfully fit statistical classifiers augmented with DKM to both datasets and report the performance on holdout data using standard metrics and metrics allowing for indeterminant diagnoses. Finally, we identify the patterns used by our statistical classifiers to generate predictions and show that these patterns agree with observations from experimental studies.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [101] [Data Encryption Battlefield: A Deep Dive into the Dynamic Confrontations in Ransomware Attacks](https://arxiv.org/abs/2504.20681)
*Arash Mahboubi,Hamed Aboutorab,Seyit Camtepe,Hang Thanh Bui,Khanh Luong,Keyvan Ansari,Shenlu Wang,Bazara Barry*

Main category: cs.CR

TL;DR: 本研究提出使用在线增量机器学习算法来检测利用熵减少（如Base64编码）和间歇性加密等复杂加密技术来规避传统检测方法的勒索软件。


<details>
  <summary>Details</summary>
Motivation: 勒索软件通过复杂的加密技术（如熵减少和间歇性加密）不断演变，对传统检测方法构成挑战，因此需要开发先进的对策来保护数据。

Method: 利用包含32.6GB数据、11,928个文件（涵盖多种格式）和75个勒索软件家族的数据集，评估了Hoeffding Tree和带热启动功能的Random Forest等在线增量机器学习算法在检测不同加密策略方面的有效性。

Result: Hoeffding Tree算法在检测传统和AES-Base64加密方面表现出优越的增量学习能力；Random Forest分类器（带热启动）在识别间歇性加密方面表现出色。

Conclusion: 需要量身定制的机器学习解决方案来应对复杂的勒索软件加密策略，Hoeffding Tree和Random Forest算法在特定场景下各具优势。

Abstract: In the rapidly evolving landscape of cybersecurity threats, ransomware represents a significant challenge. Attackers increasingly employ sophisticated encryption methods, such as entropy reduction through Base64 encoding, and partial or intermittent encryption to evade traditional detection methods. This study explores the dynamic battle between adversaries who continuously refine encryption strategies and defenders developing advanced countermeasures to protect vulnerable data. We investigate the application of online incremental machine learning algorithms designed to predict file encryption activities despite adversaries evolving obfuscation techniques. Our analysis utilizes an extensive dataset of 32.6 GB, comprising 11,928 files across multiple formats, including Microsoft Word documents (doc), PowerPoint presentations (ppt), Excel spreadsheets (xlsx), image formats (jpg, jpeg, png, tif, gif), PDFs (pdf), audio (mp3), and video (mp4) files. These files were encrypted by 75 distinct ransomware families, facilitating a robust empirical evaluation of machine learning classifiers effectiveness against diverse encryption tactics. Results highlight the Hoeffding Tree algorithms superior incremental learning capability, particularly effective in detecting traditional and AES-Base64 encryption methods employed to lower entropy. Conversely, the Random Forest classifier with warm-start functionality excels at identifying intermittent encryption methods, demonstrating the necessity of tailored machine learning solutions to counter sophisticated ransomware strategies.

</details>


### [102] [JUBILEE: Secure Debt Relief and Forgiveness](https://arxiv.org/abs/2109.07267)
*David Cerezo Sánchez*

Main category: cs.CR

TL;DR: JUBILEE是一种安全、激励相容、个体理性的债务减免和豁免机制，利用安全计算技术提高了债务结算的效率和成功率，并提供了两种实际实现方式。


<details>
  <summary>Details</summary>
Motivation: JUBILEE旨在实现一种安全、无摩擦的债务减免和豁免机制，无需第三方信任，并通过激励机制鼓励各方披露真实信息，从而促进更和谐的债务结算。

Method: JUBILEE采用新颖的安全计算技术应用于债务减免，并实现了“债务人祝福”，提高了预期利润和成功率。该机制在个体理性、激励相容、真实性/策略证明、事后效率和最优性方面优于以往所有方法。

Result: JUBILEE通过安全计算技术，为债务减免和豁免带来了“债务人祝福”，即在不使用安全计算的情况下，债务结算具有更高的预期利润和成功概率。

Conclusion: JUBILEE提供了一种在不涉及可信第三方的情况下安全、无摩擦地进行债务减免和豁免的机制。它通过引入安全计算技术，实现了更高的效率、利润和成功率，并提供了“安全电子表格”和基于区块链的Raziel智能合约两种实现方式。

Abstract: JUBILEE is a securely computed mechanism for debt relief and forgiveness in a frictionless manner without involving trusted third parties, leading to more harmonious debt settlements by incentivising the parties to truthfully reveal their private information. JUBILEE improves over all previous methods:
  - individually rational, incentive-compatible, truthful/strategy-proof, ex-post efficient, optimal mechanism for debt relief and forgiveness with private information
  - by the novel introduction of secure computation techniques to debt relief, the "blessing of the debtor" is hereby granted for the first time: debt settlements with higher expected profits and a higher probability of success than without using secure computation
  A simple and practical implementation is included for "The Secure Spreadsheet". Another implementation is realised using Raziel smart contracts on a blockchain with Pravuil consensus.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [103] [Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models](https://arxiv.org/abs/2505.23667)
*Lang Cao,Jingxian Xu,Hanbing Liu,Jinyu Wang,Mengyu Zhou,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 通过使用可执行的电子表格公式和强化学习，提升大语言模型在表格数据问答中的数值和符号推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理表格数据的数值或符号推理方面存在挑战，而电子表格公式能够编码丰富的推理模式，这部分能力尚未得到充分利用。

Method: 提出Formula Tuning（Fortune）框架，这是一个强化学习框架，通过使用二元的答案正确性作为奖励信号，训练大语言模型生成可执行的电子表格公式，以进行表格数据问答。

Result: Formula Tuning 框架显著提升了大语言模型在表格理解任务上的性能，尤其在多步数值和符号推理任务上表现突出，使得一个7B的模型在表格理解能力上优于OpenAI o1。

Conclusion: 通过强化学习驱动的公式生成方法，可以有效提升大语言模型在表格数据上的符号推理能力。

Abstract: Tables are a fundamental structure for organizing and analyzing data, making effective table understanding a critical capability for intelligent systems. While large language models (LMs) demonstrate strong general reasoning abilities, they continue to struggle with accurate numerical or symbolic reasoning over tabular data, especially in complex scenarios. Spreadsheet formulas provide a powerful and expressive medium for representing executable symbolic operations, encoding rich reasoning patterns that remain largely underutilized. In this paper, we propose Formula Tuning (Fortune), a reinforcement learning (RL) framework that trains LMs to generate executable spreadsheet formulas for question answering over general tabular data. Formula Tuning reduces the reliance on supervised formula annotations by using binary answer correctness as a reward signal, guiding the model to learn formula derivation through reasoning. We provide a theoretical analysis of its advantages and demonstrate its effectiveness through extensive experiments on seven table reasoning benchmarks. Formula Tuning substantially enhances LM performance, particularly on multi-step numerical and symbolic reasoning tasks, enabling a 7B model to outperform OpenAI o1 on table understanding. This highlights the potential of formula-driven RL to advance symbolic table reasoning in LMs.

</details>


### [104] [The AI Risk Repository: A Comprehensive Meta-Review, Database, and Taxonomy of Risks From Artificial Intelligence](https://arxiv.org/abs/2408.12622)
*Peter Slattery,Alexander K. Saeri,Emily A. C. Grundy,Jess Graham,Michael Noetel,Risto Uuk,James Dao,Soroush Pour,Stephen Casper,Neil Thompson*

Main category: cs.AI

TL;DR: AI风险的广泛关注与缺乏统一理解之间的差距，通过构建一个包含777个风险的AI风险存储库来弥合，该存储库基于系统性文献回顾和专家咨询，并按因果因素和领域进行分类，以促进对AI风险更协调的管理。


<details>
  <summary>Details</summary>
Motivation: 为了解决学术界、审计界、政策制定界、AI公司和公众对人工智能（AI）风险的担忧，但普遍缺乏对AI风险的共同理解，从而阻碍了对其进行全面讨论、研究和应对。

Method: 通过系统性地回顾现有的43个AI风险分类法和其他结构化分类，并进行专家咨询，构建了一个包含777个AI风险的活数据库。采用“最佳拟合框架综合法”开发了AI风险分类法，包括一个基于（1）实体、（2）意图、（3）时序的高层因果分类法，以及一个包含七个领域（如歧视与毒性、隐私与安全等）及其细分领域的中层领域分类法。

Result: 创建了一个AI风险存储库，这是一个公开的、可扩展的、经过分类的风险数据库，收录了777个AI风险，可以通过网站和在线电子表格访问、修改和更新，并能按因果因素和领域进行筛选。

Conclusion: 该AI风险存储库是首个全面梳理、分析和提取AI风险框架的尝试，为制定、审计和管理AI风险提供了一个更协调、更连贯、更完整的框架基础。

Abstract: The risks posed by Artificial Intelligence (AI) are of considerable concern to academics, auditors, policymakers, AI companies, and the public. However, a lack of shared understanding of AI risks can impede our ability to comprehensively discuss, research, and react to them. This paper addresses this gap by creating an AI Risk Repository to serve as a common frame of reference. This comprises a living database of 777 risks extracted from 43 taxonomies, which can be filtered based on two overarching taxonomies and easily accessed, modified, and updated via our website and online spreadsheets. We construct our Repository with a systematic review of taxonomies and other structured classifications of AI risk followed by an expert consultation. We develop our taxonomies of AI risk using a best-fit framework synthesis. Our high-level Causal Taxonomy of AI Risks classifies each risk by its causal factors (1) Entity: Human, AI; (2) Intentionality: Intentional, Unintentional; and (3) Timing: Pre-deployment; Post-deployment. Our mid-level Domain Taxonomy of AI Risks classifies risks into seven AI risk domains: (1) Discrimination & toxicity, (2) Privacy & security, (3) Misinformation, (4) Malicious actors & misuse, (5) Human-computer interaction, (6) Socioeconomic & environmental, and (7) AI system safety, failures, & limitations. These are further divided into 23 subdomains. The AI Risk Repository is, to our knowledge, the first attempt to rigorously curate, analyze, and extract AI risk frameworks into a publicly accessible, comprehensive, extensible, and categorized risk database. This creates a foundation for a more coordinated, coherent, and complete approach to defining, auditing, and managing the risks posed by AI systems.

</details>


### [105] [SpreadsheetLLM: Encoding Spreadsheets for Large Language Models](https://arxiv.org/abs/2407.09025)
*Haoyu Dong,Jianbo Zhao,Yuzhang Tian,Junyu Xiong,Shiyu Xia,Mengyu Zhou,Yun Lin,José Cambronero,Yeye He,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: SpreadsheetLLM使用一种创新的压缩框架SheetCompressor来解决大型语言模型处理电子表格的挑战，显著提高了电子表格理解和问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 由于电子表格的二维网格、灵活布局和多样化格式，大型语言模型（LLM）在处理电子表格时面临巨大挑战。

Method: 提出了一种名为SpreadsheetLLM的框架，包括：1. 基础序列化方法（单元格地址、值、格式），但受限于LLM的token限制。2. 创新的编码框架SheetCompressor，包含基于结构锚的压缩、逆索引翻译和数据格式感知聚合，以解决token限制问题。3. 针对下游任务（电子表格理解和问答）提出了Chain of Spreadsheet。

Result: SheetCompressor在电子表格表格检测任务上比基础序列化方法提高了25.6%（在GPT-4的上下文学习设置中）。经过SheetCompressor微调的LLM实现了25倍的平均压缩率，F1分数达到78.9%，比现有最佳模型高出12.3%。

Conclusion: SpreadsheetLLM能有效利用电子表格的固有布局和结构，在多种电子表格任务中表现出高效性。

Abstract: Spreadsheets are characterized by their extensive two-dimensional grids, flexible layouts, and varied formatting options, which pose significant challenges for large language models (LLMs). In response, we introduce SpreadsheetLLM, pioneering an efficient encoding method designed to unleash and optimize LLMs' powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs' token constraints, making it impractical for most applications. To tackle this challenge, we develop SheetCompressor, an innovative encoding framework that compresses spreadsheets effectively for LLMs. It comprises three modules: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in the spreadsheet table detection task, outperforming the vanilla approach by 25.6% in GPT4's in-context learning setting. Moreover, fine-tuned LLM with SheetCompressor has an average compression ratio of 25 times, and achieves a state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%. Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet understanding and validate it in a new and demanding spreadsheet QA task. We methodically leverage the inherent layout and structure of spreadsheets, demonstrating that SpreadsheetLLM is highly effective across a variety of spreadsheet tasks.

</details>


### [106] [SheetAgent: Towards A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models](https://arxiv.org/abs/2403.03636)
*Yibin Chen,Yifu Yuan,Zeyu Zhang,Yan Zheng,Jinyi Liu,Fei Ni,Jianye Hao,Hangyu Mao,Fuzheng Zhang*

Main category: cs.AI

TL;DR: SheetRM是一个包含长周期、多类别任务的基准，用于评估LLM在电子表格操作中的复杂推理能力。SheetAgent是一个利用LLM的自主代理，通过规划、信息提供和检索模块，实现了无需人工干预的迭代任务推理和反思，显著提高了电子表格操作的准确性和表格推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）在处理复杂、现实的电子表格操作任务时存在不足，尤其是在需要长周期、多步推理和处理模糊需求的任务上。因此，需要一个能够应对这些挑战的基准和代理。

Method: 提出SheetRM基准，包含长周期、多类别且需要推理的电子表格操作任务。提出SheetAgent自主代理，包含规划、信息提供和检索三个协作模块，通过迭代任务推理和反思来处理复杂的电子表格操作。

Result: SheetAgent在多个基准上比现有方法提高了20%--40%的通过率，在电子表格操作的精确度和表格推理能力方面表现出色。

Conclusion: SheetAgent能够有效应对复杂的电子表格操作任务，并显著优于现有方法，证明了其在表格推理和操作方面的潜力。

Abstract: Spreadsheets are ubiquitous across the World Wide Web, playing a critical role in enhancing work efficiency across various domains. Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce SheetRM, a benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges. To mitigate the above challenges, we further propose SheetAgent, a novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of three collaborative modules: Planner, Informer, and Retriever, achieving both advanced reasoning and accurate manipulation over spreadsheets without human interaction through iterative task reasoning and reflection. Extensive experiments demonstrate that SheetAgent delivers 20--40\% pass rate improvements on multiple benchmarks over baselines, achieving enhanced precision in spreadsheet manipulation and demonstrating superior table reasoning abilities. More details and visualizations are available at the project website: https://sheetagent.github.io/. The datasets and source code are available at https://anonymous.4open.science/r/SheetAgent.

</details>


### [107] [Large Language Model for Table Processing: A Survey](https://arxiv.org/abs/2402.05121)
*Weizheng Lu,Jing Zhang,Ju Fan,Zihao Fu,Yueguo Chen,Xiaoyong Du*

Main category: cs.AI

TL;DR: 本综述全面概述了与表格相关的任务，重点介绍了大型语言模型（LLM）和视觉语言模型（VLM）的应用，涵盖了从传统的表格问答到新兴的电子表格操作和数据分析等领域。


<details>
  <summary>Details</summary>
Motivation: 自动化处理表格数据对于数据库查询、电子表格操作、网络表格问答和图像表格信息提取等日常任务具有重要意义，能够带来显著的公共效益，引起了学术界和工业界的广泛关注。

Method: 本综述对表格相关任务进行了全面的概述，考察了用户场景和技术方面，总结了针对表格处理优化的LLM和VLM训练技术，并讨论了用于各种表格相关任务的提示工程（特别是LLM驱动的代理）。

Result: 本综述总结了用于表格处理的LLM和VLM训练技术，并讨论了提示工程（特别是LLM驱动的代理）在各种表格相关任务中的应用。

Conclusion: 尽管在表格处理方面取得了进展，但仍存在挑战，包括服务多样化的用户输入以及使用链式思考（chain-of-thought）时的慢速推理。

Abstract: Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, web table question answering, and image table information extraction. Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides a comprehensive overview of table-related tasks, examining both user scenarios and technical aspects. It covers traditional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processing. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought.

</details>


### [108] [FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language](https://arxiv.org/abs/2310.17306)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Elnaz Nouri,Mohammad Raza,Gust Verbruggen*

Main category: cs.AI

TL;DR: FormaT5是一个基于Transformer的模型，可以通过自然语言描述生成条件格式化规则，解决了用户在电子表格中自动格式化表格的难题。


<details>
  <summary>Details</summary>
Motivation: 用户在电子表格中编写数据驱动的条件格式化规则（CF）时面临挑战，因为这需要理解和实现底层逻辑。

Method: FormaT5通过预测占位符来处理用户描述中不明确或模糊的问题，这些占位符可以由第二个模型或编程由示例（PBE）系统填充。

Result: FormaT5在包含1053个CF任务的基准测试中，优于8种不同的神经方法，并且发布了该基准以鼓励相关研究。

Conclusion: FormaT5通过引入占位符和填充机制，有效解决了CF规则生成中的不明确问题，并在基准测试中取得了优于其他方法的性能，展示了构建特定领域学习系统的价值。

Abstract: Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires them to understand and implement the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system. To evaluate FormaT5 on diverse and real scenarios, we create an extensive benchmark of 1053 CF tasks, containing real-world descriptions collected from four different sources. We release our benchmarks to encourage research in this area. Abstention and filling allow FormaT5 to outperform 8 different neural approaches on our benchmarks, both with and without examples. Our results illustrate the value of building domain-specific learning systems.

</details>


### [109] [Don't Treat the Symptom, Find the Cause! Efficient Artificial-Intelligence Methods for (Interactive) Debugging](https://arxiv.org/abs/2306.12850)
*Patrick Rodler*

Main category: cs.AI

TL;DR: 模型驱动诊断是一种通用的故障排除方法，用于最小化系统故障的影响。


<details>
  <summary>Details</summary>
Motivation: 随着现代系统日益复杂和我们对其的高度依赖，系统故障可能产生严重后果，因此需要将故障造成的损害降至最低。

Method: 本文将介绍模型驱动诊断，指出其主要挑战，并讨论一些研究方法。

Result: 本文介绍了模型驱动诊断的基本概念、挑战和一些研究方法。

Conclusion: 模型驱动诊断是一种通用的故障排除方法，对于处理复杂系统中日益增长的故障至关重要。

Abstract: In the modern world, we are permanently using, leveraging, interacting with, and relying upon systems of ever higher sophistication, ranging from our cars, recommender systems in e-commerce, and networks when we go online, to integrated circuits when using our PCs and smartphones, the power grid to ensure our energy supply, security-critical software when accessing our bank accounts, and spreadsheets for financial planning and decision making. The complexity of these systems coupled with our high dependency on them implies both a non-negligible likelihood of system failures, and a high potential that such failures have significant negative effects on our everyday life. For that reason, it is a vital requirement to keep the harm of emerging failures to a minimum, which means minimizing the system downtime as well as the cost of system repair. This is where model-based diagnosis comes into play.
  Model-based diagnosis is a principled, domain-independent approach that can be generally applied to troubleshoot systems of a wide variety of types, including all the ones mentioned above, and many more. It exploits and orchestrates i.a. techniques for knowledge representation, automated reasoning, heuristic problem solving, intelligent search, optimization, stochastics, statistics, decision making under uncertainty, machine learning, as well as calculus, combinatorics and set theory to detect, localize, and fix faults in abnormally behaving systems.
  In this thesis, we will give an introduction to the topic of model-based diagnosis, point out the major challenges in the field, and discuss a selection of approaches from our research addressing these issues.

</details>


### [110] [CORNET: Learning Table Formatting Rules By Example](https://arxiv.org/abs/2208.06032)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Mohammad Raza,Gust Verbruggen*

Main category: cs.AI

TL;DR: CORNET系统能自动学习电子表格的格式规则。


<details>
  <summary>Details</summary>
Motivation: 为解决用户编写电子表格格式规则的挑战，开发了CORNET系统。

Method: CORNET系统结合了归纳编程、符号规则枚举和神经网络排序，通过用户提供的格式化单元格示例来学习条件格式规则。

Result: CORNET系统能准确学习规则，并且找到比用户编写的更短的规则，还能在用户手动格式化的电子表格中发现规则。

Conclusion: CORNET系统在学习电子表格格式规则方面表现出色，解决了现有方法的不足。

Abstract: Spreadsheets are widely used for table manipulation and presentation. Stylistic formatting of these tables is an important property for both presentation and analysis. As a result, popular spreadsheet software, such as Excel, supports automatically formatting tables based on rules. Unfortunately, writing such formatting rules can be challenging for users as it requires knowledge of the underlying rule language and data logic. We present CORNET, a system that tackles the novel problem of automatically learning such formatting rules from user examples in the form of formatted cells. CORNET takes inspiration from advances in inductive programming and combines symbolic rule enumeration with a neural ranker to learn conditional formatting rules. To motivate and evaluate our approach, we extracted tables with over 450K unique formatting rules from a corpus of over 1.8M real worksheets. Since we are the first to introduce conditional formatting, we compare CORNET to a wide range of symbolic and neural baselines adapted from related domains. Our results show that CORNET accurately learns rules across varying evaluation setups. Additionally, we show that CORNET finds shorter rules than those that a user has written and discovers rules in spreadsheets that users have manually formatted.

</details>


### [111] [Named Entity Recognition in Industrial Tables using Tabular Language Models](https://arxiv.org/abs/2209.14812)
*Aneta Koleva,Martin Ringsquandl,Mark Buckley,Rakebul Hasan,Volker Tresp*

Main category: cs.AI

TL;DR: 专门的基于Transformer的模型用于编码表格式数据越来越受到学术界的关注。尽管表格式数据在工业界无处不在，但表Transformer的应用仍然缺失。在本文中，我们研究了如何将这些模型应用于工业命名实体识别（NER）问题，其中实体出现在表格结构电子表格中。电子表格的高度技术性质以及缺乏标记数据给微调基于Transformer的模型带来了巨大挑战。因此，我们开发了一种基于现有的特定领域知识图谱的专用表数据增强策略。我们表明，这在我们的低资源场景中显著提高了性能。此外，我们研究了与线性化序列表相比，表结构作为归纳偏置的好处。我们的实验证实，表Transformer优于其他基线，并且其表归纳偏置对于Transformer模型收敛至关重要。


<details>
  <summary>Details</summary>
Motivation: 在工业界广泛存在但尚未得到充分利用的表格式数据，特别是其在命名实体识别（NER）中的应用。

Method: 提出了一种基于知识图谱的数据增强策略，并研究了表结构作为归纳偏置的优势。

Result: 所提出的数据增强策略显著提高了低资源场景下的NER性能，并且表Transformer模型优于其他基线，其归纳偏置对于模型收敛至关重要。

Conclusion: 表Transformer模型在工业NER问题中表现出色，并且其表结构归纳偏置对于Transformer模型在低资源场景下的收敛和性能至关重要。

Abstract: Specialized transformer-based models for encoding tabular data have gained interest in academia. Although tabular data is omnipresent in industry, applications of table transformers are still missing. In this paper, we study how these models can be applied to an industrial Named Entity Recognition (NER) problem where the entities are mentioned in tabular-structured spreadsheets. The highly technical nature of spreadsheets as well as the lack of labeled data present major challenges for fine-tuning transformer-based models. Therefore, we develop a dedicated table data augmentation strategy based on available domain-specific knowledge graphs. We show that this boosts performance in our low-resource scenario considerably. Further, we investigate the benefits of tabular structure as inductive bias compared to tables as linearized sequences. Our experiments confirm that a table transformer outperforms other baselines and that its tabular inductive bias is vital for convergence of transformer-based models.

</details>


### [112] [Spreadsheet computing with Finite Domain Constraint Enhancements](https://arxiv.org/abs/2203.10944)
*Ezana N. Beyenne*

Main category: cs.AI

TL;DR: 该论文提出了一个框架，将有限约束求解器与电子表格计算范式相结合，以解决约束满足问题。


<details>
  <summary>Details</summary>
Motivation: 由于单向数据流的限制，传统的电子表格应用仅限于类似簿记的任务。本论文旨在克服这一限制，扩展电子表格计算范式以解决约束满足问题。

Method: 该框架允许将有限域或指定单元格之间关系的约束附加到电子表格的各个单元格。它提供了一个约束求解接口，并包含一套特定于电子表格的约束，以帮助控制大型电子表格应用程序实现的规模。

Result: 论文通过示例展示了扩展后的电子表格范式的可用性和实用性。

Conclusion: 通过将有限约束求解器集成到电子表格中，可以扩展电子表格计算范式，使其能够解决约束满足问题，并提高处理大型应用程序的可扩展性。

Abstract: Spreadsheet computing is one of the more popular computing methodologies in today's modern society. The spreadsheet application's ease of use and usefulness has enabled non-programmers to perform programming-like tasks in a familiar setting modeled after the tabular "pen and paper" approach. However, spreadsheet applications are limited to bookkeeping-like tasks due to their single-direction data flow. This thesis demonstrates an extension of the spreadsheet computing paradigm in overcoming this limitation to solve constraint satisfaction problems. We present a framework seamlessly incorporating a finite constraint solver with the spreadsheet computing paradigm. This framework allows the individual cells in the spreadsheet to be attached to either a finite domain or a constraint specifying the relationship among the cells. The framework provides an interface for constraint solving and further enhances the spreadsheet computing paradigm by providing a set of spreadsheet-specific constraints that will aid in controlling the scalability of large spreadsheet applications implementations. Finally, we provide examples to demonstrate the usability and usefulness of the extended spreadsheet paradigm.
  Keywords: Spreadsheet computing, Constraint Logic Programming, Constraint satisfaction, Domain-Specific language, Excel, SWI Prolog, C#

</details>


### [113] [Human-Machine Collaboration for Democratizing Data Science](https://arxiv.org/abs/2004.11113)
*Clément Gautrais,Yann Dauxais,Stefano Teso,Samuel Kolb,Gust Verbruggen,Luc De Raedt*

Main category: cs.AI

TL;DR: VisualSynth是一个人机协作的数据科学框架，旨在使用户能够通过电子表格软件进行数据分析。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏数据科学专业知识，许多人无法分析他们的数据。VisualSynth旨在通过允许用户使用电子表格软件执行和自动化各种数据科学任务来普及数据科学。

Method: VisualSynth通过用户提供着色电子表格部分的彩色草图来部分指定数据科学任务，然后使用人工智能技术来确定和执行这些任务。

Result: VisualSynth能够实现从数据整理、数据选择、聚类、约束学习、预测建模到自动完成的各种数据分析任务。

Conclusion: VisualSynth通过引入一种新颖的人机协作框架和系统，解决了数据分析的专业知识障碍，使非专业人士能够利用电子表格进行复杂的数据科学任务。

Abstract: Everybody wants to analyse their data, but only few posses the data science expertise to to this. Motivated by this observation we introduce a novel framework and system \textsc{VisualSynth} for human-machine collaboration in data science.
  It wants to democratize data science by allowing users to interact with standard spreadsheet software in order to perform and automate various data analysis tasks ranging from data wrangling, data selection, clustering, constraint learning, predictive modeling and auto-completion. \textsc{VisualSynth} relies on the user providing colored sketches, i.e., coloring parts of the spreadsheet, to partially specify data science tasks, which are then determined and executed using artificial intelligence techniques.

</details>


### [114] [Automated Discovery of Data Transformations for Robotic Process Automation](https://arxiv.org/abs/2001.01007)
*Volodymyr Leno,Marlon Dumas,Marcello La Rosa,Fabrizio Maria Maggi,Artem Polyvyanyy*

Main category: cs.AI

TL;DR: 本论文提出了一种从用户交互（UI）日志中发现可机器人流程自动化（RPA）的数据传输例程的方法，并针对现有技术效率低下的问题提出了优化方案。


<details>
  <summary>Details</summary>
Motivation: 为了充分利用RPA的机遇，公司需要发现可自动化的具体例程及其实现方式。

Method: 将数据传输例程的发现问题映射到“通过示例发现数据转换”问题，并针对现有技术的低效问题，提出两种优化方案，利用UI日志信息和跨应用程序数据传输通常涉及单独复制字母和数字标记的特点。

Result: 在实际的重复数据传输例程的UI日志上评估了所提出的方法和优化方案。

Conclusion: 通过UI日志分析和优化的数据转换发现技术，可以有效地识别可用于RPA的数据传输例程。

Abstract: Robotic Process Automation (RPA) is a technology for automating repetitive routines consisting of sequences of user interactions with one or more applications. In order to fully exploit the opportunities opened by RPA, companies need to discover which specific routines may be automated, and how. In this setting, this paper addresses the problem of analyzing User Interaction (UI) logs in order to discover routines where a user transfers data from one spreadsheet or (Web) form to another. The paper maps this problem to that of discovering data transformations by example - a problem for which several techniques are available. The paper shows that a naive application of a state-of-the-art technique for data transformation discovery is computationally inefficient. Accordingly, the paper proposes two optimizations that take advantage of the information in the UI log and the fact that data transfers across applications typically involve copying alphabetic and numeric tokens separately. The proposed approach and its optimizations are evaluated using UI logs that replicate a real-life repetitive data transfer routine.

</details>


### [115] [Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples](https://arxiv.org/abs/1804.01186)
*Ashwin Kalyan,Abhishek Mohta,Oleksandr Polozov,Dhruv Batra,Prateek Jain,Sumit Gulwani*

Main category: cs.AI

TL;DR: This paper introduces Neural Guided Deductive Search (NGDS), a hybrid program synthesis technique that combines symbolic logic and neural networks to efficiently generate user-intended programs from input-output examples, outperforming existing methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Synthesizing user-intended programs from limited input-output examples is crucial for applications like spreadsheet manipulation, data wrangling, and code refactoring. Existing systems struggle with either extensive manual engineering (deductive logic) or massive data requirements (statistical models), often failing in real-time synthesis.

Method: NGDS is a hybrid approach that integrates deductive search with a neural component. The deductive search framework simplifies the learning problem for the neural model, enabling it to be trained with limited real-world data using recurrent neural network encoders. This combination ensures programs satisfy specifications by construction and generalize well.

Result: The proposed NGDS technique demonstrated effectiveness in real-world customer scenarios, synthesizing accurate programs with up to a 12x speed-up compared to state-of-the-art systems.

Conclusion: NGDS offers a more effective solution for program synthesis by combining the strengths of deductive logic and data-driven approaches, achieving high accuracy and significant speed improvements over existing methods.

Abstract: Synthesizing user-intended programs from a small number of input-output examples is a challenging problem with several important applications like spreadsheet manipulation, data wrangling and code refactoring. Existing synthesis systems either completely rely on deductive logic techniques that are extensively hand-engineered or on purely statistical models that need massive amounts of data, and in general fail to provide real-time synthesis on challenging benchmarks. In this work, we propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique that combines the best of both symbolic logic techniques and statistical models. Thus, it produces programs that satisfy the provided specifications by construction and generalize well on unseen examples, similar to data-driven systems. Our technique effectively utilizes the deductive search framework to reduce the learning problem of the neural component to a simple supervised learning setup. Further, this allows us to both train on sparingly available real-world data and still leverage powerful recurrent neural network encoders. We demonstrate the effectiveness of our method by evaluating on real-world customer scenarios by synthesizing accurate programs with up to 12x speed-up compared to state-of-the-art systems.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [116] [Pivoting the paradigm: the role of spreadsheets in K-12 data science](https://arxiv.org/abs/2506.03232)
*Oren Tirschwell,Nicholas Jon Horton*

Main category: stat.OT

TL;DR: 电子表格在K-12教育中用于数据收集、组织、探索和分析，可以培养数据素养和计算能力。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨电子表格在K-12教育中的应用潜力，以促进学生数据素养和计算能力的培养。

Method: 本文回顾了K-12数据工具的现有框架，提出了结合电子表格的教学目标，并讨论了电子表格在培养数据素养和计算能力方面的作用。此外，还提供了课堂活动示例，指出了推广中面临的挑战和障碍，并提出了相应的教学建议和专业发展需求。

Result: 电子表格能够促进学生参与数据探索和分析，培养数据素养和计算能力。

Conclusion: 尽管电子表格并非万能，但它在K-12教育中具有重要作用，能够帮助学生掌握数据科学和STEM学科的基础知识。然而，推广应用仍需克服一些挑战，并需要相应的教学支持和专业发展。

Abstract: Spreadsheet tools are widely accessible to and commonly used by K-12 students and teachers. They have an important role in data collection and organization. Beyond data organization, spreadsheets also make data visible and easy to interact with, facilitating student engagement in data exploration and analysis. Though not suitable for all circumstances, spreadsheets can and do help foster data and computing skills for K-12 students. This paper 1) reviews prior frameworks on K-12 data tools; 2) proposes data-driven learning outcomes that can be accomplished by incorporating spreadsheets into the curriculum; and 3) discusses how spreadsheets can help develop data acumen and computational fluency. We provide example class activities, identify challenges and barriers to adoption, suggest pedagogical approaches to ease the learning curve for instructors and students, and discuss the need for professional development to facilitate deeper use of spreadsheets for data science and STEM disciplines.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [117] [FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining](https://arxiv.org/abs/2109.07323)
*Zhoujun Cheng,Haoyu Dong,Ran Jia,Pengfei Wu,Shi Han,Fan Cheng,Dongmei Zhang*

Main category: cs.IR

TL;DR: 该论文提出了一种名为FORTAP的表格预训练方法，利用电子表格中的公式作为监督信号，来提升模型在表格上的数值推理能力。


<details>
  <summary>Details</summary>
Motivation: 表格中的数值推理是一个挑战，但电子表格中的公式提供了天然的监督信号，并且网上存在大量包含公式的电子表格。

Method: FORTAP通过设计两种公式预训练任务来显式地引导模型学习表格中的数值引用和计算。

Result: FORTAP在单元格类型分类和公式预测两个下游任务上取得了最先进的结果。

Conclusion: 利用公式进行数值推理的预训练方法具有巨大的潜力。

Abstract: Tables store rich numerical data, but numerical reasoning over tables is still a challenge. In this paper, we find that the spreadsheet formula, which performs calculations on numerical values in tables, is naturally a strong supervision of numerical reasoning. More importantly, large amounts of spreadsheets with expert-made formulae are available on the web and can be obtained easily. FORTAP is the first method for numerical-reasoning-aware table pretraining by leveraging large corpus of spreadsheet formulae. We design two formula pretraining tasks to explicitly guide FORTAP to learn numerical reference and calculation in semi-structured tables. FORTAP achieves state-of-the-art results on two representative downstream tasks, cell type classification and formula prediction, showing great potential of numerical-reasoning-aware pretraining.

</details>


### [118] [Detecting Layout Templates in Complex Multiregion Files](https://arxiv.org/abs/2109.06630)
*Gerardo Vitagliano,Lan Jiang,Felix Naumann*

Main category: cs.IR

TL;DR: 该研究提出了一种名为 Mondrian 的方法，用于自动识别和提取电子表格中独立的“多区域”数据集，并能在多个文件中找到相同的布局模板。


<details>
  <summary>Details</summary>
Motivation: 电子表格因其广泛应用而包含大量数据，但其灵活的结构给自动化分析带来了困难，特别是当单个文件包含多个不相关的、可能由空单元格分隔的数据区域时。

Method: Mondrian 方法包括三个阶段：1. 将电子表格渲染成图像并识别潜在的数据区域；2. 使用聚类算法将识别出的元素分组形成区域；3. 将每个文件的布局表示为图，并与其他文件进行比较以发现重复的布局模板。

Result: 与现有的表格识别算法相比，Mondrian 在检测单个文件内的可靠区域边界方面表现更优，并能准确识别跨文件出现的重复布局。

Conclusion: Mondrian 方法能够有效地处理电子表格中的多区域问题，并能从多个文件中识别出重复的布局结构，为自动化数据提取和分析提供了有效解决方案。

Abstract: Spreadsheets are among the most commonly used file formats for data management, distribution, and analysis. Their widespread employment makes it easy to gather large collections of data, but their flexible canvas-based structure makes automated analysis difficult without heavy preparation. One of the common problems that practitioners face is the presence of multiple, independent regions in a single spreadsheet, possibly separated by repeated empty cells. We define such files as "multiregion" files. In collections of various spreadsheets, we can observe that some share the same layout. We present the Mondrian approach to automatically identify layout templates across multiple files and systematically extract the corresponding regions. Our approach is composed of three phases: first, each file is rendered as an image and inspected for elements that could form regions; then, using a clustering algorithm, the identified elements are grouped to form regions; finally, every file layout is represented as a graph and compared with others to find layout templates. We compare our method to state-of-the-art table recognition algorithms on two corpora of real-world enterprise spreadsheets. Our approach shows the best performances in detecting reliable region boundaries within each file and can correctly identify recurring layouts across files.

</details>


### [119] [TUTA: Tree-based Transformers for Generally Structured Table Pre-training](https://arxiv.org/abs/2010.12537)
*Zhiruo Wang,Haoyu Dong,Ran Jia,Jia Li,Zhiyi Fu,Shi Han,Dongmei Zhang*

Main category: cs.IR

TL;DR: TUTA是一个统一的预训练架构，用于理解通用结构化表格，通过结合空间、层级和语义信息，并在不同层级进行预训练，在表格结构理解任务上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有表格理解方法主要关注关系型表格，忽略了其他常见表格结构。

Method: 提出了一种统一的预训练架构TUTA，增强了Transformer模型。设计了一个二维坐标树来描述表格的空间和层级信息，并引入了基于树的注意力机制和位置嵌入。提出了三种渐进式的预训练目标（token、cell、table级别）。

Result: TUTA在广泛的未标记网页表格和电子表格上进行了预训练，并在细胞类型分类和表格类型分类两个关键任务上进行了微调。在五个广泛研究的数据集上均达到了最先进的性能。

Conclusion: TUTA在理解通用结构化表格方面非常有效，能够有效捕捉空间、层级和语义信息，并在表格结构理解任务上取得了显著成果。

Abstract: Tables are widely used with various structures to organize and present data. Recent attempts on table understanding mainly focus on relational tables, yet overlook to other common table structures. In this paper, we propose TUTA, a unified pre-training architecture for understanding generally structured tables. Noticing that understanding a table requires spatial, hierarchical, and semantic information, we enhance transformers with three novel structure-aware mechanisms. First, we devise a unified tree-based structure, called a bi-dimensional coordinate tree, to describe both the spatial and hierarchical information of generally structured tables. Upon this, we propose tree-based attention and position embedding to better capture the spatial and hierarchical information. Moreover, we devise three progressive pre-training objectives to enable representations at the token, cell, and table levels. We pre-train TUTA on a wide range of unlabeled web and spreadsheet tables and fine-tune it on two critical tasks in the field of table structure understanding: cell type classification and table type classification. Experiments show that TUTA is highly effective, achieving state-of-the-art on five widely-studied datasets.

</details>


### [120] [TableSense: Spreadsheet Table Detection with Convolutional Neural Networks](https://arxiv.org/abs/2106.13500)
*Haoyu Dong,Shijie Liu,Shi Han,Zhouyu Fu,Dongmei Zhang*

Main category: cs.IR

TL;DR: TableSense是一个用于表格检测的新型端到端框架，通过有效的单元格特征提取、增强的卷积神经网络模型和基于主动学习的采样算法，实现了91.3%的召回率和86.5%的精确率。


<details>
  <summary>Details</summary>
Motivation: 电子表格中的自动表格检测是电子表格数据智能的关键技术和初始步骤，但面临表格结构和布局多样性的挑战。

Method: 提出了一种名为TableSense的新型端到端框架，该框架包括：1. 提出了一种有效的单元格特征提取方案，以更好地利用每个单元格中的丰富信息；2. 开发了一种增强的卷积神经网络模型用于表格检测，以满足领域内对精确表格边界检测的特定要求；3. 提出了一种有效的置信度度量方法，用于指导基于主动学习的智能采样算法，从而能够高效地构建一个包含22,176个表格和10,220个工作表的训练数据集，并广泛覆盖各种表格结构和布局。

Result: TableSense在EoB-2度量标准下达到了91.3%的召回率和86.5%的精确率，显著优于当前商品化电子表格工具中使用的检测算法以及计算机视觉领域最先进的卷积神经网络。

Conclusion: TableSense在表格检测任务上表现出高有效性，并且在召回率和精确率方面取得了显著改进。

Abstract: Spreadsheet table detection is the task of detecting all tables on a given sheet and locating their respective ranges. Automatic table detection is a key enabling technique and an initial step in spreadsheet data intelligence. However, the detection task is challenged by the diversity of table structures and table layouts on the spreadsheet. Considering the analogy between a cell matrix as spreadsheet and a pixel matrix as image, and encouraged by the successful application of Convolutional Neural Networks (CNN) in computer vision, we have developed TableSense, a novel end-to-end framework for spreadsheet table detection. First, we devise an effective cell featurization scheme to better leverage the rich information in each cell; second, we develop an enhanced convolutional neural network model for table detection to meet the domain-specific requirement on precise table boundary detection; third, we propose an effective uncertainty metric to guide an active learning based smart sampling algorithm, which enables the efficient build-up of a training dataset with 22,176 tables on 10,220 sheets with broad coverage of diverse table structures and layouts. Our evaluation shows that TableSense is highly effective with 91.3\% recall and 86.5\% precision in EoB-2 metric, a significant improvement over both the current detection algorithm that are used in commodity spreadsheet tools and state-of-the-art convolutional neural networks in computer vision.

</details>


### [121] [Recommending Related Tables](https://arxiv.org/abs/1907.03595)
*Shuo Zhang,Krisztian Balog*

Main category: cs.IR

TL;DR: 输入表格后，推荐相关的表格，并提供一个相关的表格列表。此任务可用于向电子表格程序用户主动推荐相关的结构化内容。


<details>
  <summary>Details</summary>
Motivation: 介绍并解决推荐相关表格的任务，其目标是在给定输入表格的情况下，识别并返回相关表格的排名列表。此任务的一个可能的应用场景是向电子表格程序用户主动提供有关网络上相关结构化内容的建议。

Method: 通过在多个语义空间中表示表格元素，然后使用区分性学习模型组合元素级相似性来执行表格匹配。

Result: 使用专门为此目的而构建的维基表格测试集，证明了所提出的方法具有最先进的性能。

Conclusion: 所提出的方法通过在多个语义空间中表示表格元素，并结合区分性学习模型来组合元素级相似性，从而在表格匹配方面取得了最先进的性能。

Abstract: Tables are an extremely powerful visual and interactive tool for structuring and manipulating data, making spreadsheet programs one of the most popular computer applications. In this paper we introduce and address the task of recommending related tables: given an input table, identifying and returning a ranked list of relevant tables. One of the many possible application scenarios for this task is to provide users of a spreadsheet program proactively with recommendations for related structured content on the Web. At its core, the related table recommendation task boils down to computing the similarity between a pair of tables. We develop a theoretically sound framework for performing table matching. Our approach hinges on the idea of representing table elements in multiple semantic spaces, and then combining element-level similarities using a discriminative learning model. Using a purpose-built test collection from Wikipedia tables, we demonstrate that the proposed approach delivers state-of-the-art performance.

</details>


### [122] [SmartTable: A Spreadsheet Program with Intelligent Assistance](https://arxiv.org/abs/1805.06353)
*Shuo Zhang,Vugar Abdul Zada,Krisztian Balog*

Main category: cs.IR

TL;DR: SmartTable 是一个带有智能辅助功能的在线电子表格应用程序，可以帮助用户填充表格（添加行）和扩展表格（添加列）。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是提供一个智能的在线电子表格应用程序，能够辅助用户处理关系表，包括填充实体（行）和扩展实体属性（列）。

Method: 通过提供智能辅助功能来实现，具体包括填充表格（添加行）和扩展表格（添加列）。

Result: 已实现 SmartTable 应用程序，并提供实现细节和开源版本。

Conclusion: SmartTable 是一个功能强大且易于使用的在线电子表格应用程序，具有智能辅助功能，可以提高用户处理关系表的效率。

Abstract: We introduce SmartTable, an online spreadsheet application that is equipped with intelligent assistance capabilities. With a focus on relational tables, describing entities along with their attributes, we offer assistance in two flavors: (i) for populating the table with additional entities (rows) and (ii) for extending it with additional entity attributes (columns). We provide details of our implementation, which is also released as open source. The application is available at http://smarttable.cc.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [123] [Is spreadsheet syntax better than numeric indexing for cell selection?](https://arxiv.org/abs/2505.23296)
*Philip Heltweg,Dirk Riehle,Georg-Daniel Schwarz*

Main category: cs.PL

TL;DR: 电子表格风格的语法在数据工程中比数字索引更有效


<details>
  <summary>Details</summary>
Motivation: 比较数字索引和电子表格风格的语法在数据工程中的效率

Method: 进行一项有学生参与的大规模对照实验，比较两种语法的读写代码的速度和正确性

Result: 学生在使用电子表格风格语法时，在读取代码时错误更少，在写入代码时错误更少且速度更快

Conclusion: 域名特定的语法（如电子表格语法）可能是一种有前景的工具，可以支持非软件工程背景的从业者

Abstract: Selecting a subset of cells is a common task in data engineering, for example, to remove errors or select only specific parts of a table. Multiple approaches to express this selection exist. One option is numeric indexing, commonly found in general programming languages, where a tuple of numbers identifies the cell. Alternatively, the separate dimensions can be referred to using different enumeration schemes like "A1" for the first cell, commonly found in software such as spreadsheet systems.
  In a large-scale controlled experiment with student participants as proxy for data practitioners, we compare the two options with respect to speed and correctness of reading and writing code.
  The results show that, when reading code, participants make less mistakes using spreadsheet-style syntax. Additionally, when writing code, they make fewer mistakes and are faster when using spreadsheet syntax compared to numeric syntax.
  From this, a domain-specific syntax, such as spreadsheet syntax for data engineering, appears to be a promising alternative to explore in future tools to support practitioners without a software engineering background.

</details>


### [124] [FLAME: A small language model for spreadsheet formulas](https://arxiv.org/abs/2301.13779)
*Harshit Joshi,Abishai Ebenezer,José Cambronero,Sumit Gulwani,Aditya Kanade,Vu Le,Ivan Radiček,Gust Verbruggen*

Main category: cs.PL

TL;DR: FLAME是一个更小的、仅在Excel公式上训练的Transformer模型，在公式修复、补全和检索任务上表现优于更大的模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在电子表格公式创作辅助方面难以部署，因为它们训练成本高且模型庞大。FLAME旨在解决这个问题。

Method: FLAME是一个基于Transformer的模型，它利用领域见解，使用专门的公式分词器和针对Excel的预训练目标（如掩码跨度预测和噪声自动编码）进行训练。它还使用草图去重来策划训练数据集。

Result: FLAME在14个评估设置中的10个上，在公式修复和补全任务上优于更大的模型（如Davinci、Cushman和CodeT5）。在公式检索任务上，FLAME也优于CodeT5、CodeBERT和GraphCodeBERT。

Conclusion: FLAME通过使用领域特定的方法，在保持模型尺寸和训练数据量较小的情况下，在电子表格公式处理任务上取得了有竞争力的性能，甚至优于一些更大的模型。

Abstract: Spreadsheets are a vital tool for end-user data management. Using large language models for formula authoring assistance in these environments can be difficult, as these models are expensive to train and challenging to deploy due to their size (up to billions of parameters). We present FLAME, a transformer-based model trained exclusively on Excel formulas that leverages domain insights to achieve competitive performance while being substantially smaller (60M parameters) and training on two orders of magnitude less data. We curate a training dataset using sketch deduplication, introduce an Excel-specific formula tokenizer, and use domain-specific versions of masked span prediction and noisy auto-encoding as pre-training objectives. We evaluate FLAME on formula repair, formula completion, and similarity-based formula retrieval. FLAME can outperform much larger models, such as the Davinci (175B) and Cushman (12B) variants of Codex and CodeT5 (220M), in 10 of 14 evaluation settings for the repair and completion tasks. For formula retrieval, FLAME outperforms CodeT5, CodeBERT, and GraphCodeBERT.

</details>


### [125] [Enhanced Spreadsheet Computing with Finite-Domain Constraint Satisfaction](https://arxiv.org/abs/2203.16346)
*Ezana N. Beyenne,Hai-Feng Guo*

Main category: cs.PL

TL;DR: 本文提出了一种增强的电子表格系统，通过支持有限域约束求解，打破了传统电子表格的单向数据流限制，使其能够解决约束满足问题，并为通用用户提供了一个声明式、可扩展的约束语言。


<details>
  <summary>Details</summary>
Motivation: 扩展电子表格的计算范式，打破其在解决约束满足问题时的单向数据流限制。

Method: 提出一个增强的电子表格系统，支持有限域约束求解，并构建了一个电子表格特定的约束语言，用户可以用声明式和可扩展的方式指定数据单元之间的约束。

Result: 增强的电子表格系统显著简化了使用可视化表格界面进行许多基于约束的应用程序的开发。

Conclusion: 扩展后的电子表格范式在可用性和实用性方面得到了证明，能够解决约束满足问题。

Abstract: The spreadsheet application is among the most widely used computing tools in modern society. It provides excellent usability and usefulness, and it easily enables a non-programmer to perform programming-like tasks in a visual tabular "pen and paper" approach. However, spreadsheets are mostly limited to bookkeeping-like applications due to their mono-directional data flow. This paper shows how the spreadsheet computing paradigm is extended to break this limitation for solving constraint satisfaction problems. We present an enhanced spreadsheet system where finite-domain constraint solving is well supported in a visual environment. Furthermore, a spreadsheet-specific constraint language is constructed for general users to specify constraints among data cells in a declarative and scalable way. The new spreadsheet system significantly simplifies the development of many constraint-based applications using a visual tabular interface. Examples are given to illustrate the usability and usefulness of the extended spreadsheet paradigm.
  KEYWORDS: Spreadsheet computing, Finite-domain constraint satisfaction, Constraint logic programming

</details>


### [126] [Typed Image-based Programming with Structure Editing](https://arxiv.org/abs/2110.08993)
*Jonathan Edwards,Tomas Petricek*

Main category: cs.PL

TL;DR: 图像编程通过类型和结构编辑实现协作，重点关注模式变更。


<details>
  <summary>Details</summary>
Motivation: 传统基于文件的编程在协作和部署方面有更好的支持，而图像编程在这方面存在不足，导致了如Smalltalk等系统的商业成功受限。本研究旨在解决图像编程的协作问题。

Method: 提出使用静态类型来管理和执行模式变更，并结合结构编辑来捕捉类型定义的变更，以便自动适应数据。在此基础上，提出一种用于结构编辑的版本控制理论。

Result: 提出了一种适用于结构编辑的版本控制理论，这是该研究的主要技术贡献。

Conclusion: 通过结构编辑和静态类型，可以为图像编程带来更好的协作支持，并可能扩展到整个编程体验，提供一种新的协作方式。

Abstract: Many beloved programming systems are image-based: self-contained worlds that persist both code and data in a single file. Examples include Smalltalk, LISP, HyperCard, Flash, and spreadsheets. Image-based programming avoids much of the complexity of modern programming technology stacks and encourages more casual and exploratory programming. However conventional file-based programming has better support for collaboration and deployment. These problems have been blamed for the limited commercial success of Smalltalk. We propose to enable collaboration in image-based programming via types and structure editing.
  We focus on the problem of schema change on persistent data. We turn to static types, which paradoxically require more schema change but also provide a mechanism to express and execute those changes. To determine those changes we turn to structure editing, so that we can capture changes in type definitions with sufficient fidelity to automatically adapt the data to suit. We conjecture that typical schema changes can be handled through structure editing of static types.
  That positions us to tackle collaboration with what could be called version control for structure editing. We present a theory realizing this idea, which is our main technical contribution. While we focus here on editing types, if we can extend the approach to cover the entire programming experience then it would offer a new way to collaborate in image-based programming.

</details>


### [127] [ExceLint: Automatically Finding Spreadsheet Formula Errors](https://arxiv.org/abs/1901.11100)
*Daniel W. Barowy,Emery D. Berger,Benjamin Zorn*

Main category: cs.PL

TL;DR: ExceLint 是一种用于查找 Excel 电子表格中公式错误的静态分析工具。


<details>
  <summary>Details</summary>
Motivation: 电子表格被广泛用于金融等领域，错误可能导致严重后果。因此，需要一种有效的方法来检测电子表格中的公式错误。

Method: 该分析直接利用电子表格的矩形特性，并采用信息论方法来识别那些对附近矩形区域造成意外中断的公式。ExceLint 是该静态分析在 Microsoft Excel 中的实现。

Result: ExceLint 快速有效，在 70 个电子表格的测试中，其中位数分析时间为每个 5 秒，并且其性能显著优于现有的分析方法。

Conclusion: ExceLint 是一种快速有效的静态分析工具，能够显著提高电子表格公式的准确性。

Abstract: Spreadsheets are one of the most widely used programming environments, and are widely deployed in domains like finance where errors can have catastrophic consequences. We present a static analysis specifically designed to find spreadsheet formula errors. Our analysis directly leverages the rectangular character of spreadsheets. It uses an information-theoretic approach to identify formulas that are especially surprising disruptions to nearby rectangular regions. We present ExceLint, an implementation of our static analysis for Microsoft Excel. We demonstrate that ExceLint is fast and effective: across a corpus of 70 spreadsheets, ExceLint takes a median of 5 seconds per spreadsheet, and it significantly outperforms the state of the art analysis.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [128] [Ensuring Adherence to Standards in Experiment-Related Metadata Entered Via Spreadsheets](https://arxiv.org/abs/2409.08897)
*Martin J. O'Connor,Josef Hardi,Marcos Martínez-Romero,Sowmya Somasundaram,Brendan Honick,Stephen A. Fisher,Ajay Pillai,Mark A. Musen*

Main category: cs.DL

TL;DR: 尽管有现成的工具，研究人员仍倾向于使用电子表格来提供元数据，但电子表格在确保元数据一致性和符合正式规范方面存在局限性。本文提出了一种端到端的解决方案，支持基于电子表格的元数据输入，同时确保严格遵守社区元数据标准并提供质量控制。


<details>
  <summary>Details</summary>
Motivation: 研究人员普遍偏爱使用电子表格提供元数据，尽管电子表格在确保元数据一致性和符合正式规范方面存在局限性。

Method: 该方法采用定制模板来表示元数据标准并指导电子表格的创建，使用受控术语和本体来定义可直接从电子表格访问的元数据值，并提供一个交互式 Web 工具来快速识别和修复电子表格元数据中的错误。

Result: 该方法已被部署在 HuBMAP 生物医学联盟中，用于定义和收集各种生物学检测的元数据。

Conclusion: 所提出的方法能够支持基于电子表格的元数据输入，同时确保严格遵守社区元数据标准并提供质量控制。

Abstract: Scientists increasingly recognize the importance of providing rich, standards-adherent metadata to describe their experimental results. Despite the availability of sophisticated tools to assist in the process of data annotation, investigators generally seem to prefer to use spreadsheets when supplying metadata, despite the limitations of spreadsheets in ensuring metadata consistency and compliance with formal specifications. In this paper, we describe an end-to-end approach that supports spreadsheet-based entry of metadata, while ensuring rigorous adherence to community-based metadata standards and providing quality control. Our methods employ several key components, including customizable templates that represent metadata standards and that can inform the spreadsheets that investigators use to author metadata, controlled terminologies and ontologies for defining metadata values that can be accessed directly from a spreadsheet, and an interactive Web-based tool that allows users to rapidly identify and fix errors in their spreadsheet-based metadata. We demonstrate how this approach is being deployed in a biomedical consortium known as HuBMAP to define and collect metadata about a wide range of biological assays.

</details>


### [129] [Trapped in Transformative Agreements? A Multifaceted Analysis of >1,000 Contracts](https://arxiv.org/abs/2409.20224)
*Laura Rothfritz,W. Benedikt Schmal,Ulrich Herb*

Main category: cs.DL

TL;DR: 机构似乎“陷入”了转换协议，而不是通往完全开放的学术出版的桥梁，而是被困在混合系统中，这使传统的（非开放的）出版社拥有了重要的市场力量，提高了进入壁垒，降低了竞争，并增加了图书馆和大学的成本。


<details>
  <summary>Details</summary>
Motivation: 分析转换协议的特征和转换协议领域。

Method: 通过网络抓取ESAC数据库中的每个合同的详细信息，并结合定性和定量方法。

Result: 研究机构似乎“陷入”了转换协议，而不是通往完全开放的学术出版的桥梁，而是被困在混合系统中，这使传统的（非开放的）出版社拥有了重要的市场力量，提高了进入壁垒，降低了竞争，并且增加了图书馆和大学的成本。

Conclusion: 转换协议似乎将学术机构困在了混合系统中，而不是促进向完全开放的学术出版的转变。

Abstract: Transformative agreements between academic publishers and research institutions are ubiquitous. The 'Efficiency and Standards for Article Charges' (ESAC) Initiative lists more than 1,000 contracts in its database. We make use of this unique dataset by web-scraping the details of every contract to substantially expand the overview spreadsheet provided by the ESAC Initiative. Based on that hitherto unused data source, we combine qualitative and quantitative methods to conduct an in-depth analysis of the contract characteristics and the TA landscape. Our analysis demonstrates that research institutions seem to be 'trapped' in transformative agreements. Instead of being a bridge towards a fully Open Access world, academia is stuck in the hybrid system. This endows the legacy (non-Open Access) publishing houses with substantial market power. It raises entry barriers, lowers competition, and increases costs for libraries and universities.

</details>


### [130] [A Comprehensive Approach to Ensuring Quality in Spreadsheet-Based Metadata](https://arxiv.org/abs/2312.09107)
*Martin J. O'Connor,Marcos Martínez-Romero,Mete Ugur Akdogan,Josef Hardi,Mark A. Musen*

Main category: cs.DL

TL;DR: 一种改进的电子表格元数据输入方法，通过可定制模板、受控术语和交互式Web工具来确保合规性和质量。


<details>
  <summary>Details</summary>
Motivation: 电子表格是提供元数据描述信息的主要工具，但存在合规性和质量问题。现有工具存在学习曲线陡峭和定制性差等缺点。

Method: 提出一种端到端的方法，支持基于电子表格的元数据输入，并提供严格的合规性和质量控制。该方法包括：1.用于定义元数据的可定制模板。2.对使用受控术语定义模板提供内置支持。3.提供交互式Web工具，让用户快速识别和修复电子表格元数据中的错误。

Result: 该方法已被部署在一个生物医学联盟中，用于定义和收集科学实验的元数据。

Conclusion: 所提出的方法通过结合可定制模板、受控术语和交互式Web工具，有效地解决了在电子表格中输入元数据时的合规性和质量问题。

Abstract: While scientists increasingly recognize the importance of metadata in describing their data, spreadsheets remain the preferred tool for supplying this information despite their limitations in ensuring compliance and quality. Various tools have been developed to address these limitations, but they suffer from their own shortcomings, such as steep learning curves and limited customization. In this paper, we describe an end-to-end approach that supports spreadsheet-based entry of metadata while providing rigorous compliance and quality control. Our approach employs several key strategies, including customizable templates for defining metadata, integral support for the use of controlled terminologies when defining these templates, and an interactive Web-based tool that allows users to rapidly identify and fix errors in the spreadsheet-based metadata they supply. We demonstrate how this approach is being deployed in a biomedical consortium to define and collect metadata about scientific experiments.

</details>


### [131] [Towards Semantic Interoperability in Historical Research: Documenting Research Data and Knowledge with Synthesis](https://arxiv.org/abs/2107.13957)
*Pavlos Fafalios,Konstantina Konsolaki,Lida Charami,Kostas Petrakis,Manos Paterakis,Dimitris Angelakis,Yannis Tzitzikas,Chrysoula Bekiari,Martin Doerr*

Main category: cs.DL

TL;DR: 该研究描述了一个名为Synthesis的基于Web的协作式文档系统，旨在解决历史学领域中文物和相关证据记录与研究的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有实践（如电子表格和关系数据库）在处理大量用户协作、记录证据细节、数据结构扩展和数据复用方面存在困难。本研究旨在为历史学家提供一个解决方案，以应对这些挑战。

Method: 介绍Synthesis文档系统，该系统基于Web，支持协作，并利用现有的信息记录和发布标准（CIDOC-CRM、RDF），专注于语义互操作性和高质量、长期有效的数据生产。

Result: Synthesis系统已被一个大型艺术史研究项目中的众多历史学家使用，表明其在实际应用中的可行性和有效性。

Conclusion: Synthesis系统通过提供一个基于Web的协作平台，并利用标准化的数据模型，成功解决了历史学领域文物记录中的现有问题，提高了数据的质量、可重用性和长期有效性。

Abstract: A vast area of research in historical science concerns the documentation and study of artefacts and related evidence. Current practice mostly uses spreadsheets or simple relational databases to organise the information as rows with multiple columns of related attributes. This form offers itself for data analysis and scholarly interpretation, however it also poses problems including i) the difficulty for collaborative but controlled documentation by a large number of users, ii) the lack of representation of the details from which the documented relations are inferred, iii) the difficulty to extend the underlying data structures as well as to combine and integrate data from multiple and diverse information sources, and iv) the limitation to reuse the data beyond the context of a particular research activity. To support historians to cope with these problems, in this paper we describe the Synthesis documentation system and its use by a large number of historians in the context of an ongoing research project in the field of History of Art. The system is Web-based and collaborative, and makes use of existing standards for information documentation and publication (CIDOC-CRM, RDF), focusing on semantic interoperability and the production of data of high value and long-term validity.

</details>


### [132] [FAST CAT: Collaborative Data Entry and Curation for Semantic Interoperability in Digital Humanities](https://arxiv.org/abs/2105.13733)
*Pavlos Fafalios,Kostas Petrakis,Georgios Samaritakis,Korina Doerr,Athina Kritsotaki,Yannis Tzitzikas,Martin Doerr*

Main category: cs.DL

TL;DR: FAST CAT是一个协作系统，用于在数字人文和类似实证研究中进行辅助数据输入和整理。


<details>
  <summary>Details</summary>
Motivation: 现有的电子表格和数据库工具依赖于初始研究假设，难以表示推断关系的细节，并且难以重新访问原始数据源进行验证、更正或改进。

Method: 该系统通过支持语义互操作性来应对这些挑战。

Result: FAST CAT已被应用于地中海地区蒸汽船引入的经济、社会和人口影响的欧洲（ERC）海事史项目SeaLiT。

Conclusion: FAST CAT为数字人文和实证研究中的数据管理和分析提供了一种更有效的方法。

Abstract: Descriptive and empirical sciences, such as History, are the sciences that collect, observe and describe phenomena in order to explain them and draw interpretative conclusions about influences, driving forces and impacts under given circumstances. Spreadsheet software and relational database management systems are still the dominant tools for quantitative analysis and overall data management in these these sciences, allowing researchers to directly analyse the gathered data and perform scholarly interpretation. However, this current practice has a set of limitations, including the high dependency of the collected data on the initial research hypothesis, usually useless for other research, the lack of representation of the details from which the registered relations are inferred, and the difficulty to revisit the original data sources for verification, corrections or improvements. To cope with these problems, in this paper we present FAST CAT, a collaborative system for assistive data entry and curation in Digital Humanities and similar forms of empirical research. We describe the related challenges, the overall methodology we follow for supporting semantic interoperability, and discuss the use of FAST CAT in the context of a European (ERC) project of Maritime History, called SeaLiT, which examines economic, social and demographic impacts of the introduction of steamboats in the Mediterranean area between the 1850s and the 1920s.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [133] [Using a Catenary Trajectory to Reduce Wellbore Friction in Horizontal Extended Reach Drilling](https://arxiv.org/abs/2309.12317)
*Vu Nguyen*

Main category: cs.RO

TL;DR: 钻井中的井筒摩擦是一个重要问题，会增加总成本。本文介绍了利用悬链线概念来减小井筒摩擦，并通过案例研究将悬链线轨迹设计与传统的二维圆弧设计进行了比较。


<details>
  <summary>Details</summary>
Motivation: 钻井中的井筒摩擦是一个主要成本驱动因素，而悬链线概念可以减小这种摩擦，但需要详细的分析。该项目旨在填补这一分析空白。

Method: 利用悬链线形状（自由悬挂的绳索、链条或钻杆的自然形状）来设计钻井轨迹，以期实现钻杆与井筒无接触，从而消除摩擦。通过案例研究，将悬链线轨迹设计与传统的二维圆弧设计进行了比较。计算过程已集成到易于使用的 MS Excel 电子表格中。

Result: 案例研究表明，悬链线轨迹设计与传统的二维圆弧设计在减少扭矩和阻力方面可能优于后者，尽管具体结果需要进一步的详细数据支持。

Conclusion: 悬链线轨迹设计是一种有潜力减少钻井井筒摩擦的方法，其计算方法已集成到易于使用的 Excel 工具中，可用于设计长距离水平井的轨迹。

Abstract: Wellbore friction is one of the biggest concerns when drilling due to its relation to the total cost. The catenary concept was introduced to reduce wellbore friction, but it requires detailed analyses. This project would fill this gap. A catenary shape is simply the natural shape of a rope, chain, or drill string. The drill string will then hang freely inside the wellbore. Perfectly, there should be no contact between the hole and the string, and thus no friction. Torque and drag should be minimized this way. A case study is introduced to examine the outcome between Catenary Trajectory Design and traditional 2D Arc design. The calculation procedure of Catenary Trajectory and 2D Arc Design can be found in an MS Excel spreadsheet which is easy to use and reliable for designing catenary well trajectories for extended-reach wells.

</details>


### [134] [Hazard Analysis of Collaborative Automation Systems: A Two-layer Approach based on Supervisory Control and Simulation](https://arxiv.org/abs/2209.12560)
*Tom P. Huck,Yuvaraj Selvaraj,Constantin Cronrath,Christoph Ledermann,Martin Fabian,Bengt Lennartson,Torsten Kröger*

Main category: cs.RO

TL;DR: 本研究提出一种结合形式化方法和仿真方法的两层模型驱动的危险分析方法，用于识别和分析安全关键系统的潜在危险状态。


<details>
  <summary>Details</summary>
Motivation: 现有基于人工推理、经验和简单工具的危险分析方法难以应对日益复杂的系统，而基于测试的方法成本高昂或存在危险。模型驱动的方法是解决方案，但现有方法各有优缺点。

Method: 提出一种两层方法：首先使用监督控制理论从形式化模型合成导致危险状态的不安全行为，然后将结果输入仿真环境，利用特定领域的风险指标进行详细分析。方法在工业人机协作系统上进行了演示。

Result: 该方法结合了形式化方法的穷举分析能力和仿真方法的详细分析能力，能够更全面地识别和分析系统的危险状态。

Conclusion: 该两层方法能够有效结合形式化方法和仿真方法的优势，对安全关键系统进行更准确和深入的危险分析，并在工业人机协作系统上验证了其有效性。

Abstract: Safety critical systems are typically subjected to hazard analysis before commissioning to identify and analyse potentially hazardous system states that may arise during operation. Currently, hazard analysis is mainly based on human reasoning, past experiences, and simple tools such as checklists and spreadsheets. Increasing system complexity makes such approaches decreasingly suitable. Furthermore, testing-based hazard analysis is often not suitable due to high costs or dangers of physical faults. A remedy for this are model-based hazard analysis methods, which either rely on formal models or on simulation models, each with their own benefits and drawbacks. This paper proposes a two-layer approach that combines the benefits of exhaustive analysis using formal methods with detailed analysis using simulation. Unsafe behaviours that lead to unsafe states are first synthesised from a formal model of the system using Supervisory Control Theory. The result is then input to the simulation where detailed analyses using domain-specific risk metrics are performed. Though the presented approach is generally applicable, this paper demonstrates the benefits of the approach on an industrial human-robot collaboration system.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [135] [Buckaroo: A Direct Manipulation Visual Data Wrangler](https://arxiv.org/abs/2507.16073)
*Annabelle Warner,Andrew McNutt,Paul Rosen,El Kindi Rezig*

Main category: cs.HC

TL;DR: Buckaroo是一个可视化系统，旨在通过直接操作视觉对象来突出显示数据中的差异并进行即时更正，从而解决数据准备（数据整理）耗时且易出错的问题。


<details>
  <summary>Details</summary>
Motivation: 数据科学项目中的数据准备（数据整理）阶段耗时高达80%以上，手动编码或使用电子表格的方法效率低下且容易出错，可能导致数据质量问题。

Method: Buckaroo通过（1）自动查找异常数据组并推荐检查；（2）建议用户可选择的修复异常的操作；（3）通过显示操作效果并支持撤销/重做，支持数据整理的迭代性质，使用户能够通过直接操作视觉对象来修复数据。

Result: Buckaroo可以自动发现异常数据组，推荐修复操作，并允许用户通过可视化界面进行数据操作和纠正。

Conclusion: Buckaroo通过提供可视化界面和智能建议，简化并加速了数据整理过程，提高了数据质量。

Abstract: Preparing datasets -- a critical phase known as data wrangling -- constitutes the dominant phase of data science development, consuming upwards of 80% of the total project time. This phase encompasses a myriad of tasks: parsing data, restructuring it for analysis, repairing inaccuracies, merging sources, eliminating duplicates, and ensuring overall data integrity. Traditional approaches, typically through manual coding in languages such as Python or using spreadsheets, are not only laborious but also error-prone. These issues range from missing entries and formatting inconsistencies to data type inaccuracies, all of which can affect the quality of downstream tasks if not properly corrected. To address these challenges, we present Buckaroo, a visualization system to highlight discrepancies in data and enable on-the-spot corrections through direct manipulations of visual objects. Buckaroo (1) automatically finds "interesting" data groups that exhibit anomalies compared to the rest of the groups and recommends them for inspection; (2) suggests wrangling actions that the user can choose to repair the anomalies; and (3) allows users to visually manipulate their data by displaying the effects of their wrangling actions and offering the ability to undo or redo these actions, which supports the iterative nature of data wrangling. A video companion is available at https://youtu.be/iXdCYbvpQVE

</details>


### [136] [SheetMind: An End-to-End LLM-Powered Multi-Agent Framework for Spreadsheet Automation](https://arxiv.org/abs/2506.12339)
*Ruiyan Zhu,Xi Cheng,Ke Liu,Brian Zhu,Daniel Jin,Neeraj Parihar,Zhoutian Xu,Oliver Gao*

Main category: cs.HC

TL;DR: SheetMind是一个由LLM驱动的多智能体框架，可以通过自然语言指令实现电子表格自动化。


<details>
  <summary>Details</summary>
Motivation: 将自然语言指令转化为电子表格操作，无需用户具备脚本或公式知识。

Method: 提出一个包含经理智能体、动作智能体和反思智能体的三智能体框架。经理智能体负责分解任务，动作智能体负责将任务转化为结构化命令，反思智能体负责验证。通过Google Sheets插件集成。

Result: 在基准数据集上，单步任务成功率为80%，多步任务成功率约为70%，优于其他方法。

Conclusion: 多智能体分解和基于语法的执行在连接自然语言和电子表格功能方面是有效的。

Abstract: We present SheetMind, a modular multi-agent framework powered by large language models (LLMs) for spreadsheet automation via natural language instructions. The system comprises three specialized agents: a Manager Agent that decomposes complex user instructions into subtasks; an Action Agent that translates these into structured commands using a Backus Naur Form (BNF) grammar; and a Reflection Agent that validates alignment between generated actions and the user's original intent. Integrated into Google Sheets via a Workspace extension, SheetMind supports real-time interaction without requiring scripting or formula knowledge. Experiments on benchmark datasets demonstrate an 80 percent success rate on single step tasks and approximately 70 percent on multi step instructions, outperforming ablated and baseline variants. Our results highlight the effectiveness of multi agent decomposition and grammar based execution for bridging natural language and spreadsheet functionalities.

</details>


### [137] ["How do you even know that stuff?": Barriers to expertise sharing among spreadsheet users](https://arxiv.org/abs/2506.09216)
*Qing,Xia,Advait Sarkar,Duncan Brumby,Anna Cox*

Main category: cs.HC

TL;DR: 组织中的电子表格专家难以分享他们的知识，这受到社会规范、对专业知识价值的看法以及协作带来的潜在干扰等因素的影响。


<details>
  <summary>Details</summary>
Motivation: 组织中的电子表格专家难以分享他们的知识，这阻碍了重要技术技能的保留。本研究旨在探讨社会规范和对电子表格使用价值的信念如何影响用户分享专业知识的行为。

Method: 通过对两个独立样本中的 31 名专业电子表格用户进行半结构化访谈。

Result: 电子表格专家在调整高度个性化的策略以适应主观标准、把握分享的社交时机方面面临挑战。此外，对自身专业技能的矛盾评价、对知识价值的轻视性规范信念以及对协作潜在干扰的担忧，都会阻碍分享。

Conclusion: 技术设计和社会动态之间复杂的相互作用会影响功能丰富的软件中的协作学习行为。研究结果对软件设计提出了启示，以应对在学习和协作方面的挑战。

Abstract: Spreadsheet collaboration provides valuable opportunities for learning and expertise sharing between colleagues. Sharing expertise is essential for the retention of important technical skillsets within organisations, but previous studies suggest that spreadsheet experts often fail to disseminate their knowledge to others. We suggest that social norms and beliefs surrounding the value of spreadsheet use significantly influence user engagement in sharing behaviours. To explore this, we conducted 31 semi-structured interviews with professional spreadsheet users from two separate samples. We found that spreadsheet providers face challenges in adapting highly personalised strategies to often subjective standards and evaluating the appropriate social timing of sharing. In addition, conflicted self-evaluations of one's spreadsheet expertise, dismissive normative beliefs about the value of this knowledge, and concerns about the potential disruptions associated with collaboration can further deter sharing. We suggest these observations reflect the challenges of long-term learning in feature-rich software designed primarily with initial learnability in mind. We therefore provide implications for design to navigate this tension. Overall, our findings demonstrate how the complex interaction between technology design and social dynamics can shape collaborative learning behaviours in the context of feature-rich software.

</details>


### [138] [Prompting in the Dark: Assessing Human Performance in Prompt Engineering for Data Labeling When Gold Labels Are Absent](https://arxiv.org/abs/2502.11267)
*Zeyu He,Saniya Naphade,Ting-Hao 'Kenneth' Huang*

Main category: cs.HC

TL;DR: 用户在没有金标的情况下迭代提示LLM进行数据标注的“暗提示”场景，结果表明这种方式并不可靠，并且自动化工具在此场景下也表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究在没有金标的情况下，用户迭代提示LLM进行数据标注的效果，以及自动化工具的表现。

Method: 开发了PromptingSheet插件，并通过20名参与者的实验研究“暗提示”的效果。

Result: “暗提示”效果不佳，仅9名参与者在四次或以上迭代后提高了标注准确率。DSPy等自动化工具在缺乏金标时优化效果也受限。

Conclusion: 金标对于LLM数据标注至关重要，并且在设计自动化辅助工具时需要考虑用户的需求和风险。

Abstract: Millions of users prompt large language models (LLMs) for various tasks, but how good are people at prompt engineering? Do users actually get closer to their desired outcome over multiple iterations of their prompts? These questions are crucial when no gold-standard labels are available to measure progress. This paper investigates a scenario in LLM-powered data labeling, "prompting in the dark," where users iteratively prompt LLMs to label data without using manually-labeled benchmarks. We developed PromptingSheet, a Google Sheets add-on that enables users to compose, revise, and iteratively label data through spreadsheets. Through a study with 20 participants, we found that prompting in the dark was highly unreliable -- only 9 participants improved labeling accuracy after four or more iterations. Automated prompt optimization tools like DSPy also struggled when few gold labels were available. Our findings highlight the importance of gold labels and the needs, as well as the risks, of automated support in human prompt engineering, providing insights for future tool design.

</details>


### [139] [When Copilot Becomes Autopilot: Generative AI's Critical Risk to Knowledge Work and a Critical Solution](https://arxiv.org/abs/2412.15030)
*Advait Sarkar,Xiaotong,Xu,Neil Toronto,Ian Drosos,Christian Poelitz*

Main category: cs.HC

TL;DR: 生成式AI在工作中可能带来错误，但也能帮助用户，特别是新手，学习和使用更复杂的软件功能。它在电子表格领域的应用尤为突出，主要风险并非AI的“幻觉”，而是可能削弱人类的批判性思维。因此，设计生成式AI的界面至关重要，应着重培养用户的批判性思维。我们提出了一个原型系统，用于电子表格中的批判性筛选，该系统能生成筛选条件、排序数据，并提供“挑衅”性文本来批判AI生成的条件，指出其风险、不足和替代方案。这为AI辅助的知识工作中设计批判性思维工具开辟了新的方向，并提出了关于“AI作为批评者或挑衅者”的研究议程。


<details>
  <summary>Details</summary>
Motivation: 生成式AI可能引入错误，但也为用户（特别是新手）提供了学习和应用高级软件功能的机会，从而扩大了他们能完成的任务的范围和复杂性。然而，其在电子表格等知识工作流程中的最大风险并非AI的“幻觉”，而是可能削弱人类的批判性思维能力。

Method: 提出一个原型系统，用于电子表格中的批判性筛选活动。该系统利用生成式AI提出筛选条件，并用这些条件对电子表格中的行进行排序。此外，系统还会生成“挑衅”性文本，旨在批判AI生成的条件，指出其风险、不足之处以及可行的替代方案。

Result: 开发了一个原型系统，能够利用生成式AI建议筛选条件、对电子表格数据进行排序，并通过生成“挑衅”性文本来批判性地评估AI生成的条件，从而突显了潜在的风险、缺点和替代方案。这表明存在一个针对现代AI辅助知识工作设计的批判性思维工具的丰富且未被探索的设计空间。

Conclusion: 生成式AI在知识工作中既带来了风险（如引入错误和削弱批判性思维），也带来了机遇。为了应对这些挑战，应设计生成式AI系统，特别是其用户界面，以积极鼓励和培养用户的批判性思维。我们提出的原型系统是朝着这个方向迈出的一步，并为未来研究“AI作为批评者或挑衅者”奠定了基础，探索了挑衅的呈现方式、内容和设计权衡等问题。

Abstract: Generative AI, with its tendency to "hallucinate" incorrect results, may pose a risk to knowledge work by introducing errors. On the other hand, it may also provide unprecedented opportunities for users, particularly non-experts, to learn and apply advanced software features and greatly increase the scope and complexity of tasks they can successfully achieve.
  As an example of a complex knowledge workflow that is subject to risks and opportunities from generative AI, we consider the spreadsheet. AI hallucinations are an important challenge, but they are not the greatest risk posed by generative AI to spreadsheet workflows. Rather, as more work can be safely delegated to AI, the risk is that human critical thinking -- the ability to holistically and rigorously evaluate a problem and its solutions -- is degraded in the process. The solution is to design the interfaces of generative AI systems deliberately to foster and encourage critical thinking in knowledge work, building primarily on a long history of research on critical thinking tools for education.
  We discuss a prototype system for the activity of critical shortlisting in spreadsheets. The system uses generative AI to suggest shortlisting criteria and applies these criteria to sort rows in a spreadsheet. It also generates "provocations": short text snippets that critique the AI-generated criteria, highlighting risks, shortcomings, and alternatives. Our prototype opens up a rich and completely unexplored design space of critical thinking tools for modern AI-assisted knowledge work. We outline a research agenda for AI as a critic or provocateur, including questions about where and when provocations should appear, their form and content, and potential design trade-offs.

</details>


### [140] [Understanding and Evaluating Trust in Generative AI and Large Language Models for Spreadsheets](https://arxiv.org/abs/2412.14062)
*Simon Thorne*

Main category: cs.HC

TL;DR: 生成式AI和LLM在自动化电子表格公式创建方面有很大潜力，但由于幻觉、偏差和用户技能差异，其输出的准确性和可信度无法保证。为此，本文提出了一个基于公式透明度和可依赖性评估的可信度框架。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI和LLM在电子表格公式创建中存在的幻觉、偏差和用户技能差异等问题，以确保输出的准确性和可信度。

Method: 通过可解释性和可见性来探索公式的透明度，并通过可靠性和道德考量来评估生成公式的可依赖性。同时，分析了幻觉、训练数据偏差和提示构建不当等驱动因素。

Result: 提出了一个可信度框架，用于评估生成式AI（特别是LLM）在电子表格公式创建中的透明度和可依赖性，并分析了影响这些指标的因素。

Conclusion: 生成式AI在电子表格公式创建方面具有潜力，但需要通过透明度和可依赖性框架来解决其固有的挑战，以确保其输出的准确性和可信度。

Abstract: Generative AI and Large Language Models (LLMs) hold promise for automating spreadsheet formula creation. However, due to hallucinations, bias and variable user skill, outputs obtained from generative AI cannot be assumed to be accurate or trustworthy. To address these challenges, a trustworthiness framework is proposed based on evaluating the transparency and dependability of the formula. The transparency of the formula is explored through explainability (understanding the formula's reasoning) and visibility (inspecting the underlying algorithms). The dependability of the generated formula is evaluated in terms of reliability (consistency and accuracy) and ethical considerations (bias and fairness). The paper also examines the drivers to these metrics in the form of hallucinations, training data bias and poorly constructed prompts. Finally, examples of mistrust in technology are considered and the consequences explored.

</details>


### [141] [Dynamic Prompt Middleware: Contextual Prompt Refinement Controls for Comprehension Tasks](https://arxiv.org/abs/2412.02357)
*Ian Drosos,Jack Williams,Advait Sarkar,Nicholas Wilson*

Main category: cs.HC

TL;DR: 用户在提示生成式 AI 时，尤其是在理解任务（如解释电子表格公式、Python 代码和文本段落）的上下文时，会遇到困难。提示中间件旨在通过辅助提示构建来解决这一障碍，但用户在表达足够的控制以使 AI 响应符合其偏好方面仍存在障碍。


<details>
  <summary>Details</summary>
Motivation: 生成式 AI 的有效提示对许多用户来说充满挑战，特别是在为理解任务（如解释电子表格公式、Python 代码和文本段落）提供上下文时。提示中间件旨在通过辅助提示构建来解决这一障碍，但用户在表达足够的控制以使 AI 响应符合其偏好方面仍存在障碍。

Method: 我们进行了一项形成性调查（n=38），研究用户对控制 AI 生成的解释以用于理解任务的需求，揭示了标准化但可预测的提示支持与针对用户和任务量身定制的自适应但不可预测的支持之间的权衡。为了探索这种权衡，我们实施了两种提示中间件方法：动态提示改进控制（Dynamic PRC）和静态提示改进控制（Static PRC）。动态 PRC 方法生成特定于上下文的用户界面元素，根据用户的提示和用户对 AI 的需求提供提示改进，而静态 PRC 方法提供预设的通用改进列表。我们通过一项对照用户研究（n=16）评估了这两种方法，以评估这些方法对用户控制 AI 响应以生成更好解释的影响。

Result: 结果显示，动态 PRC 方法更受欢迎，因为它提供了更多的控制，降低了提供上下文的障碍，并鼓励对任务进行探索和反思，但推理不同生成的控件对最终输出的影响仍然具有挑战性。

Conclusion: 我们的研究结果表明，动态提示中间件可以通过提供更大的控制力来改善生成式 AI 工作流程的用户体验，并引导用户获得更好的 AI 响应。

Abstract: Effective prompting of generative AI is challenging for many users, particularly in expressing context for comprehension tasks such as explaining spreadsheet formulas, Python code, and text passages. Prompt middleware aims to address this barrier by assisting in prompt construction, but barriers remain for users in expressing adequate control so that they can receive AI-responses that match their preferences.
  We conduct a formative survey (n=38) investigating user needs for control over AI-generated explanations in comprehension tasks, which uncovers a trade-off between standardized but predictable support for prompting, and adaptive but unpredictable support tailored to the user and task. To explore this trade-off, we implement two prompt middleware approaches: Dynamic Prompt Refinement Control (Dynamic PRC) and Static Prompt Refinement Control (Static PRC). The Dynamic PRC approach generates context-specific UI elements that provide prompt refinements based on the user's prompt and user needs from the AI, while the Static PRC approach offers a preset list of generally applicable refinements.
  We evaluate these two approaches with a controlled user study (n=16) to assess the impact of these approaches on user control of AI responses for crafting better explanations. Results show a preference for the Dynamic PRC approach as it afforded more control, lowered barriers to providing context, and encouraged exploration and reflection of the tasks, but that reasoning about the effects of different generated controls on the final output remains challenging. Drawing on participant feedback, we discuss design implications for future Dynamic PRC systems that enhance user control of AI responses. Our findings suggest that dynamic prompt middleware can improve the user experience of generative AI workflows by affording greater control and guide users to a better AI response.

</details>


### [142] [Exploring Higher Education Competencies through Spreadsheet Self-Assessment and Time](https://arxiv.org/abs/2409.12974)
*Maria Csernoch,Judit T. Kiss,Viktor Takács,Domicián Máté*

Main category: cs.HC

TL;DR: 学生自我评估的电子表格能力与实际表现存在差距，且在数字环境下完成任务需要更长时间。


<details>
  <summary>Details</summary>
Motivation: 探讨高等教育学生在电子表格能力和可靠性方面的自我评估与实际解决问题能力。

Method: 通过自我评估和实际解决问题（纸质和Excel）来研究学生的电子表格能力。

Result: 学生倾向于不准确地评估自己的电子表格能力，并且在数字环境下完成任务需要更长时间。研究结果未能证实“数字原住民”在Excel操作上优于纸质操作的假设。

Conclusion: 高等教育学生需要改进自我评估能力，并注意在数字环境下完成任务所需的时间，这挑战了“数字原住民”无需计算机科学教育的观点。

Abstract: The present paper aims to explore higher education students' spreadsheet competencies and reliability through self-assessment and real-world problem-solving practices. Digital natives alleged skills and competences allowed us to hypothesize that students perform better in Excel than on paper, but the findings cannot confirm this hypothesis. However, our results indicate that students tend to inaccurately assess their spreadsheet competencies compared to their actual performance in both paper-based and Excel tasks. It has also be found that students need at least twice as much time to achieve the same high scores in the digital environment as they do on paper. The results violated the widely accepted assumption that digital native students do not need computer science education, since they are born with it. This study highlights the importance of accurate self-assessment in digital skill development and time management within higher education contexts, particularly in technology-driven disciplines.

</details>


### [143] [Supporting Annotators with Affordances for Efficiently Labeling Conversational Data](https://arxiv.org/abs/2403.07762)
*Austin Z. Henley,David Piorkowski*

Main category: cs.HC

TL;DR: CAL是一个新颖的界面，旨在通过防止不当标签、提供指导、整合文档和提供查看先前标签的有效方法来辅助数据标记，旨在减轻众包标记的耗时和单调性。


<details>
  <summary>Details</summary>
Motivation: 众包标记耗时且成本高昂。CAL旨在通过提供一个新颖的界面来辅助数据标记，以减轻工作和单调性。

Method: 设计了一个名为CAL的新颖界面，它具有防止不当标签、提供指导、整合文档以及提供查看先前标签的有效方法等特点。我们还实现了一个生产级别的CAL版本，并进行了用户研究评估。

Result: 用户研究表明，使用CAL的用户报告的认知负荷较低，任务时间没有增加，并且认为CAL比标准电子表格更容易使用，并且更受欢迎。

Conclusion: CAL是一个用户友好且高效的数据标记界面，优于传统电子表格方法。

Abstract: Without well-labeled ground truth data, machine learning-based systems would not be as ubiquitous as they are today, but these systems rely on substantial amounts of correctly labeled data. Unfortunately, crowdsourced labeling is time consuming and expensive. To address the concerns of effort and tedium, we designed CAL, a novel interface to aid in data labeling. We made several key design decisions for CAL, which include preventing inapt labels from being selected, guiding users in selecting an appropriate label when they need assistance, incorporating labeling documentation into the interface, and providing an efficient means to view previous labels. We implemented a production-quality implementation of CAL and report a user-study evaluation that compares CAL to a standard spreadsheet. Key findings of our study include users using CAL reported lower cognitive load, did not increase task time, users rated CAL to be easier to use, and users preferred CAL over the spreadsheet.

</details>


### [144] [Prompting for Discovery: Flexible Sense-Making for AI Art-Making with Dreamsheets](https://arxiv.org/abs/2310.09985)
*Shm Garanganao Almeda,J. D. Zamfirescu-Pereira,Kyu Won Kim,Pradeep Mani Rathnam,Bjoern Hartmann*

Main category: cs.HC

TL;DR: DreamSheets是一个电子表格界面，通过基于LLM的函数辅助用户进行提示词构建和结果展示，帮助用户探索文本到图像（TTI）模型的巨大设计空间，并识别出有效的探索策略和界面功能。


<details>
  <summary>Details</summary>
Motivation: 用户在探索文本到图像（TTI）模型时，需要一种方法来可靠地引导提示词空间探索，以获得有趣的结果，因为即使是微小的提示词调整也会产生截然不同的图像。

Method: 我们设计了一个名为DreamSheets的探针，它提供了一个电子表格界面，并结合了基于LLM的函数，用于辅助提示词构建和同时展示生成的结果。我们通过一项初步的实验室研究和一项包含五位专业艺术家的纵向研究来评估其效果。

Result: 研究揭示了参与者用于应对TTI设计空间探索挑战的策略，以及所需的界面功能，例如使用文本生成来定义探索的局部“轴”。

Conclusion: DreamSheets的灵活布局和新颖的生成功能支持用户定义的实验流程，其研究结果可用于指导未来TTI模型交互界面的设计。

Abstract: Design space exploration (DSE) for Text-to-Image (TTI) models entails navigating a vast, opaque space of possible image outputs, through a commensurately vast input space of hyperparameters and prompt text. Minor adjustments to prompt input can surface unexpectedly disparate images. How can interfaces support end-users in reliably steering prompt-space explorations towards interesting results? Our design probe, DreamSheets, supports exploration strategies with LLM-based functions for assisted prompt construction and simultaneous display of generated results, hosted in a spreadsheet interface. The flexible layout and novel generative functions enable experimentation with user-defined workflows. Two studies, a preliminary lab study and a longitudinal study with five expert artists, revealed a set of strategies participants use to tackle the challenges of TTI design space exploration, and the interface features required to support them - like using text-generation to define local "axes" of exploration. We distill these insights into a UI mockup to guide future interfaces.

</details>


### [145] [Does Using ChatGPT Result in Human Cognitive Augmentation?](https://arxiv.org/abs/2401.11042)
*Ron Fulbright,Miranda Morrison*

Main category: cs.HC

TL;DR: ChatGPT may not always augment human cognitive performance and can even mislead users in certain tasks, highlighting the continued importance of human judgment.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of ChatGPT on human cognitive performance and determine if it leads to augmentation or potential drawbacks.

Method: Conducted two experiments comparing human performance with and without ChatGPT assistance on various tasks.

Result: ChatGPT does not consistently augment cognitive performance and can sometimes mislead users, leading to negative cognitive augmentation. Human judgment, discernment, and evaluation remain crucial for certain tasks.

Conclusion: ChatGPT does not yet replace human judgment and can even hinder cognitive performance in some cases, emphasizing the need for critical evaluation of its outputs.

Abstract: Human cognitive performance is enhanced by the use of tools. For example, a human can produce a much greater, and more accurate, volume of mathematical calculation in a unit of time using a calculator or a spreadsheet application on a computer. Such tools have taken over the burden of lower level cognitive grunt work but the human still serves the role of the expert performing higher level thinking and reasoning. Recently, however, unsupervised, deep, machine learning has produced cognitive systems able to outperform humans in several domains. When humans use these tools in a human cog ensemble, the cognitive ability of the human is augmented. In some cases, even non experts can achieve, and even exceed, the performance of experts in a particular domain, synthetic expertise. A new cognitive system, ChatGPT, has burst onto the scene during the past year. This paper investigates human cognitive augmentation due to using ChatGPT by presenting the results of two experiments comparing responses created using ChatGPT with results created not using ChatGPT. We find using ChatGPT does not always result in cognitive augmentation and does not yet replace human judgement, discernment, and evaluation in certain types of tasks. In fact, ChatGPT was observed to result in misleading users resulting in negative cognitive augmentation.

</details>


### [146] [Co-audit: tools to help humans double-check AI-generated content](https://arxiv.org/abs/2310.01297)
*Andrew D. Gordon,Carina Negreanu,José Cambronero,Rasika Chakravarthy,Ian Drosos,Hao Fang,Bhaskar Mitra,Hannah Richardson,Advait Sarkar,Stephanie Simmons,Jack Williams,Ben Zorn*

Main category: cs.HC

TL;DR: AI生成的复杂内容（如摘要、表格、代码）难以被用户审计，因此需要“协同审计”工具来辅助检查，特别是在电子表格计算等要求质量且错误有严重后果的应用中。本文探讨了协同审计工具的研究，提出其设计原则并概述了研究挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容日益复杂，用户难以审计其质量和正确性，因此需要协同审计工具来辅助用户检查AI生成的内容，尤其是在电子表格计算等应用中。

Method: 本文描述了针对由生成模型驱动的电子表格计算的协同审计工具的最新研究，提出了协同审计的初步原则，并概述了研究挑战。

Result: 本文探讨了协同审计工具在电子表格计算领域的应用，并提出了一系列设计原则和未来研究方向。

Conclusion: 协同审计工具对于任何对质量要求高且错误后果严重的生成AI应用（如电子表格计算）都是至关重要的，并提出了一系列设计原则和研究挑战。

Abstract: Users are increasingly being warned to check AI-generated content for correctness. Still, as LLMs (and other generative models) generate more complex output, such as summaries, tables, or code, it becomes harder for the user to audit or evaluate the output for quality or correctness. Hence, we are seeing the emergence of tool-assisted experiences to help the user double-check a piece of AI-generated content. We refer to these as co-audit tools. Co-audit tools complement prompt engineering techniques: one helps the user construct the input prompt, while the other helps them check the output response. As a specific example, this paper describes recent research on co-audit tools for spreadsheet computations powered by generative models. We explain why co-audit experiences are essential for any application of generative AI where quality is important and errors are consequential (as is common in spreadsheet computations). We propose a preliminary list of principles for co-audit, and outline research challenges.

</details>


### [147] ["What It Wants Me To Say": Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models](https://arxiv.org/abs/2304.06597)
*Michael Xieyang Liu,Advait Sarkar,Carina Negreanu,Ben Zorn,Jack Williams,Neil Toronto,Andrew D. Gordon*

Main category: cs.HC

TL;DR: 该研究提出了一种名为“地面抽象匹配”的方法，旨在帮助非专业用户更好地利用代码生成大语言模型进行数据分析。


<details>
  <summary>Details</summary>
Motivation: 目前的代码生成大语言模型在理解用户自然语言指令方面存在挑战，特别是对于非专业用户而言，学习如何有效地引导模型生成代码是一个难题。

Method: 研究人员提出了一种“地面抽象匹配”方法，该方法通过将模型生成的代码翻译回系统化、可预测的自然语言表述，来弥合抽象差距。在实验中，他们将该方法与一种未进行地面匹配的替代方法进行了比较，并在一个包含24名参与者的“在被试间进行、出声思考”的研究中进行了评估。

Result: 实验结果表明，“地面抽象匹配”方法能够提升末端用户对代码生成模型范围和能力的理解，并帮助他们了解何种语言能够有效地用于指导模型。

Conclusion: “地面抽象匹配”方法可以显著改善非专业用户与代码生成大语言模型交互的体验，提高用户对模型的理解和有效使用率。

Abstract: Code-generating large language models translate natural language into code. However, only a small portion of the infinite space of naturalistic utterances is effective at guiding code generation. For non-expert end-user programmers, learning this is the challenge of abstraction matching. We examine this challenge in the specific context of data analysis in spreadsheets, in a system that maps the users natural language query to Python code using the Codex generator, executes the code, and shows the result. We propose grounded abstraction matching, which bridges the abstraction gap by translating the code back into a systematic and predictable naturalistic utterance. In a between-subjects, think-aloud study (n=24), we compare grounded abstraction matching to an ungrounded alternative based on previously established query framing principles. We find that the grounded approach improves end-users' understanding of the scope and capabilities of the code-generating model, and the kind of language needed to use it effectively.

</details>


### [148] [Team OS's System for Dialogue Robot Competition 2022](https://arxiv.org/abs/2210.09928)
*Yuki Kubo,Ryo Yanagimoto,Hayato Futase,Mikio Nakano,Zhaojie Luo,Kazunori Komatani*

Main category: cs.HC

TL;DR: OSbot是一个为对话机器人竞赛2022开发的对话机器人系统，其对话流程基于手动描述的状态转换，并结合关键词提取和情感分析的结果。


<details>
  <summary>Details</summary>
Motivation: 本次竞赛的背景下，开发一个能够有效进行对话的机器人系统。

Method: OSbot系统基于状态转换的对话流，并结合了关键词提取（基于命名实体提取和预定义关键词集）和情感分析（基于SVM和Hazumi多模态对话语料库训练）技术。对话流的管理和编辑通过电子表格进行，并提供日志功能以供快速检查和编辑。

Result: 在竞赛的初步环节中，该系统取得了第三名的成绩。

Conclusion: OSbot系统通过结合状态转换、关键词提取和情感分析，实现了有效的对话管理，并在竞赛中取得了不错的成绩。

Abstract: This paper describes our dialogue robot system, OSbot, developed for Dialogue Robot Competition 2022. The dialogue flow is based on state transitions described manually and the transition conditions use the results of keyword extraction and sentiment analysis. The transitions can be easily viewed and edited by managing them on a spreadsheet. The keyword extraction is based on named entity extraction and our predefined keyword set. The sentiment analysis is text-based and uses SVM, which was trained with the multimodal dialogue corpus Hazumi. We quickly checked and edited a dialogue flow by using a logging function. In the competition's preliminary round, our system ended up in third place.

</details>


### [149] [What is it like to program with artificial intelligence?](https://arxiv.org/abs/2208.06213)
*Advait Sarkar,Andrew D. Gordon,Carina Negreanu,Christian Poelitz,Sruti Srinivasa Ragavan,Ben Zorn*

Main category: cs.HC

TL;DR: LLM编程是一种新的编程范式，它结合了现有编程方法的特性，但也带来了独特的挑战，尤其是在面向非专业用户的应用中。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLM）辅助编程与以往程序员辅助方式的异同，并关注其在末端用户编程中的应用和挑战。

Method: 通过分析公开的LLM辅助编程经验报告、可用性和设计研究，并结合一项针对非专业用户在电子表格中进行数据任务的用户研究。

Result: LLM辅助编程与编译、结对编程、通过搜索和重用进行编程有相似之处，但也存在根本性差异；LLM辅助编程应被视为一种新的编程方式；末端用户编程中存在潜在问题和开放性研究挑战。

Conclusion: LLM辅助编程是一种新的编程范式，具有独特的属性和挑战，尤其是在面向缺乏编程经验的末端用户时。

Abstract: Large language models, such as OpenAI's codex and Deepmind's AlphaCode, can generate code to solve a variety of problems expressed in natural language. This technology has already been commercialised in at least one widely-used programming editor extension: GitHub Copilot.
  In this paper, we explore how programming with large language models (LLM-assisted programming) is similar to, and differs from, prior conceptualisations of programmer assistance. We draw upon publicly available experience reports of LLM-assisted programming, as well as prior usability and design studies. We find that while LLM-assisted programming shares some properties of compilation, pair programming, and programming via search and reuse, there are fundamental differences both in the technical possibilities as well as the practical experience. Thus, LLM-assisted programming ought to be viewed as a new way of programming with its own distinct properties and challenges.
  Finally, we draw upon observations from a user study in which non-expert end user programmers use LLM-assisted tools for solving data tasks in spreadsheets. We discuss the issues that might arise, and open research challenges, in applying large language models to end-user programming, particularly with users who have little or no programming expertise.

</details>


### [150] [MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization](https://arxiv.org/abs/2209.05739)
*Lu Ying,Xinhuan Shu,Dazhen Deng,Yuchen Yang,Tan Tang,Lingyun Yu,Yingcai Wu*

Main category: cs.HC

TL;DR: MetaGlyph是一个自动生成隐喻图形可视化（MGV）的系统，可以从电子表格中提取数据，并根据数据语义自动选择隐喻和图像，然后使用蒙特卡洛树搜索算法来设计图形，以将视觉元素与数据维度相关联，并提供用户自定义选项。


<details>
  <summary>Details</summary>
Motivation: 虽然基于图形的可视化在图形设计方面取得了显著的成就，但创建具有视觉隐喻的图形可视化（MGV）需要深入的数据理解和专业设计技能，因此需要一个自动化的系统来辅助创建。

Method: 该研究首先进行定性分析，从隐喻体现和图形设计角度理解当前的MGV设计。然后，提出了一个新颖的框架，通过隐喻图像选择和MGV构建来生成MGV。MetaGlyph自动从在线资源中选择具有相应图像的隐喻，并集成蒙特卡洛树搜索算法来探索MGV设计，将视觉元素与数据维度相关联，同时考虑数据重要性、语义相关性和图形不重叠。此外，该系统还提供编辑反馈，允许用户根据自己的设计偏好自定义MGV。

Result: MetaGlyph系统能够根据输入数据自动选择隐喻和图像，并利用蒙特卡洛树搜索算法生成图形可视化。通过示例、使用场景和专家访谈证明了该系统的有效性。

Conclusion: MetaGlyph通过提供自动化的隐喻图形可视化生成流程，解决了手动创建MGV的挑战，并允许用户进行自定义，从而证明了其在简化和增强可视化设计方面的有效性。

Abstract: Glyph-based visualization achieves an impressive graphic design when associated with comprehensive visual metaphors, which help audiences effectively grasp the conveyed information through revealing data semantics. However, creating such metaphoric glyph-based visualization (MGV) is not an easy task, as it requires not only a deep understanding of data but also professional design skills. This paper proposes MetaGlyph, an automatic system for generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct a qualitative analysis to understand the design of current MGVs from the perspectives of metaphor embodiment and glyph design. Based on the results, we introduce a novel framework for generating MGVs by metaphoric image selection and an MGV construction. Specifically, MetaGlyph automatically selects metaphors with corresponding images from online resources based on the input data semantics. We then integrate a Monte Carlo tree search algorithm that explores the design of an MGV by associating visual elements with data dimensions given the data importance, semantic relevance, and glyph non-overlap. The system also provides editing feedback that allows users to customize the MGVs according to their design preferences. We demonstrate the use of MetaGlyph through a set of examples, one usage scenario, and validate its effectiveness through a series of expert interviews.

</details>


### [151] [PoVRPoint: Authoring Presentations in Mobile Virtual Reality](https://arxiv.org/abs/2201.06337)
*Verena Biener,Travis Gesslein,Daniel Schneider,Felix Kawala,Alexander Otte,Per Ola Kristensson,Michel Pahud,Eyal Ofek,Cuauhtli Campos,Matjaž Kljun,Klen Čopič Pucihar,Jens Grubert*

Main category: cs.HC

TL;DR: VR 演示文稿创作工具 PoVRPoint 在移动场景下，结合了平板电脑的触笔/触摸输入和 VR 的空间交互能力，通过扩展显示空间来辅助用户进行幻灯片选择、对象操作、动画制作和布局安排。实验结果表明，VR 显著提高了目标幻灯片识别速度和处理遮挡时对象的重新排序能力，且用户认为该交互技术可用且有趣。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索虚拟现实（VR）在移动场景下支持知识工作者创作演示文稿的设计空间，通过结合传统输入设备和VR的空间交互能力，提升创作效率和体验。

Method: 提出 PoVRPoint 工具集，将移动设备（如平板电脑）的触笔/触摸编辑功能与VR的交互能力相结合，并进行用户研究，评估其在目标幻灯片识别、对象空间操控、动画创建和多重遮挡对象布局等方面的效用。

Result: 1) VR 的宽视场显著提高了目标幻灯片的识别速度（与仅使用平板电脑相比）；2) VR 的三维视图在处理遮挡对象时，显著提高了对象重新排序的速度（与两个基线接口相比）。用户研究确认了该交互技术具有可用性和趣味性。

Conclusion: PoVRPoint 工具集利用VR的扩展显示空间和空间交互能力，有效提升了移动场景下演示文稿创作的效率和用户体验，尤其在目标识别和处理遮挡对象方面表现突出。

Abstract: Virtual Reality (VR) has the potential to support mobile knowledge workers by complementing traditional input devices with a large three-dimensional output space and spatial input. Previous research on supporting VR knowledge work explored domains such as text entry using physical keyboards and spreadsheet interaction using combined pen and touch input. Inspired by such work, this paper probes the VR design space for authoring presentations in mobile settings. We propose PoVRPoint -- a set of tools coupling pen- and touch-based editing of presentations on mobile devices, such as tablets, with the interaction capabilities afforded by VR. We study the utility of extended display space to, for example, assist users in identifying target slides, supporting spatial manipulation of objects on a slide, creating animations, and facilitating arrangements of multiple, possibly occluded, shapes. Among other things, our results indicate that 1) the wide field of view afforded by VR results in significantly faster target slide identification times compared to a tablet-only interface for visually salient targets; and 2) the three-dimensional view in VR enables significantly faster object reordering in the presence of occlusion compared to two baseline interfaces. A user study further confirmed that the interaction techniques were found to be usable and enjoyable.

</details>


### [152] [Untidy Data: The Unreasonable Effectiveness of Tables](https://arxiv.org/abs/2106.15005)
*Lyn Bartram,Michael Correll,Melanie Tory*

Main category: cs.HC

TL;DR: 数据表不仅仅是数据清理的工具，更是数据工作者在整个分析过程中进行数据交互、重塑和增强理解的重要媒介。


<details>
  <summary>Details</summary>
Motivation: 数据工作者在信息生态系统中仍依赖电子表格处理数据，但现有分析工具中数据交互方式被隐藏或抽象，本文旨在研究数据工作者如何与数据表进行交互和推理。

Method: 通过定性研究，观察数据工作者如何与数据交互和推理。

Result: 数据表在数据清理之外，还能满足用户在整个分析过程中查看、操作、重塑和增强数据的需求，以支持信息解读。用户通过重组、标注、添加细节层次和生成替代方案等方式与数据进行交互。

Conclusion: 交互式数据表是一种重要的可视化形式，其直接数据交互为可视化分析提供了丰富的 设计空间，并且可以丰富数据感知的过程。

Abstract: Working with data in table form is usually considered a preparatory and tedious step in the sensemaking pipeline; a way of getting the data ready for more sophisticated visualization and analytical tools. But for many people, spreadsheets -- the quintessential table tool -- remain a critical part of their information ecosystem, allowing them to interact with their data in ways that are hidden or abstracted in more complex tools. This is particularly true for data workers: people who work with data as part of their job but do not identify as professional analysts or data scientists. We report on a qualitative study of how these workers interact with and reason about their data. Our findings show that data tables serve a broader purpose beyond data cleanup at the initial stage of a linear analytic flow: users want to see and "get their hands on" the underlying data throughout the analytics process, reshaping and augmenting it to support sensemaking. They reorganize, mark up, layer on levels of detail, and spawn alternatives within the context of the base data. These direct interactions and human-readable table representations form a rich and cognitively important part of building understanding of what the data mean and what they can do with it. We argue that interactive tables are an important visualization idiom in their own right; that the direct data interaction they afford offers a fertile design space for visual analytics; and that sense making can be enriched by more flexible human-data interaction than is currently supported in visual analytics tools.

</details>


### [153] [Defining and Adopting an End User Computing Policy: A Case Study](https://arxiv.org/abs/1909.00855)
*Roger Turner*

Main category: cs.HC

TL;DR: 该论文通过一个案例研究，介绍了在韦斯利安保险协会引入更新的终端用户计算策略的过程，重点在于风险评估和管理。


<details>
  <summary>Details</summary>
Motivation: 终端用户计算（EUC）如果控制不当会带来重大风险，因此需要引入更新的策略来管理这些风险。

Method: 论文介绍了一个风险评估应用程序的开发，该程序根据复杂性、重要性和控制情况（或缺乏控制）来计算任何给定应用程序的风险等级。同时，论文还解释了在实施新策略过程中遇到的挑战以及如何克服这些挑战。

Result: 通过引入基于风险的方法，优先处理和缓解最高风险，从而获得最快的效益。

Conclusion: 通过案例研究表明，采用基于风险的方法来评估和管理终端用户计算的风险，可以有效地控制EUC带来的潜在风险，并带来实际效益。

Abstract: End User Computing carries significant risks if not well controlled. This paper is a case study of the introduction of an updated End User Computing policy at the Wesleyan Assurance Society. The paper outlines the plan and identifies various challenges. The paper explains how these challenges were overcome. We wrote an End User Computing Risk Assessment Application which calculates a risk rating band based on the Complexity, Materiality and Control (or lack of it) pertaining to any given application and the basis of assessment is given in this paper. The policy uses a risk based approach for assessing and mitigating against the highest risks first and obtaining the quickest benefit.

</details>


### [154] [Calliope: Automatic Visual Data Story Generation from a Spreadsheet](https://arxiv.org/abs/2010.09975)
*Danqing Shi,Xinyue Xu,Fuling Sun,Yang Shi,Nan Cao*

Main category: cs.HC

TL;DR: Calliope是一个新的视觉数据故事生成系统，它可以从电子表格生成视觉数据故事，并通过在线编辑器进行修改。


<details>
  <summary>Details</summary>
Motivation: 视觉数据故事（如海报或数据视频）常用于数据导向的叙事，但技术门槛（如数据分析、可视化和脚本编写）阻碍了它们的生成。现有的工具效率低下且难以使用。

Method: Calliope采用了一种新的面向逻辑的蒙特卡洛树搜索算法，通过探索输入电子表格的数据空间，逐步生成故事片段（数据事实）并进行逻辑排序。数据事实的重要性根据信息论进行度量，并自动生成图表和描述。

Result: 通过三个示例故事、两次对照实验和对10位领域专家的访谈进行的评估表明，Calliope有助于高效生成视觉数据故事。

Conclusion: Calliope是一个有益于高效视觉数据故事生成的系统。

Abstract: Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.

</details>


### [155] [Pen-based Interaction with Spreadsheets in Mobile Virtual Reality](https://arxiv.org/abs/2008.04543)
*Travis Gesslein,Verena Biener,Philipp Gagel,Daniel Schneider,Per Ola Kristensson,Eyal Ofek,Michel Pahud,Jens Grubert*

Main category: cs.HC

TL;DR: VR技术和笔式输入相结合，用于增强移动设备上的电子表格交互。


<details>
  <summary>Details</summary>
Motivation: 移动设备上的电子表格交互存在挑战，尤其是在空间受限的场景下。VR技术提供了沉浸式的大显示空间，可以弥补移动设备的不足。

Method: 提出了一套结合VR头显和笔式输入的工具集，利用VR空间增强电子表格的可视化和交互。具体包括：屏幕外的分层菜单、表格依赖关系可视化、以及基于注视和触摸的切换。通过视频在线调查和专家评估来研究可行性。

Result: 研究表明，该工具集在提高电子表格交互效率方面具有潜力。

Conclusion: 结合VR和笔式输入是增强移动电子表格交互的一种有前途的方法。

Abstract: Virtual Reality (VR) can enhance the display and interaction of mobile knowledge work and in particular, spreadsheet applications. While spreadsheets are widely used yet are challenging to interact with, especially on mobile devices, using them in VR has not been explored in depth. A special uniqueness of the domain is the contrast between the immersive and large display space afforded by VR, contrasted by the very limited interaction space that may be afforded for the information worker on the go, such as an airplane seat or a small work-space. To close this gap, we present a tool-set for enhancing spreadsheet interaction on tablets using immersive VR headsets and pen-based input. This combination opens up many possibilities for enhancing the productivity for spreadsheet interaction. We propose to use the space around and in front of the tablet for enhanced visualization of spreadsheet data and meta-data. For example, extending sheet display beyond the bounds of the physical screen, or easier debugging by uncovering hidden dependencies between sheet's cells. Combining the precise on-screen input of a pen with spatial sensing around the tablet, we propose tools for the efficient creation and editing of spreadsheets functions such as off-the-screen layered menus, visualization of sheets dependencies, and gaze-and-touch-based switching between spreadsheet tabs. We study the feasibility of the proposed tool-set using a video-based online survey and an expert-based assessment of indicative human performance potential.

</details>


### [156] [EQUS -- helping to see formulae](https://arxiv.org/abs/2007.00003)
*Chris Roast*

Main category: cs.HC

TL;DR: EQUS是一个交互式可视化工具，用于解决电子表格公式的易懂性问题，其设计和开发过程包括迭代和用户反馈，并被证明对广泛的用户有效。


<details>
  <summary>Details</summary>
Motivation: 电子表格被广泛用于数值处理和建模，但其公式容易被误解，因此需要一个可视化工具来帮助理解。

Method: 通过迭代设计和形成性评估，与目标用户（中学生）合作开发了EQUS，并不断改进。

Result: EQUS的可视化技术被证明对包括中学生在内的广大电子表格用户都具有广泛的适用性。

Conclusion: EQUS作为MS Excel的插件成功开发，证明了其在解决电子表格公式理解难题方面的有效性。

Abstract: Visualisation is often presented as a means of simplifying information and helping people understand complex data. In this paper we describe the design, development and evaluation of an interactive visualisation for spreadsheet formulae (EQUS). The work is justified on the grounds that these are widely used tools for significant numerical processing and modeling, yet the formula developed can be easily misunderstood. The development process was one of iterative refinement engaging an initial target audience of mid-teen learners, involving re-design and formative evaluation. The resulting visualisation techniques have been found to be broadly relevant to spreadsheet users beyond the initial target audience. EQUS has since been developed as fully integrated plug-in for MS Excel.

</details>


### [157] [Taggle: Combining Overview and Details in Tabular Data Visualizations](https://arxiv.org/abs/1712.05944)
*Katarina Furmanova,Samuel Gratzl,Holger Stitz,Thomas Zichner,Miroslava Jaresova,Alexander Lex,Marc Streit*

Main category: cs.HC

TL;DR: Taggle是一种以用户为中心的表格可视化技术，通过对数据子集进行数据驱动的聚合，并辅以定制的交互方法，以可视化方式展示大型复杂表格，解决了表格数据分析中对单个项目进行检查和关联的问题。


<details>
  <summary>Details</summary>
Motivation: 许多实际的分析任务关注检查单个感兴趣的项目，同时需要将项目与潜在的大型表格中的其余部分关联起来。现有的表格数据可视化技术主要关注概述，而忽略了对单个项目的检查。

Method: Taggle采用了一种以用户为中心的、类似电子表格的方法，使用单元格的视觉编码单独可视化源数据中的每一行。同时，Taggle引入了数据子集的数据驱动聚合。该聚合策略辅以专门用于回答特定分析问题的交互方法，例如基于多列的排序以及丰富的数据选择和过滤功能。

Result: 通过一个由领域专家在用于药物发现的复杂基因组数据分析中进行的案例研究，证明了Taggle的可行性。

Conclusion: Taggle提供了一种有效的可视化和探索大型复杂表格的方法，解决了现有技术中对单个项目进行检查和关联的不足。

Abstract: Most tabular data visualization techniques focus on overviews, yet many practical analysis tasks are concerned with investigating individual items of interest. At the same time, relating an item to the rest of a potentially large table is important. In this work we present Taggle, a tabular visualization technique for exploring and presenting large and complex tables. Taggle takes an item-centric, spreadsheet-like approach, visualizing each row in the source data individually using visual encodings for the cells. At the same time, Taggle introduces data-driven aggregation of data subsets. The aggregation strategy is complemented by interaction methods tailored to answer specific analysis questions, such as sorting based on multiple columns and rich data selection and filtering capabilities. We demonstrate Taggle using a case study conducted by a domain expert on complex genomics data analysis for the purpose of drug discovery.

</details>


### [158] [Are digital natives spreadsheet natives?](https://arxiv.org/abs/1909.00865)
*Maria Csernoch,Piroska Biró*

Main category: cs.HC

TL;DR: The paper tests informatics students' spreadsheet skills and finds they struggle, highlighting the need for algorithm-based training.


<details>
  <summary>Details</summary>
Motivation: To assess the algorithmic skills and knowledge transfer abilities of first-year informatics students in spreadsheet environments, especially considering their supposed digital nativity.

Method: Testing students on spreadsheet problem-solving, analyzing their performance based on the use of array formulas versus built-in functions.

Result: Students showed significant difficulties, performing better with algorithm-based array formulas than with problem-specific built-in functions.

Conclusion: First-year informatics students require formal, math-heavy, algorithm-based training from expert teachers, irrespective of their perceived digital nativism.

Abstract: The present paper reports the results of testing first year students of Informatics on their algorithmic skills and knowledge transfer abilities in spreadsheet environments. The selection of students plays a crucial role in the project. On the one hand, they have officially finished their spreadsheet training - they know everything - while on the other hand, they do not need any training, since they are digital natives, to whom digital skills are assigned by birth. However, we found that the students had serious difficulties in solving the spreadsheet problems presented: so low were their results that it allowed us to form broad tendencies. Considering computational thinking, algorithmic skills, and knowledge transfer abilities, it is clear that those students performed better who used algorithm-based, multilevel array formulas instead of problem specific, unconnected built-in functions. Furthermore, we can conclude that students, regardless of their birth date and digital generation assigned to them, are in great need of official, high-mathability, algorithm-based training with expert teachers.

</details>


### [159] [Somewhere Around That Number: An Interview Study of How Spreadsheet Users Manage Uncertainty](https://arxiv.org/abs/1905.13072)
*Judith Borghouts,Andrew D. Gordon,Advait Sarkar,Kenton P. O'Hara,Neil Toronto*

Main category: cs.HC

TL;DR: 用户在电子表格中处理不确定性时，其方式受到电子表格的角色和用户目标的影响。


<details>
  <summary>Details</summary>
Motivation: 了解人们在电子表格中如何处理和应对数据不确定性。

Method: 通过访谈11位来自不同领域的电子表格用户来研究。

Result: 人们处理不确定性的方式受到电子表格在工作中扮演的角色以及用户的目标的影响。电子表格被用作数据库、模板、计算工具、记事本和探索工具。用户目标包括计算和比较不同场景、理解不确定性的性质以及将数据不确定性的复杂性转化为简化的演示给决策者。

Conclusion: 目前的电子表格工具在支持用户处理不确定性方面功能有限，用户需要采用各种变通方法。

Abstract: Spreadsheet users regularly deal with uncertainty in their data, for example due to errors and estimates. While an insight into data uncertainty can help in making better informed decisions, prior research suggests that people often use informal heuristics to reason with probabilities, which leads to incorrect conclusions. Moreover, people often ignore or simplify uncertainty. To understand how people currently encounter and deal with uncertainty in spreadsheets, we conducted an interview study with 11 spreadsheet users from a range of domains. We found that how people deal with uncertainty is influenced by the role the spreadsheet plays in people's work and the user's aims. Spreadsheets are used as a database, template, calculation tool, notepad and exploration tool. In doing so, participants' aims were to compute and compare different scenarios, understand something about the nature of the uncertainty in their situation, and translate the complexity of data uncertainty into simplified presentations to other people, usually decision-makers. Spreadsheets currently provide limited tools to support these aims, and participants had various workarounds.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [160] [A costing framework for fusion power plants](https://arxiv.org/abs/2601.21724)
*Simon Woodruff*

Main category: physics.soc-ph

TL;DR: 本文总结了2017年至2024年间为ARPA-E进行的热能发电厂成本核算工作，并记录了分析框架的演变过程，从早期以资本成本为重点的研究，发展到符合行业标准、可审计的成本核算能力。


<details>
  <summary>Details</summary>
Motivation: ARPA-E资助下的聚变发电厂成本核算工作，旨在记录分析框架的演变，并提供一个标准化的、可审计的成本核算能力。

Method: 早期采用ARIES风格的成本缩放关系生成Nth-of-a-kind (NOAK)估算，并与Bechtel和Decysive Systems合作进行试点研究，以标定设备成本（BOP），并从工程、采购和施工（EPC）角度验证工厂级别的合理性。后续工作结合了Lucid Catalyst对核成本驱动因素的研究，明确地处理了间接成本，并通过面向成本的设计、模块化、集中制造和学习等实践来评估非聚变岛系统的降本路径。这些方法被应用于BETHE和GAMOW概念（以及部分ALPHA的修订），包括在普林斯顿/PPPL专业知识的支持下，加强了氚处理和工厂集成。2023年，该能力被重构为与IAEA-GEN-IV EMWG-EPRI的账户体系保持一致，用自下而上的子系统模型（针对磁体、激光器、电源和反应堆组件等主要成本驱动因素）替代了关键的ARIES衍生缩放关系，并结合了基于物理的功率平衡和工程约束的径向构建。这些开发被集成到基于电子表格的聚变经济学代码（FECONs）中，并作为开源Python框架（pyFECONs）发布，实现了从子系统估算到标准化账户的透明映射，并一致计算LCOE。

Result: 早期基于ARIES风格的成本缩放关系，后期发展为包括间接成本、降本路径分析、氚处理和工厂集成等方面的改进方法。最终重构为符合IAEA-GEN-IV EMWG-EPRI账户体系的FECONs和pyFECONs，实现了子系统估算到标准化账户的透明映射和LCOE的一致计算。

Conclusion: 该工作记录了聚变发电厂成本核算分析框架的演变，从早期资本成本估算到当前集成了物理约束、工程限制和标准化账户体系的开源能力，为聚变能经济性评估提供了更精确、透明和可审计的工具。

Abstract: This paper summarizes and consolidates fusion power-plant costing work performed in support of ARPA-E from 2017 through 2024, and documents the evolution of the associated analysis framework from early capital-cost-focused studies to a standards-aligned, auditable costing capability. Early efforts applied ARIES-style cost-scaling relations to generate Nth-of-a-kind (NOAK) estimates and were calibrated through a pilot study with Bechtel and Decysive Systems to benchmark balance-of-plant (BOP) costs and validate plant-level reasonableness from an engineering, procurement, and construction (EPC) perspective. Subsequent work, informed by Lucid Catalyst studies of nuclear cost drivers, expanded the methodology to treat indirect costs explicitly and to evaluate cost-reduction pathways for non-fusion-island systems through design-for-cost practices, modularization, centralized manufacturing, and learning. As ARPA-E's fusion portfolio expanded, these methods were applied across BETHE and GAMOW concepts (and select ALPHA revisits), including enhanced treatment of tritium handling and plant integration supported by Princeton/PPPL expertise. In 2023 the capability was refactored to align with the IAEA-GEN-IV EMWG-EPRI code-of-accounts lineage, while key ARIES-derived scaling relations were replaced by bottom-up subsystem models for dominant fusion cost drivers (e.g., magnets, lasers, power supplies, and power-core components) coupled to physics-informed power balances and engineering-constrained radial builds. These developments were implemented in the spreadsheet-based Fusion Economics code (FECONs) and released as an open-source Python framework (pyFECONs), providing a transparent mapping from subsystem estimates to standardized accounts and a consistent computation of LCOE.

</details>
