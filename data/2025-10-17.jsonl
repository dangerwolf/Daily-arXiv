{"id": "2509.26557", "title": "The Invisible Mentor: Inferring User Actions from Screen Recordings to Recommend Better Workflows", "url": "https://arxiv.org/abs/2509.26557", "pdf": "https://arxiv.org/pdf/2509.26557", "abs": "https://arxiv.org/abs/2509.26557", "authors": ["Litao Yan", "Andrew Head", "Ken Milne", "Vu Le", "Sumit Gulwani", "Chris Parnin", "Emerson Murphy-Hill"], "categories": ["cs.HC", "H.5.2"], "comment": "15 pages, 6 figures", "summary": "Many users struggle to notice when a more efficient workflow exists in\nfeature-rich tools like Excel. Existing AI assistants offer help only after\nusers describe their goals or problems, which can be effortful and imprecise.\nWe present InvisibleMentor, a system that turns screen recordings of task\ncompletion into vision-grounded reflections on tasks. It detects issues such as\nrepetitive edits and recommends more efficient alternatives based on observed\nbehavior. Unlike prior systems that rely on logs, APIs, or user prompts,\nInvisibleMentor operates directly on screen recordings. It uses a two-stage\npipeline: a vision-language model reconstructs actions and context, and a\nlanguage model generates structured, high-fidelity suggestions. In evaluation,\nInvisibleMentor accurately identified inefficient workflows, and participants\nfound its suggestions more actionable, tailored, and more helpful for learning\nand improvement compared to a prompt-based spreadsheet assistant."}
{"id": "2509.26331", "title": "AI Playing Business Games: Benchmarking Large Language Models on Managerial Decision-Making in Dynamic Simulations", "url": "https://arxiv.org/abs/2509.26331", "pdf": "https://arxiv.org/pdf/2509.26331", "abs": "https://arxiv.org/abs/2509.26331", "authors": ["Berdymyrat Ovezmyradov"], "categories": ["cs.AI", "I.2.1"], "comment": "34 pages, 7 figures, 3 tables", "summary": "The rapid advancement of LLMs sparked significant interest in their potential\nto augment or automate managerial functions. One of the most recent trends in\nAI benchmarking is performance of Large Language Models (LLMs) over longer time\nhorizons. While LLMs excel at tasks involving natural language and pattern\nrecognition, their capabilities in multi-step, strategic business\ndecision-making remain largely unexplored. Few studies demonstrated how results\ncan be different from benchmarks in short-term tasks, as Vending-Bench\nrevealed. Meanwhile, there is a shortage of alternative benchmarks for\nlong-term coherence. This research analyses a novel benchmark using a business\ngame for the decision making in business. The research contributes to the\nrecent literature on AI by proposing a reproducible, open-access management\nsimulator to the research community for LLM benchmarking. This novel framework\nis used for evaluating the performance of five leading LLMs available in free\nonline interface: Gemini, ChatGPT, Meta AI, Mistral AI, and Grok. LLM makes\ndecisions for a simulated retail company. A dynamic, month-by-month management\nsimulation provides transparently in spreadsheet model as experimental\nenvironment. In each of twelve months, the LLMs are provided with a structured\nprompt containing a full business report from the previous period and are\ntasked with making key strategic decisions: pricing, order size, marketing\nbudget, hiring, dismissal, loans, training expense, R&D expense, sales\nforecast, income forecast The methodology is designed to compare the LLMs on\nquantitative metrics: profit, revenue, and market share, and other KPIs. LLM\ndecisions are analyzed in their strategic coherence, adaptability to market\nchanges, and the rationale provided for their decisions. This approach allows\nto move beyond simple performance metrics for assessment of the long-term\ndecision-making."}
