<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.HC](#cs.HC) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Vision Language Models for Spreadsheet Understanding: Challenges and Opportunities](https://arxiv.org/abs/2405.16234)
*Shiyu Xia,Junyu Xiong,Haoyu Dong,Jianbo Zhao,Yuzhang Tian,Mengyu Zhou,Yeye He,Shi Han,Dongmei Zhang*

Main category: cs.CV

TL;DR: 本论文提出三种自监督挑战和评估指标来评估视觉语言模型（VLM）在电子表格理解方面的能力，包括OCR、空间感知和视觉格式识别。


<details>
  <summary>Details</summary>
Motivation: 评估视觉语言模型（VLM）在电子表格理解方面的能力，并指出其在空间感知和格式识别方面的不足，为未来的研究提供方向。

Method: 提出三种自监督挑战（OCR、空间感知、视觉格式识别）和相应的评估指标，并结合电子表格表格检测任务评估VLM的整体性能。同时，提出三种电子表格到图像的设置（列宽调整、样式更改、地址增强）和相应的提示变体，以更精细地探测VLM。在表格边界检测中，提出在表格四边界解码单元格值，以利用VLM的文本理解优势。

Result: VLM在OCR方面表现出潜力，但在单元格遗漏和错位方面存在问题。VLM在空间感知和格式识别方面能力不足。

Conclusion: VLM在电子表格理解方面，尤其是在空间感知和格式识别方面，仍有提升空间。本研究提出的方法和数据集可用于生成大量的电子表格-图像对，以增强VLM的电子表格数据理解能力。

Abstract: This paper explores capabilities of Vision Language Models on spreadsheet comprehension. We propose three self-supervised challenges with corresponding evaluation metrics to comprehensively evaluate VLMs on Optical Character Recognition (OCR), spatial perception, and visual format recognition. Additionally, we utilize the spreadsheet table detection task to assess the overall performance of VLMs by integrating these challenges. To probe VLMs more finely, we propose three spreadsheet-to-image settings: column width adjustment, style change, and address augmentation. We propose variants of prompts to address the above tasks in different settings. Notably, to leverage the strengths of VLMs in understanding text rather than two-dimensional positioning, we propose to decode cell values on the four boundaries of the table in spreadsheet boundary detection. Our findings reveal that VLMs demonstrate promising OCR capabilities but produce unsatisfactory results due to cell omission and misalignment, and they notably exhibit insufficient spatial and format recognition skills, motivating future work to enhance VLMs' spreadsheet data comprehension capabilities using our methods to generate extensive spreadsheet-image pairs in various settings.

</details>


### [2] [High-Throughput Phenotyping using Computer Vision and Machine Learning](https://arxiv.org/abs/2407.06354)
*Vivaan Singhvi,Langalibalele Lunga,Pragya Nidhi,Chris Keum,Varrun Prakash*

Main category: cs.CV

TL;DR: 本研究利用OCR技术和机器学习方法，结合包含物理标签的杨树数据集，实现了对植物表型特征的自动提取与分析，旨在提高高通量表型分析的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 为了提高处理大型数据集和提取特定性状的能力，本研究旨在将高通量表型分析与机器学习相结合，并利用包含物理标签的数据集来解决现有研究中数据集缺乏物理标签的问题。

Method: 本研究利用OCR技术读取杨树叶片图像上的物理标签（包含处理、区块、行、位置和基因型信息），结合图像分割和机器学习算法进行形态学分类（叶形、颜色、褐斑程度），并利用机器学习模型预测处理条件。此外，还分析了编码的EXIF标签以获取叶片大小和性状相关性信息。

Result: OCR模型在非空文本提取方面准确率达到94.31%，能够准确地将标签信息录入电子表格。形态学分类模型（叶形、颜色、褐斑程度）的平均准确率为62.82%，预测植物处理条件的准确率为60.08%。研究发现EXIF标签缺失关键信息，阻碍了叶片大小和性状相关性的评估。

Conclusion: 本研究成功应用OCR和机器学习技术实现了杨树表型数据的自动化处理，但EXIF标签的缺失限制了对叶片大小和性状相关性的深入分析。未来的研究可以改进数据采集和标签编码方式，以克服这些限制，进一步完善高通量表型分析。

Abstract: High-throughput phenotyping refers to the non-destructive and efficient evaluation of plant phenotypes. In recent years, it has been coupled with machine learning in order to improve the process of phenotyping plants by increasing efficiency in handling large datasets and developing methods for the extraction of specific traits. Previous studies have developed methods to advance these challenges through the application of deep neural networks in tandem with automated cameras; however, the datasets being studied often excluded physical labels. In this study, we used a dataset provided by Oak Ridge National Laboratory with 1,672 images of Populus Trichocarpa with white labels displaying treatment (control or drought), block, row, position, and genotype. Optical character recognition (OCR) was used to read these labels on the plants, image segmentation techniques in conjunction with machine learning algorithms were used for morphological classifications, machine learning models were used to predict treatment based on those classifications, and analyzed encoded EXIF tags were used for the purpose of finding leaf size and correlations between phenotypes. We found that our OCR model had an accuracy of 94.31% for non-null text extractions, allowing for the information to be accurately placed in a spreadsheet. Our classification models identified leaf shape, color, and level of brown splotches with an average accuracy of 62.82%, and plant treatment with an accuracy of 60.08%. Finally, we identified a few crucial pieces of information absent from the EXIF tags that prevented the assessment of the leaf size. There was also missing information that prevented the assessment of correlations between phenotypes and conditions. However, future studies could improve upon this to allow for the assessment of these features.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [3] [General Table Question Answering via Answer-Formula Joint Generation](https://arxiv.org/abs/2503.12345)
*Zhongyuan Wang,Richong Zhang,Zhijie Nie,Hangyu Mao*

Main category: cs.CL

TL;DR: LLM在表格问答（TableQA）中通过生成答案文本、SQL查询、Python代码或自定义操作来处理复杂推理问题，但缺乏对特定问题类型或表格结构的适应性。本文提出使用电子表格公式（Spreadsheet Formula）作为可执行表示，并构建了FormulaQA数据集，同时提出了TabAF框架，该框架能够用单个LLM处理多种表格问答任务。实验证明TabAF在WikiTableQuestion、HiTab和TabFact数据集上取得了新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有TableQA方法在处理特定问题类型或表格结构时通用性不足，而电子表格公式这一成熟的语言未被充分利用。

Method: 1. 构建FormulaQA数据集：从现有数据集中提取并注解了Formula。 2. 提出TabAF框架：使用单一LLM模型同时生成答案和Formula，以解决多种表格问答任务。

Result: TabAF框架在WikiTableQuestion、HiTab和TabFact数据集上达到了新的SOTA性能，证明了其通用性和泛化能力。

Conclusion: TabAF框架能够有效利用电子表格公式解决TableQA问题，并在多种表格和问题类型上展现出优越的性能和泛化能力。

Abstract: Advanced table question answering (TableQA) methods prompt large language models (LLMs) to generate answer text, SQL query, Python code, or custom operation, which impressively improve the complex reasoning problems in the TableQA task. However, these methods lack the versatility to cope with specific question types or table structures. In contrast, the Spreadsheet Formula, the widely used and well-defined operation language for tabular data, has not been thoroughly explored to solve TableQA. In this paper, we first attempt to use the Formula as the executable representation for solving complex reasoning on tables with different structures. Specifically, we construct \texttt{FromulaQA}, a large Formula-annotated TableQA dataset from existing datasets. In addition, we propose \texttt{TabAF}, a general table answering framework to solve multiple types of tasks over multiple types of tables simultaneously, which decodes answers and Formulas with a single LLM backbone. Extensive experiments demonstrate the versatility and generalization of \texttt{TabAF}. Under the same model size, \texttt{TabAF} achieves new state-of-the-art performance on the WikiTableQuestion, HiTab, and TabFact.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [4] [Knowledge engineering for open science: Building and deploying knowledge bases for metadata standards](https://arxiv.org/abs/2507.22391)
*Mark A. Musen,Martin J. O'Connor,Josef Hardi,Marcos Martinez-Romero*

Main category: cs.DL

TL;DR: The CEDAR project provides technology for scientists to create metadata templates, promoting FAIR data principles by standardizing data annotation and enabling intelligent systems for open science.


<details>
  <summary>Details</summary>
Motivation: To make scientific datasets findable, accessible, interoperable, and reusable (FAIR) by addressing the challenge of remembering all FAIR principles, particularly the need for standardized metadata.

Method: The CEDAR project develops technology enabling scientists to encode metadata standards as templates. These templates capture experimental attribute preferences and community standards, facilitating data annotation systems (Web forms, spreadsheets) and ensuring metadata adherence.

Result: CEDAR templates have been used to standardize metadata for scientific consortia, forming the basis for data-annotation systems. They aid in correcting metadata for standard compliance and capture knowledge in symbolic form for application in intelligent systems.

Conclusion: CEDAR templates offer a mechanism for scientific communities to establish shared metadata standards, encode preferences for their application, and deploy these standards in intelligent systems to advance open science.

Abstract: Scientists strive to make their datasets available in open repositories, with the goal that they be findable, accessible, interoperable, and reusable (FAIR). Although it is hard for most investigators to remember all the guiding principles associated with FAIR data, there is one overarching requirement: The data need to be annotated with rich, discipline-specific, standardized metadata. The Center for Expanded Data Annotation and Retrieval (CEDAR) builds technology that enables scientists to encode metadata standards as templates that enumerate the attributes of different kinds of experiments. These metadata templates capture preferences regarding how data should be described and what a third party needs to know to make sense of the datasets. CEDAR templates describing community metadata preferences have been used to standardize metadata for a variety of scientific consortia. They have been used as the basis for data-annotation systems that acquire metadata through Web forms or through spreadsheets, and they can help correct metadata to ensure adherence to standards. Like the declarative knowledge bases that underpinned intelligent systems decades ago, CEDAR templates capture the knowledge in symbolic form, and they allow that knowledge to be applied in a variety of settings. They provide a mechanism for scientific communities to create shared metadata standards and to encode their preferences for the application of those standards, and for deploying those standards in a range of intelligent systems to promote open science.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [5] [Consensus-Free Spreadsheet Integration](https://arxiv.org/abs/2209.14457)
*Brandon Baylor,Eric Daimler,James Hansen,Esteban Montero,Ryan Wisnesky*

Main category: cs.DB

TL;DR: 本文提出一种利用范畴论方法合并多个电子表格数据的方法。


<details>
  <summary>Details</summary>
Motivation: 在工程模型合并时，避免作者间达成一致的需求，因为所提出的方法通过保持语义的理论和模型映射来满足此条件。

Method: 将电子表格的公式表示为代数理论，将值表示为理论的模型，将表格间的重叠表示为理论和模型态射，然后使用范畴论的余极限、提升和Kan扩展构造来计算一个规范的、通用的集成理论和模型，最终将其表示为电子表格。

Result: 在一家主要能源公司对一个真实的油气计算案例进行了方法学研究，集成了两个由未相互交互的工程师构建的不同套管压力测试（MASP）计算电子表格，并描述了相关的理论和模型。此外，还描述了与验证重叠映射的语义保持性以及验证结果集成表的一致性/保守性相关的自动定理证明负担。

Conclusion: 讨论了如何将该方法学应用于企业范围内的工程工作扩展。

Abstract: We describe a method for merging multiple spreadsheets into one sheet, and/or exchanging data among the sheets, by expressing each sheet's formulae as an algebraic (equational) theory and each sheet's values as a model of its theory, expressing the overlap between the sheets as theory and model morphisms, and then performing colimit, lifting, and Kan-extension constructions from category theory to compute a canonically universal integrated theory and model, which can then be expressed as a spreadsheet. Our motivation is to find methods of merging engineering models that do not require consensus (agreement) among the authors of the models being merged, a condition fulfilled by our method because theory and model morphisms are semantics-preserving. We describe a case study of this methodology on a real-world oil and gas calculation at a major energy company, describing the theories and models that arise when integrating two different casing pressure test (MASP) calculation spreadsheets constructed by two non-interacting engineers. We also describe the automated theorem proving burden associated with both verifying the semantics preservation of the overlap mappings as well as verifying the conservativity/consistency of the resulting integrated sheet. We conclude with thoughts on how to apply the methodology to scale engineering efforts across the enterprise.

</details>


### [6] [A System and Benchmark for LLM-based Q&A on Heterogeneous Data](https://arxiv.org/abs/2409.05735)
*Achille Fokoue,Srideepika Jayaraman,Elham Khabiri,Jeffrey O. Kephart,Yingjie Li,Dhruv Shah,Youssef Drissi,Fenno F. Heath,Anu Bhamidipaty,Fateh A. Tipu,Robert J. Baseman*

Main category: cs.DB

TL;DR: siwarex平台实现了对数据库和API的无缝自然语言访问，解决了工业环境中数据源异构性问题。


<details>
  <summary>Details</summary>
Motivation: 工业用户希望能够查询结构化数据，但常常面临数据源识别、访问和整合的挑战，现有Text-to-SQL应用难以应对数据源异构性。

Method: 提出siwarex平台，实现对数据库和API的无缝自然语言访问；扩展Spider数据集和基准测试，用API替换部分表，以评估siwarex在处理数据源异构性方面的能力。

Result: siwarex平台在处理数据源异构性方面表现良好。

Conclusion: siwarex平台有效解决了工业数据访问中的异构性问题，扩展的Spider基准测试将有助于该领域的研究。

Abstract: In many industrial settings, users wish to ask questions whose answers may be found in structured data sources such as a spreadsheets, databases, APIs, or combinations thereof. Often, the user doesn't know how to identify or access the right data source. This problem is compounded even further if multiple (and potentially siloed) data sources must be assembled to derive the answer. Recently, various Text-to-SQL applications that leverage Large Language Models (LLMs) have addressed some of these problems by enabling users to ask questions in natural language. However, these applications remain impractical in realistic industrial settings because they fail to cope with the data source heterogeneity that typifies such environments. In this paper, we address heterogeneity by introducing the siwarex platform, which enables seamless natural language access to both databases and APIs. To demonstrate the effectiveness of siwarex, we extend the popular Spider dataset and benchmark by replacing some of its tables by data retrieval APIs. We find that siwarex does a good job of coping with data source heterogeneity. Our modified Spider benchmark will soon be available to the research community

</details>


### [7] [Auditable and reusable crosswalks for fast, scaled integration of scattered tabular data](https://arxiv.org/abs/2409.01517)
*Gavin Chait*

Main category: cs.DB

TL;DR: 该工具包旨在提供一个开源的策展工具集，用于生成结构良好且可互操作的数据。


<details>
  <summary>Details</summary>
Motivation: 该论文提出的主要动机是解决复杂且分散的表格数据管理问题，通过提供一个模式中心的方法来审计和重组数据，使其符合目标模式。

Method: 该工具包将策展过程划分为离散的组件，并采用模式中心的方法。它允许在没有源数据的情况下进行软件开发和分析。数据转换被捕获为描述模式到模式映射的高层顺序脚本。

Result: 该工具包支持将任何符合模式定义的数据进行重组，并提供了Python包和‘无代码’的Web应用程序。论文中还展示了一个来自纵向研究的视觉示例，其中将来自数百个地方议会的零散源数据整合到单个数据库中。

Conclusion: 该工具包为处理和重组复杂、分散的表格数据提供了一个高效、灵活且可访问的解决方案，无论用户是开发者还是非技术用户。

Abstract: This paper presents an open-source curatorial toolkit intended to produce well-structured and interoperable data. Curation is divided into discrete components, with a schema-centric focus for auditable restructuring of complex and scattered tabular data to conform to a destination schema. Task separation allows development of software and analysis without source data being present. Transformations are captured as high-level sequential scripts describing schema-to-schema mappings, reducing complexity and resource requirements. Ultimately, data are transformed, but the objective is that any data meeting a schema definition can be restructured using a crosswalk. The toolkit is available both as a Python package, and as a 'no-code' visual web application. A visual example is presented, derived from a longitudinal study where scattered source data from hundreds of local councils are integrated into a single database.

</details>


### [8] [Auto-Formula: Recommend Formulas in Spreadsheets using Contrastive Learning for Table Representations](https://arxiv.org/abs/2404.12608)
*Sibei Chen,Yeye He,Weiwei Cui,Ju Fan,Song Ge,Haidong Zhang,Dongmei Zhang,Surajit Chaudhuri*

Main category: cs.DB

TL;DR: 电子表格的普及是由于其易用性，但复杂公式的编写仍然具有挑战性。Auto-Formula 系统通过学习和适应同一组织中类似电子表格中已有的公式来解决这一痛点，其灵感来自于计算机视觉中的“类似人脸识别”技术。


<details>
  <summary>Details</summary>
Motivation: 解决非技术用户在电子表格中编写复杂公式时遇到的困难，该困难源于需要查找和理解不直观的公式语法。

Method: 利用同一组织中存在的、具有相似数据和计算逻辑的电子表格。通过学习和适应这些现有公式，并借鉴计算机视觉中的对比学习技术（“类似人脸识别”），来预测用户想要编写的目标单元格公式。

Result: 在从实际企业电子表格中提取的 2K 多个测试公式上进行了广泛评估，结果表明 Auto-Formula 的有效性优于其他方法。

Conclusion: Auto-Formula 系统通过学习类似电子表格中的现有公式，可以有效帮助用户编写复杂的电子表格公式，解决了现有用户界面和复杂公式语法带来的挑战。

Abstract: Spreadsheets are widely recognized as the most popular end-user programming tools, which blend the power of formula-based computation, with an intuitive table-based interface. Today, spreadsheets are used by billions of users to manipulate tables, most of whom are neither database experts nor professional programmers.
  Despite the success of spreadsheets, authoring complex formulas remains challenging, as non-technical users need to look up and understand non-trivial formula syntax. To address this pain point, we leverage the observation that there is often an abundance of similar-looking spreadsheets in the same organization, which not only have similar data, but also share similar computation logic encoded as formulas. We develop an Auto-Formula system that can accurately predict formulas that users want to author in a target spreadsheet cell, by learning and adapting formulas that already exist in similar spreadsheets, using contrastive-learning techniques inspired by "similar-face recognition" from compute vision.
  Extensive evaluations on over 2K test formulas extracted from real enterprise spreadsheets show the effectiveness of Auto-Formula over alternatives. Our benchmark data is available at https://github.com/microsoft/Auto-Formula to facilitate future research.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [9] [Billion-files File Systems (BfFS): A Comparison](https://arxiv.org/abs/2408.01805)
*Sohail Shaikh*

Main category: cs.PF

TL;DR: 本地文件系统对于快速处理海量数据至关重要，本研究分析了EXT4、XFS、BtrFS、ZFS和F2FS这几种流行的Linux文件系统。


<details>
  <summary>Details</summary>
Motivation: 随着数据量的爆炸式增长，需要在靠近计算设备的地方处理数据以减少传输延迟，因此本地文件系统受到了广泛关注。

Method: 通过创建、存储和读取十亿个文件来分析EXT4、XFS、BtrFS、ZFS和F2FS这几种流行的Linux文件系统，并捕获和分析了读/写吞吐量、存储块使用情况、磁盘空间利用率和开销等指标。

Result: 研究分析了文件系统在创建大量文件和文件夹期间及之后的性能下降等副作用。

Conclusion: 对EXT4、XFS、BtrFS、ZFS和F2FS等本地文件系统的性能进行了全面的分析，为系统设计者和集成商提供了有价值的见解。

Abstract: As the volume of data being produced is increasing at an exponential rate that needs to be processed quickly, it is reasonable that the data needs to be available very close to the compute devices to reduce transfer latency. Due to this need, local filesystems are getting close attention to understand their inner workings, performance, and more importantly their limitations. This study analyzes few popular Linux filesystems: EXT4, XFS, BtrFS, ZFS, and F2FS by creating, storing, and then reading back one billion files from the local filesystem. The study also captured and analyzed read/write throughput, storage blocks usage, disk space utilization and overheads, and other metrics useful for system designers and integrators. Furthermore, the study explored other side effects such as filesystem performance degradation during and after these large numbers of files and folders are created.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [10] [Excel: Automated Ledger or Analytics IDE?](https://arxiv.org/abs/2409.12976)
*Andrew Kumiega*

Main category: cs.CY

TL;DR: Excel已从简单的账簿自动化工具演变为分析的集成开发环境（IDE），但其风险管理框架尚未跟上这种演变。


<details>
  <summary>Details</summary>
Motivation: 许多人没有注意到Excel已成为一个包含数据库、OLAP引擎、多种统计编程语言、第三方软件库、动态图表和实时数据连接器的集成开发环境（IDE），这导致了对管理其日益增长的风险的需求。

Method: 分析Excel从桌面应用程序到分析IDE的演变，并提出需要扩展现有的电子表格风险框架来管理这种转变带来的新风险。

Result: 认识到Excel作为分析IDE的演变，并强调需要一个更全面的风险管理框架。

Conclusion: 现有的电子表格风险框架需要扩展，以应对将Excel用作分析IDE所带来的日益增长的风险。

Abstract: Since the inception of VisiCalc over four decades ago, spreadsheets have undergone a gradual transformation, evolving from simple ledger automation tools to the current state of Excel, which can be described as an Integrated Development Environment (IDE) for analytics. The slow evolution of Excel from an automation tool for ledgers to an IDE for analytics explains why many people have not noticed that Excel includes a fully functional database, an OLAP Engine, multiple statistical programming languages, multiple third-party software libraries, dynamic charts, and real time data connectors. The simplicity of accessing these multiple tools is a low-code framework controlled from the Excel tool that is effectively an IDE. Once we acknowledge Excel's shift from a desk top application to an IDE for analytics, the importance of establishing a comprehensive risk framework for managing this distinctive development environment becomes clear. In this paper we will explain how the current risk framework for spreadsheets needs to be expanded to manage the growing risks of using Excel as an IDE for analytics.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [TableTalk: Scaffolding Spreadsheet Development with a Language Agent](https://arxiv.org/abs/2502.09787)
*Jenny T. Liang,Aayush Kumar,Yasharth Bajpai,Sumit Gulwani,Vu Le,Chris Parnin,Arjun Radhakrishna,Ashish Tiwari,Emerson Murphy-Hill,Guastavo Soares*

Main category: cs.SE

TL;DR: TableTalk是一个用于电子表格编程的语言代理，通过提供结构化计划、灵活的下一步建议和增量构建来帮助用户，并在用户研究中显示出优于基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 电子表格编程具有挑战性，而语言代理在自动化任务方面展现出潜力。本文旨在探索如何利用语言代理来辅助电子表格的创建。

Method: TableTalk代理基于对7位电子表格程序员和85个Excel模板的研究，遵循了三个设计原则：脚手架、灵活性和增量性。它引导用户完成结构化计划，提供三个可能的下一步建议，并使用预定义工具来增量地构建电子表格。

Result: 在一项涉及20名程序员的研究中，TableTalk创建的电子表格质量更高，被选中的可能性是基线的2.3倍。此外，它将认知负荷和思考时间减少了12.6%。

Conclusion: TableTalk的成功表明了基于代理的电子表格编程工具的潜力，并为该领域的设计提供了指导方针。这项研究对电子表格编程、最终用户编程、AI辅助编程和人机协作都有重要的启示。

Abstract: Spreadsheet programming is challenging. Programmers use spreadsheet programming knowledge (e.g., formulas) and problem-solving skills to combine actions into complex tasks. Advancements in large language models have introduced language agents that observe, plan, and perform tasks, showing promise for spreadsheet creation. We present TableTalk, a spreadsheet programming agent embodying three design principles -- scaffolding, flexibility, and incrementality -- derived from studies with seven spreadsheet programmers and 85 Excel templates. TableTalk guides programmers through structured plans based on professional workflows, generating three potential next steps to adapt plans to programmer needs. It uses pre-defined tools to generate spreadsheet components and incrementally build spreadsheets. In a study with 20 programmers, TableTalk produced higher-quality spreadsheets 2.3 times more likely to be preferred than the baseline. It reduced cognitive load and thinking time by 12.6%. From this, we derive design guidelines for agentic spreadsheet programming tools and discuss implications on spreadsheet programming, end-user programming, AI-assisted programming, and human-agent collaboration.

</details>


### [12] [MaxMind: A Memory Loop Network to Enhance Software Productivity based on Large Language Models](https://arxiv.org/abs/2408.03841)
*Yuchen Dong,XiaoXiang Fang,Yuchen Hu,Renshuang Jiang,Zhe Jiang*

Main category: cs.SE

TL;DR: 本论文提出了一种名为MaxMind的新方法，通过引入Memory-Loop Networks和知识精度分段增强的RAG机制，来解决大型语言模型在软件自动化操作和工具生成（SOTG）中记忆和知识利用的问题。实验表明，MaxMind可以显著提高任务成功率和执行效率，并解决LLM在专业任务中的再训练问题。


<details>
  <summary>Details</summary>
Motivation: 当前研究在将实时任务经验转化为系统记忆以及区分知识价值方面存在不足，阻碍了大型语言模型在软件自动化操作和工具生成（SOTG）方面的应用和生产力提升。

Method: 提出Memory-Loop Networks用于及时记忆和经验引用，并增强了具有知识精度分段的RAG机制以根据价值区分来利用记忆。在此基础上设计了MaxMind模型，并开发了MaxMind4Sheet作为实例。

Result: MaxMind4Sheet在与SheetCopilot的对比实验中，任务成功率通过记忆的积累和回收稳步提升了约3%-6%，记忆回收将任务执行效率提高了高达25%，并解决了LLM在专业任务中的再训练问题。

Conclusion: MaxMind方法在SOTG领域具有显著潜力，能够提升LLM系统的能力和生产力。

Abstract: The application of large language models to facilitate automated software operations and tool generation (SOTG), thus augmenting software productivity, mirrors the early stages of human evolution when the ability to create and use tools accelerated the progress of civilization. These complex tasks require AI to continuously summarize and improve. Current research often overlooks the importance of converting real-time task experiences into system memory and differentiating the value of existing knowledge for future reference. This paper addresses these issues by evolving external memory models into Memory-Loop Networks for timely memorization and experience referencing. We also enhance a RAG mechanism with knowledge precision segmentation to utilize memory based on value differentiation, and design the MaxMind model for SOTG accordingly.To demonstrate our approach, we developed MaxMind4Sheet, an electronic spreadsheet processing system aligned with the MaxMind philosophy. Comparative experiments with SheetCopilot have demonstrated that the accumulation and recycling of task memories lead to a steady enhancement in task success rate, with an improvement rate of approximately 3%-6% per round in this implementation example. Note that as the memories continue to grow, this cumulative improvement may be substantial. The inclusion of memory recycling can also boost the system's task execution efficiency by up to 25%, and it can address the retraining issue faced by LLMs when handling specialized tasks through memories transfer.These suggest that MaxMind has significant potential to enhance the capabilities and productivity of LLM systems in SOTG.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [13] [Oh That Looks Familiar: A Novel Similarity Measure for Spreadsheet Template Discovery](https://arxiv.org/abs/2511.06973)
*Anand Krishnakumar,Vengadesh Ravikumaran*

Main category: cs.LG

TL;DR: 通过结合语义嵌入、数据类型和空间定位的混合距离度量来量化电子表格相似性，并在无监督聚类方面超越了基线。


<details>
  <summary>Details</summary>
Motivation: 传统的结构相似性识别方法未能捕捉定义模板的空间布局和类型模式。

Method: 将电子表格转换为单元格级别的嵌入，并使用 Chamfer 和 Hausdorff 距离等聚合技术来计算电子表格相似性。

Result: 在 FUSTE 数据集上，该方法实现了完美的模板重建（调整兰德指数为 1.00），优于基于图的 Mondrian 基线（0.90）。

Conclusion: 该方法能够实现大规模的自动化模板发现，从而支持检索增强生成、模型训练和批量数据清理等下游应用。

Abstract: Traditional methods for identifying structurally similar spreadsheets fail to capture the spatial layouts and type patterns defining templates. To quantify spreadsheet similarity, we introduce a hybrid distance metric that combines semantic embeddings, data type information, and spatial positioning. In order to calculate spreadsheet similarity, our method converts spreadsheets into cell-level embeddings and then uses aggregation techniques like Chamfer and Hausdorff distances. Experiments across template families demonstrate superior unsupervised clustering performance compared to the graph-based Mondrian baseline, achieving perfect template reconstruction (Adjusted Rand Index of 1.00 versus 0.90) on the FUSTE dataset. Our approach facilitates large-scale automated template discovery, which in turn enables downstream applications such as retrieval-augmented generation over tabular collections, model training, and bulk data cleaning.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [14] [Solving Data-centric Tasks using Large Language Models](https://arxiv.org/abs/2402.11734)
*Shraddha Barke,Christian Poelitz,Carina Suzana Negreanu,Benjamin Zorn,José Cambronero,Andrew D. Gordon,Vu Le,Elnaz Nouri,Nadia Polikarpova,Advait Sarkar,Brian Slininger,Neil Toronto,Jack Williams*

Main category: cs.PL

TL;DR: LLMs are useful for data-centric tasks, but deciding how much data to include in prompts is challenging. This paper introduces a dataset of NL-to-code tasks and a cluster-then-select prompting technique to improve LLM performance on these tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of effectively incorporating data into prompts for large language models (LLMs) to solve data-centric tasks, particularly for non-professional programmers.

Method: The paper introduces a cluster-then-select prompting technique. This involves creating a dataset of real-world natural language to code tasks involving tabular data and then selecting the most representative rows from the input data to include in the LLM prompt.

Result: LLM performance is sensitive to the amount of data included in the prompt. The cluster-then-select technique outperforms random selection for tasks with high syntactic variation in the input table.

Conclusion: The proposed cluster-then-select prompting technique, along with a curated dataset, can improve the performance of LLMs on data-centric tasks by strategically including representative data in the prompt.

Abstract: Large language models (LLMs) are rapidly replacing help forums like StackOverflow, and are especially helpful for non-professional programmers and end users. These users are often interested in data-centric tasks, such as spreadsheet manipulation and data wrangling, which are hard to solve if the intent is only communicated using a natural-language description, without including the data. But how do we decide how much data and which data to include in the prompt? This paper makes two contributions towards answering this question. First, we create a dataset of real-world NL-to-code tasks manipulating tabular data, mined from StackOverflow posts. Second, we introduce a cluster-then-select prompting technique, which adds the most representative rows from the input data to the LLM prompt. Our experiments show that LLM performance is indeed sensitive to the amount of data passed in the prompt, and that for tasks with a lot of syntactic variation in the input table, our cluster-then-select technique outperforms a random selection baseline.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.05587)
*Junjie Xing,Yeye He,Mengyu Zhou,Haoyu Dong,Shi Han,Lingjiao Chen,Dongmei Zhang,Surajit Chaudhuri,H. V. Jagadish*

Main category: cs.AI

TL;DR: LLMs在处理表格数据方面取得了显著进展，但目前缺乏全面的基准测试来评估它们在专业用户场景下的能力。本文提出了MMTU，一个包含25个真实世界表格任务、超过28000个问题的基准测试，旨在全面评估模型在专家级别理解、推理和操作表格的能力。研究表明，MMTU需要结合表格理解、推理和编码等多种技能，这对当前最先进的模型来说仍然是一个挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的表格相关任务的评估方法存在不足，局限于如NL-to-SQL和Table-QA等狭窄的领域，未能涵盖专业用户面临的更广泛的真实世界任务，这阻碍了对模型能力的全面理解和进步。

Method: 提出了MMTU基准测试，包含25个真实世界表格任务和超过28000个问题，旨在全面评估模型在专家级别理解、推理和操作表格的能力。这些任务来源于数十年关于表格数据的计算机科学研究，并侧重于专业用户面临的复杂表格任务。

Result: 在MMTU基准测试中，即使是像OpenAI GPT-5和DeepSeek R1这样的最先进模型，得分也仅分别约为69%和57%，表明在处理复杂表格任务方面仍有很大的提升空间。这揭示了当前模型在表格理解、推理和编码等综合技能方面仍存在挑战。

Conclusion: MMTU基准测试的提出，为评估和推动在结构化数据处理和分析领域的基础模型的发展提供了新的方向和动力，并指出了当前前沿模型在处理复杂表格任务方面的局限性。

Abstract: Tables and table-based use cases play a crucial role in many important real-world applications, such as spreadsheets, databases, and computational notebooks, which traditionally require expert-level users like data engineers, data analysts, and database administrators to operate. Although LLMs have shown remarkable progress in working with tables (e.g., in spreadsheet and database copilot scenarios), comprehensive benchmarking of such capabilities remains limited. In contrast to an extensive and growing list of NLP benchmarks, evaluations of table-related tasks are scarce, and narrowly focus on tasks like NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks that professional users face. This gap limits our understanding and model progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 28K questions across 25 real-world table tasks, designed to comprehensively evaluate models ability to understand, reason, and manipulate real tables at the expert-level. These tasks are drawn from decades' worth of computer science research on tabular data, with a focus on complex table tasks faced by professional users. We show that MMTU require a combination of skills -- including table understanding, reasoning, and coding -- that remain challenging for today's frontier models, where even frontier reasoning models like OpenAI GPT-5 and DeepSeek R1 score only around 69\% and 57\% respectively, suggesting significant room for improvement. We highlight key findings in our evaluation using MMTU and hope that this benchmark drives further advances in understanding and developing foundation models for structured data processing and analysis.
  Our code and data are available at https://github.com/MMTU-Benchmark/MMTU and https://huggingface.co/datasets/MMTU-benchmark/MMTU.

</details>


### [16] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: This paper argues that AI should model real-world entities and their relations, not just perceptions like pixels or words, and explains why relational learning, despite its importance for structured data, has not become mainstream and what is needed for its wider adoption.


<details>
  <summary>Details</summary>
Motivation: The current trend in AI focuses on modeling perceptions (pixels, words, phonemes) rather than the underlying entities and their relations, which are arguably a more accurate representation of the world. The valuable data in companies is often in relational formats (spreadsheets, databases) containing identifiers that are not naively interpretable as numbers. Relational learning, which studies such data, is not as prominent as it should be.

Method: The paper analyzes why relational learning has not achieved widespread adoption, except in limited cases, and outlines the necessary steps to elevate its prominence in the field of AI. (Note: The abstract does not provide specific methodological details beyond this analytical approach.)

Result: Relational learning has not become mainstream, with limited adoption in specific scenarios involving restricted relations. (Note: The abstract indicates this as a current state rather than a finding from the paper's own research.)

Conclusion: To achieve its rightful prominence, relational learning needs further development and strategic focus to overcome the barriers that have hindered its widespread adoption in AI. The field must shift its focus from modeling perceptions to modeling the entities and relations that constitute the real world and are represented in valuable structured data.

Abstract: Artificial intelligence seems to be taking over the world with systems that model pixels, words, and phonemes. The world is arguably made up, not of pixels, words, and phonemes but of entities (objects, things, including events) with properties and relations among them. Surely we should model these, not the perception or description of them. You might suspect that concentrating on modeling words and pixels is because all of the (valuable) data in the world is in terms of text and images. If you look into almost any company you will find their most valuable data is in spreadsheets, databases and other relational formats. These are not the form that are studied in introductory machine learning, but are full of product numbers, student numbers, transaction numbers and other identifiers that can't be interpreted naively as numbers. The field that studies this sort of data has various names including relational learning, statistical relational AI, and many others. This paper explains why relational learning is not taking over the world -- except in a few cases with restricted relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [17] [The Paradox of Spreadsheet Self-Efficacy: Social Incentives for Informal Knowledge Sharing in End-User Programming](https://arxiv.org/abs/2408.08068)
*Qing,Xia,Advait Sarkar,Duncan P. Brumby,Anna Cox*

Main category: cs.HC

TL;DR: 个人、社会和软件相关因素会影响电子表格知识共享意愿，应在设计中考虑这些因素。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解个人（自我效能感）、社会（声誉收益、同事间的信任）和软件相关（编纂工作量）等变量如何影响电子表格知识共享意愿。

Method: 对来自行政和财务岗位的电子表格用户（n=100）的调查数据进行多元回归分析。

Result: 研究发现，高水平的电子表格自我效能感和认为共享会带来声誉收益的看法可以预测更高的知识共享意愿，但认为知识编纂工作量大的个体表现出较低的知识共享意愿。此外，无论职业如何，用户倾向于报告普遍的电子表格熟练程度较低，尽管他们在工作相关场景下的电子表格使用方面表现出高自我效能感。

Conclusion: 研究结果表明，承认并设计这些社会和个人变量可以帮助避免有经验的个体不必要地回避共享，这对电子表格设计有启示意义。

Abstract: Informal Knowledge Sharing (KS) is vital for end-user programmers to gain expertise. To better understand how personal (self-efficacy), social (reputational gains, trust between colleagues), and software-related (codification effort) variables influence spreadsheet KS intention, we conducted a multiple regressions analysis based on survey data from spreadsheet users (n=100) in administrative and finance roles. We found that high levels of spreadsheet self-efficacy and a perception that sharing would result in reputational gains predicted higher KS intention, but individuals who found knowledge codification effortful showed lower KS intention. We also observed that regardless of occupation, users tended to report a lower sense of self-efficacy in their general spreadsheet proficiency, despite also reporting high self-efficacy in spreadsheet use for job-related contexts. Our findings suggest that acknowledging and designing for these social and personal variables can help avoid situations where experienced individuals refrain unnecessarily from sharing, with implications for spreadsheet design.

</details>


### [18] [Subject integration with spreadsheets -- Ignoring education is the greatest risk ever](https://arxiv.org/abs/2409.12975)
*Mária Csernoch,Ádám Gulácsi,Júlia Csernoch*

Main category: cs.HC

TL;DR: 本文探讨了在技术教学法和内容知识（TPACK）框架下，如何通过学科整合实现学校的有意义的数字化和数字化。


<details>
  <summary>Details</summary>
Motivation: 学科整合是实现学校有意义的数字化和数字化的可能途径，有助于所有学科的数字化教学、信息技术课程的背景化，并缩小‘严肃信息学’与‘数字素养’之间的差距。

Method: 通过电子表格解决三个传统的三年级任务，并分析了该过程所涉及的技能、能力以及教师和学生的计算机科学知识。

Result: 电子表格解决方案不仅能完成任务，还能培养教师和学生的技能、能力和计算机科学知识。

Conclusion: 分析、理解、规划和讨论任务与在电子表格中的活动同等重要，而这个过程对学生未来就业能力的培养起着至关重要的作用。

Abstract: Within the framework of Technological Pedagogical and Content Knowledge, subject integration is one possible solution for the introduction of meaningful digitalization and digitization in schools. This process incorporates that any school subject can be taught with digital support, informatics (computer) classes can be contextualized, and the gap between 'serious informatics' and 'digital literacy' can be minimized. The present paper details how three traditional Grade 3 tasks can be solved in spreadsheets, what skills, competencies, and computer science knowledge of both teachers and students can be developed. The solutions also reveal that analysing, understanding, planning, and discussing tasks is as important as the activity in the spreadsheets, which process plays a crucial role in the preparation of students for their future jobs.

</details>


### [19] [Data Guards: Challenges and Solutions for Fostering Trust in Data](https://arxiv.org/abs/2407.14042)
*Nicole Sultanum,Dennis Bromley,Michael Correll*

Main category: cs.HC

TL;DR: 本文研究了在数据驱动决策中建立信任的策略和障碍，并提出了数据守护者（data guards）的概念。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的决策面临着数据污染、故意欺骗等威胁，因此在使用数据，特别是新数据或不熟悉的数据时，需要信任或验证。然而，目前缺乏建立这种信任的标准。

Method: 通过对数据产出者和数据消费者的系列访谈，研究了建立数据信任的策略和障碍。

Result: 访谈显示，数据消费者尤其缺乏对数据进行验证和检查的标准和方法，尽管他们有这方面的迫切需求。

Conclusion: 为了解决这个问题，本文提出了一套数据守护者（data guards），包括用于增强数据制品信任度的方法和工具。

Abstract: From dirty data to intentional deception, there are many threats to the validity of data-driven decisions. Making use of data, especially new or unfamiliar data, therefore requires a degree of trust or verification. How is this trust established? In this paper, we present the results of a series of interviews with both producers and consumers of data artifacts (outputs of data ecosystems like spreadsheets, charts, and dashboards) aimed at understanding strategies and obstacles to building trust in data. We find a recurring need, but lack of existing standards, for data validation and verification, especially among data consumers. We therefore propose a set of data guards: methods and tools for fostering trust in data artifacts.

</details>


### [20] [Smart Portable Computer](https://arxiv.org/abs/2405.05292)
*Niladri Das*

Main category: cs.HC

TL;DR: 疫情期间，个人电脑价格昂贵且配置不足，催生了便携式智能计算机。


<details>
  <summary>Details</summary>
Motivation: 疫情期间，学生难以获得价格合理且配置足够的个人电脑；传统笔记本电脑对需要移动办公的人来说不够方便。

Method: 开发一款便携式智能计算机，它在紧凑、节能且经济高效的包装中提供与传统台式机相当的速度和性能。

Result: 该设备能够提供无缝的桌面体验，支持文档编辑、多标签浏览、电子表格管理、演示文稿创建以及 Python、C、C++ 等编程语言和 Keil、Xilinx 等编译器的使用。

Conclusion: 便携式智能计算机是一种面向程序员和普通用户的经济高效的解决方案，可提供无缝的桌面体验。

Abstract: Amidst the COVID-19 pandemic, with many organizations, schools, colleges, and universities transitioning to virtual platforms, students encountered difficulties in acquiring PCs such as desktops or laptops. The starting prices, around 15,000 INR, often failed to offer adequate system specifications, posing a challenge for consumers. Additionally, those reliant on laptops for work found the conventional approach cumbersome. Enter the "Portable Smart Computer," a leap into the future of computing. This innovative device boasts speed and performance comparable to traditional desktops but in a compact, energy-efficient, and cost-effective package. It delivers a seamless desktop experience, whether one is editing documents, browsing multiple tabs, managing spreadsheets, or creating presentations. Moreover, it supports programming languages like Python, C, C++, as well as compilers such as Keil and Xilinx, catering to the needs of programmers.

</details>


### [21] ["My toxic trait is thinking I'll remember this": gaps in the learner experience of video tutorials for feature-rich software](https://arxiv.org/abs/2404.07114)
*Ian Drosos,Advait Sarkar,Andrew D. Gordon*

Main category: cs.HC

TL;DR: Learners encounter 'gaps' (barriers to learning) in video tutorials, especially for complex software like Excel. This paper categorizes these gaps, analyzes viewer comments and creator interviews, and proposes design improvements.


<details>
  <summary>Details</summary>
Motivation: To understand and address the learning barriers ('gaps') that users face when following video tutorials for complex software, and to gain insights into the creator's perspective.

Method: Analyzed 360 viewer comments from 90 Microsoft Excel video tutorials across YouTube, TikTok, and Instagram. Conducted contextual interviews with 8 influential tutorial creators. Presented creators with design prototypes to address identified gaps.

Result: Developed a theory and taxonomy of gaps encountered in video tutorials. Identified how these gaps act as barriers to learning. Gained insights into creators' processes and frustrations. Received feedback on design prototypes.

Conclusion: Video tutorials for feature-rich software present unique learning challenges ('gaps'). A combination of analyzing viewer feedback and understanding creator challenges is crucial for designing more effective learning experiences. Proposed designs show promise in mitigating these gaps.

Abstract: Video tutorials are a popular medium for informal and formal learning. However, when learners attempt to view and follow along with these tutorials, they encounter what we call gaps, that is, issues that can prevent learning. We examine the gaps encountered by users of video tutorials for feature-rich software, such as spreadsheets. We develop a theory and taxonomy of such gaps, identifying how they act as barriers to learning, by collecting and analyzing 360 viewer comments from 90 Microsoft Excel video tutorials published by 43 creators across YouTube, TikTok, and Instagram. We conducted contextual interviews with 8 highly influential tutorial creators to investigate the gaps their viewers experience and how they address them. Further, we obtain insights into their creative process and frustrations when creating video tutorials. Finally, we present creators with two designs that aim to address gaps identified in the comment analysis for feedback and alternative design ideas.

</details>
