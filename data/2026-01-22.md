<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.CL](#cs.CL) [Total: 14]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.SE](#cs.SE) [Total: 35]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.HC](#cs.HC) [Total: 26]
- [cs.PL](#cs.PL) [Total: 5]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.DB](#cs.DB) [Total: 20]
- [cs.OH](#cs.OH) [Total: 1]
- [cs.LG](#cs.LG) [Total: 7]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.MS](#cs.MS) [Total: 1]
- [cs.AI](#cs.AI) [Total: 13]
- [math.HO](#math.HO) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [stat.OT](#stat.OT) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.DL](#cs.DL) [Total: 5]
- [cs.CY](#cs.CY) [Total: 9]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Image deidentification in the XNAT ecosystem: use cases and solutions](https://arxiv.org/abs/2504.20657)
*Alex Michie,Simon J Doran*

Main category: cs.CV

TL;DR: XNAT平台结合其生态系统中的独立工具，对DICOM数据进行去标识化处理，并在MIDI-B挑战赛中取得了99.61%的成绩，但仍需改进地址识别和图像内嵌信息移除。


<details>
  <summary>Details</summary>
Motivation: 对XNAT平台的数据去标识化工作流进行详细描述，并为MIDI-B挑战赛提供解决方案。

Method: 使用XNAT平台及其生态系统中的工具进行DICOM数据去标识化，并根据MIDI-B挑战赛的要求调整方法，包括规则化方法和机器学习方法。

Result: 在MIDI-B挑战赛的测试阶段，初始成绩为97.91%，后改进至99.61%，最终故障率为0.19%。规则化方法能移除姓名信息，但地址信息移除不完全。机器学习方法移除地址信息不当，影响整体性能。

Conclusion: 基于规则的方法在移除姓名信息方面有效，但地址信息移除仍需改进。未来的工作将侧重于提高地址识别能力和移除图像像素中的可识别信息。

Abstract: XNAT is a server-based data management platform widely used in academia for curating large databases of DICOM images for research projects. We describe in detail a deidentification workflow for DICOM data using facilities in XNAT, together with independent tools in the XNAT "ecosystem". We list different contexts in which deidentification might be needed, based on our prior experience. The starting point for participation in the Medical Image De-Identification Benchmark (MIDI-B) challenge was a set of pre-existing local methodologies, which were adapted during the validation phase of the challenge. Our result in the test phase was 97.91\%, considerably lower than our peers, due largely to an arcane technical incompatibility of our methodology with the challenge's Synapse platform, which prevented us receiving feedback during the validation phase. Post-submission, additional discrepancy reports from the organisers and via the MIDI-B Continuous Benchmarking facility, enabled us to improve this score significantly to 99.61\%. An entirely rule-based approach was shown to be capable of removing all name-related information in the test corpus, but exhibited failures in dealing fully with address data. Initial experiments using published machine-learning models to remove addresses were partially successful but showed the models to be "over-aggressive" on other types of free-text data, leading to a slight overall degradation in performance to 99.54\%. Future development will therefore focus on improving address-recognition capabilities, but also on better removal of identifiable data burned into the image pixels. Several technical aspects relating to the "answer key" are still under discussion with the challenge organisers, but we estimate that our percentage of genuine deidentification failures on the MIDI-B test corpus currently stands at 0.19\%. (Abridged from original for arXiv submission)

</details>


### [2] [Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language](https://arxiv.org/abs/2204.00598)
*Andy Zeng,Maria Attarian,Brian Ichter,Krzysztof Choromanski,Adrian Wong,Stefan Welker,Federico Tombari,Aveek Purohit,Michael Ryoo,Vikas Sindhwani,Johnny Lee,Vincent Vanhoucke,Pete Florence*

Main category: cs.CV

TL;DR: 通过Socratic Models（SMs）框架，可以零样本组合多个预训练模型，以获取新的多模态能力，并实现视频问答、多模态辅助对话和机器人感知等新应用。


<details>
  <summary>Details</summary>
Motivation: 不同的预训练模型（如视觉-语言模型和大型语言模型）在不同领域的数据上进行训练，存储了不同形式的常识知识，这种多样性可以被利用。

Method: 提出Socratic Models（SMs）框架，通过多模态信息提示，零样本组合多个预训练模型，使它们能够相互交换信息并捕获新的多模态能力，而无需进行微调。

Result: SMs在零样本图像字幕生成和视频到文本检索方面具有竞争力，并能实现回答关于自主视频的自由格式问题、通过与外部API和数据库交互进行多模态辅助对话以及机器人感知和规划等新应用。

Conclusion: Socratic Models（SMs）框架能够有效利用不同预训练模型的知识多样性，实现零样本学习和新多模态能力的捕获，并在多个下游任务中展现出优越的性能和广泛的应用潜力。

Abstract: Large pretrained (e.g., "foundation") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.

</details>


### [3] [TableParser: Automatic Table Parsing with Weak Supervision from Spreadsheets](https://arxiv.org/abs/2201.01654)
*Susie Xi Rao,Johannes Rausch,Peter Egger,Ce Zhang*

Main category: cs.CV

TL;DR: TableParser can parse tables in PDFs and scanned images using domain adaptation and weak supervision.


<details>
  <summary>Details</summary>
Motivation: Parsing table structures and extracting content from various sources like PDFs, images, spreadsheets, and CSVs is crucial for many applications.

Method: The paper introduces TableParser, a system for parsing tables in native PDFs and scanned images. It utilizes domain adaptation techniques and a weak supervision mechanism involving TableAnnotator and ExcelAnnotator.

Result: The system achieves high precision in table parsing, and extensive experiments demonstrate the effectiveness of domain adaptation.

Conclusion: The paper provides TableParser, TableAnnotator, and ExcelAnnotator as resources to advance research in table parsing.

Abstract: Tables have been an ever-existing structure to store data. There exist now different approaches to store tabular data physically. PDFs, images, spreadsheets, and CSVs are leading examples. Being able to parse table structures and extract content bounded by these structures is of high importance in many applications. In this paper, we devise TableParser, a system capable of parsing tables in both native PDFs and scanned images with high precision. We have conducted extensive experiments to show the efficacy of domain adaptation in developing such a tool. Moreover, we create TableAnnotator and ExcelAnnotator, which constitute a spreadsheet-based weak supervision mechanism and a pipeline to enable table parsing. We share these resources with the research community to facilitate further research in this interesting direction.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [4] [From Rows to Reasoning: A Retrieval-Augmented Multimodal Framework for Spreadsheet Understanding](https://arxiv.org/abs/2601.08741)
*Anmol Gulati,Sahil Sen,Waqar Sarguroh,Kevin Paul*

Main category: cs.CL

TL;DR: LLMs在处理大型企业电子表格时存在困难，我们提出了FRTR-Bench基准和FRTR框架，在处理多模态信息和提高可扩展性方面取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理大规模、多模态的企业电子表格时存在局限性，无法有效利用其中的数值和视觉信息。

Method: FRTR框架将电子表格分解为逐行、逐列和逐块的嵌入，采用混合词汇-密集检索（RRF）并集成多模态嵌入，以同时处理数值和视觉信息。

Result: FRTR在FRTR-Bench基准上达到了74%的准确率（Claude Sonnet 4.5），在SpreadsheetLLM基准上达到了87%的准确率（GPT-5），同时 token 使用量减少了约 50%。

Conclusion: FRTR框架有效解决了LLMs在处理大规模多模态电子表格时的挑战，并在准确性和效率上取得了显著的进步。

Abstract: Large Language Models (LLMs) struggle to reason over large-scale enterprise spreadsheets containing thousands of numeric rows, multiple linked sheets, and embedded visual content such as charts and receipts. Prior state-of-the-art spreadsheet reasoning approaches typically rely on single-sheet compression or full-context encoding, which limits scalability and fails to reflect how real users interact with complex, multimodal workbooks. We introduce FRTR-Bench, the first large-scale benchmark for multimodal spreadsheet reasoning, comprising 30 enterprise-grade Excel workbooks spanning nearly four million cells and more than 50 embedded images. To address these challenges, we present From Rows to Reasoning (FRTR), an advanced, multimodal retrieval-augmented generation framework that decomposes Excel workbooks into granular row, column, and block embeddings, employs hybrid lexical-dense retrieval with Reciprocal Rank Fusion (RRF), and integrates multimodal embeddings to reason over both numerical and visual information. We tested FRTR on six LLMs, achieving 74% answer accuracy on FRTR-Bench with Claude Sonnet 4.5, a substantial improvement over prior state-of-the-art approaches that reached only 24%. On the SpreadsheetLLM benchmark, FRTR achieved 87% accuracy with GPT-5 while reducing token usage by roughly 50% compared to context-compression methods.

</details>


### [5] [SQuARE: Structured Query & Adaptive Retrieval Engine For Tabular Formats](https://arxiv.org/abs/2512.04292)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Main category: cs.CL

TL;DR: SQuARE是一个混合检索框架，通过复杂的路由和轻量级代理，在真实的电子表格上进行准确的问答，解决了多行标题、合并单元格和单位注释等挑战，并且性能优于基线和ChatGPT-4o。


<details>
  <summary>Details</summary>
Motivation: 处理电子表格中的多行标题、合并单元格和单位注释等复杂性，以实现准确的问答，而传统的SQL方法在缺乏一致模式的文件上效果不佳。

Method: SQuARE是一个混合检索框架，它根据标题深度和合并密度计算连续分数，然后通过保留结构的块检索或在自动构建的关系表示上执行SQL来路由查询。一个轻量级代理在置信度低时监督跨两个路径的检索、细化或结果组合。

Result: SQuARE在多标题公司资产负债表、严重合并的世界银行工作簿和多样化的公共数据集上进行了评估，其在检索精度和端到端答案准确性方面持续优于单一策略基线和ChatGPT-4o，同时保持可预测的延迟。

Conclusion: SQuARE通过将检索与模型选择分离，能够兼容新兴的表格基础模型，为实现更鲁棒的表格理解提供了实用的桥梁。

Abstract: Accurate question answering over real spreadsheets remains difficult due to multirow headers, merged cells, and unit annotations that disrupt naive chunking, while rigid SQL views fail on files lacking consistent schemas. We present SQuARE, a hybrid retrieval framework with sheet-level, complexity-aware routing. It computes a continuous score based on header depth and merge density, then routes queries either through structure-preserving chunk retrieval or SQL over an automatically constructed relational representation. A lightweight agent supervises retrieval, refinement, or combination of results across both paths when confidence is low. This design maintains header hierarchies, time labels, and units, ensuring that returned values are faithful to the original cells and straightforward to verify. Evaluated on multi-header corporate balance sheets, a heavily merged World Bank workbook, and diverse public datasets, SQuARE consistently surpasses single-strategy baselines and ChatGPT-4o on both retrieval precision and end-to-end answer accuracy while keeping latency predictable. By decoupling retrieval from model choice, the system is compatible with emerging tabular foundation models and offers a practical bridge toward a more robust table understanding.

</details>


### [6] [An Empirical Study of Validating Synthetic Data for Formula Generation](https://arxiv.org/abs/2407.10657)
*Usneek Singh,José Cambronero,Sumit Gulwani,Aditya Kanade,Anirudh Khatry,Vu Le,Mukul Singh,Gust Verbruggen*

Main category: cs.CL

TL;DR: LLM 写的公式摘要，合成 NL 训练数据，验证很重要


<details>
  <summary>Details</summary>
Motivation: LLM 帮助写公式，但相关资源稀缺，影响预训练和微调。

Method: 使用另一模型为公式生成合成自然语言（NL）描述，并通过代理目标验证 NL 的准确性。

Result: 经验证的合成训练数据比原始数据在四个模型上都有更好的性能，并且能够解决更复杂的问题。

Conclusion: 对 LLM 生成的合成训练数据进行验证可以提高模型性能，并增强模型解决复杂问题的能力。

Abstract: Large language models (LLMs) can be leveraged to help with writing formulas in spreadsheets, but resources on these formulas are scarce, impacting both the base performance of pre-trained models and limiting the ability to fine-tune them. Given a corpus of formulas, we can use a(nother) model to generate synthetic natural language utterances for fine-tuning. However, it is important to validate whether the NL generated by the LLM is indeed accurate to be beneficial for fine-tuning. In this paper, we provide empirical results on the impact of validating these synthetic training examples with surrogate objectives that evaluate the accuracy of the synthetic annotations. We demonstrate that validation improves performance over raw data across four models (2 open and 2 closed weight). Interestingly, we show that although validation tends to prune more challenging examples, it increases the complexity of problems that models can solve after being fine-tuned on validated data.

</details>


### [7] [TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios](https://arxiv.org/abs/2403.19318)
*Xiaokang Zhang,Sijia Luo,Bohan Zhang,Zeyao Ma,Jing Zhang,Yang Li,Guanlin Li,Zijun Yao,Kangli Xu,Jinchang Zhou,Daniel Zhang-Li,Jifan Yu,Shu Zhao,Juanzi Li,Jie Tang*

Main category: cs.CL

TL;DR: TableLLM是一个拥有80亿参数的大型语言模型，专为处理文档和电子表格中的表格数据而设计，并提出了远程监督训练方法，包括推理过程扩展和交叉验证策略，以提高模型性能。我们在针对文档和电子表格格式的基准测试中评估了TableLLM，并发现其优于现有模型。相关代码、模型和基准测试已公开。 


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个能够熟练处理文档和电子表格中表格数据的大型语言模型，以满足现实世界的办公需求。

Method: 采用远程监督训练方法，包括推理过程扩展策略以增强模型对推理模式的理解，以及交叉验证策略以保证自动生成数据的质量。

Result: 通过针对文档和电子表格格式定制的基准测试和评估流程，TableLLM在与现有通用和专注于表格数据的LLM的比较中展现出优势。

Conclusion: TableLLM在处理表格数据方面表现出色，并且通过公开模型、代码和基准测试，促进了相关领域的研究和应用。

Abstract: We introduce TableLLM, a robust large language model (LLM) with 8 billion parameters, purpose-built for proficiently handling tabular data manipulation tasks, whether they are embedded within documents or spreadsheets, catering to real-world office scenarios. We propose a distant supervision method for training, which comprises a reasoning process extension strategy, aiding in training LLMs to understand reasoning patterns more effectively as well as a cross-way validation strategy, ensuring the quality of the automatically generated data. To evaluate the performance of TableLLM, we have crafted benchmarks tailored to address both document and spreadsheet formats as well as constructed a well-organized evaluation pipeline capable of handling both scenarios. Thorough evaluations underscore the advantages of TableLLM when compared to various existing general-purpose and tabular data-focused LLMs. We have publicly released the model checkpoint, source code, benchmarks, and a web application for user interaction. Our codes and data are publicly available at https://github.com/TableLLM/TableLLM.

</details>


### [8] [GiesKaNe: Bridging Past and Present in Grammatical Theory and Practical Application](https://arxiv.org/abs/2502.05113)
*Volker Emmrich*

Main category: cs.CL

TL;DR: GiesKaNe项目旨在创建一个包含历史和当代语言数据的、深度句法注释的参考语料库。该项目结合了人工和机器辅助的方法，解决了分词、标注、句法分析等基础问题，并提出了新的文本分类方法和从现有数据派生标注的标准。项目证明了利用现有基础设施和简单工具即可实现复杂语料库的构建。


<details>
  <summary>Details</summary>
Motivation: GiesKaNe项目旨在建立一个连接历史和当代语言的参考语料库，满足研究社区的需求，并应对语料库构建中的方法论挑战。

Method: 项目结合了人工专业知识和机器辅助流程，处理了分词、归一化、句子定义、词性标注、句法分析和标注者间一致性等问题。此外，还提出了一种对文本进行概念口语性和书面性连续体分类的新方法，并介绍了一种从现有标注派生出实际标注标准的方法。

Result: 项目展示了如何通过现有研究基础设施（如简单的电子表格）和战略性地使用现有工具来管理复杂语料库的编译工作流，而无需专门的标注工具。

Conclusion: 即使是像GiesKaNe这样宏大的项目，也可以通过有效利用现有资源和结合创新方法（如文本分类和标准派生）来成功实施。

Abstract: This article explores the requirements for corpus compilation within the GiesKaNe project (University of Giessen and Kassel, Syntactic Basic Structures of New High German). The project is defined by three central characteristics: it is a reference corpus, a historical corpus, and a syntactically deeply annotated treebank. As a historical corpus, GiesKaNe aims to establish connections with both historical and contemporary corpora, ensuring its relevance across temporal and linguistic contexts. The compilation process strikes the balance between innovation and adherence to standards, addressing both internal project goals and the broader interests of the research community. The methodological complexity of such a project is managed through a complementary interplay of human expertise and machine-assisted processes. The article discusses foundational topics such as tokenization, normalization, sentence definition, tagging, parsing, and inter-annotator agreement, alongside advanced considerations. These include comparisons between grammatical models, annotation schemas, and established de facto annotation standards as well as the integration of human and machine collaboration. Notably, a novel method for machine-assisted classification of texts along the continuum of conceptual orality and literacy is proposed, offering new perspectives on text selection. Furthermore, the article introduces an approach to deriving de facto standard annotations from existing ones, mediating between standardization and innovation. In the course of describing the workflow the article demonstrates that even ambitious projects like GiesKaNe can be effectively implemented using existing research infrastructure, requiring no specialized annotation tools. Instead, it is shown that the workflow can be based on the strategic use of a simple spreadsheet and integrates the capabilities of the existing infrastructure.

</details>


### [9] [MiMoTable: A Multi-scale Spreadsheet Benchmark with Meta Operations for Table Reasoning](https://arxiv.org/abs/2412.11711)
*Zheng Li,Yang Du,Mao Zheng,Mingyang Song*

Main category: cs.CL

TL;DR: MiMoTable是一个包含真实世界电子表格的多尺度基准，包含六种元操作，用于评估大型语言模型（LLM）的表格推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的表格推理基准与真实世界应用中的复杂表格和问题之间存在差距。

Method: 提出MiMoTable基准，其特点是包含真实世界的电子表格和衡量问题难度的六种元操作。

Result: 在MiMoTable基准上，Claude-3.5-Sonnet的准确率为77.4%，表明LLM仍有改进空间。使用新的元操作标准对现有基准进行评估，证明了该标准的有效性。

Conclusion: MiMoTable基准能够有效评估LLM在真实世界表格推理任务上的能力，并且提出的元操作标准能够量化表格推理任务的难度。

Abstract: Extensive research has been conducted to explore the capability of Large Language Models (LLMs) for table reasoning and has significantly improved the performance on existing benchmarks. However, tables and user questions in real-world applications are more complex and diverse, presenting an unignorable gap compared to the existing benchmarks. To fill the gap, we propose a \textbf{M}ult\textbf{i}-scale spreadsheet benchmark with \textbf{M}eta \textbf{o}perations for \textbf{Table} reasoning, named as MiMoTable. Specifically, MiMoTable incorporates two key features. First, the tables in MiMoTable are all spreadsheets used in real-world scenarios, which cover seven domains and contain different types. Second, we define a new criterion with six categories of meta operations for measuring the difficulty of each question in MiMoTable, simultaneously as a new perspective for measuring the difficulty of the existing benchmarks. Experimental results show that Claude-3.5-Sonnet achieves the best performance with 77.4\% accuracy, indicating that there is still significant room to improve for LLMs on MiMoTable. Furthermore, we grade the difficulty of existing benchmarks according to our new criteria. Experiments have shown that the performance of LLMs decreases as the difficulty of benchmarks increases, thereby proving the effectiveness of our proposed new criterion.

</details>


### [10] [SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation](https://arxiv.org/abs/2406.14991)
*Zeyao Ma,Bohan Zhang,Jing Zhang,Jifan Yu,Xiaokang Zhang,Xiaohan Zhang,Sijia Luo,Xi Wang,Jie Tang*

Main category: cs.CL

TL;DR: Introduces SpreadsheetBench, a benchmark for evaluating LLMs on real-world spreadsheet tasks using data from online forums and a new evaluation metric.


<details>
  <summary>Details</summary>
Motivation: To create a challenging benchmark that reflects real-world spreadsheet user needs and evaluates the robustness of LLMs in handling complex, real-world spreadsheet data.

Method: Developed SpreadsheetBench using 912 real questions from online Excel forums and associated spreadsheets; proposed a new evaluation metric similar to online judge platforms with multiple test cases per instruction; evaluated various LLMs.

Result: Current SOTA LLMs show a substantial performance gap compared to human performance on SpreadsheetBench, indicating the benchmark's difficulty.

Conclusion: SpreadsheetBench is a challenging benchmark that highlights the limitations of current LLMs in spreadsheet manipulation tasks and the need for more robust solutions.

Abstract: We introduce SpreadsheetBench, a challenging spreadsheet manipulation benchmark exclusively derived from real-world scenarios, designed to immerse current large language models (LLMs) in the actual workflow of spreadsheet users. Unlike existing benchmarks that rely on synthesized queries and simplified spreadsheet files, SpreadsheetBench is built from 912 real questions gathered from online Excel forums, which reflect the intricate needs of users. The associated spreadsheets from the forums contain a variety of tabular data such as multiple tables, non-standard relational tables, and abundant non-textual elements. Furthermore, we propose a more reliable evaluation metric akin to online judge platforms, where multiple spreadsheet files are created as test cases for each instruction, ensuring the evaluation of robust solutions capable of handling spreadsheets with varying values. Our comprehensive evaluation of various LLMs under both single-round and multi-round inference settings reveals a substantial gap between the state-of-the-art (SOTA) models and human performance, highlighting the benchmark's difficulty.

</details>


### [11] [NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries](https://arxiv.org/abs/2402.14853)
*Wei Zhao,Zhitao Hou,Siyuan Wu,Yan Gao,Haoyu Dong,Yao Wan,Hongyu Zhang,Yulei Sui,Haidong Zhang*

Main category: cs.CL

TL;DR: 该论文提出了NL2Formula任务，旨在将自然语言查询转换为可执行的电子表格公式，并为此构建了一个包含70,799个样本的数据集，同时提出了名为fCoder的序列到序列模型作为基线实现，实验证明fCoder优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 电子表格公式的编写对于许多最终用户来说仍然是一项繁琐且容易出错的任务，尤其是在处理复杂操作时。本研究旨在缓解这一问题。

Method: 提出NL2Formula任务，构建包含70,799个配对的自然语言查询和电子表格公式的数据集，并实现了一个名为fCoder的序列到序列基线模型。

Result: fCoder模型在NL2Formula任务上表现出色，优于其他基线模型，并且与GPT-3.5（text-davinci-003）进行了比较。

Conclusion: fCoder在NL2Formula任务上取得了优越的性能，但仍存在挑战，需要进一步研究。

Abstract: Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets, is a widespread practice among users performing data analysis. However, crafting formulas on spreadsheets remains a tedious and error-prone task for many end-users, particularly when dealing with complex operations. To alleviate the burden associated with writing spreadsheet formulas, this paper introduces a novel benchmark task called NL2Formula, with the aim to generate executable formulas that are grounded on a spreadsheet table, given a Natural Language (NL) query as input. To accomplish this, we construct a comprehensive dataset consisting of 70,799 paired NL queries and corresponding spreadsheet formulas, covering 21,670 tables and 37 types of formula functions. We realize the NL2Formula task by providing a sequence-to-sequence baseline implementation called fCoder. Experimental results validate the effectiveness of fCoder, demonstrating its superior performance compared to the baseline models. Furthermore, we also compare fCoder with an initial GPT-3.5 model (i.e., text-davinci-003). Lastly, through in-depth error analysis, we identify potential challenges in the NL2Formula task and advocate for further investigation.

</details>


### [12] [InstructExcel: A Benchmark for Natural Language Instruction in Excel](https://arxiv.org/abs/2310.14495)
*Justin Payan,Swaroop Mishra,Mukul Singh,Carina Negreanu,Christian Poelitz,Chitta Baral,Subhro Roy,Rasika Chakravarthy,Benjamin Van Durme,Elnaz Nouri*

Main category: cs.CL

TL;DR: LLMs can generate Excel OfficeScripts from natural language instructions, but it's challenging. A new benchmark, InstructExcel, with 10k+ samples, was created. GPT-4, more examples, and dynamic prompting improve performance.


<details>
  <summary>Details</summary>
Motivation: Investigate if LLMs can generate Excel OfficeScripts from natural language instructions to solve Excel-specific tasks.

Method: Introduced InstructExcel benchmark (10k+ samples, 170+ operations, 2000 spreadsheets) using Excel's 'Automate' feature. Conducted experiments in zero-shot and few-shot settings with GPT-3.5 and GPT-4.

Result: InstructExcel is a challenging benchmark for state-of-the-art LLMs. GPT-4 outperforms GPT-3.5. Performance improves with more in-context examples and dynamic prompting.

Conclusion: LLMs show potential in generating Excel scripts, but current models struggle with this complex task. Further improvements can be achieved through model choice, example provision, and prompting strategies.

Abstract: With the evolution of Large Language Models (LLMs) we can solve increasingly more complex NLP tasks across various domains, including spreadsheets. This work investigates whether LLMs can generate code (Excel OfficeScripts, a TypeScript API for executing many tasks in Excel) that solves Excel specific tasks provided via natural language user instructions. To do so we introduce a new large-scale benchmark, InstructExcel, created by leveraging the 'Automate' feature in Excel to automatically generate OfficeScripts from users' actions. Our benchmark includes over 10k samples covering 170+ Excel operations across 2,000 publicly available Excel spreadsheets. Experiments across various zero-shot and few-shot settings show that InstructExcel is a hard benchmark for state of the art models like GPT-4. We observe that (1) using GPT-4 over GPT-3.5, (2) providing more in-context examples, and (3) dynamic prompting can help improve performance on this benchmark.

</details>


### [13] [Active Learning with Tabular Language Models](https://arxiv.org/abs/2211.04128)
*Martin Ringsquandl,Aneta Koleva*

Main category: cs.CL

TL;DR: Despite advancements in tabular language models, real-world applications face challenges due to the high cost of expert labeling for technical tables. This paper explores active learning strategies for tabular language models in sub-cell named entity recognition, showing that cell-level acquisition with diversity improves efficiency, while table-level diversity is harmful. It also highlights open questions on computational efficiency and human annotator experience.


<details>
  <summary>Details</summary>
Motivation: Real-world applications of tabular language models are hindered by the high cost and expertise required for labeling technical tables. Active learning is proposed as a solution to reduce labeling costs.

Method: Investigated different acquisition functions for active learning in a real-world industrial use case of tabular language models for sub-cell named entity recognition.

Result: Cell-level acquisition functions with built-in diversity significantly reduce labeling effort. Enforced table diversity was found to be detrimental. Open questions regarding computational efficiency and the perspective of human annotators were identified.

Conclusion: Active learning, specifically with cell-level acquisition functions that incorporate diversity, can effectively reduce labeling costs for tabular language models in industrial settings, but careful consideration of table diversity, computational efficiency, and annotator experience is necessary.

Abstract: Despite recent advancements in tabular language model research, real-world applications are still challenging. In industry, there is an abundance of tables found in spreadsheets, but acquisition of substantial amounts of labels is expensive, since only experts can annotate the often highly technical and domain-specific tables. Active learning could potentially reduce labeling costs, however, so far there are no works related to active learning in conjunction with tabular language models. In this paper we investigate different acquisition functions in a real-world industrial tabular language model use case for sub-cell named entity recognition. Our results show that cell-level acquisition functions with built-in diversity can significantly reduce the labeling effort, while enforced table diversity is detrimental. We further see open fundamental questions concerning computational efficiency and the perspective of human annotators.

</details>


### [14] [Table-To-Text generation and pre-training with TabT5](https://arxiv.org/abs/2210.09162)
*Ewa Andrejczuk,Julian Martin Eisenschlos,Francesco Piccinno,Syrine Krichene,Yasemin Altun*

Main category: cs.CL

TL;DR: TABT5是一个新的编码器-解码器模型，它生成的自然语言文本基于表格和文本输入，在多个领域取得了新的最先进成果。


<details>
  <summary>Details</summary>
Motivation: 现有的Encoder-only transformer模型在表格理解任务中受限于类似单元格选择或蕴含检测的分类任务，本文提出TABT5来克服这一限制。

Method: TABT5是一个编码器-解码器模型，通过引入解码器组件，并利用特定于表格的嵌入和预训练来处理表格输入结构。

Result: TABT5在电子表格公式预测（序列准确率提高15%）、问答（序列准确率提高2.5%）和数据到文本生成（BLEU提高2.5%）等多个领域取得了新的最先进成果。

Conclusion: TABT5通过结合解码器和表格特定的输入表示，在表格理解任务中实现了最先进的性能，并扩展了应用范围。

Abstract: Encoder-only transformer models have been successfully applied to different table understanding tasks, as in TAPAS (Herzig et al., 2020). A major limitation of these architectures is that they are constrained to classification-like tasks such as cell selection or entailment detection. We present TABT5, an encoder-decoder model that generates natural language text based on tables and textual inputs. TABT5 overcomes the encoder-only limitation by incorporating a decoder component and leverages the input structure with table specific embeddings and pre-training. TABT5 achieves new state-of-the-art results on several domains, including spreadsheet formula prediction with a 15% increase in sequence accuracy, QA with a 2.5% increase in sequence accuracy and data-to-text generation with a 2.5% increase in BLEU.

</details>


### [15] [Table Pre-training: A Survey on Model Architectures, Pre-training Objectives, and Downstream Tasks](https://arxiv.org/abs/2201.09745)
*Haoyu Dong,Zhoujun Cheng,Xinyi He,Mengyu Zhou,Anda Zhou,Fan Zhou,Ao Liu,Shi Han,Dongmei Zhang*

Main category: cs.CL

TL;DR: 表格预训练已成为一个热门领域，旨在利用大量可用的表格数据来提升各种下游任务的表现。研究人员设计了多种预训练目标和模型结构，并通常结合文本进行联合预训练。本综述全面回顾了现有方法，并展望了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于网络、电子表格、PDF 等文档中存在大量表格，表格预训练已成为一个重要的研究领域，旨在利用这些未标记的表格数据来提升各种下游任务的表现。

Method: 本研究对表格预训练的不同模型设计、预训练目标和下游任务进行了全面的回顾，并探讨了相关的挑战和机遇。

Result: 表格预训练模型在表格问答、表格类型识别、列关系分类、表格搜索、公式预测等任务上取得了新的进展。

Conclusion: 表格预训练是一个充满潜力的领域，未来的研究可以关注如何更好地利用表格的结构化特性以及与自由文本的交互。

Abstract: Since a vast number of tables can be easily collected from web pages, spreadsheets, PDFs, and various other document types, a flurry of table pre-training frameworks have been proposed following the success of text and images, and they have achieved new state-of-the-arts on various tasks such as table question answering, table type recognition, column relation classification, table search, formula prediction, etc. To fully use the supervision signals in unlabeled tables, a variety of pre-training objectives have been designed and evaluated, for example, denoising cell values, predicting numerical relationships, and implicitly executing SQLs. And to best leverage the characteristics of (semi-)structured tables, various tabular language models, particularly with specially-designed attention mechanisms, have been explored. Since tables usually appear and interact with free-form text, table pre-training usually takes the form of table-text joint pre-training, which attracts significant research interests from multiple domains. This survey aims to provide a comprehensive review of different model designs, pre-training objectives, and downstream tasks for table pre-training, and we further share our thoughts and vision on existing challenges and future opportunities.

</details>


### [16] [TableQuery: Querying tabular data with natural language](https://arxiv.org/abs/2202.00454)
*Abhijith Neil Abraham,Fariz Rahman,Damanpreet Kaur*

Main category: cs.CL

TL;DR: TableQuery是一个利用预训练的深度学习模型来查询表格数据的工具，解决了现有方法需要将整个表格输入模型而导致内存限制和实时更新困难的问题。


<details>
  <summary>Details</summary>
Motivation: 现有表格数据问答方法的局限性，包括需要将整个表格输入模型导致内存限制，以及处理实时更新的数据库的困难。

Method: 使用预训练的深度学习模型将自然语言查询转换为结构化查询，可以直接在数据库或电子表格上运行，无需将整个数据加载到内存或序列化数据库。

Result: TableQuery可以处理大规模表格数据，并且能够利用更新的模型进行性能提升，无需重新训练。

Conclusion: TableQuery提供了一种高效且可扩展的表格数据查询解决方案，克服了现有方法的瓶颈。

Abstract: This paper presents TableQuery, a novel tool for querying tabular data using deep learning models pre-trained to answer questions on free text. Existing deep learning methods for question answering on tabular data have various limitations, such as having to feed the entire table as input into a neural network model, making them unsuitable for most real-world applications. Since real-world data might contain millions of rows, it may not entirely fit into the memory. Moreover, data could be stored in live databases, which are updated in real-time, and it is impractical to serialize an entire database to a neural network-friendly format each time it is updated. In TableQuery, we use deep learning models pre-trained for question answering on free text to convert natural language queries to structured queries, which can be run against a database or a spreadsheet. This method eliminates the need for fitting the entire data into memory as well as serializing databases. Furthermore, deep learning models pre-trained for question answering on free text are readily available on platforms such as HuggingFace Model Hub (7). TableQuery does not require re-training; when a newly trained model for question answering with better performance is available, it can replace the existing model in TableQuery.

</details>


### [17] [The Impact of Text Presentation on Translator Performance](https://arxiv.org/abs/2011.05978)
*Samuel Läubli,Patrick Simianer,Joern Wuebker,Geza Kovacs,Rico Sennrich,Spence Green*

Main category: cs.CL

TL;DR: CAT工具的现有设计（如句子分割和并排视图）对翻译人员的速度和准确性有显著影响，具体取决于任务类型。


<details>
  <summary>Details</summary>
Motivation: 评估计算机辅助翻译（CAT）工具的设计选择（如句子分割和并排视图）对翻译人员性能的影响。

Method: 进行了三项实验文本处理任务，测量了速度和准确性，并将句子分割的文本与未分割的文本进行了比较，并将源语言和目标语言句子的上下排列与并排排列进行了比较。

Result: 句子分割提高了文本复制的速度和句子内错误的识别能力。源语言和目标语言句子的上下排列比并排排列能更快地复制文本。然而，在修订任务中，未分割的文本在准确性和时间效率方面表现最佳。

Conclusion: CAT工具的最佳设计应根据具体任务进行调整，句子分割和上下排列视图有利于文本复制，而未分割文本则有利于修订任务。

Abstract: Widely used computer-aided translation (CAT) tools divide documents into segments such as sentences and arrange them in a side-by-side, spreadsheet-like view. We present the first controlled evaluation of these design choices on translator performance, measuring speed and accuracy in three experimental text processing tasks. We find significant evidence that sentence-by-sentence presentation enables faster text reproduction and within-sentence error identification compared to unsegmented text, and that a top-and-bottom arrangement of source and target sentences enables faster text reproduction compared to a side-by-side arrangement. For revision, on the other hand, our results suggest that presenting unsegmented text results in the highest accuracy and time efficiency. Our findings have direct implications for best practices in designing CAT tools.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [18] [Hillview: A trillion-cell spreadsheet for big data](https://arxiv.org/abs/1907.04827)
*Mihai Budiu,Parikshit Gopalan,Lalith Suresh,Udi Wieder,Han Kruiger,Marcos K. Aguilera*

Main category: cs.DC

TL;DR: Hillview是一个分布式电子表格系统，用于处理无法由单台机器处理的大型数据集。它通过vizket sketch技术，实现了对海量数据的快速交互式探索和可视化，并且在可扩展性、通信效率、渐进式可视化和准确性方面表现出色，能够处理比现有系统更大的数据集。


<details>
  <summary>Details</summary>
Motivation: 提供一个可以交互式浏览海量数据集的分布式电子表格系统，解决单机无法处理的数据问题，并实现快速的数据探索和可视化。

Method: 引入vizket sketch（可视化草图）技术，结合数据摘要算法和高效渲染的计算机图形学原理，以创建数据的紧凑可视化。通过并行计算、减少通信、提供渐进式可视化和精确的准确性保证来扩展电子表格的功能。

Result: Hillview系统能够在8台服务器上运行，导航和可视化包含数十亿行和数万亿单元格的数据集，超越了现有竞争系统的公开能力。

Conclusion: vizket sketch是一种简单但有效的技术，通过并行计算、减少通信、提供渐进式可视化和精确的准确性保证，极大地扩展了电子表格处理海量数据的能力。

Abstract: Hillview is a distributed spreadsheet for browsing very large datasets that cannot be handled by a single machine. As a spreadsheet, Hillview provides a high degree of interactivity that permits data analysts to explore information quickly along many dimensions while switching visualizations on a whim. To provide the required responsiveness, Hillview introduces visualization sketches, or vizketches, as a simple idea to produce compact data visualizations. Vizketches combine algorithmic techniques for data summarization with computer graphics principles for efficient rendering. While simple, vizketches are effective at scaling the spreadsheet by parallelizing computation, reducing communication, providing progressive visualizations, and offering precise accuracy guarantees. Using Hillview running on eight servers, we can navigate and visualize datasets of tens of billions of rows and trillions of cells, much beyond the published capabilities of competing systems.

</details>


### [19] [Streaming 1.9 Billion Hypersparse Network Updates per Second with D4M](https://arxiv.org/abs/1907.04217)
*Jeremy Kepner,Vijay Gadepally,Lauren Milechin,Siddharth Samsi,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Michael Houle,Michael Jones,Anne Klein,Peter Michaleas,Julie Mullen,Andrew Prout,Antonio Rosa,Charles Yee,Albert Reuther*

Main category: cs.DC

TL;DR: D4M库实现了用于分析网络数据的多语言关联数组和超稀疏数组数据库。本文描述了一种分层关联数组的实现，该实现通过控制更新级联的条目数来减少内存压力并提高更新速率，在单个实例中达到每秒40,000多次更新，在34,000个实例中达到每秒1,900,000,000次更新，从而能够分析大规模流式网络数据。


<details>
  <summary>Details</summary>
Motivation: 分析许多类型的网络数据，并处理D4M关联数组在流式更新中遇到的内存层次结构压力。

Method: 设计和优化了分层关联数组的实现，通过控制每个层级的条目数来控制更新的级联。

Result: 分层关联数组在单个实例中实现了超过每秒40,000次更新，在MIT SuperCloud的34,000个实例中实现了每秒1,900,000,000次的持续更新速率。

Conclusion: 分层关联数组的实现有效解决了D4M的内存压力问题，显著提高了更新速率，使得分析超大规模流式网络数据成为可能。

Abstract: The Dynamic Distributed Dimensional Data Model (D4M) library implements associative arrays in a variety of languages (Python, Julia, and Matlab/Octave) and provides a lightweight in-memory database implementation of hypersparse arrays that are ideal for analyzing many types of network data. D4M relies on associative arrays which combine properties of spreadsheets, databases, matrices, graphs, and networks, while providing rigorous mathematical guarantees, such as linearity. Streaming updates of D4M associative arrays put enormous pressure on the memory hierarchy. This work describes the design and performance optimization of an implementation of hierarchical associative arrays that reduces memory pressure and dramatically increases the update rate into an associative array. The parameters of hierarchical associative arrays rely on controlling the number of entries in each level in the hierarchy before an update is cascaded. The parameters are easily tunable to achieve optimal performance for a variety of applications. Hierarchical arrays achieve over 40,000 updates per second in a single instance. Scaling to 34,000 instances of hierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of 1,900,000,000 updates per second. This capability allows the MIT SuperCloud to analyze extremely large streaming network data sets.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [20] [Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE](https://arxiv.org/abs/2506.17330)
*Simon Thorne*

Main category: cs.SE

TL;DR: LLMs在电子表格任务中的表现有待提高，尤其是在复杂的多步操作方面，需要结合符号推理能力。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在电子表格相关任务中的有效性。

Method: 提出一个全面的基准测试框架，包含公式生成、数据操作和复杂场景。

Result: LLMs在简单任务中表现良好，但在复杂任务中常出错，输出看似合理但实际错误。

Conclusion: 当前的LLMs在需要精确逻辑推理的电子表格任务中存在局限性，需要集成符号推理能力。为此，提出了FLARE基准测试。

Abstract: Large Language Models (LLMs) have demonstrated some significant capabilities across various domains; however, their effectiveness in spreadsheet related tasks remains underexplored. This study introduces a foundation for a comprehensive benchmark framework to evaluate the performance of leading LLMs in executing spreadsheet functions, formula generation and data manipulation tasks. The benchmark encompasses tasks ranging from basic formula creation to complex, real world spreadsheet scenarios. Our findings reveal that while LLMs exhibit proficiency in straightforward tasks, they often falter in complex, multi step operations, frequently producing plausible yet incorrect outputs. These results underscore the limitations of current LLMs in handling spreadsheet tasks that require precise logical reasoning and highlight the need for integrating symbolic reasoning capabilities into LLM architectures. To support this, we introduce FLARE (Formula Logic, Auditing, Reasoning and Evaluation) a new benchmark for evaluating LLM performance on real-world spreadsheet logic, auditing, and reasoning tasks.

</details>


### [21] [Overcoming Vision Language Model Challenges in Diagram Understanding: A Proof-of-Concept with XML-Driven Large Language Models Solutions](https://arxiv.org/abs/2502.04389)
*Shue Shiinoki,Ryo Koshihara,Hayato Motegi,Masumi Morishige*

Main category: cs.SE

TL;DR: 通过解析包含图表元数据的可编辑源文件（如 .xlsx, .pptx, .docx），而非依赖视觉识别，利用LLM来理解和回答关于图表结构的问题，从而克服了现有VLM在图表理解方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLMs）在准确识别和提取图表结构与关系方面仍面临挑战，本研究旨在通过一种不依赖VLM视觉识别能力的方法来解决这些问题。

Method: 提出一种文本驱动的方法，直接从可编辑的源文件（如 .xlsx, .pptx, .docx）中提取图表元素的文本元数据，并将这些数据作为文本输入提供给大型语言模型（LLMs）进行分析和问答。

Result: 实验表明，该文本驱动方法在回答需要详细理解图表结构的问题时，比基于VLM的方法更准确，并且该方法可以扩展到其他包含源文件的文档格式（如 .pptx 和 .docx）。

Conclusion: 通过直接从源文件进行文本提取，可以有效规避VLM的局限性，利用LLM实现对图表的鲁棒理解，为提高现实世界业务场景中的工作流程效率和信息分析能力提供了有前景的途径。

Abstract: Diagrams play a crucial role in visually conveying complex relationships and processes within business documentation. Despite recent advances in Vision-Language Models (VLMs) for various image understanding tasks, accurately identifying and extracting the structures and relationships depicted in diagrams continues to pose significant challenges. This study addresses these challenges by proposing a text-driven approach that bypasses reliance on VLMs' visual recognition capabilities. Instead, it utilizes the editable source files--such as xlsx, pptx or docx--where diagram elements (e.g., shapes, lines, annotations) are preserved as textual metadata. In our proof-of-concept, we extracted diagram information from xlsx-based system design documents and transformed the extracted shape data into textual input for Large Language Models (LLMs). This approach allowed the LLM to analyze relationships and generate responses to business-oriented questions without the bottleneck of image-based processing. Experimental comparisons with a VLM-based method demonstrated that the proposed text-driven framework yielded more accurate answers for questions requiring detailed comprehension of diagram structures.The results obtained in this study are not limited to the tested .xlsx files but can also be extended to diagrams in other documents with source files, such as Office pptx and docx formats. These findings highlight the feasibility of circumventing VLM constraints through direct textual extraction from original source files. By enabling robust diagram understanding through LLMs, our method offers a promising path toward enhanced workflow efficiency and information analysis in real-world business scenarios.

</details>


### [22] [On the Workflows and Smells of Leaderboard Operations (LBOps): An Exploratory Study of Foundation Model Leaderboards](https://arxiv.org/abs/2407.04065)
*Zhimin Zhao,Abdul Ali Bangash,Filipe Roseiro Côgo,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: FM在软件工程（SE）领域展现出强大潜力，但FM排行榜的评估标准不统一，影响了FM选择的透明度。本研究旨在分析FM排行榜的实际运作（


<details>
  <summary>Details</summary>
Motivation: 评估标准不统一，影响了FM选择的透明度，需要理解FM排行榜的运作并识别问题。

Method: 收集了1,045个FM排行榜，通过卡片分类和协商一致，识别出五种工作流模式，并提出了包含关键组件及其交互的领域模型。同时，识别出八种排行榜“气味”（问题）。

Result: 识别出五种工作流模式和八种排行榜“气味”，为改进排行榜实践提供了依据。

Conclusion: 通过解决这些“气味”，可以提高FM排行榜的透明度、问责制和协作性，促进更健壮和负责任的FM比较和选择生态系统。

Abstract: Foundation models (FM), such as large language models (LLMs), which are large-scale machine learning (ML) models, have demonstrated remarkable adaptability in various downstream software engineering (SE) tasks, such as code completion, code understanding, and software development. As a result, FM leaderboards have become essential tools for SE teams to compare and select the best third-party FMs for their specific products and purposes. However, the lack of standardized guidelines for FM evaluation and comparison threatens the transparency of FM leaderboards and limits stakeholders' ability to perform effective FM selection. As a first step towards addressing this challenge, our research focuses on understanding how these FM leaderboards operate in real-world scenarios ("leaderboard operations") and identifying potential pitfalls and areas for improvement ("leaderboard smells"). In this regard, we collect up to 1,045 FM leaderboards from five different sources: GitHub, Hugging Face Spaces, Papers With Code, spreadsheet and independent platform, to examine their documentation and engage in direct communication with leaderboard operators to understand their workflows. Through card sorting and negotiated agreement, we identify five distinct workflow patterns and develop a domain model that captures the key components and their interactions within these workflows. We then identify eight unique types of leaderboard smells in LBOps. By mitigating these smells, SE teams can improve transparency, accountability, and collaboration in current LBOps practices, fostering a more robust and responsible ecosystem for FM comparison and selection.

</details>


### [23] [Will Dynamic Arrays finally change the way Models are built?](https://arxiv.org/abs/2006.14706)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: 电子表格虽然直观易用，但极易出错，可能不适合严肃的分析任务。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨动态数组能否在更专业的开发环境中取代传统技术，以提高解决方案的完整性。

Method: 通过研究Excel中新引入的动态数组功能，并将其与传统的CSE数组公式进行比较。

Result: 动态数组模型需要较少的手动更新，有潜力减少错误和风险。

Conclusion: 作者认为，动态数组的采用有可能在需要高解决方案完整性的专业开发环境中取代传统技术。

Abstract: Spreadsheets offer a supremely successful and intuitive means of processing and exchanging numerical content. Its intuitive ad-hoc nature makes it hugely popular for use in diverse areas including business and engineering, yet these very same characteristics make it extraordinarily error-prone; many would question whether it is suitable for serious analysis or modelling tasks. A previous EuSpRIG paper examined the role of Names in increasing solution transparency and providing a readable notation to forge links with the problem domain. Extensive use was made of CSE array formulas, but it is acknowledged that their use makes spreadsheet development a distinctly cumbersome task. Since that time, the new dynamic arrays have been introduced and array calculation is now the default mode of operation for Excel. This paper examines the thesis that their adoption within a more professional development environment could replace traditional techniques where solution integrity is important. A major advantage of fully dynamic models is that they require less manual intervention to keep them updated and so have the potential to reduce the attendant errors and risk.

</details>


### [24] [A Structured Approach to the development of Solutions in Excel](https://arxiv.org/abs/1704.01142)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: Excel 缺乏超越单元格公式的结构，本文提出使用非常规技术来构建可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: Excel 极大地普及了数据处理，但其默认设置缺乏传统编程的结构，难以处理复杂问题。

Method: 提出使用非传统或较少使用的技术，将问题分解为一系列类似编程语言步骤的公式来构建解决方案。

Result: 通过一系列公式模拟编程语言的步骤，创建了一个可行的解决方案。

Conclusion: 使用非常规技术可以为 Excel 解决方案带来可扩展的结构。

Abstract: Spreadsheets offer a supremely successful democratisation platform, placing the manipulation and presentation of numbers within the grasp of users that have little or no mathematical expertise or IT experience. What appears to be almost completely lacking within a "normal" solution built using Excel default settings is the deployment of any structure that extends beyond a single-cell formula. The structural elements that allow conventional code to scale without escalating errors appear to be absent. This paper considers the use of controversial or lesser-used techniques to create a coherent solution strategy in which the problem is solved by a sequence of formulas resembling the steps of a programmed language.

</details>


### [25] [Spreadsheet-based Configuration of Families of Real-Time Specifications](https://arxiv.org/abs/2310.20395)
*José Proença,David Pereira,Giann Spilere Nandi,Sina Borrami,Jonas Melchert*

Main category: cs.SE

TL;DR: 本研究提出了一种利用形式化模型和需求中的可变性来简化实时系统模型检查的方法，并将其应用于铁路领域的具体案例。


<details>
  <summary>Details</summary>
Motivation: 模型检查实时系统面临在模型细节和状态爆炸之间进行权衡的复杂性。本研究旨在利用形式化模型和待检查需求中的可变性，来简化实时规范的变体模型检查。

Method: 使用具有特定结构 的MS Excel 电子表格来配置形式化规范的可变性。通过原型工具自动处理这些电子表格，生成实例并运行模型检查器。通过分析有效的功能组合来扩展先前的工作，同时保持基于电子表格的简单接口。

Result: 开发了一个原型工具，可以自动处理Excel电子表格，生成模型检查的实例并运行模型检查器，简化了具有可变性的实时系统模型检查。

Conclusion: 通过利用形式化模型和需求中的可变性，可以有效地简化实时系统模型检查的复杂性，并且该方法易于开发人员使用，并已成功应用于实际的铁路领域案例。

Abstract: Model checking real-time systems is complex, and requires a careful trade-off between including enough detail to be useful and not too much detail to avoid state explosion. This work exploits variability of the formal model being analysed and the requirements being checked, to facilitate the model-checking of variations of real-time specifications.  This work results from the collaboration between academics and Alstom, a railway company with a concrete use-case, in the context of the VALU3S European project. The configuration of the variability of the formal specifications is described in MS Excel spreadsheets with a particular structure, making it easy to use also by developers. These spreadsheets are processed automatically by our prototype tool that generates instances and runs the model checker.  We propose the extension of our previous work by exploiting analysis over valid combination of features, while preserving the simplicity of a spreadsheet-based interface with the model checker.

</details>


### [26] [SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models](https://arxiv.org/abs/2305.19308)
*Hongxin Li,Jingran Su,Yuntao Chen,Qing Li,Zhaoxiang Zhang*

Main category: cs.SE

TL;DR: SheetCopilot 是一个利用大语言模型（LLM）通过自然语言指导电子表格完成任务的智能体。


<details>
  <summary>Details</summary>
Motivation: 旨在解决计算机日常任务（如表格数据处理、项目时间线规划）的重复性、易错性以及普通用户缺乏自动化能力的问题。

Method: 提出了一组原子动作来抽象电子表格功能，并设计了一个基于状态机的任务规划框架，使LLM能够与电子表格进行交互。

Result: 在包含221个电子表格控制任务的数据集上，SheetCopilot 单次生成任务的完成率达到了44.3%，显著优于代码生成基线。

Conclusion: SheetCopilot 在软件控制任务方面展现出强大的能力，为通过自然语言自动化电子表格操作提供了有效的解决方案。

Abstract: Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill to automate these burdensome works. With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal. In this work, we propose a SheetCopilot agent that takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for LLMs to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of LLMs in software control tasks. Our SheetCopilot correctly completes 44.3\% of tasks for a single generation, outperforming the strong code generation baseline by a wide margin. Our project page:https://sheetcopilot.github.io/.

</details>


### [27] [Excel as a Turing-complete Functional Programming Environment](https://arxiv.org/abs/2309.00115)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: Excel的动态数组功能改变了电子表格的构建方式，使其更像正式编程。


<details>
  <summary>Details</summary>
Motivation: 介绍Excel动态数组功能带来的变革以及对传统电子表格实践的影响。

Method: 探讨Excel社区中出现的新兴趋势和方法。

Result: Excel的动态数组功能为业务和工程领域带来了新的可能性，但也可能带来新的风险。

Conclusion: 虽然Excel动态数组功能的普及程度和影响尚不明确，但其已显现出与正式编程类似的趋势，预示着电子表格开发的未来。

Abstract: Since the calculation engine of Excel was the subject of a major upgrade to accommodate Dynamic Arrays in 2018 there has been a series of seismic changes to the art of building spreadsheet solutions. This paper will show the ad-hoc end user practices of traditional spreadsheets can be replaced by radically different approaches that have far more in common with formal programming. It is too early to guess the extent to which the new functionality will be adopted by the business and engineering communities and the impact that may have upon risk. Nevertheless, some trends are emerging from pioneering work within the Excel community which we will discuss here.

</details>


### [28] [A Use Case-Engineering Resources Taxonomy for Analytical Spreadsheet Models](https://arxiv.org/abs/2309.00104)
*Thomas A. Grossman,Vijay Mehrotra*

Main category: cs.SE

TL;DR: 该论文提出了一个分析型电子表格模型的分类法，考虑了用例和开发资源，将先前三类分类法扩展到九类，并区分了“分析解决方案”和“工业级分析型电子表格模型”。


<details>
  <summary>Details</summary>
Motivation: 对分析型电子表格模型进行分类，以更好地理解其用例和开发资源，区分分析解决方案和工业级模型，并为电子表格的错误、风险和随时间的变化提供一个透镜。

Method: 扩展了先前的三类分类法，提出了一个包含九种类型的电子表格模型分类法，并探讨了每种类型的性质、提出定义、联系相关文献并推测其产生方式。

Result: 确定了九种分析型电子表格模型，区分了分析解决方案和工业级模型，并阐述了该分类法在识别模型类型、理解错误风险和跟踪模型变化方面的作用。

Conclusion: 该分类法有助于指导电子表格开发，提供审视电子表格错误和风险的视角，并为理解电子表格随时间的变化提供结构，同时激发了进一步的研究问题。

Abstract: This paper presents a taxonomy for analytical spreadsheet models. It considers both the use case that a spreadsheet is meant to serve, and the engineering resources devoted to its development. We extend a previous three-type taxonomy, to identify nine types of spreadsheet models, that encompass the many analytical spreadsheet models seen in the literature. We connect disparate research literature to distinguish between an "analytical solution" and an "industrial-quality analytical spreadsheet model". We explore the nature of each of the nine types, propose definitions for some, relate them to the literature, and hypothesize on how they might arise. The taxonomy aids in identifying where various spreadsheet development guidelines are most useful, provides a lens for viewing spreadsheet errors and risk, and offers a structure for understanding how spreadsheets change over time. This taxonomy opens the door to many interesting research questions, including refinements to itself.

</details>


### [29] [Experimenting with ChatGPT for Spreadsheet Formula Generation: Evidence of Risk in AI Generated Spreadsheets](https://arxiv.org/abs/2309.00095)
*Simon Thorne*

Main category: cs.SE

TL;DR: ChatGPT在特定情况下可以生成有效的电子表格公式，但在信息有限、不确定或问题过于复杂时，其准确性和推理能力会下降，并可能产生“幻觉”。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLM）如ChatGPT在生成电子表格公式和计算输出方面的能力，特别是在需要推理、推断和解决问题的情况下。

Method: 通过一系列实验，探索ChatGPT生成电子表格公式和相关计算输出的能力，重点关注其在信息有限、不确定或问题复杂情境下的表现。

Result: 在某些情况下，ChatGPT能够生成正确的电子表格公式，并伴有正确的推理、推断和演绎。然而，当信息受限、不确定或问题过于复杂时，ChatGPT的准确性、推理、推断和演绎能力均会下降，并可能出现不准确的陈述和“幻觉”。

Conclusion: ChatGPT在生成电子表格公式方面具有潜力，但在处理复杂或信息不全的问题时仍存在局限性，需要谨慎使用。

Abstract: Large Language Models (LLM) have become sophisticated enough that complex computer programs can be created through interpretation of plain English sentences and implemented in a variety of modern languages such as Python, Java Script, C++ and Spreadsheets. These tools are powerful and relatively accurate and therefore provide broad access to computer programming regardless of the background or knowledge of the individual using them. This paper presents a series of experiments with ChatGPT to explore the tool's ability to produce valid spreadsheet formulae and related computational outputs in situations where ChatGPT has to deduce, infer and problem solve the answer. The results show that in certain circumstances, ChatGPT can produce correct spreadsheet formulae with correct reasoning, deduction and inference. However, when information is limited, uncertain or the problem is too complex, the accuracy of ChatGPT breaks down as does its ability to reason, infer and deduce. This can also result in false statements and "hallucinations" that all subvert the process of creating spreadsheet formulae.

</details>


### [30] [Demonstration of CORNET: A System For Learning Spreadsheet Formatting Rules By Example](https://arxiv.org/abs/2308.07357)
*Mukul Singh,Jose Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Gust Verbruggen*

Main category: cs.SE

TL;DR: CORNET是一个系统，可以根据用户提供的格式化单元格示例，自动学习并生成条件格式化规则。


<details>
  <summary>Details</summary>
Motivation: 现有的电子表格软件虽然支持数据依赖格式化规则，但用户需要手动编写这些规则，操作繁琐。CORNET旨在解决这一痛点，实现条件格式化规则的自动化学习。

Method: CORNET结合了归纳程序合成、半监督聚类、迭代决策树学习和神经网络排序等技术，通过用户提供的少量格式化单元格示例来生成条件格式化规则。

Result: CORNET能够根据用户提供的1-2个格式化单元格示例，生成准确的条件格式化规则建议，并以Microsoft Excel插件的形式提供给用户。

Conclusion: CORNET系统展示了通过用户示例自动学习条件格式化规则的可行性，简化了数据管理和分析任务中的格式化操作。

Abstract: Data management and analysis tasks are often carried out using spreadsheet software. A popular feature in most spreadsheet platforms is the ability to define data-dependent formatting rules. These rules can express actions such as "color red all entries in a column that are negative" or "bold all rows not containing error or failure." Unfortunately, users who want to exercise this functionality need to manually write these conditional formatting (CF) rules. We introduce CORNET, a system that automatically learns such conditional formatting rules from user examples. CORNET takes inspiration from inductive program synthesis and combines symbolic rule enumeration, based on semi-supervised clustering and iterative decision tree learning, with a neural ranker to produce accurate conditional formatting rules. In this demonstration, we show CORNET in action as a simple add-in to Microsoft Excel. After the user provides one or two formatted cells as examples, CORNET generates formatting rule suggestions for the user to apply to the spreadsheet.

</details>


### [31] [Excel Spreadsheet Analyzer](https://arxiv.org/abs/2211.06333)
*Amir Nassereldine,Patrick Chen,Jinjun Xiong*

Main category: cs.SE

TL;DR: 提出一种工具，将电子表格转换为抽象中间表示（AIR），以保留单元格之间的依赖关系，并提供一个Python库来进行数据分析。


<details>
  <summary>Details</summary>
Motivation: 当前数据科学家倾向于使用Python等科学编程语言进行数据分析，但在转换公司电子表格数据时，会丢失公式和单元格依赖关系等信息。

Method: 创建一个抽象中间表示（AIR）来表示电子表格，并构建一个基于该工具的Python库。

Result: 该工具能够保留电子表格中的公式和依赖关系，并能在Python中进行数据分析。

Conclusion: 所提出的工具和Python库可以促进从电子表格到科学编程语言的数据迁移，同时保留重要的依赖关系信息。

Abstract: Spreadsheets are widely used in various fields to do large numerical analysis. While several companies have relied on spreadsheets for decades, data scientists are going in the direction of using scientific programming languages such as python to do their data analysis due to the support, community, and vast amount of libraries. While using python to analyze a company's spreadsheets, some information such as the formulas and dependencies of a cell are lost. We propose a tool that creates an abstract intermediate representation (AIR) of a spreadsheet. This representation facilitates the transfer from spreadsheets into scientific programming languages while preserving inter-dependency information about data. In addition to that, we build a python library on top of our tool to perform some data analysis in python.

</details>


### [32] [Exploring Spreadsheet Use and Practices in a Technologically Constrained Setting](https://arxiv.org/abs/2201.07696)
*Khwima Mckinley Mkamanga,Simon Thorne*

Main category: cs.SE

TL;DR: The paper examines how spreadsheets affect business operations in a Malawian water utility, finding they enable automation but also pose risks due to management, technology, and human factors. It concludes that improved policies and governance are needed.


<details>
  <summary>Details</summary>
Motivation: To explore the impacts of spreadsheets on business operations in a water utility parastatal in Malawi, focusing on their scope of use, life cycle, and the associated organizational policy and governance.

Method: The study focused on spreadsheet scope of use and life cycle as well as organizational policy and governance.

Result: Findings indicate that the proliferation of spreadsheets has enabled business automation, but also highlights management, technological, and human factor issues contributing to high risks associated with their pervasive use.

Conclusion: There is significant room for improvement in implementing comprehensive policies and regulations governing spreadsheet development processes and adoption to mitigate risks.

Abstract: This paper explores the impacts of spreadsheets on business operations in a water utility parastatal in Malawi, Sub-Saharan Africa. The organisation is a typical example of a semi-government body operating in a technologically underdeveloped country. The study focused on spreadsheet scope of use and life cycle as well as organisational policy and governance. The results will help define future spreadsheet usage by influencing new approaches for managing potential risks associated with spreadsheets in the organization. Generally, findings indicate that the proliferation of spreadsheets in the organization has provided an enabling environment for business automation. The paper also highlights management, technological and human factor issues contributing to high risks associated with the pervasive spreadsheet use. The conclusions drawn from the research confirms that there is ample room for improvement in many areas such as implementation of comprehensive policies and regulations governing spreadsheet development processes and adoption.

</details>


### [33] [Methodology for Assessing the State of the Practice for Domain X](https://arxiv.org/abs/2110.11575)
*Spencer Smith,Jacques Carette,Peter Michalski,Ao Dong,Olu Owojaiye*

Main category: cs.SE

TL;DR: 研究软件开发实践的评估方法


<details>
  <summary>Details</summary>
Motivation: 提高研究软件的开发方法和工具，首先需要了解其当前的实践状况。

Method: 提出了一种评估研究软件开发实践状态的方法，包括确定领域、筛选软件包、收集源代码和文档、收集仓库相关数据、填写测量模板、访谈开发者、使用层次分析法（AHP）进行排序，并分析数据以回答相关问题。

Result: 该方法通过一个包含108个问题的测量模板和20个问题的访谈，来评估软件的安装性、可用性和可见性等9种质量，并使用AHP进行排序。

Conclusion: 该评估方法估计完成一个领域评估需要173个人时，并强调领域专家的参与对于确保信息准确性和分析共性和变异性的重要性。

Abstract: To improve software development methods and tools for research software, we first need to understand the current state of the practice. Therefore, we have developed a methodology for assessing the state of the software development practices for a given research software domain. For each domain we wish to answer questions such as: i) What artifacts (documents, code, test cases, etc.) are present? ii) What tools are used? iii) What principles, process and methodologies are used? iv) What are the pain points for developers? v) What actions are used to improve qualities like maintainability and reproducibility? To answer these questions, our methodology prescribes the following steps: i) Identify the domain; ii) Identify a list of candidate software packages; iii) Filter the list to a length of about 30 packages; iv) Gather source code and documentation for each package; v) Collect repository related data on each software package, like number of stars, number of open issues, number of lines of code; vi) Fill in the measurement template (the template consists of 108 questions to assess 9 qualities (including the qualities of installability, usability and visibility)); vii) Interview developers (the interview consists of 20 questions and takes about an hour); viii) Rank the software using the Analytic Hierarchy Process (AHP); and, ix) Analyze the data to answer the questions posed above. A domain expert should be engaged throughout the process, to ensure that implicit information about the domain is properly represented and to assist with conducting an analysis of the commonalities and variabilities between the 30 selected packages. Using our methodology, spreadsheet templates and AHP tool, we estimate (based on our experience with using the process) the time to complete an assessment for a given domain at 173 person hours.

</details>


### [34] [Agile Elicitation of Scalability Requirements for Open Systems: A Case Study](https://arxiv.org/abs/2108.00567)
*Gunnar Brataas,Antonio Martini,Geir Kjetil Hanssen,Georg Ræder*

Main category: cs.SE

TL;DR: ScrumScale模型是一种用于敏捷软件开发的可扩展性需求获取的工具。它通过一个简单的电子表格，结合了协调理论，并以开放银行案例研究进行了验证。该模型被证明能系统地生成可扩展性需求，并促进与其他利益相关者的沟通。


<details>
  <summary>Details</summary>
Motivation: 以往研究对敏捷软件开发中可扩展性需求获取的描述不足且复杂，因此需要一种轻量级的解决方案。

Method: 采用设计科学研究方法，结合协调理论，开发了ScrumScale模型（一个简单的电子表格），并通过开放银行案例研究进行验证。

Result: 在开放银行案例研究中，使用ScrumScale模型花费了55小时获取了可扩展性需求。TietoEVRY反馈该模型能系统地生成可扩展性需求，并有助于与其他利益相关者进行沟通。

Conclusion: ScrumScale模型为敏捷软件开发中的可扩展性需求获取提供了一种系统且有效的方法，并能在与其他利益相关者沟通时带来显著优势。

Abstract: Eliciting scalability requirements during agile software development is complicated and poorly described in previous research. This article presents a lightweight artifact for eliciting scalability requirements during agile software development: the ScrumScale model. The ScrumScale model is a simple spreadsheet. The scalability concepts underlying the ScrumScale model are clarified in this design science research, which also utilizes coordination theory. This paper describes the open banking case study, where a legacy banking system becomes open. This challenges the scalability of this legacy system. The first step in understanding this challenge is to elicit the new scalability requirements. In the open banking case study, key stakeholders from TietoEVRY spent 55 hours eliciting TietoEVRY's open banking project's scalability requirements. According to TietoEVRY, the ScrumScale model provided a systematic way of producing scalability requirements. For TietoEVRY, the scalability concepts behind the ScrumScale model also offered significant advantages in dialogues with other stakeholders.

</details>


### [35] [SpreadsheetCoder: Formula Prediction from Semi-structured Context](https://arxiv.org/abs/2106.15339)
*Xinyun Chen,Petros Maniatis,Rishabh Singh,Charles Sutton,Hanjun Dai,Max Lin,Denny Zhou*

Main category: cs.SE

TL;DR: SpreadsheetCoder使用BERT模型，结合行列和表头信息，在电子表格公式预测方面取得了显著进展。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅使用输入输出示例，忽略了电子表格的表格结构和表头信息。而表格中的行列和表头是重要的上下文信息，对公式预测至关重要。

Method: 提出了一种名为SpreadsheetCoder的基于BERT的模型架构，该架构能够同时处理行和列信息，并利用表头信息来表示表格上下文。

Result: SpreadsheetCoder的预测准确率达到了42.51%，显著优于未使用丰富表格上下文的基线方法。在实际应用中，SpreadsheetCoder比基于规则的系统更能帮助用户在Google表格中编写公式。

Conclusion: SpreadsheetCoder是第一个利用表格上下文（包括表头和半结构化数据）进行电子表格公式合成的方法，并在准确性和用户辅助方面取得了显著的改进。

Abstract: Spreadsheet formula prediction has been an important program synthesis problem with many real-world applications. Previous works typically utilize input-output examples as the specification for spreadsheet formula synthesis, where each input-output pair simulates a separate row in the spreadsheet. However, this formulation does not fully capture the rich context in real-world spreadsheets. First, spreadsheet data entries are organized as tables, thus rows and columns are not necessarily independent from each other. In addition, many spreadsheet tables include headers, which provide high-level descriptions of the cell data. However, previous synthesis approaches do not consider headers as part of the specification. In this work, we present the first approach for synthesizing spreadsheet formulas from tabular context, which includes both headers and semi-structured tabular data. In particular, we propose SpreadsheetCoder, a BERT-based model architecture to represent the tabular context in both row-based and column-based formats. We train our model on a large dataset of spreadsheets, and demonstrate that SpreadsheetCoder achieves top-1 prediction accuracy of 42.51%, which is a considerable improvement over baselines that do not employ rich tabular context. Compared to the rule-based system, SpreadsheetCoder assists 82% more users in composing formulas on Google Sheets.

</details>


### [36] [From webtables to datatables](https://arxiv.org/abs/2006.14694)
*Mária Csernoch*

Main category: cs.SE

TL;DR: 网页中的表格（Webtables）是教授电子表格技能的宝贵资源，可在商业和专业组织中用于知识转移、解决实际问题、调试以及培养计算思维。本文详细介绍了其中一个英雄联盟（LOL）战绩榜的转换过程，并提供了两种解决方案：一种在文字处理器中实现，另一种纯粹在电子表格应用程序中实现，为进一步的讨论和创新提供了空间。


<details>
  <summary>Details</summary>
Motivation: 利用和发展知识转移项目，展示和处理各种现实世界的问题和解决方案，进行讨论和调试，以及普遍地发展和利用计算思维技能，在商业和专业组织中，网页中的表格（Webtables）是教授电子表格的绝佳来源。

Method: 详细介绍其中一个英雄联盟（LOL）战绩榜的转换过程，并提供算法。

Result: 提供两种解决方案：一种在文字处理器中实现，另一种纯粹在电子表格应用程序中实现。

Conclusion: 提供两种解决方案：一种在文字处理器中实现，另一种纯粹在电子表格应用程序中实现，为进一步的讨论、发明其他解决方案和结合它们留下了空间。

Abstract: Webtables -- tables and table-like structures on webpages -- are excellent sources for teaching spreadsheeting, in commercial and professional organisations by utilizing and developing knowledge-transfer items, presenting and handling various real-world problems and solutions, discussing and debugging, and in general, developing and utilizing computational thinking skills. In the present paper the conversion process of one of the LOL Boards (League of Legends, Riot Games Inc. 2019) is detailed. After presenting the algorithm of the conversion, two solutions are offered -- one in a word processor, the other purely in a spreadsheet application -- leaving space for discussions, inventing other solutions and combining them.

</details>


### [37] [Implementation Strategies for Multidimensional Spreadsheets](https://arxiv.org/abs/2006.05814)
*Paul Mireault*

Main category: cs.SE

TL;DR: Excel开发者在实现多维变量时，主要采用两种策略：将高维变量投影到二维平面，或使用数据库方法将变量呈现为数据集表格。


<details>
  <summary>Details</summary>
Motivation: 探讨Excel开发者在处理多维变量时的实现策略。

Method: 分析Excel开发者实现多维变量的电子表格，识别并比较不同的实现策略。

Result: 大多数参与者将高维变量投影到二维平面；少数参与者采用数据库方法，将变量呈现为数据集表格，简化了公式。

Conclusion: 数据库方法在简化Excel中多维变量公式方面具有优势。

Abstract: Seasoned Excel developers were invited to participate in a challenge to implement a spreadsheet with multi-dimensional variables. We analyzed their spreadsheet to see the different implement strategies employed. We identified two strategies: most participants used a projection of three or four-dimensional variables on the two-dimensional plane used by Excel. A few participants used a database approach where the multi-dimensional variables are presented in the form of a dataset table with the appropriate primary key. This approach leads to simpler formulas.

</details>


### [38] [Abstracting spreadsheet data flow through hypergraph redrawing](https://arxiv.org/abs/2006.04794)
*David Birch,Nicolai Stawinoga,Jack Binks,Bruno Nicoletti,Paul Kelly*

Main category: cs.SE

TL;DR: 传统电子表格的易错性源于其低抽象级别。本文提出通过“重新绘制单元格边界”来提高电子表格的抽象级别，将电子表格转化为细粒度图，将运算符和值作为节点，将单元格表示为超图边，以更真实地表示最终用户的模型。通过常见子表达式识别和子树同构应用来检测向量（数组）操作。


<details>
  <summary>Details</summary>
Motivation: 传统电子表格的低抽象级别（由用户意图链接的低级单元格构成）是导致易错性的原因。

Method: 将电子表格转换为细粒度图，其中运算符和值作为节点，单元格表示为围绕一组运算符/数据节点的边界“墙壁”即超图边。提出通过重新绘制边界来创建更高层次的单元格，以更真实地表示用户的模型。通过常见子表达式识别和子树同构来检测向量（数组）操作。

Result: 通过常见子表达式识别和子树同构检测，能够识别出向量（数组）操作，这说明了所提出的方法可以提高电子表格的抽象级别。

Conclusion: 通过提高电子表格的抽象级别（例如，通过创建能够更真实地表示用户模型的新型单元格），可以潜在地减少错误。所提出的基于图的方法为实现这一目标提供了一种途径。

Abstract: We believe the error prone nature of traditional spreadsheets is due to their low level of abstraction. End user programmers are forced to construct their data models from low level cells which we define as "a data container or manipulator linked by user-intent to model their world and positioned to reflect its structure". Spreadsheet cells are limited in what they may contain (scalar values) and the links between them are inherently hidden. This paper proposes a method of raising the level of abstraction of spreadsheets by "redrawing the boundary" of the cell. To expose the hidden linkage structure we transform spreadsheets into fine-grained graphs with operators and values as nodes. "cells" are then represented as hypergraph edges by drawing a boundary "wall" around a set of operator/data nodes. To extend what cells may contain and to create a higher level model of the spreadsheet we propose that researchers should seek techniques to redraw these boundaries to create higher level "cells" which will more faithfully represent the end-user's real world/mental model. We illustrate this approach via common sub-expression identification and the application of sub-tree isomorphisms for the detection of vector (array) operations.

</details>


### [39] [Comprehensive review for common types of errors using spreadsheets](https://arxiv.org/abs/1912.09209)
*Ali Aburas*

Main category: cs.SE

TL;DR: 本研究全面回顾和分类了电子表格中查找和修复错误的方法。


<details>
  <summary>Details</summary>
Motivation: 电子表格因其灵活性和组织数据的能力而被广泛使用，但其中包含大量错误。因此，研究人员开发了各种工具来预防、检测和纠正这些错误。

Method: 本文对查找和修复电子表格错误的方法进行了全面的回顾和分类，讨论了现有研究的定义、工作原理和错误查找能力，并探讨了用户常犯的错误类型。

Result: 对现有研究的最新方法进行了分类和描述，并指出了用户在电子表格中常见的错误类型。

Conclusion: 本文全面回顾了电子表格错误查找和修复领域的研究现状，并为未来的研究提供了方向。

Abstract: Thanks to their flexibility and capability to perform different tasks and organize data in the best form and format, spreadsheets are widely used in different organizations and by different end users. Many business organizations rely on spreadsheets to fulfill their various tasks. On the other hand, the number of spreadsheets that contain errors are very high, thus researchers have developed different tools aimed at the prevention, detection, and correction of errors in spreadsheets. This research work is a comprehensive review that describes and classifies approaches on finding and fixing errors in spreadsheets. The paper discusses up-to-date research work approaches in terms of definition, how they work, and kinds of errors they can find in spreadsheets. The paper looks also for the kinds of errors that end users commonly make in spreadsheets.

</details>


### [40] [A Coding-free Software Framework of Developing Web Data Management Systems](https://arxiv.org/abs/1910.05685)
*Can Yang,Shiying Pan,Runmin Li,Yu Liu,Lizhang Peng*

Main category: cs.SE

TL;DR: 本论文提出了一种无需编码即可构建数据管理系统的理论和方法，并提供了一个实际的应用平台、一套构建方法和一套数据交换接口。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的企业将数据管理系统部署到云端，但由于软件开发的专业性，即使是小型系统，非程序员仍然难以开发。SaaS的出现使得无代码开发成为可能。

Method: 本论文提出了一套基于SaaS架构的无代码构建数据管理系统的理论和方法。通过抽象数据管理系统的通用特征，设计了一个通用的Web平台来快速生成和发布定制化的系统实例。提出了一种使用电子表格中的特定需求表来开发数据管理系统的方法，该平台通过解析表模型并在运行阶段实现目标系统来映射需求表。

Result: 实现了一个通用的Web平台，用于快速生成和发布定制化的系统实例，并实现了一种使用电子表格中的特定需求表来开发数据管理系统的方法。

Conclusion: 本论文实现并部署了所提出的框架，并通过了实证结果证明了开发Web数据管理系统中无代码方法的可行性和可用性。

Abstract: More and more enterprises recently intend to deploy data management systems in the cloud. Due to the professionalism of software development, it has still been difficult for non-programmers to develop this kind of systems, even a small one. However, the development of SaaS brings forth the more feasibility of coding-free software development than before. Based on the SaaS architecture, this paper presents a set of theory and method for coding-free construction of a data management system, on which our contributions involve in a practical application platform, a set of construction method and a set of interface on data exchange. By abstracting the common features of data management systems, we design a universal web platform to quickly generate and publish customized system instances. Moreover, we propose a kind of method to develop a data management system using a specific requirements table in spreadsheet. The corresponding platform maps the requirements table into a system instance through parsing the table model and implementing the objective system in the running stage. Finally, we implement the proposed framework and deploy it on web. The empirical result demonstrates the feasibility and availability of the coding-free method in developing web data management systems.

</details>


### [41] [Structured Spreadsheet Modelling and Implementation with Multiple Dimensions -- Part 2: Implementation](https://arxiv.org/abs/1909.00891)
*Paul Mireault*

Main category: cs.SE

TL;DR: 本文介绍了如何在电子表格中实现多维问题，以便于维护。


<details>
  <summary>Details</summary>
Motivation: 在第一部分中，我们展示了如何开发一个涉及多维变量（如产品、区域、行业和月份）的问题的概念模型。概念模型被呈现为公式图（提供变量之间交互的全局视图）和公式列表（提供变量之间交互的精确视图）。

Method: 在本文中，我们提出了在电子表格中实现多维问题的具体步骤。

Result: 本文提出了一种实现多维问题的方法，该方法可以生成易于维护的电子表格。

Conclusion: 本文介绍了在电子表格中实现多维问题的具体步骤，以提高可维护性。

Abstract: In Part 1, we showed how to develop a conceptual model of a problem involving variables of multiple dimensions, like Products, Regions, Sectors and Months. The conceptual model is presented as a Formula Diagram, giving a global view of the interaction between all the variables, and a Formula List, giving a precise view of the interaction between the variables. In this paper, we present precise steps to implement a multi-dimensional problem in a way that will produce a spreadsheet that is easy to maintain

</details>


### [42] [On the Refinement of Spreadsheet Smells by means of Structure Information](https://arxiv.org/abs/1810.04542)
*Patrick Koch,Birgit Hofer,Franz Wotawa*

Main category: cs.SE

TL;DR: 当前电子表格的缺点检测方法存在缺陷，如重复报告问题。本研究提出通过推断结构信息来改进和发现新的缺点，以提高检测准确性并减少冗余报告。


<details>
  <summary>Details</summary>
Motivation: 用户难以识别和修复电子表格中的设计缺陷，现有缺点检测技术会产生错误或冗余的报告，影响用户体验。

Method: 1. 提出静态分析方法推断相关单元格集群和块。 2. 结合结构信息改进现有缺点检测。 3. 提出三种利用推断结构的全新缺点检测技术。

Result: 改进后的缺点检测能显著减少错误和冗余的报告数量。新提出的检测技术能够发现新的潜在问题。

Conclusion: 通过推断电子表格结构信息来改进和发现缺点，可以有效提高电子表格质量评估的准确性和效率。

Abstract: Spreadsheet users are often unaware of the risks imposed by poorly designed spreadsheets. One way to assess spreadsheet quality is to detect smells which attempt to identify parts of spreadsheets that are hard to comprehend or maintain and which are more likely to be the root source of bugs. Unfortunately, current spreadsheet smell detection techniques suffer from a number of drawbacks that lead to incorrect or redundant smell reports. For example, the same quality issue is often reported for every copy of a cell, which may overwhelm users. To deal with these issues, we propose to refine spreadsheet smells by exploiting inferred structural information for smell detection. We therefore first provide a detailed description of our static analysis approach to infer clusters and blocks of related cells. We then elaborate on how to improve existing smells by providing three example refinements of existing smells that incorporate information about cell groups and computation blocks. Furthermore, we propose three novel smell detection techniques that make use of the inferred spreadsheet structures. Empirical evaluation of the proposed techniques suggests that the refinements successfully reduce the number of incorrectly and redundantly reported smells, and novel deficits are revealed by the newly introduced smells.

</details>


### [43] [Now You're Thinking With Structures: A Concept for Structure-based Interactions with Spreadsheets](https://arxiv.org/abs/1809.03435)
*Patrick Koch*

Main category: cs.SE

TL;DR: Spreadsheets are hard to understand when complex. We propose a structure-aware system to enhance spreadsheet comprehension and interaction by enriching visualizations and providing tools for proactive alteration. This system aims to work alongside existing spreadsheet tools.


<details>
  <summary>Details</summary>
Motivation: Complex spreadsheets are difficult to comprehend and adapt, hindering users. A higher-order mental model is needed for understanding complex systems, and spreadsheets currently lack this.

Method: The paper proposes a concept for structure-aware understanding and interaction with spreadsheets. This involves using structural information to enrich visualizations, enhance user actions, and provide tools for proactive alteration of the spreadsheet's structure, rather than just individual cells. An initial tool for structure inference and visualization has been implemented.

Result: An initial tool for structure inference and visualization has been implemented based on the proposed framework. The plan is to introduce proactive and reactive interaction mechanics and offer structure-aware functionality as an add-in.

Conclusion: Providing tools for thinking about and interacting with spreadsheets in a structure-aware manner will improve user productivity and the overall quality of spreadsheets.

Abstract: Spreadsheets are the go-to tool for computerized calculation and modelling, but are hard to comprehend and adapt after reaching a certain complexity. In general, cognition of complex systems is facilitated by having a higher order mental model of the system in question to work with. We therefore present a concept for structure-aware understanding of and interaction with spreadsheets that extends previous work on structure inference in the domain. Following this concept, structural information is used to enrich visualizations, reactively enhance traditional user actions, and provide tools to proactively alter the overall spreadsheet makeup instead of individual cells The intended systems should, in first approximation, not replace common spreadsheet tools, but provide an additional layer of functionality alongside the established interface. In ongoing work, we therefore implemented a tool for structure inference and visualization along the common spreadsheet layout. Based on this framework, we plan to introduce the envisioned proactive and reactive interaction mechanics, and finally provide structure-aware unctionality as an add-in for common spreadsheet processors. We believe that providing the tools for thinking about and interacting with spreadsheets in this manner will benefit users both in terms of productivity and overall spreadsheet quality.

</details>


### [44] [Typed Table Transformations](https://arxiv.org/abs/1809.02746)
*Martin Erwig*

Main category: cs.SE

TL;DR: 电子表格表格的标签可以看作是数据的类型，表格中的数据可以通过行列类型来控制。


<details>
  <summary>Details</summary>
Motivation: 介绍一种基于行列类型转换的新方法来处理电子表格的转换。

Method: 提出一种基于行列类型转换的电子表格转换新方法。

Result: 阐述了基于类型表的表构建和转换的基本思想。

Conclusion: 提出了一系列有待在未来工作中研究的问题。

Abstract: Spreadsheet tables are often labeled, and these labels effectively constitute types for the data in the table. In such cases tables can be considered to be built from typed data where the placement of values within the table is controlled by the types used for rows and columns. We present a new approach to the transformations of spreadsheet tables that is based on transformations of row and column types. We illustrate the basic idea of type-based table construction and transformation and lay out a series of research questions that should be addressed in future work.

</details>


### [45] [Implementing WHERE and ORDER BY as spreadsheet formulas](https://arxiv.org/abs/1809.00025)
*Paul Mireault*

Main category: cs.SE

TL;DR: SQL的WHERE和ORDER BY子句可以在电子表格中自动实现


<details>
  <summary>Details</summary>
Motivation: SQL的WHERE和ORDER BY子句在电子表格中缺乏自动适应性。

Method: 开发电子表格公式以实现SQL的WHERE和ORDER BY子句。

Result: 实现了SQL的WHERE和ORDER BY子句的电子表格公式。

Conclusion: SQL的WHERE和ORDER BY子句可以通过电子表格公式自动实现。

Abstract: The WHERE and ORDER BY clauses of the SQL SELECT statement select a subset of rows in the result of a database query and present the result in the specified order. In a spreadsheet program like Microsoft Excel, one could use the filter and sort buttons, or use its Query or its Pivot Table tools to achieve a similar effect. The disadvantage of using those tools is that they don't react automatically to changes in the calculated values of the spreadsheet. In this paper, we develop spreadsheet formulas that implement SQL's WHERE and ORDER BY clauses.

</details>


### [46] [The use of Charts, Pivot Tables, and Array Formulas in two Popular Spreadsheet Corpora](https://arxiv.org/abs/1808.10642)
*Bas Jansen,Felienne Hermans*

Main category: cs.SE

TL;DR: 电子表格被广泛用于工业界，但其易出错的特性可能导致错误的决策。现有研究主要集中在公式，但忽略了图表、数据透视表和数组公式等其他重要构造。为了提高电子表格质量，有必要对这些构造的使用进行研究。本文分析了Enron和EUSES两个电子表格语料库，以了解这些构造的使用情况。


<details>
  <summary>Details</summary>
Motivation: 电子表格在工业界被广泛使用，但其易出错的特性可能导致公司基于不准确的信息做出错误的决策，从而造成经济损失。现有研究主要集中在公式，但忽略了图表、数据透视表和数组公式等其他用于支持决策的构造。为了提高电子表格质量，理解这些构造的使用情况至关重要。

Method: 分析Enron和EUSES两个广泛使用的电子表格语料库，研究图表、数据透视表和数组公式等构造的使用情况。

Result: （此处省略具体结果，因原文未提供）

Conclusion: 为了提高电子表格质量，有必要将图表、数据透视表和数组公式等构造的使用纳入电子表格研究范围，以获得对其使用的完整理解。

Abstract: The use of spreadsheets in industry is widespread. Companies base decisions on information coming from spreadsheets. Unfortunately, spreadsheets are error-prone and this increases the risk that companies base their decisions on inaccurate information, which can lead to incorrect decisions and loss of money. In general, spreadsheet research is aimed to reduce the error-proneness of spreadsheets. Most research is concentrated on the use of formulas. However, there are other constructions in spreadsheets, like charts, pivot tables, and array formulas, that are also used to present decision support information to the user. There is almost no research about how these constructions are used. To improve spreadsheet quality it is important to understand how spreadsheets are used and to obtain a complete understanding, the use of charts, pivot tables, and array formulas should be included in research. In this paper, we analyze two popular spreadsheet corpora: Enron and EUSES on the use of the aforementioned constructions.

</details>


### [47] [Asheetoxy: A Taxonomy for Classifying Negative Spreadsheet-related Phenomena](https://arxiv.org/abs/1808.10231)
*Daniel Kulesz,Stefan Wagner*

Main category: cs.SE

TL;DR: Asheetoxy是一种新的电子表格现象分类法，旨在克服现有分类法的缺点，并易于非专业人士使用。


<details>
  <summary>Details</summary>
Motivation: 现有电子表格错误分类法存在问题，例如术语模糊、需要深入了解用户和流程，这使得在实际应用中难以对广泛的电子表格现象进行分类。

Method: 提出了一种名为Asheetoxy的简单、面向现象的分类法，并避免使用“错误”一词。通过对7名参与者的初步研究来评估其有效性。

Result: 初步研究表明，即使是没有电子表格研究背景的参与者，也可以使用Asheetoxy对现实世界中的电子表格现象进行分类。

Conclusion: Asheetoxy是一种有前途的分类法，可以促进电子表格错误研究的讨论，并且易于广泛使用。

Abstract: Spreadsheets (sometimes also called Excel programs) are powerful tools which play a business-critical role in many organizations. However, due to faulty spreadsheets many bad decisions have been taken in recent years. Since then, a number of researchers have been studying spreadsheet errors. However, one issue that hinders discussion among researchers and professionals is the lack of a commonly accepted taxonomy.
  Albeit a number of taxonomies for spreadsheet errors have been proposed in previous work, a major issue is that they use the term error that itself is already ambiguous. Furthermore, to apply most existing taxonomies, detailed knowledge about the underlying process and knowledge about the "brain state" of the acting spreadsheet users is required. Due to these limitations, known error-like phenomena in freely available spreadsheet corpora cannot be classified with these taxonomies.
  We propose Asheetoxy, a simple and phenomenon-oriented taxonomy that avoids the problematic term error altogether. An initial study with 7 participants indicates that even non-spreadsheet researchers similarly classify real-world spreadsheet phenomena using Asheetoxy.

</details>


### [48] [Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (SEMS'18)](https://arxiv.org/abs/1808.09174)
*Birgit Hofer,Jorge Mendes*

Main category: cs.SE

TL;DR: 该论文集是关于电子表格软件工程方法的第五届国际研讨会（SEMS'18）的论文集。


<details>
  <summary>Details</summary>
Motivation: 研讨会旨在探讨电子表格领域的软件工程方法。

Method: 该论文集收录了在SEMS'18研讨会上发表的论文，该研讨会于2018年10月1日在葡萄牙里斯本举行。

Result: 会议论文集。

Conclusion: SEMS'18是一个关于电子表格软件工程方法的研讨会。

Abstract: Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (SEMS'18), held on October 1st, 2018, in Lisbon, Portugal, and co-located with the 2018 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC).

</details>


### [49] [Combining Spreadsheet Smells for Improved Fault Prediction](https://arxiv.org/abs/1805.10493)
*Patrick Koch,Konstantin Schekotihin,Dietmar Jannach,Birgit Hofer,Franz Wotawa*

Main category: cs.SE

TL;DR: 电子表格中的代码异味可以通过 AdaBoost 集成学习器得到更准确的故障预测。


<details>
  <summary>Details</summary>
Motivation: 在电子表格中，代码异味可以用于故障预测，但单一异味预测能力有限，因此需要更有效的方法。

Method: 提出一种基于机器学习的方法，使用 AdaBoost 集成学习器来结合多个代码异味的预测能力。

Result: 通过在包含真实世界电子表格故障的两个公开数据集上进行实验，证明了该方法在故障预测准确性方面有显著提高。

Conclusion: 基于 AdaBoost 集成学习器的机器学习方法能够有效提升电子表格故障预测的准确性。

Abstract: Spreadsheets are commonly used in organizations as a programming tool for business-related calculations and decision making. Since faults in spreadsheets can have severe business impacts, a number of approaches from general software engineering have been applied to spreadsheets in recent years, among them the concept of code smells. Smells can in particular be used for the task of fault prediction. An analysis of existing spreadsheet smells, however, revealed that the predictive power of individual smells can be limited. In this work we therefore propose a machine learning based approach which combines the predictions of individual smells by using an AdaBoost ensemble classifier. Experiments on two public datasets containing real-world spreadsheet faults show significant improvements in terms of fault prediction accuracy.

</details>


### [50] [An Easy & Collaborative RDF Data Entry Method using the Spreadsheet Metaphor](https://arxiv.org/abs/1804.04175)
*Markus Schröder,Christian Jilek,Jörn Hees,Sven Hertling,Andreas Dengel*

Main category: cs.SE

TL;DR: We propose a web-based spreadsheet editor that converts spreadsheet entries into RDF statements, enabling easy semantic data creation for both experts and novices. Users created more statements in less time with similar or better quality compared to other methods.


<details>
  <summary>Details</summary>
Motivation: Spreadsheets are widely used by knowledge workers for easy data entry, but converting this data into semantic RDF statements is challenging for non-experts. This paper aims to bridge this gap by providing an accessible tool for creating semantic data from spreadsheets.

Method: A zero-configuration, web-based spreadsheet editor was developed that simultaneously transfers spreadsheet entries into RDF statements. The focus is on creating instance data for an empty knowledge base incrementally.

Result: In a user study, participants using the proposed editor were able to create more semantic statements in a shorter amount of time, with quality that was similar or significantly better than other approaches.

Conclusion: The developed spreadsheet editor is an effective tool for enabling knowledge workers, including RDF novices, to easily create semantic data from spreadsheets, outperforming existing methods in terms of speed and quality.

Abstract: Spreadsheets are widely used by knowledge workers, especially in the industrial sector. Their methodology enables a well understood, easy and fast possibility to enter data. As filling out a spreadsheet is more accessible to common knowledge workers than defining RDF statements, in this paper, we propose an easy-to-use, zero-configuration, web-based spreadsheet editor that simultaneously transfers spreadsheet entries into RDF statements. It enables various kinds of users to easily create semantic data whether they are RDF experts or novices. The typical scenario we address focuses on creating instance data starting with an empty knowledge base that is filled incrementally. In a user study, participants were able to create more statements in shorter time, having similar or even significantly outperforming quality, compared to other approaches.

</details>


### [51] [Structured Spreadsheet Modelling and Implementation with Multiple Dimensions - Part 1: Modelling](https://arxiv.org/abs/1801.09777)
*Paul Mireault*

Main category: cs.SE

TL;DR: The paper discusses the common use of dimensions, particularly the time dimension, in models and spreadsheets. It highlights the limitations of current methods for representing a second dimension, such as repeating formulas or creating multiple worksheets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations in representing multiple dimensions in spreadsheets and models, which are commonly used in daily tasks.

Method: The paper seems to describe a problem with current spreadsheet practices rather than proposing a specific method. It observes the common use of the time dimension and the manual workarounds for representing a second dimension.

Result: The result is an observation that current methods for representing a second dimension in spreadsheets are cumbersome, involving formula repetition or multiple worksheets.

Conclusion: The paper implicitly concludes that there is a need for a more efficient way to handle multiple dimensions in spreadsheet models.

Abstract: Dimensions are an integral part of many models we use every day. Without thinking about it, we frequently use the time dimension: many financial and accounting spreadsheets have columns representing months or years. Representing a second dimension is often done by repeating blocs of formulas in a worksheet or creating multiple worksheets with the same structure.

</details>


### [52] [Mitigating Spreadsheet Risk in Complex Multi-Dimensional Models in Excel](https://arxiv.org/abs/1802.01640)
*Steve Litt*

Main category: cs.SE

TL;DR: Excel 广泛用于数据分析，但手动操作易出错。本文提出 PivotModel 解决方案，利用 Excel 的强大功能，解决复杂多维模型中的风险，实现更强大的数据分析。


<details>
  <summary>Details</summary>
Motivation: Excel 易用但手动操作风险高，难以控制，特别是复杂多维模型。需要一个解决方案来缓解这类风险。

Method: 提出一个名为 "PivotModel" 的解决方案，其工作方式类似于 Excel 的 PivotTable，但旨在利用 Excel 平台强大的功能来解决复杂多维模型中的风险。

Result: PivotModel 解决方案能够缓解复杂多维模型中的电子表格风险。

Conclusion: PivotModel 是一种利用 Excel 平台功能来解决复杂多维模型中的电子表格风险的解决方案。

Abstract: Microsoft Excel is the most ubiquitous analytical tool ever built. Companies around the world leverage it for its power, flexibility and ease of use. However, spreadsheets are manually intensive and prone to error, making it difficult for companies to control spreadsheet risk. The following solution is designed to mitigate spreadsheet risk for a set of problems commonly addressed in a spreadsheet defined as "complex multi-dimensional models". "Complex" referring to certain types of applications that require functionality such as sophisticated algorithms, challenging hierarchies and database write-back (i.e. planning, forecasting, etc.) and "multi-dimensional" referring to providing capabilities such as reporting, data input forms and ad hoc analysis on the different attributes associated with the resulting model. The solution is defined as a "PivotModel" because it works similarly to a PivotTable but is designed to leverage the robust capabilities of the Microsoft Excel platform.

</details>


### [53] [Proposed Spreadsheet Transparency Definition and Measures](https://arxiv.org/abs/1802.01628)
*Craig Hatmaker*

Main category: cs.SE

TL;DR: 缺乏明确的金融模型透明度定义和衡量标准，阻碍了审计师和模型师对模型透明度的客观评估。本文提出了一个适用于电子表格模型的透明度定义，该定义足够具体，可以开发衡量和自动化工具，以帮助审计师评估模型是否满足透明度要求，并使模型师能够客观地比较和选择最符合其目标的方法。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对金融模型透明度的明确定义和量化方法，导致审计师和模型师在评估和选择模型方法时存在不确定性。

Method: 提出一个具体的电子表格模型透明度定义，该定义可用于创建衡量和自动化工具。

Result: 该定义能够帮助审计师确定模型是否满足透明度要求，并使模型师能够客观地比较和选择模型方法。

Conclusion: 提出的透明度定义为金融模型透明度的量化和评估提供了基础，有助于提高模型的可信度和可比性。

Abstract: Auditors demand financial models be transparent yet no consensus exists on what that means precisely. Without a clear modeling transparency definition we cannot know when our models are "transparent". The financial modeling community debates which methods are more or less transparent as though transparency is a quantifiable entity yet no measures exist. Without a transparency measure modelers cannot objectively evaluate methods and know which improves model transparency.
  This paper proposes a definition for spreadsheet modeling transparency that is specific enough to create measures and automation tools for auditors to determine if a model meets transparency requirements. The definition also provides modelers the ability to objectively compare spreadsheet modeling methods to select which best meets their goals.

</details>


### [54] [Alternative Spreadsheet Model Designs for an Operations Management Model Embedded in a Periodic Business Process](https://arxiv.org/abs/1802.00484)
*Thomas A. Grossman,Vijay Mehrotra,Mouwafac Sidaoui*

Main category: cs.SE

TL;DR: 该论文介绍了一种在供应链和分销规划中广泛使用的运营管理模型，并探讨了三种不同的电子表格实现方式：数据驱动设计、规范设计和表格驱动技术设计。


<details>
  <summary>Details</summary>
Motivation: 在供应链和分销规划中，运营管理模型通常嵌入在需要模型修改和重用的周期性业务流程中。

Method: 评估了数据驱动设计、规范设计和表格驱动技术设计这三种电子表格实现方式在准确性、修改性、分析性和可转移性方面的适用性，并考虑了使用每种设计所需的技术和培训。

Result: 数据驱动设计揭示了初学者建模者不当的电子表格使用习惯；表格驱动技术设计无需手动编写或编辑单元格公式即可修改以适应新数据和新结构元素，从而加快修改速度并降低出错风险；表格驱动技术设计有潜力应用于其他类别的模型。

Conclusion: 表格驱动技术设计在修改速度和降低错误风险方面优于其他设计，并且有潜力应用于更广泛的模型。论文还指出了未来研究的方向。

Abstract: We present a widely-used operations management model used in supply and distribution planning, that is typically embedded in a periodic business process that necessitates model modification and reuse. We consider three alternative spreadsheet implementations, a data-driven design, a canonical (textbook) design, and a novel (table-driven) technical design. We evaluate each regarding suitability for accuracy, modification, analysis, and transfer. We consider the degree of training and technical sophistication required to utilize each design. The data-driven design provides insight into poor spreadsheet practices by naïve modelers. The technical design can be modified for new data and new structural elements without manual writing or editing of cell formulas, thus speeding modification and reducing risk of error. The technical design has potential for use with other classes of models. We identify opportunities for future research.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [55] [FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining](https://arxiv.org/abs/2109.07323)
*Zhoujun Cheng,Haoyu Dong,Ran Jia,Pengfei Wu,Shi Han,Fan Cheng,Dongmei Zhang*

Main category: cs.IR

TL;DR: 表格包含丰富的数值数据，但对表格进行数值推理仍然是一个挑战。本研究提出FORTAP，一种利用电子表格公式进行表格预训练的方法，以增强数值推理能力。


<details>
  <summary>Details</summary>
Motivation: 电子表格公式是数值推理的天然监督信号，并且网上有大量的专家制作的公式可供使用。

Method: FORTAP通过设计两个公式预训练任务来显式地引导模型学习半结构化表格中的数值引用和计算。

Result: FORTAP在单元格类型分类和公式预测两个代表性下游任务上取得了最先进的结果。

Conclusion: 本研究表明，进行数值推理感知的预训练具有巨大潜力。

Abstract: Tables store rich numerical data, but numerical reasoning over tables is still a challenge. In this paper, we find that the spreadsheet formula, which performs calculations on numerical values in tables, is naturally a strong supervision of numerical reasoning. More importantly, large amounts of spreadsheets with expert-made formulae are available on the web and can be obtained easily. FORTAP is the first method for numerical-reasoning-aware table pretraining by leveraging large corpus of spreadsheet formulae. We design two formula pretraining tasks to explicitly guide FORTAP to learn numerical reference and calculation in semi-structured tables. FORTAP achieves state-of-the-art results on two representative downstream tasks, cell type classification and formula prediction, showing great potential of numerical-reasoning-aware pretraining.

</details>


### [56] [Detecting Layout Templates in Complex Multiregion Files](https://arxiv.org/abs/2109.06630)
*Gerardo Vitagliano,Lan Jiang,Felix Naumann*

Main category: cs.IR

TL;DR: 本研究提出 Mondrian 方法，能自动识别跨多个文件的电子表格布局模板，并系统地提取相关区域。


<details>
  <summary>Details</summary>
Motivation: 电子表格广泛用于数据管理、分发和分析，但其灵活的结构使得在没有大量预处理的情况下难以进行自动化分析。在多个独立的区域出现在单个电子表格中（可能被重复的空单元格分隔）时，这一问题尤为突出。

Method: Mondrian 方法包含三个阶段：首先，将每个文件渲染成图像并检查可能构成区域的元素；然后，使用聚类算法将识别出的元素分组形成区域；最后，将每个文件布局表示为图，并与其他文件进行比较以寻找布局模板。

Result: 与最先进的表格识别算法相比，Mondrian 方法在检测单个文件内可靠的区域边界方面表现最佳，并能正确识别跨文件的重复布局。

Conclusion: Mondrian 方法能够有效地处理电子表格中的多区域文件，并识别跨文件的布局模板，为自动化电子表格分析提供了新的解决方案。

Abstract: Spreadsheets are among the most commonly used file formats for data management, distribution, and analysis. Their widespread employment makes it easy to gather large collections of data, but their flexible canvas-based structure makes automated analysis difficult without heavy preparation. One of the common problems that practitioners face is the presence of multiple, independent regions in a single spreadsheet, possibly separated by repeated empty cells. We define such files as "multiregion" files. In collections of various spreadsheets, we can observe that some share the same layout. We present the Mondrian approach to automatically identify layout templates across multiple files and systematically extract the corresponding regions. Our approach is composed of three phases: first, each file is rendered as an image and inspected for elements that could form regions; then, using a clustering algorithm, the identified elements are grouped to form regions; finally, every file layout is represented as a graph and compared with others to find layout templates. We compare our method to state-of-the-art table recognition algorithms on two corpora of real-world enterprise spreadsheets. Our approach shows the best performances in detecting reliable region boundaries within each file and can correctly identify recurring layouts across files.

</details>


### [57] [TUTA: Tree-based Transformers for Generally Structured Table Pre-training](https://arxiv.org/abs/2010.12537)
*Zhiruo Wang,Haoyu Dong,Ran Jia,Jia Li,Zhiyi Fu,Shi Han,Dongmei Zhang*

Main category: cs.IR

TL;DR: TUTA是一个统一的预训练模型，用于理解各种结构形式的表格，通过结合空间、层级和语义信息，在表格结构理解任务上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有表格理解方法主要关注关系型表格，忽略了其他常见表格结构。

Method: 提出了一种统一的基于树的结构（二维坐标树）来描述表格的空间和层级信息，并结合了基于树的注意力和位置嵌入。设计了三种渐进式的预训练目标，分别针对标记、单元格和表格层级。在大量未标记的网络和电子表格数据上进行预训练，并在单元格类型分类和表格类型分类任务上进行微调。

Result: 在五个广泛研究的数据集上取得了最先进的成果。

Conclusion: TUTA在理解通用表格结构方面非常有效，显著优于现有方法。

Abstract: Tables are widely used with various structures to organize and present data. Recent attempts on table understanding mainly focus on relational tables, yet overlook to other common table structures. In this paper, we propose TUTA, a unified pre-training architecture for understanding generally structured tables. Noticing that understanding a table requires spatial, hierarchical, and semantic information, we enhance transformers with three novel structure-aware mechanisms. First, we devise a unified tree-based structure, called a bi-dimensional coordinate tree, to describe both the spatial and hierarchical information of generally structured tables. Upon this, we propose tree-based attention and position embedding to better capture the spatial and hierarchical information. Moreover, we devise three progressive pre-training objectives to enable representations at the token, cell, and table levels. We pre-train TUTA on a wide range of unlabeled web and spreadsheet tables and fine-tune it on two critical tasks in the field of table structure understanding: cell type classification and table type classification. Experiments show that TUTA is highly effective, achieving state-of-the-art on five widely-studied datasets.

</details>


### [58] [TableSense: Spreadsheet Table Detection with Convolutional Neural Networks](https://arxiv.org/abs/2106.13500)
*Haoyu Dong,Shijie Liu,Shi Han,Zhouyu Fu,Dongmei Zhang*

Main category: cs.IR

TL;DR: TableSense是一个用于表格检测的新型端到端框架，通过创新的细胞特征提取、增强的卷积神经网络模型和基于主动学习的采样算法，在表格检测任务上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 电子表格中的表格检测是电子表格数据智能的关键技术，但面临表格结构和布局多样性的挑战。

Method: 提出了一种名为TableSense的新型端到端框架，包括细胞特征提取方案、增强的卷积神经网络模型以及用于主动学习的度量标准。

Result: TableSense在EoB-2度量上实现了91.3%的召回率和86.5%的精确率，显著优于现有算法。

Conclusion: TableSense在电子表格表格检测方面表现出高度有效性，并在召回率和精确率方面取得了显著进步。

Abstract: Spreadsheet table detection is the task of detecting all tables on a given sheet and locating their respective ranges. Automatic table detection is a key enabling technique and an initial step in spreadsheet data intelligence. However, the detection task is challenged by the diversity of table structures and table layouts on the spreadsheet. Considering the analogy between a cell matrix as spreadsheet and a pixel matrix as image, and encouraged by the successful application of Convolutional Neural Networks (CNN) in computer vision, we have developed TableSense, a novel end-to-end framework for spreadsheet table detection. First, we devise an effective cell featurization scheme to better leverage the rich information in each cell; second, we develop an enhanced convolutional neural network model for table detection to meet the domain-specific requirement on precise table boundary detection; third, we propose an effective uncertainty metric to guide an active learning based smart sampling algorithm, which enables the efficient build-up of a training dataset with 22,176 tables on 10,220 sheets with broad coverage of diverse table structures and layouts. Our evaluation shows that TableSense is highly effective with 91.3\% recall and 86.5\% precision in EoB-2 metric, a significant improvement over both the current detection algorithm that are used in commodity spreadsheet tools and state-of-the-art convolutional neural networks in computer vision.

</details>


### [59] [Recommending Related Tables](https://arxiv.org/abs/1907.03595)
*Shuo Zhang,Krisztian Balog*

Main category: cs.IR

TL;DR: 推荐相关表格


<details>
  <summary>Details</summary>
Motivation: 在电子表格程序中，向用户主动推荐网上相关的结构化内容。

Method: 提出一种理论上可靠的表格匹配框架，该框架将表格元素表示在多个语义空间中，然后使用判别学习模型组合元素级相似性。

Result: 使用从维基百科表格专门构建的测试集合，证明了所提出的方法提供了最先进的性能。

Conclusion: 该方法在相关表格推荐任务上取得了最先进的性能。

Abstract: Tables are an extremely powerful visual and interactive tool for structuring and manipulating data, making spreadsheet programs one of the most popular computer applications. In this paper we introduce and address the task of recommending related tables: given an input table, identifying and returning a ranked list of relevant tables. One of the many possible application scenarios for this task is to provide users of a spreadsheet program proactively with recommendations for related structured content on the Web. At its core, the related table recommendation task boils down to computing the similarity between a pair of tables. We develop a theoretically sound framework for performing table matching. Our approach hinges on the idea of representing table elements in multiple semantic spaces, and then combining element-level similarities using a discriminative learning model. Using a purpose-built test collection from Wikipedia tables, we demonstrate that the proposed approach delivers state-of-the-art performance.

</details>


### [60] [SmartTable: A Spreadsheet Program with Intelligent Assistance](https://arxiv.org/abs/1805.06353)
*Shuo Zhang,Vugar Abdul Zada,Krisztian Balog*

Main category: cs.IR

TL;DR: SmartTable 是一个具有智能辅助功能的在线电子表格应用程序，可以帮助用户填充表格中的实体（行）和扩展实体属性（列）。


<details>
  <summary>Details</summary>
Motivation: 本文介绍了 SmartTable，一个在线电子表格应用程序，旨在提供智能辅助功能，特别关注关系表，用于描述实体及其属性。

Method: SmartTable 应用程序的实现细节，该应用程序能够帮助用户填充表格中的实体（行）和扩展实体属性（列）。

Result: SmartTable 应用程序的实现已发布为开源项目，并且可以在 http://smarttable.cc 访问。

Conclusion: SmartTable 是一个在线电子表格应用程序，具有智能辅助功能，可以帮助用户填充表格和扩展属性。

Abstract: We introduce SmartTable, an online spreadsheet application that is equipped with intelligent assistance capabilities. With a focus on relational tables, describing entities along with their attributes, we offer assistance in two flavors: (i) for populating the table with additional entities (rows) and (ii) for extending it with additional entity attributes (columns). We provide details of our implementation, which is also released as open source. The application is available at http://smarttable.cc.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [61] [Buckaroo: A Direct Manipulation Visual Data Wrangler](https://arxiv.org/abs/2507.16073)
*Annabelle Warner,Andrew McNutt,Paul Rosen,El Kindi Rezig*

Main category: cs.HC

TL;DR: 数据整理是数据科学项目中最耗时（高达80%）且易出错的阶段，手动编码或使用电子表格效率低下。


<details>
  <summary>Details</summary>
Motivation: 手动数据整理耗时、易出错，可能影响下游任务的质量，需要更高效、更直观的解决方案。

Method: 提出Buckaroo，一个可视化系统，通过直接操作视觉对象来高亮显示数据差异并进行即时更正。Buckaroo能够自动发现异常数据组、推荐修复建议，并通过可视化展示纠正效果，支持撤销/重做操作。

Result: Buckaroo能够自动识别异常数据组，提供修复建议，并允许用户通过可视化界面进行数据操作。

Conclusion: Buckaroo通过可视化和直接操作，简化了数据整理过程，提高了效率和数据质量。

Abstract: Preparing datasets -- a critical phase known as data wrangling -- constitutes the dominant phase of data science development, consuming upwards of 80% of the total project time. This phase encompasses a myriad of tasks: parsing data, restructuring it for analysis, repairing inaccuracies, merging sources, eliminating duplicates, and ensuring overall data integrity. Traditional approaches, typically through manual coding in languages such as Python or using spreadsheets, are not only laborious but also error-prone. These issues range from missing entries and formatting inconsistencies to data type inaccuracies, all of which can affect the quality of downstream tasks if not properly corrected. To address these challenges, we present Buckaroo, a visualization system to highlight discrepancies in data and enable on-the-spot corrections through direct manipulations of visual objects. Buckaroo (1) automatically finds "interesting" data groups that exhibit anomalies compared to the rest of the groups and recommends them for inspection; (2) suggests wrangling actions that the user can choose to repair the anomalies; and (3) allows users to visually manipulate their data by displaying the effects of their wrangling actions and offering the ability to undo or redo these actions, which supports the iterative nature of data wrangling. A video companion is available at https://youtu.be/iXdCYbvpQVE

</details>


### [62] [SheetMind: An End-to-End LLM-Powered Multi-Agent Framework for Spreadsheet Automation](https://arxiv.org/abs/2506.12339)
*Ruiyan Zhu,Xi Cheng,Ke Liu,Brian Zhu,Daniel Jin,Neeraj Parihar,Zhoutian Xu,Oliver Gao*

Main category: cs.HC

TL;DR: SheetMind是一个基于LLM的多智能体框架，可以通过自然语言指令实现电子表格自动化。


<details>
  <summary>Details</summary>
Motivation: 本研究提出SheetMind，一个由大型语言模型（LLM）驱动的模块化多智能体框架，旨在通过自然语言指令实现电子表格自动化。

Method: SheetMind包含三个专门的智能体：经理智能体（分解复杂指令）、动作智能体（将指令翻译成结构化命令）和反思智能体（验证生成动作与用户意图的一致性）。该系统通过Google Sheets的Workspace扩展集成，支持实时交互，无需脚本或公式知识。

Result: 在基准数据集上的实验表明，SheetMind在单步任务上成功率达到80%，在多步指令上成功率约为70%，优于简化和基线版本。

Conclusion: 研究结果强调了多智能体分解和基于语法执行在连接自然语言和电子表格功能方面的有效性。

Abstract: We present SheetMind, a modular multi-agent framework powered by large language models (LLMs) for spreadsheet automation via natural language instructions. The system comprises three specialized agents: a Manager Agent that decomposes complex user instructions into subtasks; an Action Agent that translates these into structured commands using a Backus Naur Form (BNF) grammar; and a Reflection Agent that validates alignment between generated actions and the user's original intent. Integrated into Google Sheets via a Workspace extension, SheetMind supports real-time interaction without requiring scripting or formula knowledge. Experiments on benchmark datasets demonstrate an 80 percent success rate on single step tasks and approximately 70 percent on multi step instructions, outperforming ablated and baseline variants. Our results highlight the effectiveness of multi agent decomposition and grammar based execution for bridging natural language and spreadsheet functionalities.

</details>


### [63] ["How do you even know that stuff?": Barriers to expertise sharing among spreadsheet users](https://arxiv.org/abs/2506.09216)
*Qing,Xia,Advait Sarkar,Duncan Brumby,Anna Cox*

Main category: cs.HC

TL;DR: 电子表格的协作可以促进同事之间的学习和专业知识共享，但专业用户在知识传播方面面临挑战，这与社会规范、对专业知识价值的看法以及协作的潜在干扰有关。


<details>
  <summary>Details</summary>
Motivation: 探究社会规范和信念如何影响电子表格用户的专业知识共享行为，以及影响这些行为的因素。

Method: 对来自两个独立样本的31位专业电子表格用户进行了半结构化访谈。

Result: 电子表格的提供者在调整高度个性化的策略以适应主观标准和评估合适的社交时机方面面临挑战。此外，对自身专业知识的矛盾自我评价、对这些知识价值的轻视性规范信念以及对协作潜在干扰的担忧会进一步阻碍知识共享。

Conclusion: 技术设计和社会动态之间复杂的相互作用会影响功能丰富的软件中的协作学习行为，并为设计提供了启示，以应对此类挑战。

Abstract: Spreadsheet collaboration provides valuable opportunities for learning and expertise sharing between colleagues. Sharing expertise is essential for the retention of important technical skillsets within organisations, but previous studies suggest that spreadsheet experts often fail to disseminate their knowledge to others. We suggest that social norms and beliefs surrounding the value of spreadsheet use significantly influence user engagement in sharing behaviours. To explore this, we conducted 31 semi-structured interviews with professional spreadsheet users from two separate samples. We found that spreadsheet providers face challenges in adapting highly personalised strategies to often subjective standards and evaluating the appropriate social timing of sharing. In addition, conflicted self-evaluations of one's spreadsheet expertise, dismissive normative beliefs about the value of this knowledge, and concerns about the potential disruptions associated with collaboration can further deter sharing. We suggest these observations reflect the challenges of long-term learning in feature-rich software designed primarily with initial learnability in mind. We therefore provide implications for design to navigate this tension. Overall, our findings demonstrate how the complex interaction between technology design and social dynamics can shape collaborative learning behaviours in the context of feature-rich software.

</details>


### [64] [Prompting in the Dark: Assessing Human Performance in Prompt Engineering for Data Labeling When Gold Labels Are Absent](https://arxiv.org/abs/2502.11267)
*Zeyu He,Saniya Naphade,Ting-Hao 'Kenneth' Huang*

Main category: cs.HC

TL;DR: Prompting LLMs for data labeling without gold labels ('prompting in the dark') is unreliable, with many users failing to improve accuracy over iterations. Automated tools also struggle, highlighting the need for gold labels and careful design of automated support for prompt engineering.


<details>
  <summary>Details</summary>
Motivation: Investigate user performance and reliability in prompt engineering for LLM-powered data labeling, especially in a scenario without manually-labeled benchmarks ('prompting in the dark'), to understand the effectiveness of iterative prompting and the need for automated tools.

Method: Developed PromptingSheet, a Google Sheets add-on for iterative prompt-based data labeling. Conducted a study with 20 participants using this tool to assess labeling accuracy improvements over multiple prompt iterations. Also evaluated the performance of automated prompt optimization tools like DSPy in low-resource label scenarios.

Result: Prompting in the dark proved highly unreliable; only 9 out of 20 participants improved their labeling accuracy after four or more prompt iterations. Automated tools like DSPy also showed limited success when gold labels were scarce.

Conclusion: Iterative prompting without gold labels is unreliable for LLM-based data labeling. The study underscores the critical importance of gold labels and points to the challenges and potential risks associated with automated prompt optimization tools, suggesting careful consideration for future tool development in human prompt engineering.

Abstract: Millions of users prompt large language models (LLMs) for various tasks, but how good are people at prompt engineering? Do users actually get closer to their desired outcome over multiple iterations of their prompts? These questions are crucial when no gold-standard labels are available to measure progress. This paper investigates a scenario in LLM-powered data labeling, "prompting in the dark," where users iteratively prompt LLMs to label data without using manually-labeled benchmarks. We developed PromptingSheet, a Google Sheets add-on that enables users to compose, revise, and iteratively label data through spreadsheets. Through a study with 20 participants, we found that prompting in the dark was highly unreliable -- only 9 participants improved labeling accuracy after four or more iterations. Automated prompt optimization tools like DSPy also struggled when few gold labels were available. Our findings highlight the importance of gold labels and the needs, as well as the risks, of automated support in human prompt engineering, providing insights for future tool design.

</details>


### [65] [When Copilot Becomes Autopilot: Generative AI's Critical Risk to Knowledge Work and a Critical Solution](https://arxiv.org/abs/2412.15030)
*Advait Sarkar,Xiaotong,Xu,Neil Toronto,Ian Drosos,Christian Poelitz*

Main category: cs.HC

TL;DR: 生成式AI在电子表格工作流中既带来了风险（幻觉、批判性思维退化）也带来了机遇（提升非专家能力），核心挑战在于如何设计AI以促进而非削弱人类的批判性思维。我们提出了一个旨在培养批判性思维的电子表格原型系统，它利用AI生成筛选标准并提供“挑衅”性反馈来促使用户进行批判性评估，并提出了相关研究议程。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在知识工作中可能引入错误（幻觉），但也为用户（尤其是新手）提供了学习和应用高级软件功能、处理更复杂任务的巨大机会。因此，研究如何应对生成式AI对知识工作（以电子表格为例）的风险与机遇至关重要，特别是如何防止批判性思维的退化。

Method: 提出一个原型系统，用于电子表格中的批判性筛选活动。该系统利用生成式AI提出筛选标准，并据此对电子表格行进行排序。此外，它还会生成“挑衅”性文本片段，批判AI生成的标准，指出其风险、不足和替代方案。

Result: 开发了一个原型系统，能够利用生成式AI提出电子表格筛选标准，对数据进行排序，并生成批判性反馈（挑衅）。这为设计支持现代AI辅助知识工作的批判性思维工具开辟了一个新的设计空间。

Conclusion: 生成式AI在电子表格工作流中的最大风险并非其“幻觉”能力，而是可能削弱人类的批判性思维。设计生成式AI的界面，使其能够主动促进批判性思维至关重要。我们提出的原型系统及其“挑衅”机制是实现这一目标的一种方式，并为AI作为批评者或挑衅者的相关研究开辟了新方向。

Abstract: Generative AI, with its tendency to "hallucinate" incorrect results, may pose a risk to knowledge work by introducing errors. On the other hand, it may also provide unprecedented opportunities for users, particularly non-experts, to learn and apply advanced software features and greatly increase the scope and complexity of tasks they can successfully achieve.
  As an example of a complex knowledge workflow that is subject to risks and opportunities from generative AI, we consider the spreadsheet. AI hallucinations are an important challenge, but they are not the greatest risk posed by generative AI to spreadsheet workflows. Rather, as more work can be safely delegated to AI, the risk is that human critical thinking -- the ability to holistically and rigorously evaluate a problem and its solutions -- is degraded in the process. The solution is to design the interfaces of generative AI systems deliberately to foster and encourage critical thinking in knowledge work, building primarily on a long history of research on critical thinking tools for education.
  We discuss a prototype system for the activity of critical shortlisting in spreadsheets. The system uses generative AI to suggest shortlisting criteria and applies these criteria to sort rows in a spreadsheet. It also generates "provocations": short text snippets that critique the AI-generated criteria, highlighting risks, shortcomings, and alternatives. Our prototype opens up a rich and completely unexplored design space of critical thinking tools for modern AI-assisted knowledge work. We outline a research agenda for AI as a critic or provocateur, including questions about where and when provocations should appear, their form and content, and potential design trade-offs.

</details>


### [66] [Understanding and Evaluating Trust in Generative AI and Large Language Models for Spreadsheets](https://arxiv.org/abs/2412.14062)
*Simon Thorne*

Main category: cs.HC

TL;DR: Generative AI在生成公式时存在不准确和不可信的问题，本文提出了一个基于透明度和可信度的框架来评估公式的可靠性，并探讨了导致这些问题的因素和潜在后果。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和大型语言模型（LLMs）有潜力自动创建电子表格公式，但其输出可能存在不准确和不可信的问题，这主要是由幻觉、偏差和用户技能差异引起的。

Method: 提出一个信任度框架，通过可解释性、可见性、可靠性和道德考量来评估公式的透明度和可信度。框架同时分析了导致这些度量问题的因素，如幻觉、训练数据偏差和不正确的提示。

Result: 该框架通过可解释性（理解公式推理）和可见性（检查底层算法）来探索公式的透明度。通过可靠性（一致性和准确性）和道德考量（偏差和公平性）来评估生成公式的可信度。

Conclusion: 通过对信任度框架的分析，可以更好地理解和解决生成式AI在电子表格公式生成方面存在的问题，从而提高其准确性和可信度。

Abstract: Generative AI and Large Language Models (LLMs) hold promise for automating spreadsheet formula creation. However, due to hallucinations, bias and variable user skill, outputs obtained from generative AI cannot be assumed to be accurate or trustworthy. To address these challenges, a trustworthiness framework is proposed based on evaluating the transparency and dependability of the formula. The transparency of the formula is explored through explainability (understanding the formula's reasoning) and visibility (inspecting the underlying algorithms). The dependability of the generated formula is evaluated in terms of reliability (consistency and accuracy) and ethical considerations (bias and fairness). The paper also examines the drivers to these metrics in the form of hallucinations, training data bias and poorly constructed prompts. Finally, examples of mistrust in technology are considered and the consequences explored.

</details>


### [67] [Dynamic Prompt Middleware: Contextual Prompt Refinement Controls for Comprehension Tasks](https://arxiv.org/abs/2412.02357)
*Ian Drosos,Jack Williams,Advait Sarkar,Nicholas Wilson*

Main category: cs.HC

TL;DR: 用户在提示生成式AI时，尤其是在解释电子表格公式、Python代码和文本段落等理解任务时，常常难以提供足够的上下文。本研究通过设计和评估两种提示中间件方法（动态 PRC 和静态 PRC）来解决这一问题，旨在提升用户对AI生成解释的控制力。


<details>
  <summary>Details</summary>
Motivation: 为了解决用户在提示生成式AI进行理解任务时，难以提供足够上下文以及有效控制AI生成结果以满足个性化需求的问题。

Method: 通过对38名用户进行调查，发现用户在标准化支持和自适应支持之间存在权衡。在此基础上，设计并实现动态PRC（根据用户提示和需求生成上下文相关的UI元素）和静态PRC（提供预设的通用优化列表）两种提示中间件。随后，招募16名用户进行对照实验，评估这两种方法对用户控制AI生成解释能力的影响。

Result: 用户研究结果显示，动态PRC方法更受青睐，因为它提供了更强的控制力，降低了提供上下文的门槛，并鼓励用户进行任务探索和反思。然而，用户在理解不同优化设置对最终输出的影响方面仍存在挑战。

Conclusion: 动态提示中间件能够通过提供更大的控制力来改善生成式AI工作流程的用户体验，并引导用户获得更好的AI响应。未来的动态PRC系统设计应着重于增强用户对AI响应的控制能力，克服用户在理解优化效果方面的障碍。

Abstract: Effective prompting of generative AI is challenging for many users, particularly in expressing context for comprehension tasks such as explaining spreadsheet formulas, Python code, and text passages. Prompt middleware aims to address this barrier by assisting in prompt construction, but barriers remain for users in expressing adequate control so that they can receive AI-responses that match their preferences.
  We conduct a formative survey (n=38) investigating user needs for control over AI-generated explanations in comprehension tasks, which uncovers a trade-off between standardized but predictable support for prompting, and adaptive but unpredictable support tailored to the user and task. To explore this trade-off, we implement two prompt middleware approaches: Dynamic Prompt Refinement Control (Dynamic PRC) and Static Prompt Refinement Control (Static PRC). The Dynamic PRC approach generates context-specific UI elements that provide prompt refinements based on the user's prompt and user needs from the AI, while the Static PRC approach offers a preset list of generally applicable refinements.
  We evaluate these two approaches with a controlled user study (n=16) to assess the impact of these approaches on user control of AI responses for crafting better explanations. Results show a preference for the Dynamic PRC approach as it afforded more control, lowered barriers to providing context, and encouraged exploration and reflection of the tasks, but that reasoning about the effects of different generated controls on the final output remains challenging. Drawing on participant feedback, we discuss design implications for future Dynamic PRC systems that enhance user control of AI responses. Our findings suggest that dynamic prompt middleware can improve the user experience of generative AI workflows by affording greater control and guide users to a better AI response.

</details>


### [68] [Exploring Higher Education Competencies through Spreadsheet Self-Assessment and Time](https://arxiv.org/abs/2409.12974)
*Maria Csernoch,Judit T. Kiss,Viktor Takács,Domicián Máté*

Main category: cs.HC

TL;DR: 本文研究了大学生在电子表格方面的能力和可靠性，通过自我评估和实际问题解决来检验。研究发现，大学生倾向于不准确地评估自己的电子表格能力，并且在数字环境中完成任务所需的时间是纸质环境的两倍。这挑战了“数字原住民”无需计算机科学教育的假设，并强调了在高等教育中准确自我评估和时间管理的重要性。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是探究大学生在电子表格方面的能力和可靠性，并通过自我评估和实际问题解决实践来验证其数字技能。作者希望检验“数字原住民”在Excel操作上是否优于纸质操作的假设。

Method: 本研究通过自我评估和实际问题解决实践（包括纸质和Excel两种环境）来探究大学生在电子表格方面的能力和可靠性。

Result: 研究结果显示，大学生倾向于不准确地评估自己的电子表格能力，并且在数字环境中完成任务所需的时间是纸质环境的两倍。尽管如此，研究未能证实数字原住民在Excel操作上优于纸质操作的假设。

Conclusion: 本研究强调了在高等教育（尤其是在技术驱动的学科）中，准确的自我评估对于数字技能发展和时间管理至关重要。研究结果反驳了‘数字原住民天生具备计算机技能，无需额外教育’的普遍假设。

Abstract: The present paper aims to explore higher education students' spreadsheet competencies and reliability through self-assessment and real-world problem-solving practices. Digital natives alleged skills and competences allowed us to hypothesize that students perform better in Excel than on paper, but the findings cannot confirm this hypothesis. However, our results indicate that students tend to inaccurately assess their spreadsheet competencies compared to their actual performance in both paper-based and Excel tasks. It has also be found that students need at least twice as much time to achieve the same high scores in the digital environment as they do on paper. The results violated the widely accepted assumption that digital native students do not need computer science education, since they are born with it. This study highlights the importance of accurate self-assessment in digital skill development and time management within higher education contexts, particularly in technology-driven disciplines.

</details>


### [69] [Supporting Annotators with Affordances for Efficiently Labeling Conversational Data](https://arxiv.org/abs/2403.07762)
*Austin Z. Henley,David Piorkowski*

Main category: cs.HC

TL;DR: CAL是一个新颖的数据标注接口，旨在减少标注工作量和繁琐性，通过防止不当标注、提供引导、整合文档和方便地查看历史标注，降低了用户的认知负荷，且不增加任务时间，用户体验更佳。


<details>
  <summary>Details</summary>
Motivation: 当前基于机器学习的系统广泛应用，但需要大量标注数据，而众包标注成本高昂且耗时。

Method: 设计并实现了一个名为CAL（Conversational AI Labeling）的数据标注接口，该接口具备防止不当标注、提供标签选择引导、整合标注文档以及提供高效的历史标注查看等功能。通过用户研究，将CAL与标准电子表格进行比较。

Result: 用户研究表明，使用CAL的用户报告的认知负荷较低，任务时间并未增加，用户认为CAL更易于使用，并且用户普遍偏爱CAL而非标准电子表格。

Conclusion: CAL在数据标注任务中优于传统的电子表格，能够有效降低用户认知负荷，提高用户体验，且不牺牲效率。

Abstract: Without well-labeled ground truth data, machine learning-based systems would not be as ubiquitous as they are today, but these systems rely on substantial amounts of correctly labeled data. Unfortunately, crowdsourced labeling is time consuming and expensive. To address the concerns of effort and tedium, we designed CAL, a novel interface to aid in data labeling. We made several key design decisions for CAL, which include preventing inapt labels from being selected, guiding users in selecting an appropriate label when they need assistance, incorporating labeling documentation into the interface, and providing an efficient means to view previous labels. We implemented a production-quality implementation of CAL and report a user-study evaluation that compares CAL to a standard spreadsheet. Key findings of our study include users using CAL reported lower cognitive load, did not increase task time, users rated CAL to be easier to use, and users preferred CAL over the spreadsheet.

</details>


### [70] [Prompting for Discovery: Flexible Sense-Making for AI Art-Making with Dreamsheets](https://arxiv.org/abs/2310.09985)
*Shm Garanganao Almeda,J. D. Zamfirescu-Pereira,Kyu Won Kim,Pradeep Mani Rathnam,Bjoern Hartmann*

Main category: cs.HC

TL;DR: DreamSheets是一个电子表格界面，利用LLM函数辅助用户进行文本到图像（TTI）模型提示词的构建和结果的展示，以支持用户探索TTI模型的设计空间。


<details>
  <summary>Details</summary>
Motivation: 在文本到图像（TTI）模型的提示词工程中，用户需要导航一个庞大且不透明的输入空间（超参数和提示词文本）来获得理想的图像输出。然而，提示词的微小改动可能导致输出结果的巨大差异。因此，如何设计用户界面来帮助终端用户可靠地引导提示词空间探索，以获得有趣的图像结果，是一个关键问题。

Method: 本研究提出了一种名为DreamSheets的设计工具，它集成在电子表格界面中。DreamSheets利用大型语言模型（LLM）函数来辅助用户构建提示词，并同时展示生成的图像结果。这种设计支持用户自定义工作流程，并通过灵活的布局和新颖的生成函数来促进实验。

Result: 通过一项初步的实验室研究和一项包含五位专业艺术家的纵向研究，本研究揭示了用户在应对TTI设计空间探索挑战时所采用的一系列策略。研究还确定了支持这些策略所需的界面功能，例如利用文本生成来定义局部“探索轴”。

Conclusion: 本研究提炼了上述研究发现，并将其转化为一个用户界面模型，以指导未来TTI模型设计工具的开发。

Abstract: Design space exploration (DSE) for Text-to-Image (TTI) models entails navigating a vast, opaque space of possible image outputs, through a commensurately vast input space of hyperparameters and prompt text. Minor adjustments to prompt input can surface unexpectedly disparate images. How can interfaces support end-users in reliably steering prompt-space explorations towards interesting results? Our design probe, DreamSheets, supports exploration strategies with LLM-based functions for assisted prompt construction and simultaneous display of generated results, hosted in a spreadsheet interface. The flexible layout and novel generative functions enable experimentation with user-defined workflows. Two studies, a preliminary lab study and a longitudinal study with five expert artists, revealed a set of strategies participants use to tackle the challenges of TTI design space exploration, and the interface features required to support them - like using text-generation to define local "axes" of exploration. We distill these insights into a UI mockup to guide future interfaces.

</details>


### [71] [Does Using ChatGPT Result in Human Cognitive Augmentation?](https://arxiv.org/abs/2401.11042)
*Ron Fulbright,Miranda Morrison*

Main category: cs.HC

TL;DR: ChatGPT can mislead users, resulting in negative cognitive augmentation for certain tasks, and does not yet replace human judgment.


<details>
  <summary>Details</summary>
Motivation: Investigate human cognitive augmentation due to using ChatGPT by comparing responses created using ChatGPT with those created without it.

Method: Conducted two experiments comparing responses generated with and without ChatGPT.

Result: Using ChatGPT does not always lead to cognitive augmentation and does not yet replace human judgment, discernment, and evaluation in certain tasks. ChatGPT was observed to mislead users, resulting in negative cognitive augmentation.

Conclusion: Human cognitive performance is enhanced by tools, but recent AI like ChatGPT does not always augment human cognition and can even be misleading in certain tasks, highlighting the continued importance of human judgment.

Abstract: Human cognitive performance is enhanced by the use of tools. For example, a human can produce a much greater, and more accurate, volume of mathematical calculation in a unit of time using a calculator or a spreadsheet application on a computer. Such tools have taken over the burden of lower level cognitive grunt work but the human still serves the role of the expert performing higher level thinking and reasoning. Recently, however, unsupervised, deep, machine learning has produced cognitive systems able to outperform humans in several domains. When humans use these tools in a human cog ensemble, the cognitive ability of the human is augmented. In some cases, even non experts can achieve, and even exceed, the performance of experts in a particular domain, synthetic expertise. A new cognitive system, ChatGPT, has burst onto the scene during the past year. This paper investigates human cognitive augmentation due to using ChatGPT by presenting the results of two experiments comparing responses created using ChatGPT with results created not using ChatGPT. We find using ChatGPT does not always result in cognitive augmentation and does not yet replace human judgement, discernment, and evaluation in certain types of tasks. In fact, ChatGPT was observed to result in misleading users resulting in negative cognitive augmentation.

</details>


### [72] [Co-audit: tools to help humans double-check AI-generated content](https://arxiv.org/abs/2310.01297)
*Andrew D. Gordon,Carina Negreanu,José Cambronero,Rasika Chakravarthy,Ian Drosos,Hao Fang,Bhaskar Mitra,Hannah Richardson,Advait Sarkar,Stephanie Simmons,Jack Williams,Ben Zorn*

Main category: cs.HC

TL;DR: LLMs的复杂输出使得用户难以审计，因此需要工具辅助用户进行二次审计，尤其是在电子表格等对质量和准确性要求高的场景。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs生成日益复杂的输出（如摘要、表格、代码），用户难以审计其质量或正确性，因此需要工具来辅助用户进行二次审计。

Method: 本文描述了用于电子表格计算的AI生成内容的二次审计工具的研究，并提出了一套二次审计的原则。

Result: 介绍了用于电子表格计算的二次审计工具的研究，并讨论了其重要性。

Conclusion: 二次审计工具对于任何对质量要求高、错误后果严重的生成AI应用（如电子表格计算）都至关重要，并提出了一套二次审计的原则和未来研究挑战。

Abstract: Users are increasingly being warned to check AI-generated content for correctness. Still, as LLMs (and other generative models) generate more complex output, such as summaries, tables, or code, it becomes harder for the user to audit or evaluate the output for quality or correctness. Hence, we are seeing the emergence of tool-assisted experiences to help the user double-check a piece of AI-generated content. We refer to these as co-audit tools. Co-audit tools complement prompt engineering techniques: one helps the user construct the input prompt, while the other helps them check the output response. As a specific example, this paper describes recent research on co-audit tools for spreadsheet computations powered by generative models. We explain why co-audit experiences are essential for any application of generative AI where quality is important and errors are consequential (as is common in spreadsheet computations). We propose a preliminary list of principles for co-audit, and outline research challenges.

</details>


### [73] ["What It Wants Me To Say": Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models](https://arxiv.org/abs/2304.06597)
*Michael Xieyang Liu,Advait Sarkar,Carina Negreanu,Ben Zorn,Jack Williams,Neil Toronto,Andrew D. Gordon*

Main category: cs.HC

TL;DR: 代码生成大语言模型可以将自然语言转换为代码，但只有一小部分自然语言可以有效地指导代码生成。对于非专业终端用户程序员来说，学习这一点是抽象匹配的挑战。我们在电子表格数据分析的特定环境中检查了这个挑战，该系统使用Codex生成器将用户的自然语言查询映射到Python代码，执行代码并显示结果。我们提出了基于现实的抽象匹配，它通过将代码转换回系统化和可预测的自然语言来弥合抽象鸿沟。在一个涉及24名参与者的被试间“出声思考”研究中，我们将基于现实的抽象匹配与基于先前已建立的查询构建原则的非基于现实的替代方法进行了比较。我们发现，基于现实的方法提高了终端用户对代码生成模型的作用域和能力的理解，以及有效使用它所需的语言类型。


<details>
  <summary>Details</summary>
Motivation: 代码生成大语言模型在将自然语言转换为代码方面存在挑战，特别是对于非专业用户而言，学习如何有效地引导代码生成是关键。该研究旨在解决“抽象匹配”问题，即用户理解和模型生成代码之间的差距。

Method: 提出了一种名为“基于现实的抽象匹配”的方法，该方法将代码转换回系统化、可预测的自然语言，以弥合用户理解和模型生成之间的差距。通过一项包含24名参与者的“出声思考”研究，将此方法与一种基于现有查询构建原则的“非基于现实”方法进行了比较。

Result: 基于现实的抽象匹配方法在研究中被证明可以提高终端用户对代码生成模型的作用域和能力的理解，并帮助他们了解生成有效代码所需的语言。

Conclusion: 基于现实的抽象匹配方法是一种有效的方式，可以提高非专业用户在使用代码生成大语言模型进行数据分析等任务时的理解和效率。

Abstract: Code-generating large language models translate natural language into code. However, only a small portion of the infinite space of naturalistic utterances is effective at guiding code generation. For non-expert end-user programmers, learning this is the challenge of abstraction matching. We examine this challenge in the specific context of data analysis in spreadsheets, in a system that maps the users natural language query to Python code using the Codex generator, executes the code, and shows the result. We propose grounded abstraction matching, which bridges the abstraction gap by translating the code back into a systematic and predictable naturalistic utterance. In a between-subjects, think-aloud study (n=24), we compare grounded abstraction matching to an ungrounded alternative based on previously established query framing principles. We find that the grounded approach improves end-users' understanding of the scope and capabilities of the code-generating model, and the kind of language needed to use it effectively.

</details>


### [74] [Team OS's System for Dialogue Robot Competition 2022](https://arxiv.org/abs/2210.09928)
*Yuki Kubo,Ryo Yanagimoto,Hayato Futase,Mikio Nakano,Zhaojie Luo,Kazunori Komatani*

Main category: cs.HC

TL;DR: OSbot 是一个基于状态转换的对话机器人系统，通过关键词提取和情感分析来驱动对话流程，并在 2022 年对话机器人竞赛中获得第三名。


<details>
  <summary>Details</summary>
Motivation: 为对话机器人竞赛 2022 开发一个对话机器人系统。

Method: 该对话机器人系统（OSbot）采用基于状态转换的对话流程，并通过关键词提取和情感分析的结果来触发状态转换。关键词提取基于命名实体提取和预定义的关键词集。情感分析是基于文本的，使用支持向量机（SVM）模型，该模型使用多模态对话语料库 Hazumi 进行训练。对话流程可以通过电子表格进行管理和编辑，并提供日志记录功能以方便检查和修改。

Result: 在竞赛的初步评审环节中，该系统取得了第三名的成绩。

Conclusion: OSbot 系统通过结合手动定义的状态转换、关键词提取和情感分析，并提供易于管理的界面，在对话机器人竞赛中取得了不错的成绩。

Abstract: This paper describes our dialogue robot system, OSbot, developed for Dialogue Robot Competition 2022. The dialogue flow is based on state transitions described manually and the transition conditions use the results of keyword extraction and sentiment analysis. The transitions can be easily viewed and edited by managing them on a spreadsheet. The keyword extraction is based on named entity extraction and our predefined keyword set. The sentiment analysis is text-based and uses SVM, which was trained with the multimodal dialogue corpus Hazumi. We quickly checked and edited a dialogue flow by using a logging function. In the competition's preliminary round, our system ended up in third place.

</details>


### [75] [What is it like to program with artificial intelligence?](https://arxiv.org/abs/2208.06213)
*Advait Sarkar,Andrew D. Gordon,Carina Negreanu,Christian Poelitz,Sruti Srinivasa Ragavan,Ben Zorn*

Main category: cs.HC

TL;DR: LLM 辅助编程是一种新的编程范式，它借鉴了现有的一些编程辅助概念，但也存在根本性的不同，并带来了独特的挑战，尤其是在面向非专业用户时。


<details>
  <summary>Details</summary>
Motivation: 探讨 LLM 辅助编程与以往程序员辅助概念的异同，并识别其在面向终端用户编程时的潜在问题和研究挑战。

Method: 借鉴公开的 LLM 辅助编程经验报告以及先前可用性和设计研究，并结合一项用户研究，观察非专业终端用户程序员如何使用 LLM 辅助工具解决电子表格中的数据任务。

Result: LLM 辅助编程在技术可能性和实践体验上与编译、结对编程、通过搜索和重用进行编程等方式存在根本性差异，应被视为一种新的编程方式。研究还指出了在将 LLM 应用于非专业用户编程时可能出现的问题。

Conclusion: LLM 辅助编程是一种新的编程范式，具有其独特的属性和挑战。在将 LLM 应用于终端用户编程时，尤其是在用户几乎没有编程经验的情况下，需要特别关注潜在问题和开放性研究挑战。

Abstract: Large language models, such as OpenAI's codex and Deepmind's AlphaCode, can generate code to solve a variety of problems expressed in natural language. This technology has already been commercialised in at least one widely-used programming editor extension: GitHub Copilot.
  In this paper, we explore how programming with large language models (LLM-assisted programming) is similar to, and differs from, prior conceptualisations of programmer assistance. We draw upon publicly available experience reports of LLM-assisted programming, as well as prior usability and design studies. We find that while LLM-assisted programming shares some properties of compilation, pair programming, and programming via search and reuse, there are fundamental differences both in the technical possibilities as well as the practical experience. Thus, LLM-assisted programming ought to be viewed as a new way of programming with its own distinct properties and challenges.
  Finally, we draw upon observations from a user study in which non-expert end user programmers use LLM-assisted tools for solving data tasks in spreadsheets. We discuss the issues that might arise, and open research challenges, in applying large language models to end-user programming, particularly with users who have little or no programming expertise.

</details>


### [76] [MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization](https://arxiv.org/abs/2209.05739)
*Lu Ying,Xinhuan Shu,Dazhen Deng,Yuchen Yang,Tan Tang,Lingyun Yu,Yingcai Wu*

Main category: cs.HC

TL;DR: MetaGlyph是一个自动生成隐喻图形可视化（MGV）的系统，通过分析在线资源中的隐喻图像并结合蒙特卡洛树搜索算法来设计图形，支持用户自定义编辑，并通过案例、场景和专家访谈验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 创建隐喻图形可视化（MGV）需要深厚的数据理解和专业设计技能，而MetaGlyph旨在自动化这一过程。

Method: MetaGlyph通过隐喻图像选择和MGV构建框架来生成MGV。它首先根据输入数据语义从在线资源中选择具有相应图像的隐喻，然后集成蒙特卡洛树搜索算法，根据数据重要性、语义相关性和图形不重叠性来探索视觉元素与数据维度的关联设计。系统还提供编辑反馈以支持用户自定义。

Result: MetaGlyph系统能够根据输入数据自动选择隐喻图像，并利用蒙特卡洛树搜索算法生成与之匹配的图形可视化。系统还支持用户通过编辑反馈进行自定义调整。

Conclusion: MetaGlyph成功地展示了其在自动生成隐喻图形可视化方面的能力，并通过多种方式验证了其有效性，为用户提供了一种更便捷的创建MGV的方法。

Abstract: Glyph-based visualization achieves an impressive graphic design when associated with comprehensive visual metaphors, which help audiences effectively grasp the conveyed information through revealing data semantics. However, creating such metaphoric glyph-based visualization (MGV) is not an easy task, as it requires not only a deep understanding of data but also professional design skills. This paper proposes MetaGlyph, an automatic system for generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct a qualitative analysis to understand the design of current MGVs from the perspectives of metaphor embodiment and glyph design. Based on the results, we introduce a novel framework for generating MGVs by metaphoric image selection and an MGV construction. Specifically, MetaGlyph automatically selects metaphors with corresponding images from online resources based on the input data semantics. We then integrate a Monte Carlo tree search algorithm that explores the design of an MGV by associating visual elements with data dimensions given the data importance, semantic relevance, and glyph non-overlap. The system also provides editing feedback that allows users to customize the MGVs according to their design preferences. We demonstrate the use of MetaGlyph through a set of examples, one usage scenario, and validate its effectiveness through a series of expert interviews.

</details>


### [77] [PoVRPoint: Authoring Presentations in Mobile Virtual Reality](https://arxiv.org/abs/2201.06337)
*Verena Biener,Travis Gesslein,Daniel Schneider,Felix Kawala,Alexander Otte,Per Ola Kristensson,Michel Pahud,Eyal Ofek,Cuauhtli Campos,Matjaž Kljun,Klen Čopič Pucihar,Jens Grubert*

Main category: cs.HC

TL;DR: 该论文提出了一种名为PoVRPoint的工具集，用于在移动设备（如平板电脑）上结合VR技术进行演示文稿创作，以增强移动知识工作者的效率。


<details>
  <summary>Details</summary>
Motivation: 为了探索VR在移动场景下支持知识工作（特别是演示文稿创作）的设计空间，借鉴了先前在VR中进行文本输入和电子表格交互的研究。

Method: 提出PoVRPoint工具集，该工具集将移动设备上的笔和触摸输入与VR的空间交互能力相结合，用于演示文稿的编辑。通过用户研究，评估了VR提供的扩展显示空间在识别目标幻灯片、空间对象操作、动画创建和多重遮挡对象排列等方面的效用。

Result: 研究结果表明，VR的宽阔视野显著提高了目标幻灯片的识别速度（与仅使用平板电脑相比），并且VR的三维视图在处理遮挡情况下的对象重新排序方面比基线界面更快。用户研究证实了所提出的交互技术的可用性和趣味性。

Conclusion: PoVRPoint工具集通过利用VR提供的扩展显示空间和空间交互能力，能够有效地支持移动设备上的演示文稿创作，并提供比传统方法更好的用户体验。

Abstract: Virtual Reality (VR) has the potential to support mobile knowledge workers by complementing traditional input devices with a large three-dimensional output space and spatial input. Previous research on supporting VR knowledge work explored domains such as text entry using physical keyboards and spreadsheet interaction using combined pen and touch input. Inspired by such work, this paper probes the VR design space for authoring presentations in mobile settings. We propose PoVRPoint -- a set of tools coupling pen- and touch-based editing of presentations on mobile devices, such as tablets, with the interaction capabilities afforded by VR. We study the utility of extended display space to, for example, assist users in identifying target slides, supporting spatial manipulation of objects on a slide, creating animations, and facilitating arrangements of multiple, possibly occluded, shapes. Among other things, our results indicate that 1) the wide field of view afforded by VR results in significantly faster target slide identification times compared to a tablet-only interface for visually salient targets; and 2) the three-dimensional view in VR enables significantly faster object reordering in the presence of occlusion compared to two baseline interfaces. A user study further confirmed that the interaction techniques were found to be usable and enjoyable.

</details>


### [78] [Untidy Data: The Unreasonable Effectiveness of Tables](https://arxiv.org/abs/2106.15005)
*Lyn Bartram,Michael Correll,Melanie Tory*

Main category: cs.HC

TL;DR: Data tables, especially spreadsheets, are crucial for data workers beyond initial cleanup, enabling direct interaction and reasoning throughout the analysis process. These interactions, including reorganization, markup, and layering, are vital for understanding data meaning and potential. Interactive tables should be recognized as a key visualization and design space for visual analytics.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand how data workers, who use data in their jobs but aren't professional analysts, interact with and reason about data using tables, challenging the view of tables as merely a preparatory step.

Method: The study involved a qualitative analysis of how data workers interact with and reason about their data in tables.

Result: Data tables serve a purpose beyond initial data cleanup; users engage with them throughout the analysis process by reorganizing, marking up, and layering data. These direct interactions with human-readable tables are important for building understanding.

Conclusion: Interactive tables are a significant visualization idiom, offering valuable design opportunities for visual analytics and enriching sensemaking through flexible human-data interaction.

Abstract: Working with data in table form is usually considered a preparatory and tedious step in the sensemaking pipeline; a way of getting the data ready for more sophisticated visualization and analytical tools. But for many people, spreadsheets -- the quintessential table tool -- remain a critical part of their information ecosystem, allowing them to interact with their data in ways that are hidden or abstracted in more complex tools. This is particularly true for data workers: people who work with data as part of their job but do not identify as professional analysts or data scientists. We report on a qualitative study of how these workers interact with and reason about their data. Our findings show that data tables serve a broader purpose beyond data cleanup at the initial stage of a linear analytic flow: users want to see and "get their hands on" the underlying data throughout the analytics process, reshaping and augmenting it to support sensemaking. They reorganize, mark up, layer on levels of detail, and spawn alternatives within the context of the base data. These direct interactions and human-readable table representations form a rich and cognitively important part of building understanding of what the data mean and what they can do with it. We argue that interactive tables are an important visualization idiom in their own right; that the direct data interaction they afford offers a fertile design space for visual analytics; and that sense making can be enriched by more flexible human-data interaction than is currently supported in visual analytics tools.

</details>


### [79] [Defining and Adopting an End User Computing Policy: A Case Study](https://arxiv.org/abs/1909.00855)
*Roger Turner*

Main category: cs.HC

TL;DR: 用户计算策略的更新带来风险，但通过风险评估应用程序和基于风险的方法得到控制。


<details>
  <summary>Details</summary>
Motivation: End User Computing (EUC) 存在重大风险，需要进行控制。本文研究了 Wesleyan Assurance Society 更新 EUC 策略的过程，旨在解决这些风险。

Method: 开发了一个 EUC 风险评估应用程序，根据复杂性、重要性和控制情况（或缺乏控制）来计算风险等级。该策略采用基于风险的方法，优先处理和缓解最高风险。

Result: 成功实施了更新后的 EUC 策略，并克服了在实施过程中遇到的各种挑战。

Conclusion: 通过采用基于风险的方法和风险评估工具，EUC 策略的更新能够有效识别、评估和缓解相关风险，从而在 Wesleyan Assurance Society 实现了更有效的风险管理。

Abstract: End User Computing carries significant risks if not well controlled. This paper is a case study of the introduction of an updated End User Computing policy at the Wesleyan Assurance Society. The paper outlines the plan and identifies various challenges. The paper explains how these challenges were overcome. We wrote an End User Computing Risk Assessment Application which calculates a risk rating band based on the Complexity, Materiality and Control (or lack of it) pertaining to any given application and the basis of assessment is given in this paper. The policy uses a risk based approach for assessing and mitigating against the highest risks first and obtaining the quickest benefit.

</details>


### [80] [Calliope: Automatic Visual Data Story Generation from a Spreadsheet](https://arxiv.org/abs/2010.09975)
*Danqing Shi,Xinyue Xu,Fuling Sun,Yang Shi,Nan Cao*

Main category: cs.HC

TL;DR: Calliope是一个新的视觉数据故事生成系统，可以从电子表格自动生成视觉数据故事，并通过在线编辑器进行编辑。


<details>
  <summary>Details</summary>
Motivation: 视觉数据故事（如海报或数据视频）虽然有助于理解和记忆，但由于技术门槛（数据分析、可视化、脚本），生成过程困难且低效。

Method: Calliope采用一种新的面向逻辑的蒙特卡洛树搜索算法来探索数据空间，逐步生成故事片段（数据事实），并根据信息论的重要性对它们进行排序。每个数据事实都配有图表和自动生成的描述。

Result: 通过三个示例故事、两次对照实验和10位领域专家的访谈评估，证明Calliope有助于高效生成视觉数据故事。

Conclusion: Calliope通过自动化过程和易于使用的编辑器，显著提高了视觉数据故事的生成效率。

Abstract: Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.

</details>


### [81] [Pen-based Interaction with Spreadsheets in Mobile Virtual Reality](https://arxiv.org/abs/2008.04543)
*Travis Gesslein,Verena Biener,Philipp Gagel,Daniel Schneider,Per Ola Kristensson,Eyal Ofek,Michel Pahud,Jens Grubert*

Main category: cs.HC

TL;DR: VR可以增强移动知识工作，特别是电子表格应用程序的显示和交互，但将其应用于移动设备交互的探索尚不深入。


<details>
  <summary>Details</summary>
Motivation: 随着移动设备普及，电子表格的交互性挑战日益凸显，尤其是在空间受限的移动场景下，VR在其中具有巨大潜力。

Method: 提出了一套结合VR头显和笔式输入的工具集，用于增强平板电脑上的电子表格交互，利用VR空间扩展屏幕显示，并结合精确的笔输入和空间感知，实现屏幕外菜单、依赖关系可视化、注视-触摸切换等功能。

Result: 通过视频在线调查和专家评估，研究了所提出工具集的可行性以及对人类工作潜力的影响。

Conclusion: 结合VR和笔式输入为提高电子表格交互的生产力提供了新的可能性，尤其是在移动场景下，通过扩展可视空间和提供更丰富的交互方式。

Abstract: Virtual Reality (VR) can enhance the display and interaction of mobile knowledge work and in particular, spreadsheet applications. While spreadsheets are widely used yet are challenging to interact with, especially on mobile devices, using them in VR has not been explored in depth. A special uniqueness of the domain is the contrast between the immersive and large display space afforded by VR, contrasted by the very limited interaction space that may be afforded for the information worker on the go, such as an airplane seat or a small work-space. To close this gap, we present a tool-set for enhancing spreadsheet interaction on tablets using immersive VR headsets and pen-based input. This combination opens up many possibilities for enhancing the productivity for spreadsheet interaction. We propose to use the space around and in front of the tablet for enhanced visualization of spreadsheet data and meta-data. For example, extending sheet display beyond the bounds of the physical screen, or easier debugging by uncovering hidden dependencies between sheet's cells. Combining the precise on-screen input of a pen with spatial sensing around the tablet, we propose tools for the efficient creation and editing of spreadsheets functions such as off-the-screen layered menus, visualization of sheets dependencies, and gaze-and-touch-based switching between spreadsheet tabs. We study the feasibility of the proposed tool-set using a video-based online survey and an expert-based assessment of indicative human performance potential.

</details>


### [82] [EQUS -- helping to see formulae](https://arxiv.org/abs/2007.00003)
*Chris Roast*

Main category: cs.HC

TL;DR: EQUS是一个交互式可视化工具，用于简化电子表格公式，帮助用户理解复杂数据。它已被证明对广大电子表格用户具有广泛的适用性，并已作为MS Excel的插件进行开发。


<details>
  <summary>Details</summary>
Motivation: 电子表格公式易于被用户误解，而它们被广泛应用于数值处理和建模，因此需要一个可视化工具来帮助理解。

Method: 通过与目标用户（中学生）的互动，并经过反复设计和评估来开发EQUS。

Result: EQUS可视化技术已被证明对更广泛的电子表格用户群体具有相关性。

Conclusion: EQUS的开发和评估表明，可视化技术可以有效帮助用户理解复杂的电子表格公式，并且该工具已成功集成到MS Excel中。

Abstract: Visualisation is often presented as a means of simplifying information and helping people understand complex data. In this paper we describe the design, development and evaluation of an interactive visualisation for spreadsheet formulae (EQUS). The work is justified on the grounds that these are widely used tools for significant numerical processing and modeling, yet the formula developed can be easily misunderstood. The development process was one of iterative refinement engaging an initial target audience of mid-teen learners, involving re-design and formative evaluation. The resulting visualisation techniques have been found to be broadly relevant to spreadsheet users beyond the initial target audience. EQUS has since been developed as fully integrated plug-in for MS Excel.

</details>


### [83] [Taggle: Combining Overview and Details in Tabular Data Visualizations](https://arxiv.org/abs/1712.05944)
*Katarina Furmanova,Samuel Gratzl,Holger Stitz,Thomas Zichner,Miroslava Jaresova,Alexander Lex,Marc Streit*

Main category: cs.HC

TL;DR: Taggle是一种以数据项为中心的可视化技术，用于探索和呈现大型复杂表格，它通过数据驱动的聚合和交互方法来辅助分析。


<details>
  <summary>Details</summary>
Motivation: 现有的表格数据可视化技术大多关注概览，但实际分析任务常涉及对特定数据项的深入研究，并将这些项与整个数据集联系起来。因此，需要一种能够关注单个数据项并能将其与大数据集关联的可视化技术。

Method: Taggle 采用类似电子表格的方法，对数据源中的每一行进行单独可视化，并使用单元格的视觉编码。同时，它引入了数据驱动的数据子集聚合方法，并辅以专门用于回答特定分析问题（如多列排序、丰富的数据选择和过滤）的交互方法。

Result: 通过一个由领域专家在药物发现的复杂基因组数据分析领域进行的案例研究，证明了 Taggle 的有效性。

Conclusion: Taggle 能够有效地处理大型复杂表格，并通过其以数据项为中心的可视化、数据驱动的聚合以及定制的交互方法，满足了数据项的深入研究和关联分析的需求。

Abstract: Most tabular data visualization techniques focus on overviews, yet many practical analysis tasks are concerned with investigating individual items of interest. At the same time, relating an item to the rest of a potentially large table is important. In this work we present Taggle, a tabular visualization technique for exploring and presenting large and complex tables. Taggle takes an item-centric, spreadsheet-like approach, visualizing each row in the source data individually using visual encodings for the cells. At the same time, Taggle introduces data-driven aggregation of data subsets. The aggregation strategy is complemented by interaction methods tailored to answer specific analysis questions, such as sorting based on multiple columns and rich data selection and filtering capabilities. We demonstrate Taggle using a case study conducted by a domain expert on complex genomics data analysis for the purpose of drug discovery.

</details>


### [84] [Are digital natives spreadsheet natives?](https://arxiv.org/abs/1909.00865)
*Maria Csernoch,Piroska Biró*

Main category: cs.HC

TL;DR: 即使是自认为是“数字原住民”的计算机专业一年级学生，在电子表格环境中也存在算法技能和知识迁移能力方面的显著困难，他们需要接受正式的、基于算法的、高水平的数学培训。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在测试计算机专业一年级学生在电子表格环境中的算法技能和知识迁移能力，尽管他们已完成相关培训且被认为是“数字原住民”。

Method: 对计算机专业一年级学生在电子表格环境中解决问题的能力进行测试，并分析其结果，重点关注算法技能、计算思维和知识迁移能力。

Result: 学生在解决电子表格问题时遇到严重困难。使用基于算法的多层数组公式的学生比使用特定函数且不相关的内置函数学生表现更好。这表明学生在算法技能和知识迁移方面存在普遍不足。

Conclusion: 所有学生，无论其年龄或数字代别，都需要接受由专家教师进行的正规的、高水平的、基于算法的培训。

Abstract: The present paper reports the results of testing first year students of Informatics on their algorithmic skills and knowledge transfer abilities in spreadsheet environments. The selection of students plays a crucial role in the project. On the one hand, they have officially finished their spreadsheet training - they know everything - while on the other hand, they do not need any training, since they are digital natives, to whom digital skills are assigned by birth. However, we found that the students had serious difficulties in solving the spreadsheet problems presented: so low were their results that it allowed us to form broad tendencies. Considering computational thinking, algorithmic skills, and knowledge transfer abilities, it is clear that those students performed better who used algorithm-based, multilevel array formulas instead of problem specific, unconnected built-in functions. Furthermore, we can conclude that students, regardless of their birth date and digital generation assigned to them, are in great need of official, high-mathability, algorithm-based training with expert teachers.

</details>


### [85] [Somewhere Around That Number: An Interview Study of How Spreadsheet Users Manage Uncertainty](https://arxiv.org/abs/1905.13072)
*Judith Borghouts,Andrew D. Gordon,Advait Sarkar,Kenton P. O'Hara,Neil Toronto*

Main category: cs.HC

TL;DR: 用户在处理包含不确定性的电子表格数据时，常采用非正式的启发式方法，导致结论不准确。用户倾向于忽略或简化不确定性。研究发现，用户处理不确定性的方式受电子表格在工作中的作用及用户目标的影响。电子表格被用作数据库、模板、计算工具、记事本和探索工具。用户的目标包括计算和比较不同场景、理解不确定性的性质，以及将数据不确定性的复杂性简化呈现给决策者。


<details>
  <summary>Details</summary>
Motivation: 了解用户在电子表格中如何处理和应对数据不确定性。

Method: 通过对11位来自不同领域的电子表格用户进行访谈。

Result: 用户处理不确定性的方式受到电子表格在工作中扮演的角色以及用户的目标的影响。电子表格可被用作数据库、模板、计算工具、记事本和探索工具。用户的主要目标是计算和比较不同场景，理解不确定性的性质，并将复杂的数据不确定性简化为易于理解的格式呈现给他人（通常是决策者）。

Conclusion: 现有的电子表格工具在支持用户处理不确定性方面的功能有限，用户通常需要采取变通方法来应对。

Abstract: Spreadsheet users regularly deal with uncertainty in their data, for example due to errors and estimates. While an insight into data uncertainty can help in making better informed decisions, prior research suggests that people often use informal heuristics to reason with probabilities, which leads to incorrect conclusions. Moreover, people often ignore or simplify uncertainty. To understand how people currently encounter and deal with uncertainty in spreadsheets, we conducted an interview study with 11 spreadsheet users from a range of domains. We found that how people deal with uncertainty is influenced by the role the spreadsheet plays in people's work and the user's aims. Spreadsheets are used as a database, template, calculation tool, notepad and exploration tool. In doing so, participants' aims were to compute and compare different scenarios, understand something about the nature of the uncertainty in their situation, and translate the complexity of data uncertainty into simplified presentations to other people, usually decision-makers. Spreadsheets currently provide limited tools to support these aims, and participants had various workarounds.

</details>


### [86] [Characterizing Scalability Issues in Spreadsheet Software using Online Forums](https://arxiv.org/abs/1801.03829)
*Kelly Mack,John Lee,Kevin Chang,Karrie Karahalios,Aditya Parameswaran*

Main category: cs.HC

TL;DR: 本文研究了如何利用在线论坛数据（如Reddit）来研究用户在使用Excel表格时遇到的挑战，并讨论了这些发现对未来表格软件设计的启示。


<details>
  <summary>Details</summary>
Motivation: 传统的可用性研究耗时耗力，因此本文提出一种利用在线论坛数据来大规模、低成本地研究用户挑战的新方法。

Method: 通过抓取Reddit上的帖子，收集用户关于Excel表格的问题和抱怨，分析其中关于数据量过大导致的用户挑战。

Result: 分析了用户在使用Excel表格时遇到的普遍问题，特别是与处理大量数据相关的挑战。

Conclusion: 探讨了研究结果对设计下一代Excel表格软件的启示。

Abstract: In traditional usability studies, researchers talk to users of tools to understand their needs and challenges. Insights gained via such interviews offer context, detail, and background. Due to costs in time and money, we are beginning to see a new form of tool interrogation that prioritizes scale, cost, and breadth by utilizing existing data from online forums. In this case study, we set out to apply this method of using online forum data to a specific issue---challenges that users face with Excel spreadsheets. Spreadsheets are a versatile and powerful processing tool if used properly. However, with versatility and power come errors, from both users and the software, which make using spreadsheets less effective. By scraping posts from the website Reddit, we collected a dataset of questions and complaints about Excel. Specifically, we explored and characterized the issues users were facing with spreadsheet software in general, and in particular, as resulting from a large amount of data in their spreadsheets. We discuss the implications of our findings on the design of next-generation spreadsheet software.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [87] [Is spreadsheet syntax better than numeric indexing for cell selection?](https://arxiv.org/abs/2505.23296)
*Philip Heltweg,Dirk Riehle,Georg-Daniel Schwarz*

Main category: cs.PL

TL;DR: 两种单元格选择方法（数字索引和电子表格样式）在速度和正确性方面进行了比较。


<details>
  <summary>Details</summary>
Motivation: 为了在数据工程中选择单元格子集，需要比较不同的表示方法。

Method: 通过包含学生参与者的大规模对照实验，比较了数字索引和电子表格样式的代码读取和写入速度及正确性。

Result: 与数字索引相比，电子表格样式在读取代码时错误更少，在写入代码时错误更少且速度更快。

Conclusion: 对于没有软件工程背景的数据从业者，领域特定语法（如电子表格语法）可能是一个有希望的选择。

Abstract: Selecting a subset of cells is a common task in data engineering, for example, to remove errors or select only specific parts of a table. Multiple approaches to express this selection exist. One option is numeric indexing, commonly found in general programming languages, where a tuple of numbers identifies the cell. Alternatively, the separate dimensions can be referred to using different enumeration schemes like "A1" for the first cell, commonly found in software such as spreadsheet systems.
  In a large-scale controlled experiment with student participants as proxy for data practitioners, we compare the two options with respect to speed and correctness of reading and writing code.
  The results show that, when reading code, participants make less mistakes using spreadsheet-style syntax. Additionally, when writing code, they make fewer mistakes and are faster when using spreadsheet syntax compared to numeric syntax.
  From this, a domain-specific syntax, such as spreadsheet syntax for data engineering, appears to be a promising alternative to explore in future tools to support practitioners without a software engineering background.

</details>


### [88] [FLAME: A small language model for spreadsheet formulas](https://arxiv.org/abs/2301.13779)
*Harshit Joshi,Abishai Ebenezer,José Cambronero,Sumit Gulwani,Aditya Kanade,Vu Le,Ivan Radiček,Gust Verbruggen*

Main category: cs.PL

TL;DR: FLAME是一个6000万参数的Transformer模型，仅使用Excel公式进行训练，在公式修复、补全和检索任务上表现优于更大的模型，如Codex和CodeT5。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在电子表格公式创作辅助方面存在部署困难，因为它们体积庞大且训练成本高。FLAME旨在创建一个更小、更高效的模型。

Method: FLAME使用专门为Excel公式设计的Transformer架构，通过草图去重数据集、公式分词器以及掩码跨度预测和噪声自动编码等预训练任务进行训练。

Result: 在公式修复和补全任务中，FLAME在14个评估设置中的10个优于更大的模型。在公式检索任务中，FLAME也优于CodeT5、CodeBERT和GraphCodeBERT。

Conclusion: FLAME展示了在特定领域进行模型训练的有效性，证明了即使模型规模较小，也能在电子表格公式处理任务上达到或超过大型模型的性能。

Abstract: Spreadsheets are a vital tool for end-user data management. Using large language models for formula authoring assistance in these environments can be difficult, as these models are expensive to train and challenging to deploy due to their size (up to billions of parameters). We present FLAME, a transformer-based model trained exclusively on Excel formulas that leverages domain insights to achieve competitive performance while being substantially smaller (60M parameters) and training on two orders of magnitude less data. We curate a training dataset using sketch deduplication, introduce an Excel-specific formula tokenizer, and use domain-specific versions of masked span prediction and noisy auto-encoding as pre-training objectives. We evaluate FLAME on formula repair, formula completion, and similarity-based formula retrieval. FLAME can outperform much larger models, such as the Davinci (175B) and Cushman (12B) variants of Codex and CodeT5 (220M), in 10 of 14 evaluation settings for the repair and completion tasks. For formula retrieval, FLAME outperforms CodeT5, CodeBERT, and GraphCodeBERT.

</details>


### [89] [Enhanced Spreadsheet Computing with Finite-Domain Constraint Satisfaction](https://arxiv.org/abs/2203.16346)
*Ezana N. Beyenne,Hai-Feng Guo*

Main category: cs.PL

TL;DR: 该论文扩展了电子表格的计算范式，以解决约束满足问题，通过引入支持有限域约束求解的可视化环境和用于用户指定约束的特定约束语言。


<details>
  <summary>Details</summary>
Motivation: 电子表格虽然广泛使用且易于非程序员操作，但主要局限于类似簿记的应用，因为它们是单向数据流。本研究旨在打破这一限制。

Method: 提出一个增强的电子表格系统，支持可视化环境中的有限域约束求解，并构建了一种电子表格特定的约束语言，供用户以声明式和可扩展的方式指定单元格之间的数据约束。

Result: 新的电子表格系统显著简化了使用可视化表格界面开发许多基于约束的应用。

Conclusion: 通过提供支持约束求解的可视化环境和易于使用的约束语言，扩展后的电子表格范式提高了约束满足问题的可用性和实用性。

Abstract: The spreadsheet application is among the most widely used computing tools in modern society. It provides excellent usability and usefulness, and it easily enables a non-programmer to perform programming-like tasks in a visual tabular "pen and paper" approach. However, spreadsheets are mostly limited to bookkeeping-like applications due to their mono-directional data flow. This paper shows how the spreadsheet computing paradigm is extended to break this limitation for solving constraint satisfaction problems. We present an enhanced spreadsheet system where finite-domain constraint solving is well supported in a visual environment. Furthermore, a spreadsheet-specific constraint language is constructed for general users to specify constraints among data cells in a declarative and scalable way. The new spreadsheet system significantly simplifies the development of many constraint-based applications using a visual tabular interface. Examples are given to illustrate the usability and usefulness of the extended spreadsheet paradigm.
  KEYWORDS: Spreadsheet computing, Finite-domain constraint satisfaction, Constraint logic programming

</details>


### [90] [Typed Image-based Programming with Structure Editing](https://arxiv.org/abs/2110.08993)
*Jonathan Edwards,Tomas Petricek*

Main category: cs.PL

TL;DR: 图像化编程系统可以通过结构化编辑和静态类型实现协作，特别是在处理模式更改时。


<details>
  <summary>Details</summary>
Motivation: 图像化编程系统（如Smalltalk, LISP, HyperCard, Flash, 电子表格）易于上手，但缺乏协作和部署支持，限制了其商业成功。本研究旨在解决图像化编程的协作问题。

Method: 提出使用静态类型来处理模式更改，并结合结构化编辑来捕获类型定义的更改，从而自动适应数据。在此基础上，提出一种用于结构化编辑的版本控制理论，以实现协作。

Result: 提出了一种用于结构化编辑的版本控制理论，这是该研究的主要技术贡献。该方法有望通过编辑类型来支持图像化编程的协作。

Conclusion: 通过静态类型和结构化编辑，可以为图像化编程系统带来更好的协作支持，尤其是在处理数据模式更改方面。如果该方法能扩展到整个编程体验，将为图像化编程的协作提供新的途径。

Abstract: Many beloved programming systems are image-based: self-contained worlds that persist both code and data in a single file. Examples include Smalltalk, LISP, HyperCard, Flash, and spreadsheets. Image-based programming avoids much of the complexity of modern programming technology stacks and encourages more casual and exploratory programming. However conventional file-based programming has better support for collaboration and deployment. These problems have been blamed for the limited commercial success of Smalltalk. We propose to enable collaboration in image-based programming via types and structure editing.
  We focus on the problem of schema change on persistent data. We turn to static types, which paradoxically require more schema change but also provide a mechanism to express and execute those changes. To determine those changes we turn to structure editing, so that we can capture changes in type definitions with sufficient fidelity to automatically adapt the data to suit. We conjecture that typical schema changes can be handled through structure editing of static types.
  That positions us to tackle collaboration with what could be called version control for structure editing. We present a theory realizing this idea, which is our main technical contribution. While we focus here on editing types, if we can extend the approach to cover the entire programming experience then it would offer a new way to collaborate in image-based programming.

</details>


### [91] [ExceLint: Automatically Finding Spreadsheet Formula Errors](https://arxiv.org/abs/1901.11100)
*Daniel W. Barowy,Emery D. Berger,Benjamin Zorn*

Main category: cs.PL

TL;DR: ExceLint 是一种用于查找电子表格公式错误的静态分析工具，它利用电子表格的矩形特性，通过信息论方法识别出可能令人惊讶的异常公式，并在实际应用中表现出快速和高效的性能。


<details>
  <summary>Details</summary>
Motivation: 鉴于电子表格在金融等关键领域的广泛应用以及其中错误可能导致的严重后果，需要一种专门用于检测电子表格公式错误的工具。

Method: 该分析方法直接利用了电子表格的矩形结构，并采用信息论方法来识别那些可能对相邻矩形区域造成令人惊讶的干扰的公式。ExceLint 是该静态分析方法在 Microsoft Excel 中的具体实现。

Result: 在对 70 个电子表格进行的测试中，ExceLint 的运行速度快，中位数分析时间仅为每个电子表格 5 秒，并且其错误检测性能显著优于现有的分析方法。

Conclusion: ExceLint 是一种快速有效的静态分析工具，能够显著提高电子表格公式的准确性，对于在关键领域使用电子表格的应用具有重要价值。

Abstract: Spreadsheets are one of the most widely used programming environments, and are widely deployed in domains like finance where errors can have catastrophic consequences. We present a static analysis specifically designed to find spreadsheet formula errors. Our analysis directly leverages the rectangular character of spreadsheets. It uses an information-theoretic approach to identify formulas that are especially surprising disruptions to nearby rectangular regions. We present ExceLint, an implementation of our static analysis for Microsoft Excel. We demonstrate that ExceLint is fast and effective: across a corpus of 70 spreadsheets, ExceLint takes a median of 5 seconds per spreadsheet, and it significantly outperforms the state of the art analysis.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [92] [Radif Corpus: A Symbolic Dataset for Non-Metric Iranian Classical Music](https://arxiv.org/abs/2507.10456)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.SD

TL;DR: 该研究介绍了伊朗古典音乐的数字radif语料库，包含228首乐曲的MIDI文件和详细数据，支持计算分析。


<details>
  <summary>Details</summary>
Motivation: 伊朗古典音乐的核心是radif，但缺乏可用于计算研究的数字资源。本研究旨在创建一个完整的非公制radif语料库，以促进对伊朗古典音乐的计算分析。

Method: 创建了一个包含228首乐曲（总计约281分钟）的完整非公制radif语料库，提供MIDI文件和包含音符、时值、音程、层级结构等信息的电子表格数据，并忠实表示了包括四分音符和非公制特征在内的音调。

Result: 生成了一个包含228首乐曲的数字radif语料库，涵盖了radif的所有13个组成部分，并提供了MIDI文件、数据电子表格、基本统计数据以及复杂性和相似性度量。

Conclusion: 该语料库为伊朗古典音乐的计算研究提供了一个平台，研究人员可以利用它来研究旋律模式、即兴风格，或在音乐信息检索、音乐理论和计算（民族）音乐学等领域进行其他任务。

Abstract: Non-metric music forms the core of the repertoire in Iranian classical music. Dastgahi music serves as the underlying theoretical system for both Iranian art music and certain folk traditions. At the heart of Iranian classical music lies the radif, a foundational repertoire that organizes melodic material central to performance and pedagogy.
  In this study, we introduce a digital corpus representing the complete non-metrical radif repertoire, covering all 13 existing components of this repertoire. We provide MIDI files (about 281 minutes in total) and data spreadsheets describing notes, note durations, intervals, and hierarchical structures for 228 pieces of music. We faithfully represent the tonality including quarter-tones, and the non-metric aspect. Furthermore, we provide supporting basic statistics, and measures of complexity and similarity over the corpus.
  Our corpus provides a platform for computational studies of Iranian classical music. Researchers might employ it in studying melodic patterns, investigating improvisational styles, or for other tasks in music information retrieval, music theory, and computational (ethno)musicology.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [93] [Data-Semantics-Aware Recommendation of Diverse Pivot Tables](https://arxiv.org/abs/2507.06171)
*Whanhee Cho,Anna Fariha*

Main category: cs.DB

TL;DR: SAGE是一个数据语义感知系统，用于推荐k个预算内多样化的透视表，解决了现有工作中排名靠前推荐的冗余问题。SAGE确保每个透视表都有用、可解释且能适应用户操作和偏好，同时保证透视表集合彼此不同，提供多样化推荐。


<details>
  <summary>Details</summary>
Motivation: 为高维数据集中的用户识别有用的透视表组合仍然是一个挑战，需要消除繁琐的手动过程。

Method: SAGE提出了一种数据语义感知模型来衡量单个透视表的效用和透视表集合的多样性，并设计了一种可扩展的贪心算法来高效地选择一组高效用的多样化透视表，利用数据语义显著减少了组合搜索空间。

Result: 在三个真实世界数据集上的大量实验表明，SAGE的表现优于其他方法，并且能够有效地扩展以适应高维数据集。

Conclusion: SAGE系统通过提供数据语义感知模型和可扩展的贪心算法，有效地解决了推荐多样化透视表的问题，并在实验和案例研究中证明了其优越性。

Abstract: Data summarization is essential to discover insights from large datasets. In a spreadsheets, pivot tables offer a convenient way to summarize tabular data by computing aggregates over some attributes, grouped by others. However, identifying attribute combinations that will result in useful pivot tables remains a challenge, especially for high-dimensional datasets. We formalize the problem of automatically recommending insightful and interpretable pivot tables, eliminating the tedious manual process. A crucial aspect of recommending a set of pivot tables is to diversify them. Traditional works inadequately address the table-diversification problem, which leads us to consider the problem of pivot table diversification.
  We present SAGE, a data-semantics-aware system for recommending k-budgeted diverse pivot tables, overcoming the shortcomings of prior work for top-k recommendations that cause redundancy. SAGE ensures that each pivot table is insightful, interpretable, and adaptive to the user's actions and preferences, while also guaranteeing that the set of pivot tables are different from each other, offering a diverse recommendation. We make two key technical contributions: (1) a data-semantics-aware model to measure the utility of a single pivot table and the diversity of a set of pivot tables, and (2) a scalable greedy algorithm that can efficiently select a set of diverse pivot tables of high utility, by leveraging data semantics to significantly reduce the combinatorial search space. Our extensive experiments on three real-world datasets show that SAGE outperforms alternative approaches, and efficiently scales to accommodate high-dimensional datasets. Additionally, we present several case studies to highlight SAGE's qualitative effectiveness over commercial software and Large Language Models (LLMs).

</details>


### [94] [Facilitating Digital Agriculture with Simple Databases](https://arxiv.org/abs/2312.06517)
*Dennis Buckmaster,Sami Basir,Hanae Sakata*

Main category: cs.DB

TL;DR: 提供开源的数据库模板，方便农业工作者进行数据管理和分析。


<details>
  <summary>Details</summary>
Motivation: 为农业工作者提供易于使用的数据库解决方案，帮助他们管理和分析数据。

Method: 提供结构化的私有数据库模板（基于Airtable），使用简单的数据验证表单，并提供工作坊视频教程。

Result: 生成整洁、机器和人类可读、可编辑、可导出的运营数据，以支持物流、提供元数据和改进企业分析。

Conclusion: 这些资源有望通过推广和结构化教育项目，促进数字农业原则的普及。

Abstract: As an on-ramp to databases, we offer several well-structured private database templates as open source resources for agriculturalists, particularly those with modest spreadsheet skills. These farmer-oriented Air table databases use simple data-validated forms, with the look and feel of a customized app, to yield operational data that is tidy, machine- and human-readable, editable, and exportable for analysis in other software. Such data can facilitate logistics, provide contextual metadata, and improve enterprise analysis. A recorded workshop explaining how to build a database for activity records is presented. These resources may facilitate infusion of digital agriculture principles through Extension and structured educational programming.

</details>


### [95] [Streamlining Knowledge Graph Construction with a façade: The SPARQL Anything project](https://arxiv.org/abs/2310.16700)
*Luigi Asprino,Enrico Daga,Justin Dowdy,Paul Mulholland,Aldo Gangemi,Marco Ratta*

Main category: cs.DB

TL;DR: SPARQL Anything 是一个用于查询异构资源的框架，它通过重载 SERVICE 子句，支持多种格式（包括 CSV、JSON、XML、Markdown、YAML、DOCx、Bibtex 等），并提供灵活的 API 查询、参数化查询和复杂管道构建等功能。本文阐述了其设计理念和软件架构，并通过社区调查和行业报告验证了其设计假设的有效性。


<details>
  <summary>Details</summary>
Motivation: 设计一个面向知识工程师的数据集成框架。

Method: 通过重载 SERVICE 子句，实现 SPARQL Anything 系统，支持多种文件格式和 API 查询，构建复杂管道。

Result: SPARQL Anything 支持广泛的数据格式和查询功能，并通过社区调查和行业报告证明了其有效性。

Conclusion: SPARQL Anything 提供了一个强大且灵活的数据集成框架，能够满足知识工程师在处理异构数据时的需求。

Abstract: What should a data integration framework for knowledge engineers look like? Recent research on Knowledge Graph construction proposes the design of a façade, a notion borrowed from object-oriented software engineering. This idea is applied to SPARQL Anything, a system that allows querying heterogeneous resources as-if they were in RDF, in plain SPARQL 1.1, by overloading the SERVICE clause. SPARQL Anything supports a wide variety of file formats, from popular ones (CSV, JSON, XML, Spreadsheets) to others that are not supported by alternative solutions (Markdown, YAML, DOCx, Bibtex). Features include querying Web APIs with high flexibility, parametrised queries, and chaining multiple transformations into complex pipelines. In this paper, we describe the design rationale and software architecture of the SPARQL Anything system. We provide references to an extensive set of reusable, real-world scenarios from various application domains. We report on the value-to-users of the founding assumptions of its design, compared to alternative solutions through a community survey and a field report from the industry.

</details>


### [96] [DataVinci: Learning Syntactic and Semantic String Repairs](https://arxiv.org/abs/2308.10922)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Gust Verbruggen*

Main category: cs.DB

TL;DR: DataVinci是一个全自动的字符串数据错误检测和修复系统，它学习基于正则表达式的模式来识别和修复错误，并利用LLM处理包含语义子串的字符串，同时结合程序执行信息来改进修复效果，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有字符串数据清理系统在处理真实世界数据时存在不足，例如：1. 许多系统仅限于错误检测或需要用户提供标注、示例或约束。2. 现有方法通常独立处理句法错误或语义错误，忽略了字符串中同时存在的句法和语义子串。3. 并非所有数据都能形成明确的多数模式。

Method: 1. **无监督模式学习**: DataVinci 学习覆盖列中大部分值的基于正则表达式的模式，并将不符合这些模式的值报告为数据错误。 2. **自动修复**: 系统可以根据学习到的多数模式和从其他列推导出的约束自动生成数据错误的编辑。 3. **处理语义子串**: 利用大型语言模型（LLM）在学习模式和生成编辑之前，抽象（并重新具体化）字符串中的语义部分。 4. **利用执行信息**: 对于无法形成多数模式的数据，DataVinci 利用读取目标数据的现有程序的执行信息来识别和纠正数据修复。

Result: DataVinci 在错误检测和修复方面均优于 7 个基线方法，并在 4 个现有和新的基准测试中得到了验证。

Conclusion: DataVinci 作为一个全无监督的系统，能够有效地检测和修复字符串数据中的句法和语义错误，尤其擅长处理复杂的、混合类型的错误，并在各种场景下表现出优越的性能。

Abstract: String data is common in real-world datasets: 67.6% of values in a sample of 1.8 million real Excel spreadsheets from the web were represented as text. Systems that successfully clean such string data can have a significant impact on real users. While prior work has explored errors in string data, proposed approaches have often been limited to error detection or require that the user provide annotations, examples, or constraints to fix the errors. Furthermore, these systems have focused independently on syntactic errors or semantic errors in strings, but ignore that strings often contain both syntactic and semantic substrings. We introduce DataVinci, a fully unsupervised string data error detection and repair system. DataVinci learns regular-expression-based patterns that cover a majority of values in a column and reports values that do not satisfy such patterns as data errors. DataVinci can automatically derive edits to the data error based on the majority patterns and constraints learned over other columns without the need for further user interaction. To handle strings with both syntactic and semantic substrings, DataVinci uses an LLM to abstract (and re-concretize) portions of strings that are semantic prior to learning majority patterns and deriving edits. Because not all data can result in majority patterns, DataVinci leverages execution information from an existing program (which reads the target data) to identify and correct data repairs that would not otherwise be identified. DataVinci outperforms 7 baselines on both error detection and repair when evaluated on 4 existing and new benchmarks.

</details>


### [97] [Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples](https://arxiv.org/abs/2307.14565)
*Peng Li,Yeye He,Cong Yan,Yue Wang,Surajit Chaudhuri*

Main category: cs.DB

TL;DR: 超过30%的现实世界表格不符合关系标准，需要复杂的重组才能使用SQL进行查询。本研究开发了一个名为Auto-Tables的系统，可以自动合成多步转换流水线，将非关系表转换为关系表，并构建了一个包含244个真实测试用例的基准。评估表明，Auto-Tables在交互速度下能成功处理超过70%的测试用例，无需用户干预。


<details>
  <summary>Details</summary>
Motivation: 处理现实世界中超过30%不符合关系标准、难以直接用SQL查询的表格，解决技术和非技术用户在表格转换上面临的痛点。

Method: 开发Auto-Tables系统，自动合成多步转换流水线（Python或其他语言），将非关系表转换为关系表。构建了一个包含244个真实表格的基准数据集进行评估。

Result: Auto-Tables系统在交互速度下，成功处理了超过70%的测试用例，无需用户输入，有效解决了表格数据准备的难题。

Conclusion: Auto-Tables是一个有效的工具，可以帮助技术和非技术用户自动准备数据以进行分析，显著提高了处理不规范表格的效率和可行性。

Abstract: Relational tables, where each row corresponds to an entity and each column corresponds to an attribute, have been the standard for tables in relational databases. However, such a standard cannot be taken for granted when dealing with tables "in the wild". Our survey of real spreadsheet-tables and web-tables shows that over 30% of such tables do not conform to the relational standard, for which complex table-restructuring transformations are needed before these tables can be queried easily using SQL-based analytics tools. Unfortunately, the required transformations are non-trivial to program, which has become a substantial pain point for technical and non-technical users alike, as evidenced by large numbers of forum questions in places like StackOverflow and Excel/Power-BI/Tableau forums.
  We develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations (in Python or other languages), to transform non-relational tables into standard relational forms for downstream analytics, obviating the need for users to manually program transformations. We compile an extensive benchmark for this new task, by collecting 244 real test cases from user spreadsheets and online forums. Our evaluation suggests that Auto-Tables can successfully synthesize transformations for over 70% of test cases at interactive speeds, without requiring any input from users, making this an effective tool for both technical and non-technical users to prepare data for analytics.

</details>


### [98] [Efficient and Compact Spreadsheet Formula Graphs](https://arxiv.org/abs/2302.05482)
*Dixin Tang,Fanchao Chen,Christopher De Leon,Tana Wattanawaroon,Jeaseok Yun,Srinivasan Seshadri,Aditya G. Parameswaran*

Main category: cs.DB

TL;DR: TACO框架通过利用表格局部性原理，提出了一种压缩公式图的算法，显著提高了电子表格中公式图的查询和维护效率。


<details>
  <summary>Details</summary>
Motivation: 电子表格的公式图通常庞大且复杂，导致查询和维护耗时，降低了交互性。因此，需要一种有效的方法来压缩公式图，以提高查询和维护的速度。

Method: TACO框架利用了电子表格的“表格局部性”特性（即相邻单元格的公式结构相似），识别并应用了四种基于表格局部性的模式来压缩公式图。该框架可以直接在压缩后的图上进行查询，而无需解压，并能对更新进行增量维护。

Result: TACO框架显著减小了公式图的大小。在查询公式图方面，与基线实现和商业电子表格系统相比，TACO的速度分别提高了34,972倍和632倍。

Conclusion: TACO框架通过利用表格局部性原理，成功地压缩了公式图，并在查询和维护方面实现了显著的性能提升，解决了电子表格交互性受公式图复杂度影响的问题。

Abstract: Spreadsheets are one of the most popular data analysis tools, wherein users can express computation as formulae alongside data. The ensuing dependencies are tracked as formula graphs. Efficiently querying and maintaining these formula graphs is critical for interactivity across multiple settings. Unfortunately, formula graphs are often large and complex such that querying and maintaining them is time-consuming, reducing interactivity. We propose TACO, a framework for efficiently compressing formula graphs, thereby reducing the time for querying and maintenance. The efficiency of TACO stems from a key spreadsheet property: tabular locality, which means that cells close to each other are likely to have similar formula structures. We leverage four such tabular locality-based patterns and develop algorithms for compressing formula graphs using these patterns, directly querying the compressed graph without decompression, and incrementally maintaining the graph during updates. We integrate TACO into an open-source spreadsheet system and show that TACO can significantly reduce formula graph sizes. For querying formula graphs, the speedups of TACO over a baseline implemented in our framework and a commercial spreadsheet system are up to 34,972x and 632x, respectively.

</details>


### [99] [Sigma Workbook: A Spreadsheet for Cloud Data Warehouses](https://arxiv.org/abs/2204.03128)
*James Gale,Max Seiden,Deepanshu Utkarsh,Jason Frantz,Rob Woollen,Çağatay Demiralp*

Main category: cs.DB

TL;DR: Sigma Workbook 是一个交互式系统，通过类似电子表格的界面，让业务用户能够轻松地对云数据仓库中的大规模数据进行可视化分析。


<details>
  <summary>Details</summary>
Motivation: 现有的云数据仓库（CDW）分析工具要么在即席转换方面功能有限，要么对业务用户来说难以使用。

Method: Sigma Workbook 提供了一个类似电子表格的直观界面，通过直接操作实现分析。它能够根据用户交互动态构建 SQL 查询，并将查询直接在 CDW 上执行，从而利用新一代 CDW 的可扩展性。

Result: 通过对队列分析、会话化和数据增强这三个实际用例的演示，证明了 Sigma Workbook 的易用性、可扩展性和表现力。

Conclusion: Sigma Workbook 使得业务用户能够轻松地对云数据仓库中的大规模数据进行可视化分析，克服了现有工具的局限性。

Abstract: Cloud data warehouses (CDWs) bring large-scale data and compute power closer to users in enterprises. However, existing tools for analyzing data in CDWs are either limited in ad-hoc transformations or difficult to use for business users. Here we introduce Sigma Workbook, a new interactive system that enables business users to easily perform a visual analysis of data in CDWs at scale. For this, Sigma Workbook provides an accessible spreadsheet-like interface for analysis through direct manipulation. Sigma Workbook dynamically constructs matching SQL queries from user interactions, building on the versatility and expressivity of SQL. Constructed queries are directly executed on CDWs, leveraging the superior characteristics of the new generation CDWs, including scalability. We demonstrate Sigma Workbook through 3 real-life use cases -- cohort analysis, sessionization, and data augmentation -- and underline Workbook's ease of use, scalability, and expressivity.

</details>


### [100] [Efficient Specialized Spreadsheet Parsing for Data Science](https://arxiv.org/abs/2202.13189)
*Felix Henze,Haralampos Gavriilidis,Eleni Tzirita Zacharatou,Volker Markl*

Main category: cs.DB

TL;DR: 该研究提出了一种新的Excel表格解析器，以提高在R环境中加载Excel表格的效率，并显著减少内存使用。


<details>
  <summary>Details</summary>
Motivation: 当前将电子表格加载到数据科学环境（如R）以进行高级分析的方法存在运行时长或内存占用过高的问题，这在普通计算机上阻碍了数据探索的进行。

Method: 该研究引入了一种新的解析器，通过将解压缩和解析紧密结合来最小化内存使用。此外，为了减少运行时间，研究采用了优化的特定于电子表格的解析例程，并利用了并行处理技术。

Result: 研究实现的用于将Excel表格加载到R环境中的原型，与现有最先进的方法相比，速度提高了3倍，内存消耗降低了40倍。

Conclusion: 研究提出的新方法在速度和内存效率方面都优于现有方法，使得在普通计算机上进行电子表格数据探索更加实用。

Abstract: Spreadsheets are widely used for data exploration. Since spreadsheet systems have limited capabilities, users often need to load spreadsheets to other data science environments to perform advanced analytics. However, current approaches for spreadsheet loading suffer from either high runtime or memory usage, which hinders data exploration on commodity systems. To make spreasheet loading practical on commodity systems, we introduce a novel parser that minimizes memory usage by tightly coupling decompression and parsing. Furthermore, to reduce the runtime, we introduce optimized spreadsheet-specific parsing routines and employ parallelism. To evaluate our approach, we implement a prototype for loading Excel spreadsheets into R environments. Our evaluation shows that our novel approach is up to 3x faster while consuming up to 40x less memory than state-of-the-art approaches.

</details>


### [101] [Spread2RML: Constructing Knowledge Graphs by Predicting RML Mappings on Messy Spreadsheets](https://arxiv.org/abs/2110.12829)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: RML 可以将包括电子表格在内的半结构化数据映射到 RDF 知识图谱。然而，由于电子表格复杂且混乱的数据模型，映射创建过程非常耗时。本文提出了 Spread2RML，它通过一组可扩展的 RML 对象映射模板，利用启发式方法为每列预测 RML 映射，从而简化此过程。


<details>
  <summary>Details</summary>
Motivation: 电子表格的复杂且混乱的数据模型使得创建 RML 映射的过程非常耗时，Spread2RML 旨在通过自动预测映射来减少这种工作量。

Method: Spread2RML 使用一组可扩展的 RML 对象映射模板，并基于启发式方法将这些模板应用于电子表格的每一列来预测 RML 映射。

Result: 在包含从高度混乱的合成数据到 data.gov 的不太混乱的电子表格的三个数据集上进行了评估，获得了有希望的结果，特别是对于全自动方法和处理混乱数据方面。

Conclusion: Spread2RML 能够自动预测 RML 映射，尤其是在处理混乱的电子表格数据方面，并取得了初步的积极成果。

Abstract: The RDF Mapping Language (RML) allows to map semi-structured data to RDF knowledge graphs. Besides CSV, JSON and XML, this also includes the mapping of spreadsheet tables. Since spreadsheets have a complex data model and can become rather messy, their mapping creation tends to be very time consuming. In order to reduce such efforts, this paper presents Spread2RML which predicts RML mappings on messy spreadsheets. This is done with an extensible set of RML object map templates which are applied for each column based on heuristics. In our evaluation, three datasets are used ranging from very messy synthetic data to spreadsheets from data.gov which are less messy. We obtained first promising results especially with regard to our approach being fully automatic and dealing with rather messy data.

</details>


### [102] [Supercomputing Enabled Deployable Analytics for Disaster Response](https://arxiv.org/abs/2108.11525)
*Kaira Samuel,Jeremy Kepner,Michael Jones,Lauren Milechin,Vijay Gadepally,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Michael Houle,Anna Klein,Victor Lopez,Julie Mullen,Andrew Prout,Albert Reuther,Antonio Rosa,Sid Samsi,Charles Yee,Peter Michaleas*

Main category: cs.DB

TL;DR: 本篇论文提出了一种为一线应急响应人员提供地理空间人口普查数据的分析方法，以应对紧急情况。


<details>
  <summary>Details</summary>
Motivation: 由于网络访问受限和安全要求，标准云分析平台不适用于前线部署。因此，需要一种无需网络连接或额外软件即可运行的本地化分析解决方案。

Method: 该方法预先计算了多种分析数据，并将其打包成易于使用的文件（如 Google Earth 的 KML 文件和 Microsoft Excel 的 XLSX 文件）。这些文件可以在标准预装软件中运行，无需网络或额外安装。数据在 MIT SuperCloud 上处理，生成了数千个分析文件，包括总人口、15 岁以下人口、65 岁以上人口和中位年龄等关键指标，并按县排序。

Result: 该方法能够快速生成地理空间人口普查数据分析文件。Excel 文件提供了坐标单位转换功能，Google Earth 文件则通过不同颜色映射展示人口密度，直观易懂。利用 MIT SuperCloud 的计算能力，可在几分钟内生成新的分析数据。

Conclusion: 该方法通过提供易于访问的地理空间人口普查数据，为应急响应人员提供了一个强大的工具，有望提高应急准备能力。

Abstract: First responders and other forward deployed essential workers can benefit from advanced analytics. Limited network access and software security requirements prevent the usage of standard cloud based microservice analytic platforms that are typically used in industry. One solution is to precompute a wide range of analytics as files that can be used with standard preinstalled software that does not require network access or additional software and can run on a wide range of legacy hardware. In response to the COVID-19 pandemic, this approach was tested for providing geo-spatial census data to allow quick analysis of demographic data for better responding to emergencies. These data were processed using the MIT SuperCloud to create several thousand Google Earth and Microsoft Excel files representative of many advanced analytics. The fast mapping of census data using Google Earth and Microsoft Excel has the potential to give emergency responders a powerful tool to improve emergency preparedness. Our approach displays relevant census data (total population, population under 15, population over 65, median age) per census block, sorted by county, through a Microsoft Excel spreadsheet (xlsx file) and Google Earth map (kml file). The spreadsheet interface includes features that allow users to convert between different longitude and latitude coordinate units. For the Google Earth files, a variety of absolute and relative colors maps of population density have been explored to provide an intuitive and meaningful interface. Using several hundred cores on the MIT SuperCloud, new analytics can be generated in a few minutes.

</details>


### [103] [Table2Charts: Recommending Charts by Learning Shared Table Representations](https://arxiv.org/abs/2008.11015)
*Mengyu Zhou,Qingtao Li,Xinyi He,Yuejiang Li,Yibo Liu,Wei Ji,Shi Han,Yining Chen,Daxin Jiang,Dongmei Zhang*

Main category: cs.DB

TL;DR: Table2Charts框架通过学习大量（表格，图表）配对数据中的常见模式，解决了图表推荐中的效率、数据不平衡和表格上下文等挑战，并在多类型任务和人工评估中表现优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 推荐常用的图表需要考虑效率、数据不平衡和表格上下文等挑战。

Method: 提出Table2Charts框架，利用深度Q学习和复制机制以及启发式搜索，将表格转换为遵循图表模板的序列。

Result: 在包含16.5万个表格和26.6万个图表的大型电子表格语料库上进行实验，Table2Charts学习到了表格字段的共享表示，使得不同图表类型的推荐任务可以相互促进。在多类型任务中，Table2Charts的表现优于其他图表推荐系统，召回率（R@3=0.61, R@1=0.43）翻倍，并在人工评估中获得更好结果。

Conclusion: Table2Charts框架在图表推荐任务中表现出色，能够有效处理复杂情况并超越现有方法。

Abstract: It is common for people to create different types of charts to explore a multi-dimensional dataset (table). However, to recommend commonly composed charts in real world, one should take the challenges of efficiency, imbalanced data and table context into consideration. In this paper, we propose Table2Charts framework which learns common patterns from a large corpus of (table, charts) pairs. Based on deep Q-learning with copying mechanism and heuristic searching, Table2Charts does table-to-sequence generation, where each sequence follows a chart template. On a large spreadsheet corpus with 165k tables and 266k charts, we show that Table2Charts could learn a shared representation of table fields so that recommendation tasks on different chart types could mutually enhance each other. Table2Charts outperforms other chart recommendation systems in both multi-type task (with doubled recall numbers R@3=0.61 and R@1=0.43) and human evaluations.

</details>


### [104] [Sigma Worksheet: Interactive Construction of OLAP Queries](https://arxiv.org/abs/2012.00697)
*James Gale,Max Seiden,Gretchen Atwood,Jason Frantz,Rob Woollen,Çağatay Demiralp*

Main category: cs.DB

TL;DR: Sigma Worksheet是一个新的交互式系统，它提供了一个类似电子表格的界面，用户可以通过直接操作轻松地对云数据仓库中的数据进行即席可视化分析，并动态构建SQL查询。


<details>
  <summary>Details</summary>
Motivation: 现有的云数据仓库分析工具要么在即席转换方面功能有限，要么对业务用户来说难以使用，而业务用户是企业中最大的用户群体。

Method: Sigma Worksheet提供了一个类似电子表格的、易于使用的界面，用户可以通过直接操作来分析数据。它会根据用户在熟悉界面上的交互动态地构建匹配的SQL查询，并直接在云数据仓库上执行这些查询，从而利用新一代云数据仓库的可扩展性等优点。

Result: 在两个真实用例（队列分析和会话化）中，Sigma Worksheet展示了其表达能力。其生成的SQL查询在TPC-H基准测试中的性能与参考查询相当。一项包含100名参与者的调查和一项对70名参与者的半结构化访谈研究表明，Sigma Worksheet更易于使用和学习，并提高了用户的工作效率。

Conclusion: Sigma Worksheet使业务用户能够轻松地对云数据仓库中的数据进行大规模的即席可视化分析，并且易于使用和学习，提高了用户的工作效率。该研究还表明，通过在数据分析的各个阶段提供指导，可以进一步改善用户体验。

Abstract: The new generation of cloud data warehouses (CDWs) brings large amounts of data and compute power closer to users in enterprises. The ability to directly access the warehouse data, interactively analyze and explore it at scale can empower users to improve their decision making cycles. However, existing tools for analyzing data in CDWs are either limited in ad-hoc transformations or difficult to use for business users, the largest user segment in enterprises. Here we introduce Sigma Worksheet, a new interactive system that enables users to easily perform ad-hoc visual analysis of data in CDWs at scale. For this, Sigma Worksheet provides an accessible spreadsheet-like interface for data analysis through direct manipulation. Sigma Worksheet dynamically constructs matching SQL queries from user interactions on this familiar interface, building on the versatility and expressivity of SQL. Sigma Worksheet executes constructed queries directly on CDWs, leveraging the superior characteristics of the new generation CDWs, including scalability. To evaluate Sigma Worksheet, we first demonstrate its expressivity through two real life use cases, cohort analysis and sessionization. We then measure the performance of the Worksheet generated queries with a set of experiments using the TPC-H benchmark. Results show the performance of our compiled SQL queries is comparable to that of the reference queries of the benchmark. Finally, to assess the usefulness of Sigma Worksheet in deployment, we elicit feedback through a 100-person survey followed by a semi-structured interview study with 70 participants. We find that Sigma Worksheet is easier to use and learn, improving the productivity of users. Our findings also suggest Sigma Worksheet can further improve user experience by providing guidance to users at various steps of data analysis.

</details>


### [105] [Mapping Spreadsheets to RDF: Supporting Excel in RML](https://arxiv.org/abs/2104.13600)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: RML Mapper has been extended to support Excel files, enabling the mapping of spreadsheets to RDF graphs.


<details>
  <summary>Details</summary>
Motivation: The RML specification and its implementations neglect the widely used spreadsheet format, which this paper addresses by extending RML Mapper to support Excel files.

Method: The RML Mapper tool was extended to support Microsoft Excel spreadsheet files, allowing access to various meta data of spreadsheet cells within RML maps. Experimental features for specific use cases were also included. The implementation is available on GitHub.

Result: RML Mapper now supports Excel files, with capabilities demonstrated in an online demo. Experimental features offer advanced functionalities for specific use cases.

Conclusion: The extended RML Mapper facilitates the mapping of spreadsheet data to RDF graphs, overcoming a limitation in the current RML ecosystem.

Abstract: The RDF Mapping Language (RML) enables, among other formats, the mapping of tabular data as Comma-Separated Values (CSV) files to RDF graphs. Unfortunately, the widely used spreadsheet format is currently neglected by its specification and well-known implementations. Therefore, we extended one of the tools which is RML Mapper to support Microsoft Excel spreadsheet files and demonstrate its capabilities in an interactive online demo. Our approach allows to access various meta data of spreadsheet cells in typical RML maps. Some experimental features for more specific use cases are also provided. The implementation code is publicly available in a GitHub fork.

</details>


### [106] [Dataset Generation Patterns for Evaluating Knowledge Graph Construction](https://arxiv.org/abs/2104.13576)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: Confidentiality hinders the publication of authentic, labeled datasets of personal and enterprise data, although they could be useful for evaluating knowledge graph construction approaches in industrial scenarios. Therefore, our plan is to synthetically generate such data in a way that it appears as authentic as possible.


<details>
  <summary>Details</summary>
Motivation: Confidentiality hinders the publication of authentic, labeled datasets of personal and enterprise data, which are useful for evaluating knowledge graph construction approaches in industrial scenarios.

Method: The paper derives 11 distinct patterns from real spreadsheets from industry and demonstrates a suitable generator called Data Sprout that is able to reproduce them. It describes how the generator produces spreadsheets in general and what altering effects the implemented patterns have.

Result: The paper demonstrates a generator called Data Sprout that can reproduce 11 distinct patterns found in real spreadsheets from industry.

Conclusion: Synthetic data generation can be a viable solution to the lack of authentic datasets for knowledge graph construction in industrial scenarios, by discovering and imitating patterns found in real-world data.

Abstract: Confidentiality hinders the publication of authentic, labeled datasets of personal and enterprise data, although they could be useful for evaluating knowledge graph construction approaches in industrial scenarios. Therefore, our plan is to synthetically generate such data in a way that it appears as authentic as possible. Based on our assumption that knowledge workers have certain habits when they produce or manage data, generation patterns could be discovered which can be utilized by data generators to imitate real datasets. In this paper, we initially derived 11 distinct patterns found in real spreadsheets from industry and demonstrate a suitable generator called Data Sprout that is able to reproduce them. We describe how the generator produces spreadsheets in general and what altering effects the implemented patterns have.

</details>


### [107] [Interactively Constructing Knowledge Graphs from Messy User-Generated Spreadsheets](https://arxiv.org/abs/2103.03537)
*Markus Schröder,Christian Jilek,Michael Schulze,Andreas Dengel*

Main category: cs.DB

TL;DR: 知识工作者填写的电子表格可能包含非结构化内容，难以被机器解读。本文提出了一种交互式方法，通过图形化用户界面让知识工程师标注电子表格单元格，从而构建知识图谱，解决了数据维护缺失导致的数据混乱问题。


<details>
  <summary>Details</summary>
Motivation: 处理和解释知识工作者随意填写的、可能包含非结构化内容的电子表格数据，特别是为机器提供更明确、形式化和结构化的数据表示（如知识图谱）。

Method: 提出了一种交互式方法，包括一个图形用户界面，允许知识工程师批量注释电子表格单元格中的信息，并基于这些注释构建知识图谱。

Result: 使用来自工业场景的五个电子表格，构建了一个包含 25,000 个三元组的知识图谱。将此方法与最先进的 RDF 映射语言 (RML) 进行了比较，并突出了所提出方法的优势。

Conclusion: 所提出的交互式方法能够有效地解决因数据维护策略缺失导致的用户生成数据混乱问题，并通过实例验证了其在构建知识图谱方面的有效性和优越性。

Abstract: When spreadsheets are filled freely by knowledge workers, they can contain rather unstructured content. For humans and especially machines it becomes difficult to interpret such data properly. Therefore, spreadsheets are often converted to a more explicit, formal and structured form, for example, to a knowledge graph. However, if a data maintenance strategy has been missing and user-generated data becomes "messy", the construction of knowledge graphs will be a challenging task. In this paper, we catalog several of those challenges and propose an interactive approach to solve them. Our approach includes a graphical user interface which enables knowledge engineers to bulk-annotate spreadsheet cells with extracted information. Based on the cells' annotations a knowledge graph is ultimately formed. Using five spreadsheets from an industrial scenario, we built a 25k-triple graph during our evaluation. We compared our method with the state-of-the-art RDF Mapping Language (RML) attempt. The comparison highlights contributions of our approach.

</details>


### [108] [Leam: An Interactive System for In-situ Visual Text Analysis](https://arxiv.org/abs/2009.03520)
*Sajjadur Rahman,Peter Griggs,Çağatay Demiralp*

Main category: cs.DB

TL;DR: Leam是一个结合了计算笔记本、电子表格和可视化工具优点的文本分析系统，解决了现有系统在数据异构性、可追溯性、工作流可重用性和可重复性方面存在的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有文本分析系统通常只能处理部分分析阶段，并且在数据异构性、可追溯性、工作流可重用性和可重复性以及与既有实践的兼容性方面存在挑战。

Method: 提出Leam系统，将文本分析过程视为一个连续体，结合了计算笔记本、电子表格和可视化工具的优点。Leam具有交互式用户界面、用于管理多种数据类型的新数据模型以及表达性代数，能够捕获文本分析的各个阶段并协调数据、代码和可视化等组件。

Result: 展示了Leam的开发进展和使用示例，证明了其有效性。

Conclusion: 概述了Leam的未来增强计划，并指出了开发交互式可视化文本分析系统的研究方向。

Abstract: With the increase in scale and availability of digital text generated on the web, enterprises such as online retailers and aggregators often use text analytics to mine and analyze the data to improve their services and products alike. Text data analysis is an iterative, non-linear process with diverse workflows spanning multiple stages, from data cleaning to visualization. Existing text analytics systems usually accommodate a subset of these stages and often fail to address challenges related to data heterogeneity, provenance, workflow reusability and reproducibility, and compatibility with established practices. Based on a set of design considerations we derive from these challenges, we propose Leam, a system that treats the text analysis process as a single continuum by combining advantages of computational notebooks, spreadsheets, and visualization tools. Leam features an interactive user interface for running text analysis workflows, a new data model for managing multiple atomic and composite data types, and an expressive algebra that captures diverse sets of operations representing various stages of text analysis and enables coordination among different components of the system, including data, code, and visualizations. We report our current progress in Leam development while demonstrating its usefulness with usage examples. Finally, we outline a number of enhancements to Leam and identify several research directions for developing an interactive visual text analysis system.

</details>


### [109] [ObjTables: structured spreadsheets that promote data quality, reuse, and integration](https://arxiv.org/abs/2005.05227)
*Jonathan R. Karr,Wolfram Liebermeister,Arthur P. Goldberg,John A. P. Sekar,Bilal Shaikh*

Main category: cs.DB

TL;DR: ObjTables工具包通过结合模式和对象关系映射系统，使电子表格具有人类和机器可读性，从而使研究人员能够重用和组合电子表格。


<details>
  <summary>Details</summary>
Motivation: 当前的科学研究面临着理解复杂系统行为如何从网络中涌现的挑战，而异构信息的聚合、重用和集成至关重要。文章的补充电子表格是关键的数据来源，但由于缺乏定义对象、关系和属性的模式，这些电子表格难以进行再分析。

Method: 开发了ObjTables工具包，该工具包包含一种用于模式的格式，用于指示每个电子表格和列所代表的类和属性的标记，以及多种科学信息数据类型。此外，还提供高级软件，用于使用模式读取、写入、验证、比较、合并、修订和分析电子表格。

Result: ObjTables工具包通过使电子表格更易于重用，有望实现前所未有的二次元分析。通过轻松构建新格式和相关软件来处理新型数据，ObjTables还可以加速新兴的科学领域。

Conclusion: ObjTables通过提供一种结合模式和对象关系映射的解决方案，解决了科学研究中电子表格数据难以重用和分析的问题，有望促进科学数据的集成和分析，并加速科学领域的发展。

Abstract: A central challenge in science is to understand how systems behaviors emerge from complex networks. This often requires aggregating, reusing, and integrating heterogeneous information. Supplementary spreadsheets to articles are a key data source. Spreadsheets are popular because they are easy to read and write. However, spreadsheets are often difficult to reanalyze because they capture data ad hoc without schemas that define the objects, relationships, and attributes that they represent. To help researchers reuse and compose spreadsheets, we developed ObjTables, a toolkit that makes spreadsheets human- and machine-readable by combining spreadsheets with schemas and an object-relational mapping system. ObjTables includes a format for schemas; markup for indicating the class and attribute represented by each spreadsheet and column; numerous data types for scientific information; and high-level software for using schemas to read, write, validate, compare, merge, revision, and analyze spreadsheets. By making spreadsheets easier to reuse, ObjTables could enable unprecedented secondary meta-analyses. By making it easy to build new formats and associated software for new types of data, ObjTables can also accelerate emerging scientific fields.

</details>


### [110] [Needles in the 'Sheet'stack: Augmented Analytics to get Insights from Spreadsheets](https://arxiv.org/abs/2006.08224)
*Medha Atre,Anand Deshpande,Reshma Godse,Pooja Deokar,Sandip Moharir,Dhruva Ray,Akshay Chitlangia,Trupti Phadnis,Yugansh Goyal*

Main category: cs.DB

TL;DR: This paper presents a tool that provides insights from periodic spreadsheets without requiring knowledge of the database schema, targeting users unfamiliar with traditional analytics methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide business intelligence and analytics capabilities to users who lack familiarity with database schemas or resources for traditional analytics.

Method: The method involves examining periodic spreadsheets of different reports (views) to extract insights, without prior knowledge of the schema, data, or information.

Result: The tool provides ready insights or visual query explorations from the given reports.

Conclusion: The solution is targeted at users without the familiarity with the database schema or resources to conduct analytics in the contemporary way.

Abstract: Business intelligence (BI) tools for database analytics have come a long way and nowadays also provide ready insights or visual query explorations, e.g. QuickInsights by Microsoft Power BI, SpotIQ by ThoughtSpot, Zenvisage, etc. In this demo, we focus on providing insights by examining periodic spreadsheets of different reports (aka views), without prior knowledge of the schema of the database or reports, or data information. Such a solution is targeted at users without the familiarity with the database schema or resources to conduct analytics in the contemporary way.

</details>


### [111] [A Billion Updates per Second Using 30,000 Hierarchical In-Memory D4M Databases](https://arxiv.org/abs/1902.00846)
*Jeremy Kepner,Vijay Gadepally,Lauren Milechin,Siddharth Samsi,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Micheal Houle,Micheal Jones,Anne Klein,Peter Michaleas,Julie Mullen,Andrew Prout,Antonio Rosa,Charles Yee,Albert Reuther*

Main category: cs.DB

TL;DR: D4M库使用分层关联数组在MIT超级计算集群上实现了每秒19亿次更新，以支持大规模流网络数据的分析。


<details>
  <summary>Details</summary>
Motivation: 分析大规模网络数据需要高性能的流更新图表示，而关联数组非常适合此任务。

Method: 实现了分层的D4M关联数组，并在MIT超级计算集群上进行了大规模测试，运行了34,000个实例，分布在1,100个服务器节点上。

Result: 实现了每秒1,900,000,000次更新的持续速率。

Conclusion: D4M库提供的高性能流更新能力使得MIT超级计算集群能够分析极大规模的流网络数据集。

Abstract: Analyzing large scale networks requires high performance streaming updates of graph representations of these data. Associative arrays are mathematical objects combining properties of spreadsheets, databases, matrices, and graphs, and are well-suited for representing and analyzing streaming network data. The Dynamic Distributed Dimensional Data Model (D4M) library implements associative arrays in a variety of languages (Python, Julia, and Matlab/Octave) and provides a lightweight in-memory database. Associative arrays are designed for block updates. Streaming updates to a large associative array requires a hierarchical implementation to optimize the performance of the memory hierarchy. Running 34,000 instances of a hierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of 1,900,000,000 updates per second. This capability allows the MIT SuperCloud to analyze extremely large streaming network data sets.

</details>


### [112] [Towards Semantically Enhanced Data Understanding](https://arxiv.org/abs/1806.04952)
*Markus Schröder,Christian Jilek,Jörn Hees,Andreas Dengel*

Main category: cs.DB

TL;DR: 我们将数据与其文档关联起来，创建一个单一的语义模型，以便于数据科学家的查询和利用。


<details>
  <summary>Details</summary>
Motivation: 目前数据科学家的文档查找和利用效率低下，因为文档通常与数据分离，并且是非结构化的，导致查找开销大，并且其他应用程序难以利用。我们提出了一种将数据与其文档链接起来的单一语义模型，以解决这些问题。

Method: 我们提出了一种使用单一语义模型将数据与其文档链接起来的方法。数据科学家可以直接链接到相关数据，或者浏览始终引用数据的文档。该模型还可以被其他应用程序用于搜索、比较、集成或可视化数据。

Result: 我们展示了一个早期原型来证明该方法。

Conclusion: 我们提出的单一语义模型能够有效地将数据与其文档链接起来，提高了数据科学家的查询效率，并为其他数据处理应用程序提供了支持。

Abstract: In the field of machine learning, data understanding is the practice of getting initial insights in unknown datasets. Such knowledge-intensive tasks require a lot of documentation, which is necessary for data scientists to grasp the meaning of the data. Usually, documentation is separate from the data in various external documents, diagrams, spreadsheets and tools which causes considerable look up overhead. Moreover, other supporting applications are not able to consume and utilize such unstructured data. That is why we propose a methodology that uses a single semantic model that interlinks data with its documentation. Hence, data scientists are able to directly look up the connected information about the data by simply following links. Equally, they can browse the documentation which always refers to the data. Furthermore, the model can be used by other approaches providing additional support, like searching, comparing, integrating or visualizing data. To showcase our approach we also demonstrate an early prototype.

</details>


<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [113] [Real-time stock analysis for blending recipes in industrial plants](https://arxiv.org/abs/1909.02960)
*Florin Zamfir,Nicolae Paraschiv,Emil Pricop*

Main category: cs.OH

TL;DR: 该论文介绍了一个实时库存分析应用程序，用于管理和优化混合过程中的产品库存。


<details>
  <summary>Details</summary>
Motivation: Excel表格在管理库存和过程数据时存在难以理解、追踪困难以及公式被误删等问题，因此需要一个集中化、易于操作的应用程序来辅助决策。

Method: 开发了一个使用C#实现的应用程序，集成了特定的计划算法，能够实时读取生产数据，并支持分步操作。

Result: 该应用程序能够识别混合过程所需的原料、确定现有原料可生产的成品数量，以及优化成品数量。

Conclusion: 该应用程序能有效解决传统库存管理中存在的问题，并通过集成算法为操作员提供决策支持。

Abstract: Many companies use Excel spreadsheets to keep stock records and to calculate process-specific data. These spreadsheets are often hard to understand and track. And if the user does not protect them, there is a risk that the user randomly changes or erase formulas. The paper focuses on the stocks of products used in a blending process with a known recipe. Developing an application that can bring this data in a centralized form and that can assist the operator in decide is a necessity. When a programmer implements an application that uses data from plants he needs to consider one fundamental aspect as reading real-time data from the process. The real-time stock analysis application takes into account all the above elements. The application is easy to use by an operator in the command room of installation because of the planning algorithms integrated into it. The algorithms proposed and implemented in this paper have well-defined goals: identifying the ingredients needed to achieve the blending process for required quantities, determine the quantities of the finished product that can be made with the existing ingredients and determine the optimum quantities of the finished product. The application implemented in C# intensively uses these algorithms and gives the user the ability to build the result step by step.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [114] [Reducing Aleatoric and Epistemic Uncertainty through Multi-modal Data Acquisition](https://arxiv.org/abs/2501.18268)
*Arthur Hoarau,Benjamin Quost,Sébastien Destercke,Willem Waegeman*

Main category: cs.LG

TL;DR: 多模态数据中，通过增加数据模态数量可减少不确定性，通过增加观测数量可减少认知不确定性。


<details>
  <summary>Details</summary>
Motivation: 多模态数据为分离不确定性带来了新的机遇和挑战，并对机器学习社区中普遍存在的关于认知不确定性和随机不确定性的假设提出了质疑。

Method: 提出一种新颖的数据采集框架，该框架通过在样本量和数据模态两个方向上进行采样，实现不确定性分离，从而做出可行的决策。该框架结合了主动学习、主动特征采集和不确定性量化等思想。

Result: 在两个多模态数据集上提供了概念验证实现，以展示所提出的数据采集框架。

Conclusion: 所提出的数据采集框架通过在样本量和数据模态两个方向上进行采样，可以实现不确定性分离，从而做出可行的决策。

Abstract: To generate accurate and reliable predictions, modern AI systems need to combine data from multiple modalities, such as text, images, audio, spreadsheets, and time series. Multi-modal data introduces new opportunities and challenges for disentangling uncertainty: it is commonly assumed in the machine learning community that epistemic uncertainty can be reduced by collecting more data, while aleatoric uncertainty is irreducible. However, this assumption is challenged in modern AI systems when information is obtained from different modalities. This paper introduces an innovative data acquisition framework where uncertainty disentanglement leads to actionable decisions, allowing sampling in two directions: sample size and data modality. The main hypothesis is that aleatoric uncertainty decreases as the number of modalities increases, while epistemic uncertainty decreases by collecting more observations. We provide proof-of-concept implementations on two multi-modal datasets to showcase our data acquisition framework, which combines ideas from active learning, active feature acquisition and uncertainty quantification.

</details>


### [115] [Large Scale Transfer Learning for Tabular Data via Language Modeling](https://arxiv.org/abs/2406.12031)
*Josh Gardner,Juan C. Perdomo,Ludwig Schmidt*

Main category: cs.LG

TL;DR: TabuLa-8B是一个针对表格预测任务的大型语言模型，通过在新提取的大型数据集上微调Llama 3-8B模型，并在零样本和少样本设置下展现出优于XGBoost和TabPFN等现有最先进模型的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管在语言和视觉领域已出现强大的基础模型，但在表格数据领域，这种迁移学习范式的影响有限。本文旨在缩小这一差距，为表格预测开发类似的基础模型。

Method: 从TabLib语料库中提取并筛选了一个包含超过21亿行、来自400万张表格的大型训练数据集。然后，使用一种新颖的表格数据打包和注意力机制，在Llama 3-8B模型上进行微调，以进行表格预测（分类和分箱回归）。

Result: 在329个数据集的测试套件上，TabuLa-8B在零样本设置下，对未见过表格的准确率比随机猜测高出15个百分点以上。在少样本设置下（1-32个样本），TabuLa-8B比经过相同数据量甚至16倍数据量显式训练的XGBoost和TabPFN模型准确率高5-15个百分点。

Conclusion: TabuLa-8B在表格预测任务上取得了显著进展，尤其是在零样本和少样本场景下，其性能超越了现有的最先进模型。该模型、代码和数据已公开发布。

Abstract: Tabular data -- structured, heterogeneous, spreadsheet-style data with rows and columns -- is widely used in practice across many domains. However, while recent foundation models have reduced the need for developing task-specific datasets and predictors in domains such as language modeling and computer vision, this transfer learning paradigm has not had similar impact in the tabular domain. In this work, we seek to narrow this gap and present TabuLa-8B, a language model for tabular prediction. We define a process for extracting a large, high-quality training dataset from the TabLib corpus, proposing methods for tabular data filtering and quality control. Using the resulting dataset, which comprises over 2.1B rows from over 4M unique tables, we fine-tune a Llama 3-8B large language model (LLM) for tabular data prediction (classification and binned regression) using a novel packing and attention scheme for tabular prediction. Through evaluation across a test suite of 329 datasets, we find that TabuLa-8B has zero-shot accuracy on unseen tables that is over 15 percentage points (pp) higher than random guessing, a feat that is not possible with existing state-of-the-art tabular prediction models (e.g. XGBoost, TabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the target datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN models that are explicitly trained on equal, or even up to 16x more data. We release our model, code, and data along with the publication of this paper.

</details>


### [116] [Generating tabular datasets under differential privacy](https://arxiv.org/abs/2308.14784)
*Gianluca Truda*

Main category: cs.LG

TL;DR: 本研究提出了一种新的生成模型TableDiffusion，用于在保持差分隐私的前提下生成高质量的合成表格数据，解决了现有生成模型在隐私和数据质量之间的权衡问题以及GANs在训练中的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型在合成表格数据时，尤其是在隐私保护方面存在不足，容易泄露原始数据。同时，GANs在处理表格数据时存在训练不稳定和模式崩溃的问题，而差分隐私的引入又加剧了数据质量和隐私之间的权衡。

Method: 本研究引入了两种新的端到端模型，利用注意力机制学习可逆的表格数据表示，并提出了TableDiffusion，这是第一个用于表格数据合成的差分隐私扩散模型。通过预测添加的噪声，模型能够绕过重建混合类型表格数据的挑战。

Result: 实验结果表明，TableDiffusion能够生成更高保真度的合成数据集，避免了模式崩溃问题，并在隐私保护的表格数据合成方面取得了最先进的性能。与GANs相比，扩散模型在数据和隐私效率方面表现更优，训练过程更平滑，并且能够更好地利用每个数据批次。

Conclusion: 扩散模型范式在生成合成表格数据方面比对抗范式更具数据和隐私效率，能够优化数据质量与隐私的权衡，生成更高质量的合成表格数据，同时保持差分隐私的保护。

Abstract: Machine Learning (ML) is accelerating progress across fields and industries, but relies on accessible and high-quality training data. Some of the most important datasets are found in biomedical and financial domains in the form of spreadsheets and relational databases. But this tabular data is often sensitive in nature. Synthetic data generation offers the potential to unlock sensitive data, but generative models tend to memorise and regurgitate training data, which undermines the privacy goal. To remedy this, researchers have incorporated the mathematical framework of Differential Privacy (DP) into the training process of deep neural networks. But this creates a trade-off between the quality and privacy of the resulting data. Generative Adversarial Networks (GANs) are the dominant paradigm for synthesising tabular data under DP, but suffer from unstable adversarial training and mode collapse, which are exacerbated by the privacy constraints and challenging tabular data modality. This work optimises the quality-privacy trade-off of generative models, producing higher quality tabular datasets with the same privacy guarantees. We implement novel end-to-end models that leverage attention mechanisms to learn reversible tabular representations. We also introduce TableDiffusion, the first differentially-private diffusion model for tabular data synthesis. Our experiments show that TableDiffusion produces higher-fidelity synthetic datasets, avoids the mode collapse problem, and achieves state-of-the-art performance on privatised tabular data synthesis. By implementing TableDiffusion to predict the added noise, we enabled it to bypass the challenges of reconstructing mixed-type tabular data. Overall, the diffusion paradigm proves vastly more data and privacy efficient than the adversarial paradigm, due to augmented re-use of each data batch and a smoother iterative training process.

</details>


### [117] [Smart Metro: Deep Learning Approaches to Forecasting the MRT Line 3 Ridership](https://arxiv.org/abs/2304.07303)
*Jayrald Empino,Jean Allyson Junsay,Mary Grace Verzon,Mideth Abisado,Shekinah Lor Huyo-a,Gabriel Avelino Sampedro*

Main category: cs.LG

TL;DR: This paper presents a time series prediction model to forecast the daily ridership of Metro Rail Transit Line 3 (MRT3) in Metro Manila, addressing the challenges commuters and the transportation department face due to fluctuating daily passenger counts.


<details>
  <summary>Details</summary>
Motivation: The Metro Rail Transit Line 3 (MRT3) experiences daily ridership fluctuations due to various factors, making it difficult for commuters to plan and for the DOTr to analyze historical data. This study aims to address this challenge by providing a method to predict future passenger counts.

Method: The study utilizes time series prediction to forecast the daily traffic and anticipate future attendance at specific MRT3 stations on given days.

Result: The paper presents a time series prediction of daily traffic to anticipate future attendance at a particular station on specific days.

Conclusion: The study aims to provide a more efficient way to manage and understand MRT3 ridership through time series prediction.

Abstract: Since its establishment in 1999, the Metro Rail Transit Line 3 (MRT3) has served as a transportation option for numerous passengers in Metro Manila, Philippines. The Philippine government's transportation department records more than a thousand people using the MRT3 daily and forecasting the daily passenger count may be rather challenging. The MRT3's daily ridership fluctuates owing to variables such as holidays, working days, and other unexpected issues. Commuters do not know how many other commuters are on their route on a given day, which may hinder their ability to plan an efficient itinerary. Currently, the DOTr depends on spreadsheets containing historical data, which might be challenging to examine. This study presents a time series prediction of daily traffic to anticipate future attendance at a particular station on specific days.

</details>


### [118] [Adversarial Networks and Machine Learning for File Classification](https://arxiv.org/abs/2301.11964)
*Ken St. Germain,Josh Angichiodo*

Main category: cs.LG

TL;DR: 机器学习可用于在文件扩展名或文件头被混淆时识别文件类型。


<details>
  <summary>Details</summary>
Motivation: 文件类型识别对于取证调查至关重要，但所有者可能会试图隐藏文件类型。

Method: 使用半监督生成对抗网络（SGAN）进行文件类型分类。

Result: SGAN 在 11 种不同类型的文件上实现了 97.6% 的准确率，并且在监督样本较少的情况下优于传统方法。

Conclusion: SGAN 是一种精确的文件分类器，尤其适用于样本量有限的情况。

Abstract: Correctly identifying the type of file under examination is a critical part of a forensic investigation. The file type alone suggests the embedded content, such as a picture, video, manuscript, spreadsheet, etc. In cases where a system owner might desire to keep their files inaccessible or file type concealed, we propose using an adversarially-trained machine learning neural network to determine a file's true type even if the extension or file header is obfuscated to complicate its discovery. Our semi-supervised generative adversarial network (SGAN) achieved 97.6% accuracy in classifying files across 11 different types. We also compared our network against a traditional standalone neural network and three other machine learning algorithms. The adversarially-trained network proved to be the most precise file classifier especially in scenarios with few supervised samples available. Our implementation of a file classifier using an SGAN is implemented on GitHub (https://ksaintg.github.io/SGAN-File-Classier).

</details>


### [119] [TabularNet: A Neural Network Architecture for Understanding Semantic Structures of Tabular Data](https://arxiv.org/abs/2106.03096)
*Lun Du,Fei Gao,Xu Chen,Ran Jia,Junshan Wang,Jiang Zhang,Shi Han,Dongmei Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为TabularNet的新型神经网络架构，用于同时提取表格的空间和关系信息，以解决表格数据语义结构理解的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究在理解表格数据时，虽然利用卷积神经网络（CNN）提取空间信息，但忽略了单元格之间多样的关系信息（如层级和并列关系）。

Method: TabularNet包含一个空间编码器（利用行/列池化和双向门控循环单元（Bi-GRU）捕捉统计信息和局部位置相关性）和一个关系编码器（基于WordNet树构建图，并利用图卷积网络（GCN）关注单元格间的层级和并列关系）。

Result: 在三个分类任务和两个真实世界电子表格数据集上的广泛实验表明，TabularNet在各项指标上均优于最先进的基线模型。

Conclusion: TabularNet能够作为多任务场景下不同理解任务的统一神经骨干，有效提取表格的空间和关系信息。

Abstract: Tabular data are ubiquitous for the widespread applications of tables and hence have attracted the attention of researchers to extract underlying information. One of the critical problems in mining tabular data is how to understand their inherent semantic structures automatically. Existing studies typically adopt Convolutional Neural Network (CNN) to model the spatial information of tabular structures yet ignore more diverse relational information between cells, such as the hierarchical and paratactic relationships. To simultaneously extract spatial and relational information from tables, we propose a novel neural network architecture, TabularNet. The spatial encoder of TabularNet utilizes the row/column-level Pooling and the Bidirectional Gated Recurrent Unit (Bi-GRU) to capture statistical information and local positional correlation, respectively. For relational information, we design a new graph construction method based on the WordNet tree and adopt a Graph Convolutional Network (GCN) based encoder that focuses on the hierarchical and paratactic relationships between cells. Our neural network architecture can be a unified neural backbone for different understanding tasks and utilized in a multitask scenario. We conduct extensive experiments on three classification tasks with two real-world spreadsheet data sets, and the results demonstrate the effectiveness of our proposed TabularNet over state-of-the-art baselines.

</details>


### [120] [Visual Backpropagation](https://arxiv.org/abs/1906.04011)
*Roy S. Freedman*

Main category: cs.LG

TL;DR: 在电子表格中通过函数式编程实现反向传播，称为可视化反向传播。


<details>
  <summary>Details</summary>
Motivation: 本论文的动机是展示如何通过函数式编程在电子表格中实现一个可视化且透明的反向传播。

Method: 可视化反向传播利用数组工作表公式、手动计算和类似脉动阵列的处理顺序。

Result: 将可视化反向传播与 Tensorflow（Python）解决方案在标准回归问题上进行了比较。

Conclusion: 可视化反向传播在电子表格中提供了一种无需宏或用户定义函数即可实现反向传播的方法。

Abstract: We show how a declarative functional programming specification of backpropagation yields a visual and transparent implementation within spreadsheets. We call our method Visual Backpropagation. This backpropagation implementation exploits array worksheet formulas, manual calculation, and has a sequential order of computation similar to the processing of a systolic array. The implementation uses no hidden macros nor user-defined functions; there are no loops, assignment statements, or links to any procedural programs written in conventional languages. As an illustration, we compare a Visual Backpropagation solution to a Tensorflow (Python) solution on a standard regression problem.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [121] [A CNN toolbox for skin cancer classification](https://arxiv.org/abs/1908.08187)
*Fabrizio Nunnari,Daniel Sonntag*

Main category: eess.IV

TL;DR: 该软件工具箱简化了皮肤癌分类中深度神经网络的配置，使得技术和非技术用户都能快速设置和探索CNN架构。


<details>
  <summary>Details</summary>
Motivation: 为了简化皮肤癌分类领域中深度神经网络的配置过程，并方便技术和非技术用户进行模型调优。

Method: 实现了一个软件工具箱，允许开发者快速设置CNN架构和超参数，并提供了一个简单的电子表格式用户界面供非技术用户探索配置。

Result: 通过在黑色素瘤检测任务中应用该工具箱，并对图像增强、分辨率和重缩放滤波器等因素进行分析，初步量化了它们对检测性能和训练时间的影响。

Conclusion: 该软件工具箱能够有效地支持皮肤癌分类中深度神经网络的配置和探索，并为未来集成元学习和AutoML系统奠定了基础。

Abstract: We describe a software toolbox for the configuration of deep neural networks in the domain of skin cancer classification. The implemented software architecture allows developers to quickly set up new convolutional neural network (CNN) architectures and hyper-parameter configurations. At the same time, the user interface, manageable as a simple spreadsheet, allows non-technical users to explore different configuration settings that need to be explored when switching to different data sets. In future versions, meta leaning frameworks can be added, or AutoML systems that continuously improve over time. Preliminary results, conducted with two CNNs in the context melanoma detection on dermoscopic images, quantify the impact of image augmentation, image resolution, and rescaling filter on the overall detection performance and training time.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [122] [Data Encryption Battlefield: A Deep Dive into the Dynamic Confrontations in Ransomware Attacks](https://arxiv.org/abs/2504.20681)
*Arash Mahboubi,Hamed Aboutorab,Seyit Camtepe,Hang Thanh Bui,Khanh Luong,Keyvan Ansari,Shenlu Wang,Bazara Barry*

Main category: cs.CR

TL;DR: 研究人员研究了在线增量机器学习算法在检测不断变化的加密策略以应对勒索软件方面的应用。


<details>
  <summary>Details</summary>
Motivation: 由于攻击者不断改进加密策略（如通过Base64编码降低熵和采用部分/间歇性加密）以逃避检测，因此需要开发先进的对策来保护数据。

Method: 研究人员使用了一个包含11,928个文件（多种格式）的32.6 GB数据集，这些文件由75个不同的勒索软件家族加密。他们评估了Hoeffding树和随机森林（带热启动）等在线增量机器学习算法的有效性。

Result: Hoeffding树算法在检测传统的和AES-Base64加密方面效果更好，而随机森林算法在检测间歇性加密方面效果更好。

Conclusion: 需要根据具体的加密策略采用定制的机器学习解决方案来有效应对复杂的勒索软件策略。

Abstract: In the rapidly evolving landscape of cybersecurity threats, ransomware represents a significant challenge. Attackers increasingly employ sophisticated encryption methods, such as entropy reduction through Base64 encoding, and partial or intermittent encryption to evade traditional detection methods. This study explores the dynamic battle between adversaries who continuously refine encryption strategies and defenders developing advanced countermeasures to protect vulnerable data. We investigate the application of online incremental machine learning algorithms designed to predict file encryption activities despite adversaries evolving obfuscation techniques. Our analysis utilizes an extensive dataset of 32.6 GB, comprising 11,928 files across multiple formats, including Microsoft Word documents (doc), PowerPoint presentations (ppt), Excel spreadsheets (xlsx), image formats (jpg, jpeg, png, tif, gif), PDFs (pdf), audio (mp3), and video (mp4) files. These files were encrypted by 75 distinct ransomware families, facilitating a robust empirical evaluation of machine learning classifiers effectiveness against diverse encryption tactics. Results highlight the Hoeffding Tree algorithms superior incremental learning capability, particularly effective in detecting traditional and AES-Base64 encryption methods employed to lower entropy. Conversely, the Random Forest classifier with warm-start functionality excels at identifying intermittent encryption methods, demonstrating the necessity of tailored machine learning solutions to counter sophisticated ransomware strategies.

</details>


### [123] [JUBILEE: Secure Debt Relief and Forgiveness](https://arxiv.org/abs/2109.07267)
*David Cerezo Sánchez*

Main category: cs.CR

TL;DR: JUBILEE是一种安全、无摩擦的债务减免和债务豁免机制，无需可信第三方，通过激励机制促使各方披露真实信息，从而实现更和谐的债务结算。它在个体理性、激励相容、策略证明、事后有效和最优性方面优于以往所有方法。


<details>
  <summary>Details</summary>
Motivation: JUBILEE旨在通过引入安全计算技术，实现更优化的债务减免和豁免，并首次实现了“债务人祝福”，即在不使用安全计算的情况下，提高债务结算的预期利润和成功率。

Method: JUBILEE利用安全计算技术（包括“The Secure Spreadsheet”和基于Pravuil共识的Raziel智能合约）来实现债务减免和豁免。

Result: JUBILEE在个体理性、激励相容、策略证明、事后有效和最优性方面均优于以往所有方法，并能提高债务结算的预期利润和成功率。

Conclusion: JUBILEE是一种创新的安全计算机制，能够以无摩擦、可信的方式实现债务减免和豁免，并为债务人带来更高的预期利润和成功率。

Abstract: JUBILEE is a securely computed mechanism for debt relief and forgiveness in a frictionless manner without involving trusted third parties, leading to more harmonious debt settlements by incentivising the parties to truthfully reveal their private information. JUBILEE improves over all previous methods:
  - individually rational, incentive-compatible, truthful/strategy-proof, ex-post efficient, optimal mechanism for debt relief and forgiveness with private information
  - by the novel introduction of secure computation techniques to debt relief, the "blessing of the debtor" is hereby granted for the first time: debt settlements with higher expected profits and a higher probability of success than without using secure computation
  A simple and practical implementation is included for "The Secure Spreadsheet". Another implementation is realised using Raziel smart contracts on a blockchain with Pravuil consensus.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [124] [Mathematics of Digital Hyperspace](https://arxiv.org/abs/2103.15203)
*Jeremy Kepner,Timothy Davis,Vijay Gadepally,Hayden Jananthan,Lauren Milechin*

Main category: cs.MS

TL;DR: 该论文提出了一种名为“semilink”的新型数学概念，它结合了半环对，为图分析、数据库操作和机器学习提供了基本运算。该概念基于超图、超稀疏矩阵和关联数组代数，并提出将其集成到GraphBLAS标准中，以增强对数字假想空间中非结构化数据的导航能力。


<details>
  <summary>Details</summary>
Motivation: 当前数字假想空间中的非结构化数据流对标准的类型和维度概念提出了挑战。需要一种新的数学方法来优雅地表示、遍历和转换这些数据。

Method: 提出了一种名为“semilink”的新型数学概念，该概念结合了成对的半环，以提供图分析、数据库操作和机器学习所必需的基本运算。同时，该论文探讨了将“semilink”概念和基于键的索引（如指向字符串的指针）添加到GraphBLAS标准中的可能性。

Result: 通过将键基索引（例如指向字符串的指针）和 semilinks 添加到 GraphBLAS，可以使其成为更丰富的关联数组代数，并可作为电子表格、数据库表和数据中心操作系统的插件式替代品，从而增强对数字假想空间中非结构化数据的导航能力。

Conclusion: Semilinks 和 GraphBLAS 的结合为处理数字假想空间中的非结构化数据提供了一种强大的新方法，有望在图分析、数据库操作、机器学习以及更广泛的数据管理领域带来改进。

Abstract: Social media, e-commerce, streaming video, e-mail, cloud documents, web pages, traffic flows, and network packets fill vast digital lakes, rivers, and oceans that we each navigate daily. This digital hyperspace is an amorphous flow of data supported by continuous streams that stretch standard concepts of type and dimension. The unstructured data of digital hyperspace can be elegantly represented, traversed, and transformed via the mathematics of hypergraphs, hypersparse matrices, and associative array algebra. This paper explores a novel mathematical concept, the semilink, that combines pairs of semirings to provide the essential operations for graph analytics, database operations, and machine learning. The GraphBLAS standard currently supports hypergraphs, hypersparse matrices, the mathematics required for semilinks, and seamlessly performs graph, network, and matrix operations. With the addition of key based indices (such as pointers to strings) and semilinks, GraphBLAS can become a richer associative array algebra and be a plug-in replacement for spreadsheets, database tables, and data centric operating systems, enhancing the navigation of unstructured data found in digital hyperspace.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [125] [Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models](https://arxiv.org/abs/2505.23667)
*Lang Cao,Jingxian Xu,Hanbing Liu,Jinyu Wang,Mengyu Zhou,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 使用基于RL的Formula Tuning（Fortune）框架，通过生成可执行的电子表格公式来提升大语言模型在表格数据问答上的数值和符号推理能力，减少对标注数据的依赖，并在多个基准测试中取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理表格数据的数值或符号推理方面能力不足，尤其是在复杂场景下，而电子表格公式能够编码丰富的推理模式，但尚未得到充分利用。

Method: 提出Formula Tuning（Fortune）框架，这是一个强化学习（RL）框架，通过使用二元的答案正确性作为奖励信号，训练大语言模型生成可执行的电子表格公式，以进行表格数据的问答，从而减少对监督公式标注的依赖。

Result: Formula Tuning显著提升了大语言模型在表格理解任务上的性能，特别是在多步数值和符号推理任务上。一个7B的模型通过Fortune框架的能力超过了OpenAI o1。

Conclusion: Formula Tuning框架有潜力通过驱动公式的强化学习来推进大语言模型在符号表格推理方面的能力。

Abstract: Tables are a fundamental structure for organizing and analyzing data, making effective table understanding a critical capability for intelligent systems. While large language models (LMs) demonstrate strong general reasoning abilities, they continue to struggle with accurate numerical or symbolic reasoning over tabular data, especially in complex scenarios. Spreadsheet formulas provide a powerful and expressive medium for representing executable symbolic operations, encoding rich reasoning patterns that remain largely underutilized. In this paper, we propose Formula Tuning (Fortune), a reinforcement learning (RL) framework that trains LMs to generate executable spreadsheet formulas for question answering over general tabular data. Formula Tuning reduces the reliance on supervised formula annotations by using binary answer correctness as a reward signal, guiding the model to learn formula derivation through reasoning. We provide a theoretical analysis of its advantages and demonstrate its effectiveness through extensive experiments on seven table reasoning benchmarks. Formula Tuning substantially enhances LM performance, particularly on multi-step numerical and symbolic reasoning tasks, enabling a 7B model to outperform OpenAI o1 on table understanding. This highlights the potential of formula-driven RL to advance symbolic table reasoning in LMs.

</details>


### [126] [The AI Risk Repository: A Comprehensive Meta-Review, Database, and Taxonomy of Risks From Artificial Intelligence](https://arxiv.org/abs/2408.12622)
*Peter Slattery,Alexander K. Saeri,Emily A. C. Grundy,Jess Graham,Michael Noetel,Risto Uuk,James Dao,Soroush Pour,Stephen Casper,Neil Thompson*

Main category: cs.AI

TL;DR: 该论文通过创建一个包含777个AI风险的公共数据库，解决了AI风险理解不一致的问题，为AI风险的管理和审计奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 为了解决学术界、审计界、政策制定者、AI公司和公众对AI风险缺乏共同理解的问题，本研究旨在创建一个AI风险的共同参考框架。

Method: 本研究通过系统性地审查现有的43种AI风险分类法，并结合专家咨询，利用“最佳拟合框架综合法”构建了一个包含777个AI风险的数据库，该数据库可通过网站和在线电子表格进行访问、修改和更新。论文还提出了两个高层级的分类法：因果分类法（区分实体、意图和时机）和领域分类法（包含七个主要领域和23个子领域）。

Result: 创建了一个包含777个AI风险的、可公开访问的、可扩展的、经过严格整理和分析的AI风险数据库。该数据库利用因果分类法和领域分类法进行组织，为AI风险的研究、讨论和管理提供了一个共同的参考框架。

Conclusion: 本研究创建的AI风险知识库是第一个全面、公开的AI风险数据库，它为更协调、更连贯、更完整的AI风险定义、审计和管理方法奠定了基础。

Abstract: The risks posed by Artificial Intelligence (AI) are of considerable concern to academics, auditors, policymakers, AI companies, and the public. However, a lack of shared understanding of AI risks can impede our ability to comprehensively discuss, research, and react to them. This paper addresses this gap by creating an AI Risk Repository to serve as a common frame of reference. This comprises a living database of 777 risks extracted from 43 taxonomies, which can be filtered based on two overarching taxonomies and easily accessed, modified, and updated via our website and online spreadsheets. We construct our Repository with a systematic review of taxonomies and other structured classifications of AI risk followed by an expert consultation. We develop our taxonomies of AI risk using a best-fit framework synthesis. Our high-level Causal Taxonomy of AI Risks classifies each risk by its causal factors (1) Entity: Human, AI; (2) Intentionality: Intentional, Unintentional; and (3) Timing: Pre-deployment; Post-deployment. Our mid-level Domain Taxonomy of AI Risks classifies risks into seven AI risk domains: (1) Discrimination & toxicity, (2) Privacy & security, (3) Misinformation, (4) Malicious actors & misuse, (5) Human-computer interaction, (6) Socioeconomic & environmental, and (7) AI system safety, failures, & limitations. These are further divided into 23 subdomains. The AI Risk Repository is, to our knowledge, the first attempt to rigorously curate, analyze, and extract AI risk frameworks into a publicly accessible, comprehensive, extensible, and categorized risk database. This creates a foundation for a more coordinated, coherent, and complete approach to defining, auditing, and managing the risks posed by AI systems.

</details>


### [127] [SpreadsheetLLM: Encoding Spreadsheets for Large Language Models](https://arxiv.org/abs/2407.09025)
*Haoyu Dong,Jianbo Zhao,Yuzhang Tian,Junyu Xiong,Shiyu Xia,Mengyu Zhou,Yun Lin,José Cambronero,Yeye He,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: SpreadsheetLLM 提出了一种新的编码方法来优化 LLM 在电子表格上的理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 电子表格的二维网格、灵活布局和多样化格式给 LLM 带来了挑战。

Method: 提出了一种结合单元格地址、值和格式的序列化方法，并开发了包括结构锚定压缩、逆索引翻译和数据格式感知聚合的 SheetCompressor 框架。还提出了用于电子表格理解的 Chain of Spreadsheet。

Result: SheetCompressor 在电子表格表格检测任务上比普通方法提高了 25.6%。使用 SheetCompressor 微调的 LLM 实现了 25 倍的平均压缩率和 78.9% 的 F1 分数，优于现有模型 12.3%。

Conclusion: SpreadsheetLLM 在各种电子表格任务中都非常有效，能够有效利用电子表格的布局和结构。

Abstract: Spreadsheets are characterized by their extensive two-dimensional grids, flexible layouts, and varied formatting options, which pose significant challenges for large language models (LLMs). In response, we introduce SpreadsheetLLM, pioneering an efficient encoding method designed to unleash and optimize LLMs' powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs' token constraints, making it impractical for most applications. To tackle this challenge, we develop SheetCompressor, an innovative encoding framework that compresses spreadsheets effectively for LLMs. It comprises three modules: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in the spreadsheet table detection task, outperforming the vanilla approach by 25.6% in GPT4's in-context learning setting. Moreover, fine-tuned LLM with SheetCompressor has an average compression ratio of 25 times, and achieves a state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%. Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet understanding and validate it in a new and demanding spreadsheet QA task. We methodically leverage the inherent layout and structure of spreadsheets, demonstrating that SpreadsheetLLM is highly effective across a variety of spreadsheet tasks.

</details>


### [128] [SheetAgent: Towards A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models](https://arxiv.org/abs/2403.03636)
*Yibin Chen,Yifu Yuan,Zeyu Zhang,Yan Zheng,Jinyi Liu,Fei Ni,Jianye Hao,Hangyu Mao,Fuzheng Zhang*

Main category: cs.AI

TL;DR: SheetRM是一个包含长时限、多类别、需要推理的电子表格操作任务的基准测试。SheetAgent是一个利用LLM的自主代理，通过迭代任务推理和反思来处理这些任务，并在实验中显示出比基线有显著的改进。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）在处理复杂和现实的电子表格任务方面存在不足，特别是那些需要长时序推理和处理模糊需求的任务。本项目旨在弥合这一差距。

Method: 提出SheetRM基准测试，包含长时限、多类别、需要推理的电子表格操作任务。提出SheetAgent自主代理，包含规划器、信息员和检索器三个协作模块，通过迭代任务推理和反思，实现无需人工干预的电子表格操作和推理。

Result: SheetAgent在多个基准测试上比现有方法提高了20%--40%的成功率，提高了电子表格操作的准确性，并展示了其在表格推理方面的优越能力。

Conclusion: SheetAgent能够有效地处理复杂和现实的电子表格任务，展现出优越的推理和操作能力，为电子表格的自动化处理提供了一个有前景的解决方案。

Abstract: Spreadsheets are ubiquitous across the World Wide Web, playing a critical role in enhancing work efficiency across various domains. Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce SheetRM, a benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges. To mitigate the above challenges, we further propose SheetAgent, a novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of three collaborative modules: Planner, Informer, and Retriever, achieving both advanced reasoning and accurate manipulation over spreadsheets without human interaction through iterative task reasoning and reflection. Extensive experiments demonstrate that SheetAgent delivers 20--40\% pass rate improvements on multiple benchmarks over baselines, achieving enhanced precision in spreadsheet manipulation and demonstrating superior table reasoning abilities. More details and visualizations are available at the project website: https://sheetagent.github.io/. The datasets and source code are available at https://anonymous.4open.science/r/SheetAgent.

</details>


### [129] [Large Language Model for Table Processing: A Survey](https://arxiv.org/abs/2402.05121)
*Weizheng Lu,Jing Zhang,Ju Fan,Zihao Fu,Yueguo Chen,Xiaoyong Du*

Main category: cs.AI

TL;DR: 这是一篇关于使用大型语言模型（LLM）和视觉语言模型（VLM）来处理表格数据的调查论文，涵盖了从传统任务到新兴领域的各种应用、训练技术、提示工程以及面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 自动化处理表格数据，如数据库查询、电子表格操作、网络表格问答和图像表格信息提取，具有重要的公众效益，并引起了学术界和工业界的广泛关注。

Method: 该调查论文全面概述了表格相关任务，检查了用户场景和技术方面，包括传统的表格问答以及新兴的电子表格操作和表格数据分析领域。此外，还总结了针对表格处理的LLM和VLM的训练技术，并讨论了提示工程，特别是LLM驱动的代理在各种表格相关任务中的应用。

Result: 论文总结了处理表格数据的LLM和VLM的训练技术，并讨论了提示工程（特别是LLM驱动的代理）在各种表格相关任务中的应用。

Conclusion: 尽管LLM和VLM在表格处理方面取得了进展，但仍存在一些挑战，包括用户服务的多样化输入以及使用链式思考（chain-of-thought）时处理速度缓慢的问题。

Abstract: Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, web table question answering, and image table information extraction. Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides a comprehensive overview of table-related tasks, examining both user scenarios and technical aspects. It covers traditional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processing. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought.

</details>


### [130] [FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language](https://arxiv.org/abs/2310.17306)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Elnaz Nouri,Mohammad Raza,Gust Verbruggen*

Main category: cs.AI

TL;DR: FormaT5是一个基于Transformer的模型，可以通过自然语言描述生成条件格式规则，解决了用户在电子表格中自动格式化表格的难题。


<details>
  <summary>Details</summary>
Motivation: 用户在电子表格中自动格式化表格时，需要理解和实现底层逻辑，编写条件格式规则具有挑战性。

Method: FormaT5通过学习预测占位符来解决描述不充分或模棱两可的问题，占位符可以由第二个模型或编程 by-example 系统填充。

Result: FormaT5在一个包含1053个条件格式任务的基准上，优于8种不同的神经方法，并且在有无示例的情况下都表现良好。

Conclusion: FormaT5通过引入占位符和处理不充分描述的能力，在生成条件格式规则方面取得了显著成果，并强调了构建特定领域学习系统的重要性。

Abstract: Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires them to understand and implement the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system. To evaluate FormaT5 on diverse and real scenarios, we create an extensive benchmark of 1053 CF tasks, containing real-world descriptions collected from four different sources. We release our benchmarks to encourage research in this area. Abstention and filling allow FormaT5 to outperform 8 different neural approaches on our benchmarks, both with and without examples. Our results illustrate the value of building domain-specific learning systems.

</details>


### [131] [Don't Treat the Symptom, Find the Cause! Efficient Artificial-Intelligence Methods for (Interactive) Debugging](https://arxiv.org/abs/2306.12850)
*Patrick Rodler*

Main category: cs.AI

TL;DR: 模型驱动诊断是一种通用的系统故障排除方法，旨在通过利用人工智能技术（如知识表示、自动推理、机器学习等）来检测、定位和修复故障，以最大限度地减少系统停机时间和修复成本。


<details>
  <summary>Details</summary>
Motivation: 现代社会高度依赖日益复杂的系统，系统故障可能导致严重的负面影响，因此需要最小化故障造成的损害。

Method: 介绍模型驱动诊断，指出该领域的挑战，并讨论解决这些挑战的研究方法。

Result: 该论文将介绍模型驱动诊断，讨论其主要挑战，并概述相关的研究方法。

Conclusion: 模型驱动诊断是一种通用且强大的故障排除技术，对于确保复杂系统的可靠性至关重要。

Abstract: In the modern world, we are permanently using, leveraging, interacting with, and relying upon systems of ever higher sophistication, ranging from our cars, recommender systems in e-commerce, and networks when we go online, to integrated circuits when using our PCs and smartphones, the power grid to ensure our energy supply, security-critical software when accessing our bank accounts, and spreadsheets for financial planning and decision making. The complexity of these systems coupled with our high dependency on them implies both a non-negligible likelihood of system failures, and a high potential that such failures have significant negative effects on our everyday life. For that reason, it is a vital requirement to keep the harm of emerging failures to a minimum, which means minimizing the system downtime as well as the cost of system repair. This is where model-based diagnosis comes into play.
  Model-based diagnosis is a principled, domain-independent approach that can be generally applied to troubleshoot systems of a wide variety of types, including all the ones mentioned above, and many more. It exploits and orchestrates i.a. techniques for knowledge representation, automated reasoning, heuristic problem solving, intelligent search, optimization, stochastics, statistics, decision making under uncertainty, machine learning, as well as calculus, combinatorics and set theory to detect, localize, and fix faults in abnormally behaving systems.
  In this thesis, we will give an introduction to the topic of model-based diagnosis, point out the major challenges in the field, and discuss a selection of approaches from our research addressing these issues.

</details>


### [132] [CORNET: Learning Table Formatting Rules By Example](https://arxiv.org/abs/2208.06032)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Mohammad Raza,Gust Verbruggen*

Main category: cs.AI

TL;DR: CORNET是一个自动学习电子表格样式规则的系统，它通过用户提供的格式化单元格示例，结合符号规则枚举和神经网络排序，能够准确地学习、发现甚至生成比用户编写的更简洁的规则。


<details>
  <summary>Details</summary>
Motivation: 用户难以编写电子表格的样式格式化规则，需要了解底层规则语言和数据逻辑。现有软件（如Excel）支持自动格式化，但编写规则具有挑战性。

Method: CORNET系统受到归纳编程的启发，结合了符号规则枚举和神经网络排序，以学习条件格式化规则。它从用户提供的格式化单元格示例中学习规则。

Result: CORNET在各种评估设置下都能准确地学习规则。此外，它找到的规则比用户编写的更短，并且能够发现用户手动格式化的电子表格中的规则。在从180万个实际工作表中提取的包含超过45万个独特格式化规则的语料库上进行了评估。

Conclusion: CORNET成功地解决了自动学习电子表格条件格式化规则的问题，证明了其在准确性和规则简洁性方面的优势，并能发现用户手动格式化的规则。

Abstract: Spreadsheets are widely used for table manipulation and presentation. Stylistic formatting of these tables is an important property for both presentation and analysis. As a result, popular spreadsheet software, such as Excel, supports automatically formatting tables based on rules. Unfortunately, writing such formatting rules can be challenging for users as it requires knowledge of the underlying rule language and data logic. We present CORNET, a system that tackles the novel problem of automatically learning such formatting rules from user examples in the form of formatted cells. CORNET takes inspiration from advances in inductive programming and combines symbolic rule enumeration with a neural ranker to learn conditional formatting rules. To motivate and evaluate our approach, we extracted tables with over 450K unique formatting rules from a corpus of over 1.8M real worksheets. Since we are the first to introduce conditional formatting, we compare CORNET to a wide range of symbolic and neural baselines adapted from related domains. Our results show that CORNET accurately learns rules across varying evaluation setups. Additionally, we show that CORNET finds shorter rules than those that a user has written and discovers rules in spreadsheets that users have manually formatted.

</details>


### [133] [Named Entity Recognition in Industrial Tables using Tabular Language Models](https://arxiv.org/abs/2209.14812)
*Aneta Koleva,Martin Ringsquandl,Mark Buckley,Rakebul Hasan,Volker Tresp*

Main category: cs.AI

TL;DR: 专门的基于 transformer 的模型在学术界引起了人们对编码表数据的兴趣。然而，这些模型在工业界尚未得到应用。本文研究了如何将这些模型应用于工业命名实体识别（NER）问题，其中实体出现在表格结构电子表格中。本文提出了一种基于知识图谱的数据增强策略，以解决电子表格的技术性质和标记数据稀缺的挑战。实验结果表明，该策略在低资源场景下显著提高了性能。此外，本文还探讨了与线性化序列相比，表格结构作为归纳偏置的优势。实验证明，表格 transformer 的性能优于其他基线模型，并且其归纳偏置对于 transformer 模型的收敛至关重要。


<details>
  <summary>Details</summary>
Motivation: 研究如何将学术界提出的专门的基于 transformer 的模型应用于工业命名实体识别（NER）问题，并解决该领域面临的挑战，如电子表格的技术性质和标记数据稀缺。

Method: 开发了一种基于知识图谱的数据增强策略，以提高模型在低资源场景下的性能。并通过实验比较了表格 transformer 和其他基线模型（包括将表格视为线性化序列的方法）的性能，以验证表格结构作为归纳偏置的优势。

Result: 所提出的数据增强策略在低资源场景下显著提高了表格 transformer 在 NER 任务上的性能。实验结果表明，表格 transformer 优于其他基线模型，并且其表格归纳偏置对于模型的收敛至关重要。

Conclusion: 表格 transformer 模型及其表格归纳偏置对于在低资源场景下解决工业 NER 问题至关重要，并且通过结合基于知识图谱的数据增强策略可以进一步提高性能。

Abstract: Specialized transformer-based models for encoding tabular data have gained interest in academia. Although tabular data is omnipresent in industry, applications of table transformers are still missing. In this paper, we study how these models can be applied to an industrial Named Entity Recognition (NER) problem where the entities are mentioned in tabular-structured spreadsheets. The highly technical nature of spreadsheets as well as the lack of labeled data present major challenges for fine-tuning transformer-based models. Therefore, we develop a dedicated table data augmentation strategy based on available domain-specific knowledge graphs. We show that this boosts performance in our low-resource scenario considerably. Further, we investigate the benefits of tabular structure as inductive bias compared to tables as linearized sequences. Our experiments confirm that a table transformer outperforms other baselines and that its tabular inductive bias is vital for convergence of transformer-based models.

</details>


### [134] [Spreadsheet computing with Finite Domain Constraint Enhancements](https://arxiv.org/abs/2203.10944)
*Ezana N. Beyenne*

Main category: cs.AI

TL;DR: 本文提出了一种将有限约束求解器与电子表格计算范式相结合的框架，以克服电子表格在处理约束满足问题时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的电子表格应用功能有限，仅限于类似簿记的任务，因为它们是单向数据流。然而，电子表格的易用性使其成为非程序员的流行选择。因此，有必要扩展电子表格的计算范式，以处理更复杂的任务，如约束满足问题。

Method: 提出了一种框架，将有限约束求解器无缝集成到电子表格计算范式中。在此框架中，电子表格中的单元格可以附加到有限域或指定单元格之间关系的约束。该框架还提供了一个约束求解接口，并包含一组特定于电子表格的约束，以帮助控制大型电子表格应用程序实现的规模。

Result: 通过示例证明了扩展电子表格范式的可用性和实用性。

Conclusion: 所提出的框架成功地将电子表格计算的易用性与约束求解能力相结合，从而扩展了电子表格在解决约束满足问题方面的能力。

Abstract: Spreadsheet computing is one of the more popular computing methodologies in today's modern society. The spreadsheet application's ease of use and usefulness has enabled non-programmers to perform programming-like tasks in a familiar setting modeled after the tabular "pen and paper" approach. However, spreadsheet applications are limited to bookkeeping-like tasks due to their single-direction data flow. This thesis demonstrates an extension of the spreadsheet computing paradigm in overcoming this limitation to solve constraint satisfaction problems. We present a framework seamlessly incorporating a finite constraint solver with the spreadsheet computing paradigm. This framework allows the individual cells in the spreadsheet to be attached to either a finite domain or a constraint specifying the relationship among the cells. The framework provides an interface for constraint solving and further enhances the spreadsheet computing paradigm by providing a set of spreadsheet-specific constraints that will aid in controlling the scalability of large spreadsheet applications implementations. Finally, we provide examples to demonstrate the usability and usefulness of the extended spreadsheet paradigm.
  Keywords: Spreadsheet computing, Constraint Logic Programming, Constraint satisfaction, Domain-Specific language, Excel, SWI Prolog, C#

</details>


### [135] [Human-Machine Collaboration for Democratizing Data Science](https://arxiv.org/abs/2004.11113)
*Clément Gautrais,Yann Dauxais,Stefano Teso,Samuel Kolb,Gust Verbruggen,Luc De Raedt*

Main category: cs.AI

TL;DR: VisualSynth 是一个新颖的框架和系统，用于数据科学中的人机协作，它允许用户通过与电子表格软件交互来执行和自动化各种数据分析任务。


<details>
  <summary>Details</summary>
Motivation: 许多人都希望分析自己的数据，但缺乏专业的数据科学知识。VisualSynth 旨在通过允许用户利用电子表格软件来解决这个问题，从而普及数据科学。

Method: VisualSynth 利用用户提供的彩色草图（即对电子表格的某些部分进行着色）来部分指定数据科学任务，然后利用人工智能技术来确定和执行这些任务。

Result: VisualSynth 支持从数据整理、数据选择、聚类、约束学习、预测建模到自动补全的各种数据分析任务。

Conclusion: VisualSynth 通过人机协作和基于草图的交互，使非专业用户能够执行和自动化复杂的数据科学任务，从而实现数据科学的民主化。

Abstract: Everybody wants to analyse their data, but only few posses the data science expertise to to this. Motivated by this observation we introduce a novel framework and system \textsc{VisualSynth} for human-machine collaboration in data science.
  It wants to democratize data science by allowing users to interact with standard spreadsheet software in order to perform and automate various data analysis tasks ranging from data wrangling, data selection, clustering, constraint learning, predictive modeling and auto-completion. \textsc{VisualSynth} relies on the user providing colored sketches, i.e., coloring parts of the spreadsheet, to partially specify data science tasks, which are then determined and executed using artificial intelligence techniques.

</details>


### [136] [Automated Discovery of Data Transformations for Robotic Process Automation](https://arxiv.org/abs/2001.01007)
*Volodymyr Leno,Marlon Dumas,Marcello La Rosa,Fabrizio Maria Maggi,Artem Polyvyanyy*

Main category: cs.AI

TL;DR: 本篇论文提出了一种从用户交互日志中发现数据传输例程的方法，并对其进行了优化，以提高效率。


<details>
  <summary>Details</summary>
Motivation: 为了充分利用机器人流程自动化（RPA）的机遇，公司需要发现可自动化的具体例程及其实现方式。

Method: 本研究将分析用户交互（UI）日志以发现用户在电子表格或（Web）表单之间传输数据的例程的问题，映射到通过示例发现数据转换的问题。研究提出了两种优化方法，利用UI日志信息和跨应用程序数据传输通常涉及单独复制字母和数字标记的特点。

Result: 评估表明，在UI日志上复制真实世界数据传输例程的拟议方法和优化是有效的。

Conclusion: 本研究提出了一种从UI日志中发现数据传输例程的方法，并通过优化解决了计算效率问题。

Abstract: Robotic Process Automation (RPA) is a technology for automating repetitive routines consisting of sequences of user interactions with one or more applications. In order to fully exploit the opportunities opened by RPA, companies need to discover which specific routines may be automated, and how. In this setting, this paper addresses the problem of analyzing User Interaction (UI) logs in order to discover routines where a user transfers data from one spreadsheet or (Web) form to another. The paper maps this problem to that of discovering data transformations by example - a problem for which several techniques are available. The paper shows that a naive application of a state-of-the-art technique for data transformation discovery is computationally inefficient. Accordingly, the paper proposes two optimizations that take advantage of the information in the UI log and the fact that data transfers across applications typically involve copying alphabetic and numeric tokens separately. The proposed approach and its optimizations are evaluated using UI logs that replicate a real-life repetitive data transfer routine.

</details>


### [137] [Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples](https://arxiv.org/abs/1804.01186)
*Ashwin Kalyan,Abhishek Mohta,Oleksandr Polozov,Dhruv Batra,Prateek Jain,Sumit Gulwani*

Main category: cs.AI

TL;DR: 神经引导演绎搜索（NGDS）是一种结合了逻辑推理和统计模型优势的混合程序合成技术，能够在少量示例下生成用户意图的程序。


<details>
  <summary>Details</summary>
Motivation: 解决现有程序合成系统在依赖纯粹的演绎逻辑或海量数据统计模型时遇到的挑战，实现实时合成并泛化到未见示例。

Method: 提出了一种名为神经引导演绎搜索（NGDS）的混合技术，利用演绎搜索框架简化神经网络的学习问题，结合了符号逻辑技术和统计模型，并使用循环神经网络编码器。

Result: 在真实客户场景的评估中，NGDS能够合成准确的程序，并且相比最先进的系统有高达12倍的速度提升。

Conclusion: NGDS通过结合逻辑推理和神经网络，有效地解决了程序合成的挑战，在准确性和效率方面都表现出色。

Abstract: Synthesizing user-intended programs from a small number of input-output examples is a challenging problem with several important applications like spreadsheet manipulation, data wrangling and code refactoring. Existing synthesis systems either completely rely on deductive logic techniques that are extensively hand-engineered or on purely statistical models that need massive amounts of data, and in general fail to provide real-time synthesis on challenging benchmarks. In this work, we propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique that combines the best of both symbolic logic techniques and statistical models. Thus, it produces programs that satisfy the provided specifications by construction and generalize well on unseen examples, similar to data-driven systems. Our technique effectively utilizes the deductive search framework to reduce the learning problem of the neural component to a simple supervised learning setup. Further, this allows us to both train on sparingly available real-world data and still leverage powerful recurrent neural network encoders. We demonstrate the effectiveness of our method by evaluating on real-world customer scenarios by synthesizing accurate programs with up to 12x speed-up compared to state-of-the-art systems.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [138] [Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!](https://arxiv.org/abs/2309.02110)
*James P. Dilger*

Main category: math.HO

TL;DR: Wordle玩家的作弊行为、偏好和易受影响性可以通过分析游戏数据来量化。


<details>
  <summary>Details</summary>
Motivation: 本文旨在利用信息论和游戏数据分析，量化评估Wordle玩家的行为，包括作弊、对起始词的偏好以及易受外部信息影响的程度。

Method: 通过收集和分析2023年5月至8月Wordle玩家的第一猜测数据，运用信息论评估玩家的运气和技能。

Result: A) 约0.2%-0.5%的玩家在一局中就解出谜题，远高于随机猜测的概率（0.043%），表明存在约4000-10000名玩家通过非正常途径获取答案。B) 至少有1/3的玩家有固定的起始词或偏好的起始词列表，即使在目标词出现后，他们仍然忠于自己的起始词。C) 2023年8月15日，约3万名玩家突然更换了起始词，这可能与一个填字游戏线索有关，表明玩家容易受到外部信息的影响。

Conclusion: 该研究超越了传统的社交媒体、调查和谷歌趋势分析，提供了关于Wordle玩家作弊行为、玩家偏好以及易受影响性的可靠、定量的证据。

Abstract: Wordle is a popular, online word game offered by the New York Times (nytimes.com). Currently there are some 2 million players of the English version worldwide. Players have 6 attempts to guess the daily word (target word) and after each attempt, the player receives color-coded information about the correctness and position of each letter in the guess. After either a successful completion of the puzzle or the final unsuccessful attempt, software can assess the player's luck and skill using Information Theory and can display data for the first, second, ..., sixth guesses of a random sample of all players. Recently, I discovered that the latter data is presented in a format that can easily be copied and pasted into a spreadsheet. I compiled data on Wordle players' first guesses from May 2023 - August 2023 and inferred some interesting information about Wordle players. A) Every day, about 0.2-0.5% of players solve the puzzle in one attempt. Because the odds of guessing the one of 2,315 possible target words at random is 0.043%, this implies that 4,000 - 10,000 players cheat by obtaining the target word outside of playing the game! B) At least 1/3 of the players have a favorite starting word, or cycle through several. And even though players should be aware that target words are never repeated, most players appear to remain loyal to their starting word even after its appearance as a target word. C) On August 15, 2023, about 30,000 players abruptly changed their starting word, presumably based on a crossword puzzle clue! Wordle players can be influenced! This study goes beyond social media postings, surveys, and Google Trends to provide solid, quantitative evidence about cheating in Wordle.

</details>


### [139] [GeoGebra e situações que envolvem modelação numa abordagem STEAM](https://arxiv.org/abs/1907.02099)
*J. M. D. S. Dos Santos,A. P. Silveira,A. E. S. Trocado*

Main category: math.HO

TL;DR: 该研究提出了一个STEAM教学方法，利用GeoGebra软件在数学课上解决二维和三维几何问题，旨在教师培训和学生学习中推广应用。


<details>
  <summary>Details</summary>
Motivation: 为了在葡语区数学教学中实施STEAM方法，并引入交互式数学软件GeoGebra，为教师和学生设计了相关教学材料。

Method: 利用GeoGebra软件的2D、3D、CAS、电子表格等窗口，通过脚本指导，让用户能够学习和操作，以解决几何建模问题，研究切割平面和某些表面。

Result: 设计了一套可供教师和学生使用的GeoGebra互动任务，能够连接其他科学和艺术领域，促进STEAM教学和项目式学习。该任务将从葡语区推广到西语和英语区。

Conclusion: 该研究提出的GeoGebra互动任务符合STEAM教学理念，易于上手，有助于数学内容的学习和应用，并具有国际推广潜力。

Abstract: In order to implement a STEAM approach including the use of technology, namely the use of interactive mathematics software GeoGebra, in mathematics classes, in the lusophone space, the materials presented here were conceived, to be implemented in a first phase among teachers. Later, with the necessary adaptations, these tasks will be applied to the students. The tasks deal with modeling situations, in two- and three-dimensional geometric problems, in order to apply GeoGebra software in its analysis to illustrate its capabilities. The different windows of this software are used, namely the 2D and 3D windows, CAS window, spreadsheet and extra two dimensional windows in order to study cutting planes in solids and some surfaces. The tasks are presented so that any user, regardless of the degree of knowledge they have of the software, can follow them, being supported in scripts with some indications of the tools and commands to use. Designed for the teaching and learning of Mathematics, from a STEAM approach, these tasks allow connections with other Sciences and the Arts, and allow the development of projects using and consolidating relevant mathematical contents. These tasks are part of the proposals of activities of the participants of the Training Courses for Trainers in GeoGebra for Portuguese Speaking Countries, which from 2019 have an impact on the STEAM approach. These courses are carried out with the high sponsorship of the Organization of Ibero-American States for Education, Science and Culture (OEI). Given the interest that the tasks have for the users of the Iberian space, as well as their dissemination at a global level, the materials initially developed in Portuguese language will be adapted for Spanish and English speakers.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [140] [LEI2JSON: Schema-based Validation and Conversion of Livestock Event Information](https://arxiv.org/abs/2310.17414)
*Mahir Habib,Muhammad Ashad Kabir,Lihong Zheng*

Main category: eess.SY

TL;DR: LEI2JSON是一个用于Google表格的插件，可以将畜牧业事件数据标准化为JSON格式，以节省时间和资源。


<details>
  <summary>Details</summary>
Motivation: 畜牧业生产者需要一种标准化的方法来处理他们的畜牧业事件数据。

Method: LEI2JSON通过创建包含适当列标题、注释和验证规则的电子表格模板，将电子表格数据转换为JSON格式，并根据模式验证输出来实现数据标准化。

Result: LEI2JSON可以促进畜牧业事件信息在本地或Google云端硬盘中的无缝存储，并且在一次广泛的实验评估中证明了其有效性。

Conclusion: LEI2JSON为畜牧业生产者提供了一种高效的数据标准化工具，可以节省时间和资源。

Abstract: Livestock producers often need help in standardising (i.e., converting and validating) their livestock event data. This article introduces a novel solution, LEI2JSON (Livestock Event Information To JSON). The tool is an add-on for Google Sheets, adhering to the livestock event information (LEI) schema. The core objective of LEI2JSON is to provide livestock producers with an efficient mechanism to standardise their data, leading to substantial savings in time and resources. This is achieved by building the spreadsheet template with the appropriate column headers, notes, and validation rules, converting the spreadsheet data into JSON format, and validating the output against the schema. LEI2JSON facilitates the seamless storage of livestock event information locally or on Google Drive in JSON. Additionally, we have conducted an extensive experimental evaluation to assess the effectiveness of the tool.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [141] [Pivoting the paradigm: the role of spreadsheets in K-12 data science](https://arxiv.org/abs/2506.03232)
*Oren Tirschwell,Nicholas Jon Horton*

Main category: stat.OT

TL;DR: 电子表格有助于培养K-12学生的数据和计算能力，但需要克服一些挑战。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨电子表格在K-12教育中的潜力，特别是在数据收集、组织、探索、分析以及培养数据素养和计算能力方面的作用。

Method: 本研究回顾了现有的K-12数据工具框架，提出了通过整合电子表格可以实现的数据驱动的学习成果，并讨论了电子表格在培养数据素养和计算能力方面的作用。此外，研究还提供了课堂活动示例，指出了推广过程中面临的挑战和障碍，并提出了旨在减轻师生学习负担的教学方法，同时强调了促进电子表格在数据科学和STEM学科中应用的专业发展需求。

Result: 电子表格在K-12教育中可以有效地促进数据收集、组织、探索和分析。通过将电子表格纳入课程，可以实现特定的数据驱动的学习成果，有助于培养学生的数据素养和计算能力。

Conclusion: 电子表格是一种有价值的K-12教育工具，可以增强学生的数据处理和计算能力。虽然其应用存在挑战，但通过合适的教学方法和专业发展，可以充分发挥其潜力，特别是在STEM领域。

Abstract: Spreadsheet tools are widely accessible to and commonly used by K-12 students and teachers. They have an important role in data collection and organization. Beyond data organization, spreadsheets also make data visible and easy to interact with, facilitating student engagement in data exploration and analysis. Though not suitable for all circumstances, spreadsheets can and do help foster data and computing skills for K-12 students. This paper 1) reviews prior frameworks on K-12 data tools; 2) proposes data-driven learning outcomes that can be accomplished by incorporating spreadsheets into the curriculum; and 3) discusses how spreadsheets can help develop data acumen and computational fluency. We provide example class activities, identify challenges and barriers to adoption, suggest pedagogical approaches to ease the learning curve for instructors and students, and discuss the need for professional development to facilitate deeper use of spreadsheets for data science and STEM disciplines.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [142] [Dynamic Kernel Matching for Non-conforming Data: A Case Study of T-cell Receptor Datasets](https://arxiv.org/abs/2103.10472)
*Jared Ostmeyer,Scott Christley,Lindsay Cowell*

Main category: q-bio.QM

TL;DR: 本研究提出了一种名为动态核匹配（DKM）的方法，用于修改现有的统计分类器以处理非结构化数据，并在T细胞受体（TCR）序列和TCR图谱数据集上进行了验证，证明了其在疾病诊断中的潜力。


<details>
  <summary>Details</summary>
Motivation: 大多数统计分类器都无法处理非结构化数据，而本研究旨在解决这一问题，并探索非结构化数据在疾病诊断中的应用。

Method: 提出动态核匹配（DKM）方法，用于修改现有的统计分类器以处理非结构化数据。将DKM应用于TCR序列和TCR图谱数据集，并使用标准指标和允许不确定诊断的指标评估性能。最后，识别用于预测的模式，并与实验研究结果进行比较。

Result: 成功地将增强DKM的统计分类器应用于TCR序列和TCR图谱数据集，并在保持数据和允许不确定诊断方面取得了良好的性能。识别出的模式与实验研究结果一致。

Conclusion: DKM是一种有效的方法，可以扩展统计分类器的能力，以处理非结构化数据，并在疾病诊断方面具有应用前景。识别出的模式为进一步研究提供了见解。

Abstract: Most statistical classifiers are designed to find patterns in data where numbers fit into rows and columns, like in a spreadsheet, but many kinds of data do not conform to this structure. To uncover patterns in non-conforming data, we describe an approach for modifying established statistical classifiers to handle non-conforming data, which we call dynamic kernel matching (DKM). As examples of non-conforming data, we consider (i) a dataset of T-cell receptor (TCR) sequences labelled by disease antigen and (ii) a dataset of sequenced TCR repertoires labelled by patient cytomegalovirus (CMV) serostatus, anticipating that both datasets contain signatures for diagnosing disease. We successfully fit statistical classifiers augmented with DKM to both datasets and report the performance on holdout data using standard metrics and metrics allowing for indeterminant diagnoses. Finally, we identify the patterns used by our statistical classifiers to generate predictions and show that these patterns agree with observations from experimental studies.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [143] [Ensuring Adherence to Standards in Experiment-Related Metadata Entered Via Spreadsheets](https://arxiv.org/abs/2409.08897)
*Martin J. O'Connor,Josef Hardi,Marcos Martínez-Romero,Sowmya Somasundaram,Brendan Honick,Stephen A. Fisher,Ajay Pillai,Mark A. Musen*

Main category: cs.DL

TL;DR: 科学家们越来越认识到提供丰富的、符合标准的元数据来描述他们的实验结果的重要性。尽管有现成的复杂工具来协助数据注释过程，但研究人员通常还是倾向于使用电子表格来提供元数据，尽管电子表格在确保元数据一致性和符合正式规范方面存在局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管有现成的复杂工具来协助数据注释过程，但研究人员通常还是倾向于使用电子表格来提供元数据，尽管电子表格在确保元数据一致性和符合正式规范方面存在局限性。

Method: 本研究描述了一种端到端的方法，该方法支持基于电子表格的元数据输入，同时确保严格遵守社区元数据标准并提供质量控制。我们的方法包括几个关键组成部分：1. 可自定义的模板，代表元数据标准并可用于指导研究人员用于编写元数据的电子表格。2. 受控术语和本体，用于定义可直接从电子表格访问的元数据值。3. 一个交互式 Web 工具，允许用户快速识别和修复其电子表格元数据中的错误。

Result: 该方法已被部署在名为 HuBMAP 的生物医学联盟中，用于定义和收集关于多种生物检测的元数据。

Conclusion: 本研究介绍了一种支持基于电子表格的元数据输入的方法，该方法通过使用可自定义的模板、受控术语和本体以及交互式 Web 工具，确保了元数据的一致性、标准化和质量控制。该方法已成功应用于 HuBMAP 生物医学联盟。

Abstract: Scientists increasingly recognize the importance of providing rich, standards-adherent metadata to describe their experimental results. Despite the availability of sophisticated tools to assist in the process of data annotation, investigators generally seem to prefer to use spreadsheets when supplying metadata, despite the limitations of spreadsheets in ensuring metadata consistency and compliance with formal specifications. In this paper, we describe an end-to-end approach that supports spreadsheet-based entry of metadata, while ensuring rigorous adherence to community-based metadata standards and providing quality control. Our methods employ several key components, including customizable templates that represent metadata standards and that can inform the spreadsheets that investigators use to author metadata, controlled terminologies and ontologies for defining metadata values that can be accessed directly from a spreadsheet, and an interactive Web-based tool that allows users to rapidly identify and fix errors in their spreadsheet-based metadata. We demonstrate how this approach is being deployed in a biomedical consortium known as HuBMAP to define and collect metadata about a wide range of biological assays.

</details>


### [144] [Trapped in Transformative Agreements? A Multifaceted Analysis of >1,000 Contracts](https://arxiv.org/abs/2409.20224)
*Laura Rothfritz,W. Benedikt Schmal,Ulrich Herb*

Main category: cs.DL

TL;DR: 学术机构与学术出版商之间的转型协议（TA）普遍存在，但本研究利用ESAC数据库的合同数据分析发现，机构似乎被困在混合式出版系统中，这增强了传统出版商的市场力量，提高了成本。


<details>
  <summary>Details</summary>
Motivation: 分析学术机构与学术出版商之间的转型协议（TA）的特征和现状，探讨其对开放获取（OA）的实际影响。

Method: 通过网络爬虫技术获取ESAC数据库中的合同数据，并结合定性和定量方法进行深入分析。

Result: 研究发现，学术机构似乎被困在转型协议中，未能有效过渡到完全开放获取。这种混合系统增强了传统出版商的市场力量，提高了进入门槛，降低了竞争，并增加了图书馆和大学的成本。

Conclusion: 转型协议未能成为通往完全开放获取世界的桥梁，反而使学术界陷入了混合系统，这有利于传统出版商，并对学术界造成了经济负担。

Abstract: Transformative agreements between academic publishers and research institutions are ubiquitous. The 'Efficiency and Standards for Article Charges' (ESAC) Initiative lists more than 1,000 contracts in its database. We make use of this unique dataset by web-scraping the details of every contract to substantially expand the overview spreadsheet provided by the ESAC Initiative. Based on that hitherto unused data source, we combine qualitative and quantitative methods to conduct an in-depth analysis of the contract characteristics and the TA landscape. Our analysis demonstrates that research institutions seem to be 'trapped' in transformative agreements. Instead of being a bridge towards a fully Open Access world, academia is stuck in the hybrid system. This endows the legacy (non-Open Access) publishing houses with substantial market power. It raises entry barriers, lowers competition, and increases costs for libraries and universities.

</details>


### [145] [A Comprehensive Approach to Ensuring Quality in Spreadsheet-Based Metadata](https://arxiv.org/abs/2312.09107)
*Martin J. O'Connor,Marcos Martínez-Romero,Mete Ugur Akdogan,Josef Hardi,Mark A. Musen*

Main category: cs.DL

TL;DR: 该研究提出了一种端到端的解决方案，通过可定制的模板、受控术语和交互式Web工具，来改进基于电子表格的元数据输入、合规性和质量控制，并已在生物医学联盟中得到部署。


<details>
  <summary>Details</summary>
Motivation: 科学家们越来越认识到元数据的重要性，但电子表格在确保合规性和质量方面存在局限性。现有工具也存在学习曲线陡峭和定制化程度低等问题。

Method: 采用了一种端到端的方法，包括：1. 可定制的模板用于定义元数据。2. 对使用受控术语定义模板提供内置支持。3. 开发了一个交互式Web工具，使用户能够快速识别和修复电子表格元数据中的错误。

Result: 该方法已被部署在生物医学联盟中，用于定义和收集科学实验的元数据。

Conclusion: 该研究提出了一种改进电子表格元数据输入、合规性和质量控制的有效方法，并已在实际应用中得到验证。

Abstract: While scientists increasingly recognize the importance of metadata in describing their data, spreadsheets remain the preferred tool for supplying this information despite their limitations in ensuring compliance and quality. Various tools have been developed to address these limitations, but they suffer from their own shortcomings, such as steep learning curves and limited customization. In this paper, we describe an end-to-end approach that supports spreadsheet-based entry of metadata while providing rigorous compliance and quality control. Our approach employs several key strategies, including customizable templates for defining metadata, integral support for the use of controlled terminologies when defining these templates, and an interactive Web-based tool that allows users to rapidly identify and fix errors in the spreadsheet-based metadata they supply. We demonstrate how this approach is being deployed in a biomedical consortium to define and collect metadata about scientific experiments.

</details>


### [146] [Towards Semantic Interoperability in Historical Research: Documenting Research Data and Knowledge with Synthesis](https://arxiv.org/abs/2107.13957)
*Pavlos Fafalios,Konstantina Konsolaki,Lida Charami,Kostas Petrakis,Manos Paterakis,Dimitris Angelakis,Yannis Tzitzikas,Chrysoula Bekiari,Martin Doerr*

Main category: cs.DL

TL;DR: 该研究提出了Synthesis文档系统，以解决历史文物研究中数据组织和协作的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前历史科学研究中，文物信息主要使用电子表格或关系数据库进行组织，存在协作困难、细节表示不足、数据结构扩展性差、数据复用性受限等问题。

Method: 该论文描述了Synthesis文档系统，该系统是一个基于Web的协作系统，利用CIDOC-CRM和RDF等现有标准，支持信息文档化和发布，并注重语义互操作性和高质量、长效数据的生成。

Result: Synthesis系统已被大量历史学家用于一个正在进行的艺术史研究项目中，有效支持了历史学家的工作。

Conclusion: Synthesis系统通过利用现有标准和Web技术，为历史文物研究提供了有效的解决方案，提高了数据的协作性、详细性、灵活性和可复用性。

Abstract: A vast area of research in historical science concerns the documentation and study of artefacts and related evidence. Current practice mostly uses spreadsheets or simple relational databases to organise the information as rows with multiple columns of related attributes. This form offers itself for data analysis and scholarly interpretation, however it also poses problems including i) the difficulty for collaborative but controlled documentation by a large number of users, ii) the lack of representation of the details from which the documented relations are inferred, iii) the difficulty to extend the underlying data structures as well as to combine and integrate data from multiple and diverse information sources, and iv) the limitation to reuse the data beyond the context of a particular research activity. To support historians to cope with these problems, in this paper we describe the Synthesis documentation system and its use by a large number of historians in the context of an ongoing research project in the field of History of Art. The system is Web-based and collaborative, and makes use of existing standards for information documentation and publication (CIDOC-CRM, RDF), focusing on semantic interoperability and the production of data of high value and long-term validity.

</details>


### [147] [FAST CAT: Collaborative Data Entry and Curation for Semantic Interoperability in Digital Humanities](https://arxiv.org/abs/2105.13733)
*Pavlos Fafalios,Kostas Petrakis,Georgios Samaritakis,Korina Doerr,Athina Kritsotaki,Yannis Tzitzikas,Martin Doerr*

Main category: cs.DL

TL;DR: FAST CAT是一个协作系统，用于数字人文和类似实证研究中的辅助数据录入和策展。


<details>
  <summary>Details</summary>
Motivation: 描述性科学（如历史）依赖电子表格和关系数据库进行数据管理和分析，但这带来了数据局限性，例如数据依赖于初始研究假设，缺乏原始关系推断的细节，以及难以重新访问原始数据源进行验证、更正或改进。

Method: 提出FAST CAT系统，一个支持语义互操作性的协作系统，并将在一个关于蒸汽船对地中海经济、社会和人口影响的欧洲海事历史项目中进行讨论。

Result: FAST CAT系统旨在解决数字人文和实证研究中的数据录入和策展挑战。

Conclusion: FAST CAT系统为数字人文和实证研究提供了一种改进数据管理和分析的方法，特别是在解决与传统工具相关的数据局限性方面。

Abstract: Descriptive and empirical sciences, such as History, are the sciences that collect, observe and describe phenomena in order to explain them and draw interpretative conclusions about influences, driving forces and impacts under given circumstances. Spreadsheet software and relational database management systems are still the dominant tools for quantitative analysis and overall data management in these these sciences, allowing researchers to directly analyse the gathered data and perform scholarly interpretation. However, this current practice has a set of limitations, including the high dependency of the collected data on the initial research hypothesis, usually useless for other research, the lack of representation of the details from which the registered relations are inferred, and the difficulty to revisit the original data sources for verification, corrections or improvements. To cope with these problems, in this paper we present FAST CAT, a collaborative system for assistive data entry and curation in Digital Humanities and similar forms of empirical research. We describe the related challenges, the overall methodology we follow for supporting semantic interoperability, and discuss the use of FAST CAT in the context of a European (ERC) project of Maritime History, called SeaLiT, which examines economic, social and demographic impacts of the introduction of steamboats in the Mediterranean area between the 1850s and the 1920s.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [148] [Improving Feedback from Automated Reviews of Student Spreadsheets](https://arxiv.org/abs/2311.10728)
*Sören Aguirre Reid,Frank Kammer,Jonas-Ian Kuche,Pia-Doreen Ritzke,Markus Siepermann,Max Stephan,Armin Wagenknecht*

Main category: cs.CY

TL;DR: 本研究针对教学环境中电子表格作业评估工具的缺乏，开发了一套智能辅导系统（ITS）。


<details>
  <summary>Details</summary>
Motivation: 目前，在教学环境中，用于评估学生电子表格作业的数字工具仍然很少。然而，电子表格（如Excel）是广泛使用的工具，并被纳入许多课程。因此，开发一个自动评估学生Excel作业并提供个性化反馈的系统具有重要意义。

Method: 该系统能够自动分析学生的Excel提交内容，包括值匹配、公式详细分析和解决方案质量评估。ITS包含不同反馈级别，可根据学生的学习水平提供不同详细程度的错误信息。学生提交的作业通过与唯一的参考解决方案进行比较来评估。

Result: 研究表明，更高反馈级别的ITS能够提高学生正确提交的百分比，并且学生认为这种反馈易于理解且有帮助。

Conclusion: 本研究开发的智能辅导系统（ITS）能够有效地自动评估学生的Excel作业，并提供个性化反馈，有助于提高学生的学习效果和满意度。

Abstract: Spreadsheets are one of the most widely used tools for end users. As a result, spreadsheets such as Excel are now included in many curricula. However, digital solutions for assessing spreadsheet assignments are still scarce in the teaching context. Therefore, we have developed an Intelligent Tutoring System (ITS) to review students' Excel submissions and provide individualized feedback automatically. Although the lecturer only needs to provide one reference solution, the students' submissions are analyzed automatically in several ways: value matching, detailed analysis of the formulas, and quality assessment of the solution. To take the students' learning level into account, we have developed feedback levels for an ITS that provide gradually more information about the error by using one of the different analyses. Feedback at a higher level has been shown to lead to a higher percentage of correct submissions and was also perceived as well understandable and helpful by the students.

</details>


### [149] [How Beaufort, Neumann and Gates met? Subject integration with spreadsheeting](https://arxiv.org/abs/2309.12353)
*Maria Csernoch,Julia Csernoch*

Main category: cs.CY

TL;DR: 计算思维应作为继读写算之后的第四项基本技能。本研究提出了一种新方法，以 Beaufort 尺为基础，支持学科整合和数字图式构建，并在 8 年级行动研究中进行了演示。


<details>
  <summary>Details</summary>
Motivation: 计算思维，特别是数字问题解决方法，需要建立专门的图式，而目前在这方面还有很长的路要走。

Method: 提出了一种新方法，将传统的纸质问题和数据检索过程整合到 Beaufort 尺中，并在 8 年级行动研究中进行。

Result: 研究发现，与传统的教科书和非情境化数字环境相比，学生的内容知识和数字技能都得到了更有效的提升。

Conclusion: 该方法可以应用于任何纸质问题，将其转化为数字环境可以更有效地解决，并为构建学科和信息学图式提供多种形式。

Abstract: Computational thinking should be the fourth fundamental skill, along with reading, writing, and arithmetic (3R). To reach the level where computational thinking skills, especially digital problem solving have their own schemata, there is a long way to go. In the present paper, a novel approach is detailed to support subject integration and building digital schemata, on the well-known Beaufort scale. The conversion of a traditional, paper-based problem and a data retrieval process are presented within the frame of a Grade 8 action research study. It is found that both students content knowledge and their digital skills developed more efficiently than in traditional course book and decontextualized digital environments. Furthermore, the method presented here can be adapted to any paper-based problems whose solutions would be more effective in a digital environment and which offer various forms for building schemata both in the subject matter and informatics.

</details>


### [150] [A Simpler Method for Understanding Emergency Shelter Access Patterns](https://arxiv.org/abs/2210.13619)
*Geoffrey G. Messier*

Main category: cs.CY

TL;DR: SAM是一种新的紧急避难所可访问性指标，用于衡量避难所客户的脆弱性，并能被非技术人员使用。


<details>
  <summary>Details</summary>
Motivation: 提供一个直观的、可由非技术人员实现的、用于理解避难所可访问性模式的方法，以作为衡量客户脆弱性的标准。

Method: 使用来自北美大型避难所的客户数据来演示SAM与传统的过渡性、偶发性和慢性客户聚类分析产生相似的结果。

Result: SAM产生与传统聚类分析相似的结果，但需要更少的数据，并能生成实时图景，展示外部因素如何影响避难所的可访问性模式。使用SAM对九年客户数据生成的时间线证明了“住房优先”计划和COVID-19封锁对人们如何访问避难所的影响。

Conclusion: SAM允许避难所工作人员超越分配传统的客户标签（过渡性、偶发性和慢性性），而是直接使用SAM的“软”输出作为脆弱性指标。

Abstract: The Simplified Access Metric (SAM) is a new approach for characterizing emergency shelter access patterns as a measure of shelter client vulnerability. The goal of SAM is to provide shelter operators with an intuitive way to understand access patterns that can be implemented by non-technical staff using spreadsheet operations. Client data from a large North American shelter will be used to demonstrate that SAM produces similar results to traditional transitional, episodic and chronic client cluster analysis. Since SAM requires less data than cluster analysis, it is also able to generate a real time picture of how shelter access patterns are affected by external factors. Timelines generated from nine years of shelter client data using SAM demonstrate the impact of Housing First programming and the COVID-19 lockdown on how people access shelter. Finally, SAM allows shelter staff to move beyond assigning transitional, episodic and chronic labels and instead use the "soft" output of SAM directly as a measure of vulnerability.

</details>


### [151] [Long-Term Mentoring for Computer Science Researchers](https://arxiv.org/abs/2208.04738)
*Emily Ruppel,Sihang Liu,Elba Garza,Sukyoung Ryu,Alexandra Silva,Talia Ringer*

Main category: cs.CY

TL;DR: PL和CA社区都建立了长期待遇计划，以解决社区内建立新联系的难题，PL社区的SIGPLAN-M覆盖了328名受训者和234名导师，CA社区的CALM正在试点阶段，均获得了积极反馈。


<details>
  <summary>Details</summary>
Motivation: 解决PL和CA社区中建立新联系的难题，特别是缺乏长期待遇计划。

Method: PL社区设立了SIGPLAN-M，CA社区设立了CALM。分享了两个项目的经验、影响和挑战。

Result: SIGPLAN-M覆盖328名受训者和234名导师，遍布41个国家，受训者评价“改变人生”、“挽救职业”。CALM正在试点，有13名导师和21名受训者，遍布7个国家，反馈积极。

Conclusion: 希望SIGPLAN-M和CALM的经验能促进整个计算机科学领域长期待遇计划的发展。

Abstract: Early in the pandemic, we -- leaders in the research areas of programming languages (PL) and computer architecture (CA) -- realized that we had a problem: the only way to form new lasting connections in the community was to already have lasting connections in the community. Both of our academic communities had wonderful short-term mentoring programs to address this problem, but it was clear that we needed long-term mentoring programs.
  Those of us in CA approached this scientifically, making an evidence-backed case for community-wide long-term mentoring. In the meantime, one of us in PL had impulsively launched an unofficial long-term mentoring program, founded on chaos and spreadsheets. In January 2021, the latter grew to an official cross-institutional long-term mentoring program called SIGPLAN-M; in January 2022, the former grew to Computer Architecture Long-term Mentoring (CALM).
  The impacts have been strong: SIGPLAN-M reaches 328 mentees and 234 mentors across 41 countries, and mentees have described it as "life changing" and "a career saver." And while CALM is in its pilot phase -- with 13 mentors and 21 mentees across 7 countries -- it has received very positive feedback. The leaders of SIGPLAN-M and CALM shared our designs, impacts, and challenges along the way. Now, we wish to share those with you. We hope this will kick-start a larger long-term mentoring effort across all of computer science.

</details>


### [152] [Optimization Helps Scheduling Nursing Staff at the Long-Term Care Homes of the City of Toronto](https://arxiv.org/abs/2102.09461)
*Manion Anderson,Merve Bodur,Scott Rathwell,Vahid Sarhangian*

Main category: cs.CY

TL;DR: 该研究开发了一个基于分层优化模型的电子表格调度工具，用于解决多伦多长期护理机构的护士排班问题，该工具在满足复杂工龄要求的同时，最大限度地提高了护士的偏好度，显著缩短了排班时间并提高了班次偏好度。


<details>
  <summary>Details</summary>
Motivation: 多伦多长期护理机构及服务部（LTCH&S）在安大略省最大的长期护理服务提供商之一，面临着日益严峻的护士排班挑战和高离职率问题，特别是兼职护士的缺勤率较高。

Method: 开发了一个基于分层优化模型的电子表格调度工具，该工具能够自动生成排班表，并在满足护士对不同班次偏好的同时，优先考虑资深护士的工龄要求，以最大化总偏好得分和满足尽可能多的需求。

Result: 该工具在多伦多一家拥有391个床位的护理机构进行了实施。与手动排班相比，该工具能够在几分之一小时内生成可行排班表，而手动方法则需要长达数小时。此外，生成的排班表在满足偏好方面表现出色，平均有94%以上的分配班次被列为最优先。

Conclusion: 该研究开发的电子表格调度工具成功地解决了多伦多长期护理机构的护士排班难题，通过自动化排班流程、考虑护士偏好以及满足复杂的工龄要求，显著提高了排班效率和护士满意度。

Abstract: The City of Toronto Long Term Care Homes & Services (LTCH&S) division is one of the largest providers of long-term care in the Canadian province of Ontario, providing care to 2,640 residents at 10 homes across Toronto. Our collaboration with LTCH&S was initiated to facilitate the increasingly challenging task of scheduling nursing staff and reduce high absenteeism rate observed among the part-time nurses. We developed a spreadsheet-based scheduling tool to automate the generation of schedules and incorporate nurses' preferences for different shifts into the schedules. At the core of the scheduling tool is a hierarchical optimization model that generates a feasible schedule with the highest total preference score while satisfying the maximum possible demand. Feasible schedules had to abide by a set of complex seniority requirements which prioritized more senior nurses when allocating the available shifts. Our scheduling tool was implemented in a 391-bed home in Toronto. The tool allowed nursing managers to generate feasible schedules within a fraction of an hour, in contrast to the status-quo manual approach which could took up to tens of hours. In addition, the schedules successfully accounted for preferences with on average above 94% of the allocated shifts ranked as most preferred.

</details>


### [153] [Developing Excel Thought Leadership](https://arxiv.org/abs/2006.04793)
*David Lyford-Smith*

Main category: cs.CY

TL;DR: ICAEW发布了三份关于电子表格使用和工作环境的“思想领导力”报告，总结了五年来的经验教训，并巩固了其在该领域的地位。


<details>
  <summary>Details</summary>
Motivation: 回顾ICAEW发布的关于电子表格使用和工作环境的“思想领导力”报告的历史，总结其经验教训，并展示该过程如何帮助ICAEW在该领域发展其地位。

Method: 回顾ICAEW发布的关于电子表格使用和工作环境的三份“思想领导力”报告的历史，总结其关键经验教训，并讨论该过程如何帮助ICAEW在该领域发展其地位。

Result: ICAEW发布了三份关于电子表格使用和工作环境的“思想领导力”报告，这些报告总结了五年的经验教训，并帮助ICAEW在该领域建立了其地位。

Conclusion: ICAEW通过发布关于电子表格使用和工作环境的三份“思想领导力”报告，成功地总结了经验教训，并巩固了其在该领域的领先地位。

Abstract: Over a period of five years, the Institute of Chartered Accountants in England and Wales (ICAEW) has developed a suite of three 'thought leadership' papers surrounding good practice in spreadsheet use and spreadsheet work environments. We will review the history of these three papers, the key lessons which each has to teach, and discuss how the process of making them has helped ICAEW to develop its position in the field.

</details>


### [154] [A Case Study of Spreadsheet Use within the Finance and Academic Registry units within a Higher Education Institution](https://arxiv.org/abs/1909.07462)
*Simon Thorne,Jamie Hancock*

Main category: cs.CY

TL;DR: 本研究的案例研究表明，在英国一所高等教育机构的学术注册和财务部门中，电子表格被大量使用。研究结果强调了制定明确的电子表格开发原则和指南的必要性，以确保数据完整性、减少重复工作并优化电子表格的使用以实现机构目标。


<details>
  <summary>Details</summary>
Motivation: 探讨英国一所高等教育机构（学术注册和财务部门）的电子表格使用情况，重点关注其重要性、培训、经验、目的、所用技术、创建的电子表格大小以及共享情况。

Method: 通过案例研究，调查了英国一所高等教育机构的学术注册和财务部门的电子表格使用情况，分析了电子表格的重要性、培训、经验、目的、技术、大小和共享。

Result: 研究发现，该机构创建和使用了大量的电子表格。电子表格开发者的特征与其他研究中的情况相似。有必要制定明确的电子表格开发原则和指南，以确保数据完整性，减少重复劳动，并优化电子表格的使用以实现机构目标。

Conclusion: 为了确保数据完整性、减少重复工作并优化电子表格的使用以实现机构目标，该机构需要制定明确的电子表格开发原则和指南。

Abstract: This paper presents the findings of a case study of spreadsheet use in a higher education institution in the UK. The paper considers the use of spreadsheets in two units of the organisation, academic registry and finance. Spreadsheet use is explored in terms of importance, training, experience, purpose, techniques deployed, size of spreadsheets created and sharing of spreadsheets. The implications of the results are then considered in terms of accurate reporting to external funding bodies such the funding councils, internal data integrity and internal data efficiencies. The results show a large volume of spreadsheets being created and used, that the profile of spreadsheet developers is typical of other studies of spreadsheet use and the need for the organisation to have clear principles and guidelines for the development of spreadsheet models in the organisation to ensure data integrity, reduce duplication of effort and to optimise the use of spreadsheets to meet the institutions goals.

</details>


### [155] [Computer Simulation of Neural Networks Using Spreadsheets: The Dawn of the Age of Camelot](https://arxiv.org/abs/1807.00018)
*Serhiy O. Semerikov,Illia O. Teplytskyi,Yuliia V. Yechkalo,Arnold E. Kiv*

Main category: cs.CY

TL;DR: 该文章探讨了在电子表格环境中训练神经网络模拟方法的必要性，并回顾了现有方法。文章识别出几种主要方法，包括联合使用电子表格和神经网络模拟工具、使用第三方插件、开发嵌入式宏、使用标准插件进行非线性优化，以及不使用插件和宏直接在电子表格中创建神经网络。


<details>
  <summary>Details</summary>
Motivation: 文章旨在论证在电子表格环境中开发训练神经网络模拟方法的必要性，并系统性地回顾了现有应用。

Method: 文章通过对1890-1950年间的文献进行系统性回顾，分析了在电子表格环境中模拟神经网络的各种方法，包括联合使用电子表格和神经网络模拟工具、使用第三方插件、开发嵌入式宏、使用标准插件进行非线性优化，以及不使用插件和宏直接在电子表格中创建神经网络。此外，文章还探讨了“数学生物学公报”期刊、其创始人Nicolas Rashevsky以及围绕该期刊的科学界在创建和发展计算神经科学模型和方法中的作用，识别了神经网络创建的心理物理学基础、神经计算的数学基础以及神经工程方法（特别是图像识别）。文章还讨论了Walter Pitts在结合描述性和定量训练理论中的作用，并指出掌握基于历史和遗传学方法的模型对于获得电子表格环境中的神经模拟能力至关重要。

Result: 文章识别了三种有前景的模型，可以用于开发相应的训练方法：Rashevsky的连续双因素模型、McCulloch和Pitts的离散模型以及Householder和Landahl的离散-连续模型。

Conclusion: 为了在电子表格环境中掌握神经模拟能力，应掌握基于历史和遗传学的方法。文章指出了三种有前景的模型类别：连续双因素模型、离散模型和离散-连续模型。

Abstract: The article substantiates the necessity to develop training methods of computer simulation of neural networks in the spreadsheet environment. The systematic review of their application to simulating artificial neural networks is performed. The authors distinguish basic approaches to solving the problem of network computer simulation training in the spreadsheet environment, joint application of spreadsheets and tools of neural network simulation, application of third-party add-ins to spreadsheets, development of macros using the embedded languages of spreadsheets; use of standard spreadsheet add-ins for non-linear optimization, creation of neural networks in the spreadsheet environment without add-ins and macros. After analyzing a collection of writings of 1890-1950, the research determines the role of the scientific journal "Bulletin of Mathematical Biophysics", its founder Nicolas Rashevsky and the scientific community around the journal in creating and developing models and methods of computational neuroscience. There are identified psychophysical basics of creating neural networks, mathematical foundations of neural computing and methods of neuroengineering (image recognition, in particular). The role of Walter Pitts in combining the descriptive and quantitative theories of training is discussed. It is shown that to acquire neural simulation competences in the spreadsheet environment, one should master the models based on the historical and genetic approach. It is indicated that there are three groups of models, which are promising in terms of developing corresponding methods - the continuous two-factor model of Rashevsky, the discrete model of McCulloch and Pitts, and the discrete-continuous models of Householder and Landahl.

</details>


### [156] [Edu-Edition Spreadsheet Competency Framework](https://arxiv.org/abs/1802.00496)
*Maria Csernoch,Piroska Biró*

Main category: cs.CY

TL;DR: 该论文提出了电子表格能力框架的教育版（E2SCF），强调在教育早期阶段培养电子表格能力的重要性，并引入了高数学能力的计算机支持的现实世界问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 在教育早期阶段开始培养电子表格能力，并由专家教师提供支持，可以使这一过程更加有效。

Method: E2SCF的主要特点是高数学能力的计算机支持的现实世界问题解决能力。这种方法从培训一开始就基于双向知识转移、数据和错误分析与处理，以及电子表格的编程方面。

Result: E2SCF旨在为基础用户和普通用户建立扎实的电子表格知识，并培养可转移的问题解决技能和能力。

Conclusion: E2SCF为基础用户和普通用户建立扎实的电子表格知识，并培养可转移的问题解决技能和能力。

Abstract: Based on the Spreadsheet Competency Framework for finance professionals, in the present paper we introduce the Edu-Edition of the Spreadsheet Competency Framework (E2SCF). We claim that building spreadsheet competences should start in education, as early as possible, and this process is a lot more effective if support arrives from expert teachers. The main feature of E2SCF is high mathability computer-supported real world problem solving. This approach is based on - from the very beginning of training - a two-directional knowledge transfer, data and error analysis and handling, and the programming aspect of spreadsheets. Based on these features, E2SCF is set up for basic and general users to build up firm spreadsheet knowledge and to develop transferable problem solving skills and competences.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [157] [Using a Catenary Trajectory to Reduce Wellbore Friction in Horizontal Extended Reach Drilling](https://arxiv.org/abs/2309.12317)
*Vu Nguyen*

Main category: cs.RO

TL;DR: 钻井时井筒摩擦力是主要考虑因素之一，因此该项目提出悬链线模型以减小井筒摩擦力。


<details>
  <summary>Details</summary>
Motivation: 由于井筒摩擦力是钻井过程中的主要成本因素，因此需要研究减小井筒摩擦力的方法。

Method: 通过比较悬链线轨迹设计和传统二维圆弧设计的案例研究，研究悬链线模型的有效性。计算程序可在 Excel 电子表格中找到。

Result: 悬链线模型有望实现无接触钻井，从而最大限度地减小扭矩和阻力。

Conclusion: 悬链线轨迹设计是一种用于长半径井的有效方法，可以有效减小井筒摩擦力。

Abstract: Wellbore friction is one of the biggest concerns when drilling due to its relation to the total cost. The catenary concept was introduced to reduce wellbore friction, but it requires detailed analyses. This project would fill this gap. A catenary shape is simply the natural shape of a rope, chain, or drill string. The drill string will then hang freely inside the wellbore. Perfectly, there should be no contact between the hole and the string, and thus no friction. Torque and drag should be minimized this way. A case study is introduced to examine the outcome between Catenary Trajectory Design and traditional 2D Arc design. The calculation procedure of Catenary Trajectory and 2D Arc Design can be found in an MS Excel spreadsheet which is easy to use and reliable for designing catenary well trajectories for extended-reach wells.

</details>


### [158] [Hazard Analysis of Collaborative Automation Systems: A Two-layer Approach based on Supervisory Control and Simulation](https://arxiv.org/abs/2209.12560)
*Tom P. Huck,Yuvaraj Selvaraj,Constantin Cronrath,Christoph Ledermann,Martin Fabian,Bengt Lennartson,Torsten Kröger*

Main category: cs.RO

TL;DR: 该论文提出了一种结合形式化方法和仿真的两层方法，用于对安全关键系统进行危害分析，以克服现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前基于人工推理和经验的危害分析方法难以应对日益复杂的系统，而基于测试的方法成本高昂或存在危险。需要一种更有效的危害分析方法。

Method: 该论文提出了一种两层危害分析方法。首先，利用监督控制理论从系统形式化模型中综合出导致不安全状态的不安全行为。然后，将这些行为作为输入，在仿真环境中进行详细分析，并使用特定领域的风险度量。该方法在工业人机协作系统中进行了演示。

Result: 该方法能够结合形式化方法的穷尽分析和仿真方法的详细分析的优点，对安全关键系统进行危害分析，并在工业人机协作系统上展示了其有效性。

Conclusion: 提出的两层危害分析方法结合了形式化方法和仿真方法的优势，能够更有效地识别和分析安全关键系统中的潜在危害，尤其适用于复杂系统如工业人机协作系统。

Abstract: Safety critical systems are typically subjected to hazard analysis before commissioning to identify and analyse potentially hazardous system states that may arise during operation. Currently, hazard analysis is mainly based on human reasoning, past experiences, and simple tools such as checklists and spreadsheets. Increasing system complexity makes such approaches decreasingly suitable. Furthermore, testing-based hazard analysis is often not suitable due to high costs or dangers of physical faults. A remedy for this are model-based hazard analysis methods, which either rely on formal models or on simulation models, each with their own benefits and drawbacks. This paper proposes a two-layer approach that combines the benefits of exhaustive analysis using formal methods with detailed analysis using simulation. Unsafe behaviours that lead to unsafe states are first synthesised from a formal model of the system using Supervisory Control Theory. The result is then input to the simulation where detailed analyses using domain-specific risk metrics are performed. Though the presented approach is generally applicable, this paper demonstrates the benefits of the approach on an industrial human-robot collaboration system.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [159] [Using the Abstract Computer Architecture Description Language to Model AI Hardware Accelerators](https://arxiv.org/abs/2402.00069)
*Mika Markus Müller,Alexander Richard Manfred Borst,Konstantin Lübeck,Alexander Louis-Ferdinand Jung,Oliver Bringmann*

Main category: cs.AR

TL;DR: AI硬件加速器需要专门的硬件来运行DNN，但选择和配置它们很困难。本文提出使用ACADL来模拟AI硬件加速器，将DNN映射到它们上面，并通过模拟收集性能结果。


<details>
  <summary>Details</summary>
Motivation: 制造商需要选择合适的AI硬件加速器来满足其产品的性能要求，但目前的方法（数据表、电子表格计算、黑盒模拟器）效率低下且理解粗略。

Method: 使用ACADL来模拟AI硬件加速器，将DNN映射到它们上面，并解释用于收集性能结果的计时模拟语义。

Result: 通过ACADL模拟和计时语义，可以更有效地收集AI硬件加速器的性能结果。

Conclusion: ACADL可以有效地用于模拟AI硬件加速器，将DNN映射到它们上面，并收集性能结果，从而解决当前在选择和配置AI硬件加速器时面临的挑战。

Abstract: Artificial Intelligence (AI) has witnessed remarkable growth, particularly through the proliferation of Deep Neural Networks (DNNs). These powerful models drive technological advancements across various domains. However, to harness their potential in real-world applications, specialized hardware accelerators are essential. This demand has sparked a market for parameterizable AI hardware accelerators offered by different vendors.
  Manufacturers of AI-integrated products face a critical challenge: selecting an accelerator that aligns with their product's performance requirements. The decision involves choosing the right hardware and configuring a suitable set of parameters. However, comparing different accelerator design alternatives remains a complex task. Often, engineers rely on data sheets, spreadsheet calculations, or slow black-box simulators, which only offer a coarse understanding of the performance characteristics.
  The Abstract Computer Architecture Description Language (ACADL) is a concise formalization of computer architecture block diagrams, which helps to communicate computer architecture on different abstraction levels and allows for inferring performance characteristics. In this paper, we demonstrate how to use the ACADL to model AI hardware accelerators, use their ACADL description to map DNNs onto them, and explain the timing simulation semantics to gather performance results.

</details>


### [160] [Online Model Swapping in Architectural Simulation](https://arxiv.org/abs/2012.01571)
*Patrick Lavin,Jeffrey Young,Rich Vuduc,Jonathan Beard*

Main category: cs.AR

TL;DR: 当系统和应用程序变得越来越复杂时，详细仿真所需的时间也越来越长。为了快速迭代设计，架构师必须依赖简单的模型，比如电子表格。然而，将简单的仿真迁移到更详细的仿真需要多次执行才能找到简单的模型能够有效运行的地方，这可能比运行详细的模型本身更昂贵。此外，架构师常常需要依靠直觉来选择这些简单的模型，这使得问题更加复杂。本研究提出了一种连接简单和详细仿真之间的方法，通过在线监控仿真行为，并自动用更简单的统计模型替换详细模型。我们在开源模拟器SVE-Cachesim中实现了该方法，用于替换内存层次结构中的一级数据缓存（L1D）。这个概念验证表明，我们的技术不仅可以处理本地时间不变统计数据的近似，还可以处理随时间变化的统计数据（例如，L1D 是一种时间序列函数），以及下游的副作用（例如，L1D 会过滤掉 L2 缓存的访问）。我们的模拟用 8% 的模拟周期计数误差替换了内置缓存模型，同时使用了近似缓存模型超过 90% 的模拟时间，并且我们的简单模型每个模型“执行”所需的计算量减少了 2 到 8 倍。


<details>
  <summary>Details</summary>
Motivation: 随着系统和应用程序的复杂性增加，详细仿真所需的时间也越来越长，导致设计迭代速度减慢。架构师倾向于使用简单的模型（如电子表格）来快速迭代，但从简单模型迁移到详细模型的过程复杂且耗时。此外，选择合适的简单模型往往依赖于直觉。

Method: 提出了一种在线监控仿真行为的方法，自动用更简单的统计模型替换详细模型，以连接简单和详细仿真之间的差距。该方法在 SVE-Cachesim 模拟器中实现，用于替换内存层次结构中的 L1D 缓存。

Result: 该方法能够处理本地时间不变统计数据和随时间变化的统计数据（如L1D），以及下游的副作用。在模拟中，该方法用近似缓存模型替换了内置缓存模型超过90%的模拟时间，同时仅引入了8%的模拟周期计数误差。并且，近似模型所需的计算量是原始模型的2到8倍。

Conclusion: 所提出的方法能够有效地在详细仿真和简单仿真之间架起桥梁，通过在线监控和自动替换模型，在保证较高精度的前提下显著减少计算量和模拟时间，解决了复杂系统设计中仿真效率和迭代速度的问题。

Abstract: As systems and applications grow more complex, detailed simulation takes an ever increasing amount of time. The prospect of increased simulation time resulting in slower design iteration forces architects to use simpler models, such as spreadsheets, when they want to iterate quickly on a design. However, the task of migrating from a simple simulation to one with more detail often requires multiple executions to find where simple models could be effective, which could be more expensive than running the detailed model in the first place. Also, architects must often rely on intuition to choose these simpler models, further complicating the problem.
  In this work, we present a method of bridging the gap between simple and detailed simulation by monitoring simulation behavior online and automatically swapping out detailed models with simpler statistical approximations. We demonstrate the potential of our methodology by implementing it in the open-source simulator SVE-Cachesim to swap out the level one data cache (L1D) within a memory hierarchy. This proof of concept demonstrates that our technique can handle a non-trivial use-case in not just approximation of local time-invariant statistics, but also those that vary with time (e.g., the L1D is a form of a time-series function), and downstream side-effects (e.g., the L1D filters accesses for the level two cache). Our simulation swaps out the built-in cache model with only an 8% error in the simulated cycle count while using the approximated cache models for over 90% of the simulation, and our simpler models require two to eight times less computation per "execution" of the model

</details>
