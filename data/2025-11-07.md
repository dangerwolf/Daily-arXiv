<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution](https://arxiv.org/abs/2510.25726)
*Junlong Li,Wenshuo Zhao,Jian Zhao,Weihao Zeng,Haoze Wu,Xiaochen Wang,Rui Ge,Yuxuan Cao,Yuzhen Huang,Wei Liu,Junteng Liu,Zhaochen Su,Yiyang Guo,Fan Zhou,Lueyang Zhang,Juan Michelini,Xingyao Wang,Xiang Yue,Shuyan Zhou,Graham Neubig,Junxian He*

Main category: cs.CL

TL;DR: Toolathlon是一个新的语言智能体基准测试，旨在评估智能体在跨多个复杂应用程序和多步骤工作流中的真实世界性能，现有最先进模型在该基准上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有的语言智能体基准测试过于狭窄或简化，缺乏评估智能体真实世界性能所需的任务多样性、真实性和长期复杂性。

Method: 研究者引入了Tool Decathlon（Toolathlon）基准，该基准包含32个软件应用和604个工具，提供真实的初始环境状态，并设计了108个需要多应用交互、平均约20个回合才能完成的严格可验证任务。

Result: 最先进的模型在Toolathlon上的表现存在显著不足：Claude-4.5-Sonnet的成功率仅为38.6%，平均工具调用次数为20.2次；而顶级的开源模型DeepSeek-V3.2-Exp达到20.1%。

Conclusion: Toolathlon有望推动开发出更强大的语言智能体，以执行真实世界的长期任务。

Abstract: Real-world language agents must handle complex, multi-step workflows across
diverse Apps. For instance, an agent may manage emails by coordinating with
calendars and file systems, or monitor a production database to detect
anomalies and generate reports following an operating manual. However, existing
language agent benchmarks often focus on narrow domains or simplified tasks
that lack the diversity, realism, and long-horizon complexity required to
evaluate agents' real-world performance. To address this gap, we introduce the
Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering
diverse Apps and tools, realistic environment setup, and reliable
execution-based evaluation. Toolathlon spans 32 software applications and 604
tools, ranging from everyday platforms such as Google Calendar and Notion to
professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools
are based on a high-quality set of Model Context Protocol (MCP) servers that we
may have revised or implemented ourselves. Unlike prior works, which primarily
ensure functional realism but offer limited environment state diversity, we
provide realistic initial environment states from real software, such as Canvas
courses with dozens of students or real financial spreadsheets. This benchmark
includes 108 manually sourced or crafted tasks in total, requiring interacting
with multiple Apps over around 20 turns on average to complete. Each task is
strictly verifiable through dedicated evaluation scripts. Comprehensive
evaluation of SOTA models highlights their significant shortcomings: the
best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate
with 20.2 tool calling turns on average, while the top open-weights model
DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development
of more capable language agents for real-world, long-horizon task execution.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Benchmark Dataset Generation and Evaluation for Excel Formula Repair with LLMs](https://arxiv.org/abs/2508.11715)
*Ananya Singha,Harshita Sahijwani,Walt Williams,Emmanuel Aboah Boateng,Nick Hausman,Miguel Di Luca,Keegan Choudhury,Chaya Binet,Vu Le,Tianwei Chen,Oryan Rokeah Chen,Sulaiman Vesal,Sadid Hasan*

Main category: cs.SE

TL;DR: 本文介绍了一个用于Excel公式修复的新型基准数据集和上下文感知基线技术，以解决高质量数据缺乏的问题，并利用大型语言模型改进语义运行时错误的自动化纠正。


<details>
  <summary>Details</summary>
Motivation: Excel对新手用户来说是一个复杂工具，经常导致因逻辑错误或函数误解引起的运行时错误。尽管大型语言模型（LLMs）在解释公式错误方面有帮助，但自动化纠正这些语义运行时错误仍是一个未解决的问题。主要挑战是缺乏高质量、全面的数据集用于训练和严格评估。

Method: 提出了一种构建Excel公式修复基准数据集的新方法。该方法包含一个数据生成管道：利用少量从在线论坛整理的种子样本，通过与LLMs的少样本提示进行合成扩展，并采用强大的“LLM-as-a-Judge”验证框架，结合基于执行的检查来确保生成数据的正确性和语义忠实性。此外，提出了一种上下文感知的Excel公式修复基线技术，该技术利用LLMs结合有错误的公式和相关的电子表格上下文。评估了各种LLMs（GPT-4o, GPT-4.1, Phi-3, Mistral）在该新生成的基准上使用基于执行的指标的性能。

Result: 生成了一个包含618个高质量样本的基准数据集，涵盖了常见的运行时错误。通过手动标注证明了数据集的质量，并提供了对错误和函数分布的深入见解。

Conclusion: 所提出的生成方法具有高度可扩展性，可以很容易地调整，以创建用于其他低资源编程语言中类似代码修复任务的评估基准。

Abstract: Excel is a pervasive yet often complex tool, particularly for novice users,
where runtime errors arising from logical mistakes or misinterpretations of
functions pose a significant challenge. While large language models (LLMs)
offer promising assistance by explaining formula errors, the automated
correction of these semantic runtime errors remains an open problem. A primary
challenge to advancing models for such scenarios is the severe lack of
high-quality, comprehensive datasets for training and rigorous evaluation. This
paper addresses this gap by introducing a novel approach for constructing a
benchmark dataset specifically designed for Excel formula repair. We propose a
data generation pipeline, which leverages a small set of curated seed samples
from online forums to synthetically expand the dataset. Our pipeline integrates
few-shot prompting with LLMs and employs a robust \textit{LLM-as-a-Judge}
validation framework, combined with execution-based checks to ensure the
correctness and semantic fidelity of the generated data. This process produced
a benchmark dataset of 618 high-quality samples, covering common runtime
errors. Furthermore, we propose a context-aware baseline technique for Excel
formula repair that utilizes LLMs to leverage both the faulty formula, and
relevant spreadsheet context. We evaluate the performance of various LLMs
(GPT-4o, GPT-4.1, Phi-3, Mistral) on our newly generated benchmark using
execution-based metrics. Our analysis demonstrates the dataset's quality
through manual annotation and provides insights into error and function
distributions. The proposed generation methodology is highly scalable and can
be readily adapted to create evaluation benchmarks for similar code repair
tasks in other low-resource programming languages.

</details>
