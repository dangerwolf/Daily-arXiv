<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.HC](#cs.HC) [Total: 5]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Vision Language Models for Spreadsheet Understanding: Challenges and Opportunities](https://arxiv.org/abs/2405.16234)
*Shiyu Xia,Junyu Xiong,Haoyu Dong,Jianbo Zhao,Yuzhang Tian,Mengyu Zhou,Yeye He,Shi Han,Dongmei Zhang*

Main category: cs.CV

TL;DR: Vision Language Models (VLMs) are evaluated on spreadsheet comprehension using self-supervised challenges for OCR, spatial perception, and format recognition. While VLMs show good OCR, they struggle with spatial and format tasks, indicating a need for improvement in these areas. The paper proposes methods to generate more training data to enhance VLM performance on spreadsheets.


<details>
  <summary>Details</summary>
Motivation: This paper aims to evaluate the capabilities of Vision Language Models (VLMs) in understanding spreadsheets and to identify areas for improvement.

Method: The study proposes three self-supervised challenges (OCR, spatial perception, visual format recognition) and evaluation metrics. It also uses spreadsheet table detection and three spreadsheet-to-image settings (column width adjustment, style change, address augmentation) with prompt variants. A method to decode cell values on table boundaries is introduced to leverage VLM strengths in text understanding over 2D positioning.

Result: VLMs show promising OCR capabilities but perform unsatisfactorily in cell omission and misalignment. Their spatial and format recognition skills are insufficient.

Conclusion: VLMs exhibit potential in spreadsheet comprehension, particularly in OCR, but require significant improvements in spatial and format recognition. The proposed methods for data generation and evaluation can guide future research to enhance VLM performance on spreadsheets.

Abstract: This paper explores capabilities of Vision Language Models on spreadsheet comprehension. We propose three self-supervised challenges with corresponding evaluation metrics to comprehensively evaluate VLMs on Optical Character Recognition (OCR), spatial perception, and visual format recognition. Additionally, we utilize the spreadsheet table detection task to assess the overall performance of VLMs by integrating these challenges. To probe VLMs more finely, we propose three spreadsheet-to-image settings: column width adjustment, style change, and address augmentation. We propose variants of prompts to address the above tasks in different settings. Notably, to leverage the strengths of VLMs in understanding text rather than two-dimensional positioning, we propose to decode cell values on the four boundaries of the table in spreadsheet boundary detection. Our findings reveal that VLMs demonstrate promising OCR capabilities but produce unsatisfactory results due to cell omission and misalignment, and they notably exhibit insufficient spatial and format recognition skills, motivating future work to enhance VLMs' spreadsheet data comprehension capabilities using our methods to generate extensive spreadsheet-image pairs in various settings.

</details>


### [2] [High-Throughput Phenotyping using Computer Vision and Machine Learning](https://arxiv.org/abs/2407.06354)
*Vivaan Singhvi,Langalibalele Lunga,Pragya Nidhi,Chris Keum,Varrun Prakash*

Main category: cs.CV

TL;DR: 本研究使用 OCR 和机器学习技术对杨树表型进行高通量表征，但存在数据标签和叶片大小评估的局限性。


<details>
  <summary>Details</summary>
Motivation: 使用机器学习和自动化相机提高大规模植物表型数据集的处理效率和特定性状提取能力，解决现有研究中数据标签缺失的问题。

Method: 1. 使用 OCR 技术读取包含处理、区组、行、位置和基因型信息的植物白标签。 2. 结合图像分割技术和机器学习算法进行形态学分类（叶形、颜色、褐斑）。 3. 使用机器学习模型预测植物处理（对照或干旱）。 4. 分析 EXIF 标签以确定叶片大小并寻找表型之间的相关性。

Result: 1. OCR 模型在提取非空文本方面准确率达到 94.31%，可准确录入电子表格。 2. 形态学分类模型（叶形、颜色、褐斑）平均准确率为 62.82%。 3. 植物处理预测模型的准确率为 60.08%。 4. EXIF 标签缺失关键信息，无法评估叶片大小及表型与条件之间的相关性。

Conclusion: 虽然本研究在 OCR 和某些表型分类方面取得了进展，但 EXIF 标签数据的缺失阻碍了对叶片大小和表型相关性的全面评估，未来研究可弥补这些不足。

Abstract: High-throughput phenotyping refers to the non-destructive and efficient evaluation of plant phenotypes. In recent years, it has been coupled with machine learning in order to improve the process of phenotyping plants by increasing efficiency in handling large datasets and developing methods for the extraction of specific traits. Previous studies have developed methods to advance these challenges through the application of deep neural networks in tandem with automated cameras; however, the datasets being studied often excluded physical labels. In this study, we used a dataset provided by Oak Ridge National Laboratory with 1,672 images of Populus Trichocarpa with white labels displaying treatment (control or drought), block, row, position, and genotype. Optical character recognition (OCR) was used to read these labels on the plants, image segmentation techniques in conjunction with machine learning algorithms were used for morphological classifications, machine learning models were used to predict treatment based on those classifications, and analyzed encoded EXIF tags were used for the purpose of finding leaf size and correlations between phenotypes. We found that our OCR model had an accuracy of 94.31% for non-null text extractions, allowing for the information to be accurately placed in a spreadsheet. Our classification models identified leaf shape, color, and level of brown splotches with an average accuracy of 62.82%, and plant treatment with an accuracy of 60.08%. Finally, we identified a few crucial pieces of information absent from the EXIF tags that prevented the assessment of the leaf size. There was also missing information that prevented the assessment of correlations between phenotypes and conditions. However, future studies could improve upon this to allow for the assessment of these features.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [3] [General Table Question Answering via Answer-Formula Joint Generation](https://arxiv.org/abs/2503.12345)
*Zhongyuan Wang,Richong Zhang,Zhijie Nie,Hangyu Mao*

Main category: cs.CL

TL;DR: LLM在表格问答中表现出色，但缺乏通用性。本文提出使用电子表格公式解决表格问答问题，并构建了FormulaQA数据集和TabAF框架，在多个数据集上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的表格问答方法虽然在复杂推理方面表现出色，但缺乏处理特定类型问题或表格结构的通用性。电子表格公式作为一种成熟的表格数据操作语言，尚未得到充分利用。

Method: 提出使用电子表格公式作为可执行表示，用于解决不同结构的表格上的复杂推理问题。构建了FormulaQA数据集，并提出了TabAF框架，该框架使用单一LLM骨干网络来解码答案和公式，以同时解决多种表格类型的多种任务。

Result: TabAF框架在WikiTableQuestion、HiTab和TabFact数据集上取得了新的SOTA性能，证明了其通用性和泛化能力，即使在相同的模型大小下也表现出色。

Conclusion: TabAF框架通过利用电子表格公式，成功解决了表格问答中的通用性和复杂性问题，并在多个数据集上取得了先进的性能。

Abstract: Advanced table question answering (TableQA) methods prompt large language models (LLMs) to generate answer text, SQL query, Python code, or custom operation, which impressively improve the complex reasoning problems in the TableQA task. However, these methods lack the versatility to cope with specific question types or table structures. In contrast, the Spreadsheet Formula, the widely used and well-defined operation language for tabular data, has not been thoroughly explored to solve TableQA. In this paper, we first attempt to use the Formula as the executable representation for solving complex reasoning on tables with different structures. Specifically, we construct \texttt{FromulaQA}, a large Formula-annotated TableQA dataset from existing datasets. In addition, we propose \texttt{TabAF}, a general table answering framework to solve multiple types of tasks over multiple types of tables simultaneously, which decodes answers and Formulas with a single LLM backbone. Extensive experiments demonstrate the versatility and generalization of \texttt{TabAF}. Under the same model size, \texttt{TabAF} achieves new state-of-the-art performance on the WikiTableQuestion, HiTab, and TabFact.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [4] [Billion-files File Systems (BfFS): A Comparison](https://arxiv.org/abs/2408.01805)
*Sohail Shaikh*

Main category: cs.PF

TL;DR: 对EXT4、XFS、BtrFS、ZFS和F2FS这几种流行的Linux文件系统进行了分析，研究了它们在处理海量文件时的读写吞吐量、存储使用情况及性能下降等问题。


<details>
  <summary>Details</summary>
Motivation: 随着数据量的爆炸式增长，需要在靠近计算设备的本地存储上快速处理数据，因此了解本地文件系统的性能和局限性至关重要。

Method: 创建、存储和读取10亿个文件，并分析了读写吞吐量、存储块使用情况、磁盘空间利用率和开销等指标，同时研究了大量文件创建期间和之后文件系统的性能下降情况。

Result: 分析了几种流行的Linux文件系统（EXT4、XFS、BtrFS、ZFS和F2FS）的读写吞吐量、存储使用情况和性能下降情况。

Conclusion: 对EXT4、XFS、BtrFS、ZFS和F2FS这几种流行的Linux文件系统进行了分析，研究了它们在处理海量文件时的读写吞吐量、存储使用情况及性能下降等问题。

Abstract: As the volume of data being produced is increasing at an exponential rate that needs to be processed quickly, it is reasonable that the data needs to be available very close to the compute devices to reduce transfer latency. Due to this need, local filesystems are getting close attention to understand their inner workings, performance, and more importantly their limitations. This study analyzes few popular Linux filesystems: EXT4, XFS, BtrFS, ZFS, and F2FS by creating, storing, and then reading back one billion files from the local filesystem. The study also captured and analyzed read/write throughput, storage blocks usage, disk space utilization and overheads, and other metrics useful for system designers and integrators. Furthermore, the study explored other side effects such as filesystem performance degradation during and after these large numbers of files and folders are created.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [5] [Solving Data-centric Tasks using Large Language Models](https://arxiv.org/abs/2402.11734)
*Shraddha Barke,Christian Poelitz,Carina Suzana Negreanu,Benjamin Zorn,José Cambronero,Andrew D. Gordon,Vu Le,Elnaz Nouri,Nadia Polikarpova,Advait Sarkar,Brian Slininger,Neil Toronto,Jack Williams*

Main category: cs.PL

TL;DR: LLM在处理包含表格数据的自然语言到代码任务时，数据选择至关重要。本文提出了一个名为“聚类后选择”的技术，以选择最具代表性的数据行加入提示，并通过实验证明了该方法优于随机选择。


<details>
  <summary>Details</summary>
Motivation: LLM在处理自然语言描述的数据操作任务时，如何有效地在提示中包含表格数据是一个关键问题，尤其是对于非专业程序员和终端用户。

Method: 创建了一个从StackOverflow帖子中挖掘的真实世界自然语言到代码任务数据集，并提出了一种“聚类后选择”的提示技术，该技术将输入数据中最具代表性的行添加到LLM提示中。

Result: 实验表明，LLM在处理包含表格数据的自然语言到代码任务时的性能对提示中包含的数据量敏感。所提出的“聚类后选择”技术在输入表格具有大量语法变异的任务上，优于随机选择基线。

Conclusion: 对于涉及表格数据的自然语言到代码任务，选择合适的样本数据并将其包含在提示中，能够显著提升LLM的性能。聚类后选择技术是一种有效的数据选择策略。

Abstract: Large language models (LLMs) are rapidly replacing help forums like StackOverflow, and are especially helpful for non-professional programmers and end users. These users are often interested in data-centric tasks, such as spreadsheet manipulation and data wrangling, which are hard to solve if the intent is only communicated using a natural-language description, without including the data. But how do we decide how much data and which data to include in the prompt? This paper makes two contributions towards answering this question. First, we create a dataset of real-world NL-to-code tasks manipulating tabular data, mined from StackOverflow posts. Second, we introduce a cluster-then-select prompting technique, which adds the most representative rows from the input data to the LLM prompt. Our experiments show that LLM performance is indeed sensitive to the amount of data passed in the prompt, and that for tasks with a lot of syntactic variation in the input table, our cluster-then-select technique outperforms a random selection baseline.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [Oh That Looks Familiar: A Novel Similarity Measure for Spreadsheet Template Discovery](https://arxiv.org/abs/2511.06973)
*Anand Krishnakumar,Vengadesh Ravikumaran*

Main category: cs.LG

TL;DR: 该研究提出了一种结合语义嵌入、数据类型和空间布局的混合距离度量方法，用于量化电子表格之间的相似性，并在FUSTE数据集上实现了比现有方法更好的无监督聚类和模板重建效果。


<details>
  <summary>Details</summary>
Motivation: 传统方法未能捕捉定义模板的空间布局和类型模式，因此需要新的方法来量化电子表格的相似性。

Method: 将电子表格转换为单元格级别的嵌入，并结合Chamfer和Hausdorff距离等聚合技术来计算相似性。

Result: 在FUSTE数据集上，该方法实现了比基于图的Mondrian基线模型更优越的无监督聚类性能，模板重建的调整兰德指数（ARI）达到1.00，而基线模型为0.90。

Conclusion: 该方法能够有效地进行大规模自动化模板发现，从而支持检索增强生成、模型训练和批量数据清理等下游应用。

Abstract: Traditional methods for identifying structurally similar spreadsheets fail to capture the spatial layouts and type patterns defining templates. To quantify spreadsheet similarity, we introduce a hybrid distance metric that combines semantic embeddings, data type information, and spatial positioning. In order to calculate spreadsheet similarity, our method converts spreadsheets into cell-level embeddings and then uses aggregation techniques like Chamfer and Hausdorff distances. Experiments across template families demonstrate superior unsupervised clustering performance compared to the graph-based Mondrian baseline, achieving perfect template reconstruction (Adjusted Rand Index of 1.00 versus 0.90) on the FUSTE dataset. Our approach facilitates large-scale automated template discovery, which in turn enables downstream applications such as retrieval-augmented generation over tabular collections, model training, and bulk data cleaning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.05587)
*Junjie Xing,Yeye He,Mengyu Zhou,Haoyu Dong,Shi Han,Lingjiao Chen,Dongmei Zhang,Surajit Chaudhuri,H. V. Jagadish*

Main category: cs.AI

TL;DR: LLMs在处理表格数据方面取得了显著进展，但现有的基准测试未能全面评估其在现实世界中的专家级能力。本研究提出了MMTU，一个包含25个真实世界表格任务的超大规模基准测试，用于评估模型在理解、推理和操作表格方面的能力。该基准测试揭示了当前最先进的模型在处理复杂表格任务时仍面临挑战，并期望能推动结构化数据处理和分析领域基础模型的发展。


<details>
  <summary>Details</summary>
Motivation: 传统的表格操作需要专业用户，而现有LLM基准测试未能全面评估其在真实世界表格任务中的专家级能力，限制了模型的进步和理解。

Method: 提出了MMTU，一个包含28K多条问题和25个真实世界表格任务的大规模基准测试，旨在全面评估模型理解、推理和操作真实表格的专家级能力。这些任务源于数十年的计算机科学研究，专注于专业用户面临的复杂表格任务。

Result: MMTU测试表明，当前最先进的模型（如OpenAI GPT-5和DeepSeek R1）在这些任务上的得分分别为69%和57%，表明在表格理解、推理和编码等综合技能方面仍有显著的提升空间。

Conclusion: MMTU基准测试揭示了当前模型在处理复杂表格任务方面的局限性，并期望通过提供全面的评估来推动结构化数据处理和分析领域基础模型的发展。

Abstract: Tables and table-based use cases play a crucial role in many important real-world applications, such as spreadsheets, databases, and computational notebooks, which traditionally require expert-level users like data engineers, data analysts, and database administrators to operate. Although LLMs have shown remarkable progress in working with tables (e.g., in spreadsheet and database copilot scenarios), comprehensive benchmarking of such capabilities remains limited. In contrast to an extensive and growing list of NLP benchmarks, evaluations of table-related tasks are scarce, and narrowly focus on tasks like NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks that professional users face. This gap limits our understanding and model progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 28K questions across 25 real-world table tasks, designed to comprehensively evaluate models ability to understand, reason, and manipulate real tables at the expert-level. These tasks are drawn from decades' worth of computer science research on tabular data, with a focus on complex table tasks faced by professional users. We show that MMTU require a combination of skills -- including table understanding, reasoning, and coding -- that remain challenging for today's frontier models, where even frontier reasoning models like OpenAI GPT-5 and DeepSeek R1 score only around 69\% and 57\% respectively, suggesting significant room for improvement. We highlight key findings in our evaluation using MMTU and hope that this benchmark drives further advances in understanding and developing foundation models for structured data processing and analysis.
  Our code and data are available at https://github.com/MMTU-Benchmark/MMTU and https://huggingface.co/datasets/MMTU-benchmark/MMTU.

</details>


### [8] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: 当前人工智能主要关注像素和词语的模式识别，但忽视了对现实世界实体及其关系的建模。


<details>
  <summary>Details</summary>
Motivation: 大多数公司最有价值的数据是关系型的（如电子表格、数据库），而非文本和图像。然而，当前机器学习领域并未充分研究这类数据。

Method: 探讨了关系学习（或称统计关系人工智能）为何未能普及，并提出了提升其地位的建议。

Result: 该领域的研究（包括关系学习）并未取得预期的广泛成功，仅在关系受限的情况下有所应用。

Conclusion: 需要进一步的研究和发展，以使关系学习在人工智能领域占据应有的重要地位。

Abstract: Artificial intelligence seems to be taking over the world with systems that model pixels, words, and phonemes. The world is arguably made up, not of pixels, words, and phonemes but of entities (objects, things, including events) with properties and relations among them. Surely we should model these, not the perception or description of them. You might suspect that concentrating on modeling words and pixels is because all of the (valuable) data in the world is in terms of text and images. If you look into almost any company you will find their most valuable data is in spreadsheets, databases and other relational formats. These are not the form that are studied in introductory machine learning, but are full of product numbers, student numbers, transaction numbers and other identifiers that can't be interpreted naively as numbers. The field that studies this sort of data has various names including relational learning, statistical relational AI, and many others. This paper explains why relational learning is not taking over the world -- except in a few cases with restricted relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [9] [The Paradox of Spreadsheet Self-Efficacy: Social Incentives for Informal Knowledge Sharing in End-User Programming](https://arxiv.org/abs/2408.08068)
*Qing,Xia,Advait Sarkar,Duncan P. Brumby,Anna Cox*

Main category: cs.HC

TL;DR: 个人（自我效能感）、社会（声誉收益、同事间的信任）和软件相关（编码工作量）的变量会影响电子表格知识共享的意愿。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解个人、社会和软件相关变量如何影响电子表格知识共享的意愿。

Method: 通过对来自行政和财务岗位的电子表格用户（n=100）的调查数据进行多元回归分析。

Result: 高水平的电子表格自我效能感和认为分享会带来声誉收益的认知，可以预测更高的知识共享意愿；然而，发现知识编码工作量大的个体，其知识共享意愿较低。此外，无论何种职业，用户倾向于报告较低的总体电子表格熟练度自我效能感，尽管他们在工作相关环境下的电子表格使用方面报告了较高的自我效能感。

Conclusion: 承认并设计这些社会和个人变量，有助于避免有经验的人不必要地回避分享，这对电子表格设计有启示。

Abstract: Informal Knowledge Sharing (KS) is vital for end-user programmers to gain expertise. To better understand how personal (self-efficacy), social (reputational gains, trust between colleagues), and software-related (codification effort) variables influence spreadsheet KS intention, we conducted a multiple regressions analysis based on survey data from spreadsheet users (n=100) in administrative and finance roles. We found that high levels of spreadsheet self-efficacy and a perception that sharing would result in reputational gains predicted higher KS intention, but individuals who found knowledge codification effortful showed lower KS intention. We also observed that regardless of occupation, users tended to report a lower sense of self-efficacy in their general spreadsheet proficiency, despite also reporting high self-efficacy in spreadsheet use for job-related contexts. Our findings suggest that acknowledging and designing for these social and personal variables can help avoid situations where experienced individuals refrain unnecessarily from sharing, with implications for spreadsheet design.

</details>


### [10] [Subject integration with spreadsheets -- Ignoring education is the greatest risk ever](https://arxiv.org/abs/2409.12975)
*Mária Csernoch,Ádám Gulácsi,Júlia Csernoch*

Main category: cs.HC

TL;DR: Subject integration with spreadsheets can enhance digitalization in schools by contextualizing informatics classes and developing crucial skills for students and teachers.


<details>
  <summary>Details</summary>
Motivation: Subject integration within the framework of Technological Pedagogical and Content Knowledge is proposed as a solution for meaningful digitalization in schools, aiming to teach any subject with digital support, contextualize informatics classes, and bridge the gap between 'serious informatics' and 'digital literacy'.

Method: The paper details how three traditional Grade 3 tasks can be solved using spreadsheets and outlines the skills, competencies, and computer science knowledge that can be developed in both teachers and students through this process.

Result: The use of spreadsheets for solving traditional Grade 3 tasks demonstrates that the analysis, understanding, planning, and discussion of these tasks are as important as the practical activity within the spreadsheets, highlighting the role of this process in preparing students for future jobs.

Conclusion: The paper concludes that integrating informatics through spreadsheet-based problem-solving in traditional subjects is an effective way to foster digitalization in schools, developing essential skills for students and teachers and preparing them for the future workforce.

Abstract: Within the framework of Technological Pedagogical and Content Knowledge, subject integration is one possible solution for the introduction of meaningful digitalization and digitization in schools. This process incorporates that any school subject can be taught with digital support, informatics (computer) classes can be contextualized, and the gap between 'serious informatics' and 'digital literacy' can be minimized. The present paper details how three traditional Grade 3 tasks can be solved in spreadsheets, what skills, competencies, and computer science knowledge of both teachers and students can be developed. The solutions also reveal that analysing, understanding, planning, and discussing tasks is as important as the activity in the spreadsheets, which process plays a crucial role in the preparation of students for their future jobs.

</details>


### [11] [Data Guards: Challenges and Solutions for Fostering Trust in Data](https://arxiv.org/abs/2407.14042)
*Nicole Sultanum,Dennis Bromley,Michael Correll*

Main category: cs.HC

TL;DR: 数据造假威胁数据驱动决策的有效性。本研究通过访谈数据生产者和消费者，探讨建立数据信任的策略和障碍，发现数据验证和核查的需求普遍存在但缺乏标准。因此，研究提出了一系列“数据卫士”方法和工具，以增强数据产品的可信度。


<details>
  <summary>Details</summary>
Motivation: 数据造假威胁数据驱动决策的有效性，因此需要建立对数据的信任或进行验证。

Method: 通过访谈数据生产者和消费者，研究建立数据信任的策略和障碍。

Result: 研究发现，数据消费者普遍需要数据验证和核查，但现有标准缺乏。

Conclusion: 提出了一系列“数据卫士”方法和工具，以增强数据产品的可信度。

Abstract: From dirty data to intentional deception, there are many threats to the validity of data-driven decisions. Making use of data, especially new or unfamiliar data, therefore requires a degree of trust or verification. How is this trust established? In this paper, we present the results of a series of interviews with both producers and consumers of data artifacts (outputs of data ecosystems like spreadsheets, charts, and dashboards) aimed at understanding strategies and obstacles to building trust in data. We find a recurring need, but lack of existing standards, for data validation and verification, especially among data consumers. We therefore propose a set of data guards: methods and tools for fostering trust in data artifacts.

</details>


### [12] [Smart Portable Computer](https://arxiv.org/abs/2405.05292)
*Niladri Das*

Main category: cs.HC

TL;DR: 便携式智能计算机在疫情期间为难以购买个人电脑的学生提供了具有成本效益的替代方案，性能可与台式机媲美。


<details>
  <summary>Details</summary>
Motivation: 疫情期间，学生难以获得价格在15000印度卢比以上且配置不足的台式机或笔记本电脑。

Method: 提出了一种名为“便携式智能计算机”的新型设备，它在紧凑、节能且经济高效的封装中提供了与传统台式机相媲美的速度和性能。

Result: 该设备提供了无缝的桌面体验，支持文档编辑、多标签浏览、电子表格管理、演示文稿创建以及Python、C、C++等编程语言和Keil、Xilinx等编译器的运行。

Conclusion: 便携式智能计算机为需要强大计算能力的个人（包括学生和程序员）提供了一种便携、节能且经济高效的解决方案。

Abstract: Amidst the COVID-19 pandemic, with many organizations, schools, colleges, and universities transitioning to virtual platforms, students encountered difficulties in acquiring PCs such as desktops or laptops. The starting prices, around 15,000 INR, often failed to offer adequate system specifications, posing a challenge for consumers. Additionally, those reliant on laptops for work found the conventional approach cumbersome. Enter the "Portable Smart Computer," a leap into the future of computing. This innovative device boasts speed and performance comparable to traditional desktops but in a compact, energy-efficient, and cost-effective package. It delivers a seamless desktop experience, whether one is editing documents, browsing multiple tabs, managing spreadsheets, or creating presentations. Moreover, it supports programming languages like Python, C, C++, as well as compilers such as Keil and Xilinx, catering to the needs of programmers.

</details>


### [13] ["My toxic trait is thinking I'll remember this": gaps in the learner experience of video tutorials for feature-rich software](https://arxiv.org/abs/2404.07114)
*Ian Drosos,Advait Sarkar,Andrew D. Gordon*

Main category: cs.HC

TL;DR: 视频教程存在“沟壑”问题，影响用户学习，本研究通过分析用户评论和访谈创作者，提出沟壑理论和分类，并给出设计建议以解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是解决视频教程在用户学习过程中遇到的“沟壑”问题，特别是针对像Excel这样功能丰富的软件。

Method: 本研究采用的方法包括：1. 收集和分析来自YouTube、TikTok和Instagram上90个Microsoft Excel视频教程的360条用户评论，以识别沟壑问题。2. 建立沟壑的理论和分类。3. 访谈8位有影响力的教程创作者，了解他们观众遇到的沟壑以及如何解决。4. 收集创作者在制作视频教程过程中的见解、遇到的困难以及对解决沟壑的设计的反馈。

Result: 本研究识别并分类了视频教程中的沟壑问题，这些问题会阻碍用户学习。通过分析用户评论和访谈创作者，研究揭示了沟壑的性质以及创作者应对这些问题的策略。此外，研究还收集了创作者对旨在解决这些沟壑的设计的反馈和替代想法。

Conclusion: 本研究提出了一个关于视频教程中“沟壑”问题的理论和分类，强调了这些沟壑如何成为学习的障碍。研究结果为理解用户在观看视频教程时遇到的挑战提供了见解，并为改进视频教程的设计和交付提供了基于证据的建议，以提高学习体验。

Abstract: Video tutorials are a popular medium for informal and formal learning. However, when learners attempt to view and follow along with these tutorials, they encounter what we call gaps, that is, issues that can prevent learning. We examine the gaps encountered by users of video tutorials for feature-rich software, such as spreadsheets. We develop a theory and taxonomy of such gaps, identifying how they act as barriers to learning, by collecting and analyzing 360 viewer comments from 90 Microsoft Excel video tutorials published by 43 creators across YouTube, TikTok, and Instagram. We conducted contextual interviews with 8 highly influential tutorial creators to investigate the gaps their viewers experience and how they address them. Further, we obtain insights into their creative process and frustrations when creating video tutorials. Finally, we present creators with two designs that aim to address gaps identified in the comment analysis for feedback and alternative design ideas.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [14] [Knowledge engineering for open science: Building and deploying knowledge bases for metadata standards](https://arxiv.org/abs/2507.22391)
*Mark A. Musen,Martin J. O'Connor,Josef Hardi,Marcos Martinez-Romero*

Main category: cs.DL

TL;DR: Scientists want their data to be FAIR (findable, accessible, interoperable, and reusable), which requires rich, standardized metadata. CEDAR develops technology to create metadata templates based on community standards, enabling better data annotation and promoting open science.


<details>
  <summary>Details</summary>
Motivation: The motivation is to make scientific datasets FAIR by addressing the challenge of remembering and applying complex FAIR principles, particularly the need for rich, standardized metadata.

Method: CEDAR builds technology to encode metadata standards as templates. These templates enumerate experiment attributes, capture preferences for data description, and specify what is needed for third parties to understand datasets. They are used to standardize metadata, power data-annotation systems (Web forms, spreadsheets), and correct metadata for standards adherence.

Result: CEDAR templates have been used by scientific consortia to standardize metadata, forming the basis for data-annotation systems. They help ensure metadata adheres to community preferences and standards.

Conclusion: CEDAR templates provide a mechanism for scientific communities to establish shared metadata standards, encode preferences, and deploy these standards in intelligent systems to promote open science. They capture knowledge in symbolic form, applicable across various settings, similar to declarative knowledge bases used in intelligent systems.

Abstract: Scientists strive to make their datasets available in open repositories, with the goal that they be findable, accessible, interoperable, and reusable (FAIR). Although it is hard for most investigators to remember all the guiding principles associated with FAIR data, there is one overarching requirement: The data need to be annotated with rich, discipline-specific, standardized metadata. The Center for Expanded Data Annotation and Retrieval (CEDAR) builds technology that enables scientists to encode metadata standards as templates that enumerate the attributes of different kinds of experiments. These metadata templates capture preferences regarding how data should be described and what a third party needs to know to make sense of the datasets. CEDAR templates describing community metadata preferences have been used to standardize metadata for a variety of scientific consortia. They have been used as the basis for data-annotation systems that acquire metadata through Web forms or through spreadsheets, and they can help correct metadata to ensure adherence to standards. Like the declarative knowledge bases that underpinned intelligent systems decades ago, CEDAR templates capture the knowledge in symbolic form, and they allow that knowledge to be applied in a variety of settings. They provide a mechanism for scientific communities to create shared metadata standards and to encode their preferences for the application of those standards, and for deploying those standards in a range of intelligent systems to promote open science.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [15] [Consensus-Free Spreadsheet Integration](https://arxiv.org/abs/2209.14457)
*Brandon Baylor,Eric Daimler,James Hansen,Esteban Montero,Ryan Wisnesky*

Main category: cs.DB

TL;DR: 通过将电子表格公式表示为代数理论，将值表示为理论模型，并利用范畴论中的 colimit、lifting 和 Kan 扩展来合并电子表格。


<details>
  <summary>Details</summary>
Motivation: 开发一种无需模型作者达成共识即可合并工程模型的方法，该方法通过保持语义的理论和模型形态来实现。

Method: 将电子表格公式表达为代数理论，将值表达为理论模型，将重叠部分表达为理论和模型形态，然后使用范畴论中的 colimit、lifting 和 Kan 扩展来计算和表示合并后的理论和模型。

Result: 通过一个在能源公司进行的实际案例研究，展示了集成两个独立开发的井筒压力测试（MASP）计算电子表格的理论和模型，并讨论了相关的自动定理证明负担。

Conclusion: 探讨了将此方法扩展到企业级工程应用的潜力。

Abstract: We describe a method for merging multiple spreadsheets into one sheet, and/or exchanging data among the sheets, by expressing each sheet's formulae as an algebraic (equational) theory and each sheet's values as a model of its theory, expressing the overlap between the sheets as theory and model morphisms, and then performing colimit, lifting, and Kan-extension constructions from category theory to compute a canonically universal integrated theory and model, which can then be expressed as a spreadsheet. Our motivation is to find methods of merging engineering models that do not require consensus (agreement) among the authors of the models being merged, a condition fulfilled by our method because theory and model morphisms are semantics-preserving. We describe a case study of this methodology on a real-world oil and gas calculation at a major energy company, describing the theories and models that arise when integrating two different casing pressure test (MASP) calculation spreadsheets constructed by two non-interacting engineers. We also describe the automated theorem proving burden associated with both verifying the semantics preservation of the overlap mappings as well as verifying the conservativity/consistency of the resulting integrated sheet. We conclude with thoughts on how to apply the methodology to scale engineering efforts across the enterprise.

</details>


### [16] [A System and Benchmark for LLM-based Q&A on Heterogeneous Data](https://arxiv.org/abs/2409.05735)
*Achille Fokoue,Srideepika Jayaraman,Elham Khabiri,Jeffrey O. Kephart,Yingjie Li,Dhruv Shah,Youssef Drissi,Fenno F. Heath,Anu Bhamidipaty,Fateh A. Tipu,Robert J. Baseman*

Main category: cs.DB

TL;DR: Siwarex 平台实现了对数据库和 API 的无缝自然语言访问，有效解决了工业环境中数据源异构性问题。


<details>
  <summary>Details</summary>
Motivation: 用户希望能够用自然语言查询结构化数据，但现实中的数据源往往是异构的，难以访问和整合。

Method: 提出了 siwarex 平台，扩展了 Spider 数据集和基准测试，用数据检索 API 替换部分表格，以处理数据源异构性。

Result: Siwarex 平台能够很好地处理数据源异构性。

Conclusion: Siwarex 平台为解决工业环境中的数据源异构性问题提供了有效方案，其改进的 Spider 基准测试将有助于该领域的研究。

Abstract: In many industrial settings, users wish to ask questions whose answers may be found in structured data sources such as a spreadsheets, databases, APIs, or combinations thereof. Often, the user doesn't know how to identify or access the right data source. This problem is compounded even further if multiple (and potentially siloed) data sources must be assembled to derive the answer. Recently, various Text-to-SQL applications that leverage Large Language Models (LLMs) have addressed some of these problems by enabling users to ask questions in natural language. However, these applications remain impractical in realistic industrial settings because they fail to cope with the data source heterogeneity that typifies such environments. In this paper, we address heterogeneity by introducing the siwarex platform, which enables seamless natural language access to both databases and APIs. To demonstrate the effectiveness of siwarex, we extend the popular Spider dataset and benchmark by replacing some of its tables by data retrieval APIs. We find that siwarex does a good job of coping with data source heterogeneity. Our modified Spider benchmark will soon be available to the research community

</details>


### [17] [Auditable and reusable crosswalks for fast, scaled integration of scattered tabular data](https://arxiv.org/abs/2409.01517)
*Gavin Chait*

Main category: cs.DB

TL;DR: 该工具包旨在生成结构良好且可互操作的数据，以应对复杂和分散的表格数据。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏用于结构良好且可互操作的数据生成的工具。为了解决这个问题，本研究提出了一种开放源代码的策展工具包。

Method: 该工具包将策展过程划分为离散的组件，并采用以模式为中心的关注点，以实现对复杂且分散的表格数据的可审核重组，使其符合目标模式。任务分离允许在没有源数据的情况下进行软件开发和分析。转换被捕获为描述模式到模式映射的高级顺序脚本，从而降低了复杂性和资源需求。

Result: 该工具包提供了一个 Python 包和一个‘无代码’的 Web 应用程序，并展示了一个从数百个地方议会的分散源数据集成到单个数据库中的纵向研究的视觉示例。

Conclusion: 该工具包可用于重组任何符合模式定义的通用数据，而不仅仅是用于原始研究中收集的数据。

Abstract: This paper presents an open-source curatorial toolkit intended to produce well-structured and interoperable data. Curation is divided into discrete components, with a schema-centric focus for auditable restructuring of complex and scattered tabular data to conform to a destination schema. Task separation allows development of software and analysis without source data being present. Transformations are captured as high-level sequential scripts describing schema-to-schema mappings, reducing complexity and resource requirements. Ultimately, data are transformed, but the objective is that any data meeting a schema definition can be restructured using a crosswalk. The toolkit is available both as a Python package, and as a 'no-code' visual web application. A visual example is presented, derived from a longitudinal study where scattered source data from hundreds of local councils are integrated into a single database.

</details>


### [18] [Auto-Formula: Recommend Formulas in Spreadsheets using Contrastive Learning for Table Representations](https://arxiv.org/abs/2404.12608)
*Sibei Chen,Yeye He,Weiwei Cui,Ju Fan,Song Ge,Haidong Zhang,Dongmei Zhang,Surajit Chaudhuri*

Main category: cs.DB

TL;DR: 该研究提出了一种名为Auto-Formula的系统，通过学习和适应组织内相似电子表格中已存在的公式，来帮助用户更轻松地编写复杂公式。


<details>
  <summary>Details</summary>
Motivation: 尽管电子表格很受欢迎，但对于非技术用户来说，编写复杂公式仍然很困难，需要查找和理解复杂的公式语法。

Method: 利用组织内相似电子表格中存在的相似数据和计算逻辑，通过对比学习技术（受计算机视觉中“相似人脸识别”的启发）来学习和适应现有公式，从而预测用户想要在目标电子表格单元格中编写的公式。

Result: 在从实际企业电子表格中提取的2000多个测试公式上进行的广泛评估表明，Auto-Formula的有效性优于其他方法。

Conclusion: Auto-Formula系统通过学习和适应相似电子表格中已有的公式，有效解决了非技术用户在编写复杂电子表格公式时遇到的困难。

Abstract: Spreadsheets are widely recognized as the most popular end-user programming tools, which blend the power of formula-based computation, with an intuitive table-based interface. Today, spreadsheets are used by billions of users to manipulate tables, most of whom are neither database experts nor professional programmers.
  Despite the success of spreadsheets, authoring complex formulas remains challenging, as non-technical users need to look up and understand non-trivial formula syntax. To address this pain point, we leverage the observation that there is often an abundance of similar-looking spreadsheets in the same organization, which not only have similar data, but also share similar computation logic encoded as formulas. We develop an Auto-Formula system that can accurately predict formulas that users want to author in a target spreadsheet cell, by learning and adapting formulas that already exist in similar spreadsheets, using contrastive-learning techniques inspired by "similar-face recognition" from compute vision.
  Extensive evaluations on over 2K test formulas extracted from real enterprise spreadsheets show the effectiveness of Auto-Formula over alternatives. Our benchmark data is available at https://github.com/microsoft/Auto-Formula to facilitate future research.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [19] [TableTalk: Scaffolding Spreadsheet Development with a Language Agent](https://arxiv.org/abs/2502.09787)
*Jenny T. Liang,Aayush Kumar,Yasharth Bajpai,Sumit Gulwani,Vu Le,Chris Parnin,Arjun Radhakrishna,Ashish Tiwari,Emerson Murphy-Hill,Guastavo Soares*

Main category: cs.SE

TL;DR: TableTalk是一个智能代理，可以帮助用户更轻松地创建电子表格。它通过提供结构化计划、生成多个下一步选项和逐步构建电子表格来做到这一点。在实验中，TableTalk创建的电子表格质量更高，用户满意度更高，并且减少了认知负荷和思考时间。


<details>
  <summary>Details</summary>
Motivation: 电子表格编程具有挑战性，需要结合编程知识和解决问题的技能。现有的模型可以帮助创建电子表格，但TableTalk旨在通过模拟专业工作流程和提供适应性强的计划来进一步简化这一过程。

Method: TableTalk的设计基于对7位电子表格程序员和85个Excel模板的研究，遵循了脚手架、灵活性和增量性三个原则。它通过结构化计划、生成三个潜在的下一步选项以及使用预定义工具逐步构建电子表格来工作。

Result: 在20位程序员的实验中，TableTalk生成的电子表格被用户偏好的可能性是基线的2.3倍。此外，它将认知负荷和思考时间减少了12.6%。

Conclusion: TableTalk在电子表格创建方面取得了显著成效，表明其在AI辅助编程和人机协作方面的潜力。研究结果还为设计未来的智能电子表格编程工具提供了指导方针。

Abstract: Spreadsheet programming is challenging. Programmers use spreadsheet programming knowledge (e.g., formulas) and problem-solving skills to combine actions into complex tasks. Advancements in large language models have introduced language agents that observe, plan, and perform tasks, showing promise for spreadsheet creation. We present TableTalk, a spreadsheet programming agent embodying three design principles -- scaffolding, flexibility, and incrementality -- derived from studies with seven spreadsheet programmers and 85 Excel templates. TableTalk guides programmers through structured plans based on professional workflows, generating three potential next steps to adapt plans to programmer needs. It uses pre-defined tools to generate spreadsheet components and incrementally build spreadsheets. In a study with 20 programmers, TableTalk produced higher-quality spreadsheets 2.3 times more likely to be preferred than the baseline. It reduced cognitive load and thinking time by 12.6%. From this, we derive design guidelines for agentic spreadsheet programming tools and discuss implications on spreadsheet programming, end-user programming, AI-assisted programming, and human-agent collaboration.

</details>


### [20] [MaxMind: A Memory Loop Network to Enhance Software Productivity based on Large Language Models](https://arxiv.org/abs/2408.03841)
*Yuchen Dong,XiaoXiang Fang,Yuchen Hu,Renshuang Jiang,Zhe Jiang*

Main category: cs.SE

TL;DR: 该论文提出了一种名为MaxMind的新模型，用于通过改进的外部记忆模型和检索增强生成（RAG）机制来增强大型语言模型（LLM）在软件操作和工具生成（SOTG）方面的能力，旨在提高软件生产力。


<details>
  <summary>Details</summary>
Motivation: 当前研究在将实时任务经验转化为系统记忆以及区分现有知识的价值方面存在不足，限制了LLM在SOTG领域的应用。

Method: 提出Memory-Loop Networks用于及时记忆和经验引用，并增强了具有知识精度分段的RAG机制，以根据价值差异化利用内存，并据此设计了MaxMind模型。

Result: 通过MaxMind4Sheet（一个电子表格处理系统）的实验表明，任务记忆的积累和回收可将任务成功率稳定提升约3%-6%，并将任务执行效率提高高达25%，同时解决了LLM在处理专业任务时面临的再训练问题。

Conclusion: MaxMind模型在增强LLM在SOTG方面的能力和生产力方面具有巨大潜力。

Abstract: The application of large language models to facilitate automated software operations and tool generation (SOTG), thus augmenting software productivity, mirrors the early stages of human evolution when the ability to create and use tools accelerated the progress of civilization. These complex tasks require AI to continuously summarize and improve. Current research often overlooks the importance of converting real-time task experiences into system memory and differentiating the value of existing knowledge for future reference. This paper addresses these issues by evolving external memory models into Memory-Loop Networks for timely memorization and experience referencing. We also enhance a RAG mechanism with knowledge precision segmentation to utilize memory based on value differentiation, and design the MaxMind model for SOTG accordingly.To demonstrate our approach, we developed MaxMind4Sheet, an electronic spreadsheet processing system aligned with the MaxMind philosophy. Comparative experiments with SheetCopilot have demonstrated that the accumulation and recycling of task memories lead to a steady enhancement in task success rate, with an improvement rate of approximately 3%-6% per round in this implementation example. Note that as the memories continue to grow, this cumulative improvement may be substantial. The inclusion of memory recycling can also boost the system's task execution efficiency by up to 25%, and it can address the retraining issue faced by LLMs when handling specialized tasks through memories transfer.These suggest that MaxMind has significant potential to enhance the capabilities and productivity of LLM systems in SOTG.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [21] [Excel: Automated Ledger or Analytics IDE?](https://arxiv.org/abs/2409.12976)
*Andrew Kumiega*

Main category: cs.CY

TL;DR: Excel已经从一个简单的账本自动化工具演变成一个集成开发环境(IDE)，但许多人并未意识到其内置的数据库、OLAP引擎、统计编程语言、第三方库、动态图表和实时数据连接器。这导致了管理Excel作为分析IDE所带来的风险的框架需要扩展。


<details>
  <summary>Details</summary>
Motivation: Excel已演变成一个包含数据库、OLAP引擎、统计编程语言、第三方库、动态图表和实时数据连接器的分析集成开发环境(IDE)，但目前的风险管理框架未能跟上这一演变，需要进行扩展以应对由此产生的日益增长的风险。

Method: 本文将阐述如何扩展现有的电子表格风险框架，以管理将Excel用作分析IDE所带来的日益增长的风险。

Result: Excel已具备数据库、OLAP引擎、统计编程语言、第三方库、动态图表和实时数据连接器等多种功能，并可通过低代码框架进行控制，本质上是一个分析IDE。

Conclusion: 鉴于Excel已转变为一个分析IDE，有必要扩展现有的风险管理框架，以应对其带来的日益增长的风险。

Abstract: Since the inception of VisiCalc over four decades ago, spreadsheets have undergone a gradual transformation, evolving from simple ledger automation tools to the current state of Excel, which can be described as an Integrated Development Environment (IDE) for analytics. The slow evolution of Excel from an automation tool for ledgers to an IDE for analytics explains why many people have not noticed that Excel includes a fully functional database, an OLAP Engine, multiple statistical programming languages, multiple third-party software libraries, dynamic charts, and real time data connectors. The simplicity of accessing these multiple tools is a low-code framework controlled from the Excel tool that is effectively an IDE. Once we acknowledge Excel's shift from a desk top application to an IDE for analytics, the importance of establishing a comprehensive risk framework for managing this distinctive development environment becomes clear. In this paper we will explain how the current risk framework for spreadsheets needs to be expanded to manage the growing risks of using Excel as an IDE for analytics.

</details>
