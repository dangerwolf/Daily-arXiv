<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations](https://arxiv.org/abs/2510.19864)
*Amila Indika,Igor Molybog*

Main category: cs.SE

TL;DR: 该论文提出了电子表格操作文档（SOD）任务，旨在从电子表格操作中生成人类可读的解释。


<details>
  <summary>Details</summary>
Motivation: 缺乏系统的电子表格文档记录方法阻碍了自动化、协作和知识转移，从而可能导致关键机构知识的丢失。

Method: 创建了一个包含111个电子表格操作代码片段及其对应的自然语言摘要的基准数据集，并使用BLEU、GLEU、ROUGE-L和METEOR指标评估了五个LLM（GPT-4o, GPT-4o-mini, LLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B）。

Result: 实验结果表明，LLM可以生成准确的电子表格文档。

Conclusion: SOD是提高电子表格的可重复性、可维护性和协作工作流程的可行前提步骤，但仍存在需要解决的挑战。

Abstract: Numerous knowledge workers utilize spreadsheets in business, accounting, and finance. However, a lack of systematic documentation methods for spreadsheets hinders automation, collaboration, and knowledge transfer, which risks the loss of crucial institutional knowledge. This paper introduces Spreadsheet Operations Documentation (SOD), an AI task that involves generating human-readable explanations from spreadsheet operations. Many previous studies have utilized Large Language Models (LLMs) for generating spreadsheet manipulation code; however, translating that code into natural language for SOD is a less-explored area. To address this, we present a benchmark of 111 spreadsheet manipulation code snippets, each paired with a corresponding natural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini, LLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and METEOR metrics. Our findings suggest that LLMs can generate accurate spreadsheet documentation, making SOD a feasible prerequisite step toward enhancing reproducibility, maintainability, and collaborative workflows in spreadsheets, although there are challenges that need to be addressed.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [IIKL: Isometric Immersion Kernel Learning with Riemannian Manifold for Geometric Preservation](https://arxiv.org/abs/2505.06288)
*Zihao Chen,Wenyong Wang,Jiachen Yang,Yu Xiang*

Main category: cs.LG

TL;DR: This paper introduces Isometric Immersion Kernel Learning (IIKL) to preserve geometric properties during representation learning of non-Euclidean data.


<details>
  <summary>Details</summary>
Motivation: Previous methods lose critical geometric information by mapping non-Euclidean data into Euclidean space.

Method: IIKL builds a Riemannian manifold and isometrically induces a Riemannian metric, proving isometric immersion is equivalent to the kernel function in the tangent bundle.

Result: IIKL preserves intrinsic geometric representation, reducing inner product invariant loss by 90%, improving reconstruction accuracy by 40%, and reducing error for geometric metrics by 90%.

Conclusion: The learned Riemannian manifold and metric preserve the intrinsic geometric representation of data, significantly improving downstream task accuracy.

Abstract: Geometric representation learning in preserving the intrinsic geometric and topological properties for discrete non-Euclidean data is crucial in scientific applications. Previous research generally mapped non-Euclidean discrete data into Euclidean space during representation learning, which may lead to the loss of some critical geometric information. In this paper, we propose a novel Isometric Immersion Kernel Learning (IIKL) method to build Riemannian manifold and isometrically induce Riemannian metric from discrete non-Euclidean data. We prove that Isometric immersion is equivalent to the kernel function in the tangent bundle on the manifold, which explicitly guarantees the invariance of the inner product between vectors in the arbitrary tangent space throughout the learning process, thus maintaining the geometric structure of the original data. Moreover, a novel parameterized learning model based on IIKL is introduced, and an alternating training method for this model is derived using Maximum Likelihood Estimation (MLE), ensuring efficient convergence. Experimental results proved that using the learned Riemannian manifold and its metric, our model preserved the intrinsic geometric representation of data in both 3D and high-dimensional datasets successfully, and significantly improved the accuracy of downstream tasks, such as data reconstruction and classification. It is showed that our method could reduce the inner product invariant loss by more than 90% compared to state-of-the-art (SOTA) methods, also achieved an average 40% improvement in downstream reconstruction accuracy and a 90% reduction in error for geometric metrics involving isometric and conformal.

</details>
