<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution](https://arxiv.org/abs/2510.25726)
*Junlong Li,Wenshuo Zhao,Jian Zhao,Weihao Zeng,Haoze Wu,Xiaochen Wang,Rui Ge,Yuxuan Cao,Yuzhen Huang,Wei Liu,Junteng Liu,Zhaochen Su,Yiyang Guo,Fan Zhou,Lueyang Zhang,Juan Michelini,Xingyao Wang,Xiang Yue,Shuyan Zhou,Graham Neubig,Junxian He*

Main category: cs.CL

TL;DR: Toolathlon是一个包含32个应用程序和604个工具的基准测试，用于评估语言代理处理复杂、长时限工作流的能力，其中包含来自真实软件的现实环境状态和108个任务，测试表明现有模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有的语言代理基准测试通常过于狭窄或简化，无法充分评估代理在真实世界中的表现，尤其是在处理需要跨越多个应用程序和长时间运行的复杂任务时。

Method: 引入Toolathlon基准测试，该测试包含32个应用程序、604个工具、来自真实软件的环境状态以及108个可验证的任务，平均每个任务需要约20个回合。

Result: 现有的最先进模型在Toolathlon基准测试中的表现不佳，表现最好的Claude-4.5-Sonnet成功率仅为38.6%，而开源模型DeepSeek-V3.2-Exp的成功率为20.1%。

Conclusion: Toolathlon基准测试旨在推动更强大、能够执行真实世界长时限任务的语言代理的发展。

Abstract: Real-world language agents must handle complex, multi-step workflows across diverse Apps. For instance, an agent may manage emails by coordinating with calendars and file systems, or monitor a production database to detect anomalies and generate reports following an operating manual. However, existing language agent benchmarks often focus on narrow domains or simplified tasks that lack the diversity, realism, and long-horizon complexity required to evaluate agents' real-world performance. To address this gap, we introduce the Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering diverse Apps and tools, realistic environment setup, and reliable execution-based evaluation. Toolathlon spans 32 software applications and 604 tools, ranging from everyday platforms such as Google Calendar and Notion to professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools are based on a high-quality set of Model Context Protocol (MCP) servers that we may have revised or implemented ourselves. Unlike prior works, which primarily ensure functional realism but offer limited environment state diversity, we provide realistic initial environment states from real software, such as Canvas courses with dozens of students or real financial spreadsheets. This benchmark includes 108 manually sourced or crafted tasks in total, requiring interacting with multiple Apps over around 20 turns on average to complete. Each task is strictly verifiable through dedicated evaluation scripts. Comprehensive evaluation of SOTA models highlights their significant shortcomings: the best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate with 20.2 tool calling turns on average, while the top open-weights model DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development of more capable language agents for real-world, long-horizon task execution.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Benchmark Dataset Generation and Evaluation for Excel Formula Repair with LLMs](https://arxiv.org/abs/2508.11715)
*Ananya Singha,Harshita Sahijwani,Walt Williams,Emmanuel Aboah Boateng,Nick Hausman,Miguel Di Luca,Keegan Choudhury,Chaya Binet,Vu Le,Tianwei Chen,Oryan Rokeah Chen,Sulaiman Vesal,Sadid Hasan*

Main category: cs.SE

TL;DR: 该研究提出了一个用于 Excel 公式修复的数据集构建新方法，并构建了一个包含618个样本的数据集，同时提出了一个基于LLM的公式修复基线技术。


<details>
  <summary>Details</summary>
Motivation: 现有的Excel公式错误修复研究缺乏高质量、全面的数据集进行训练和评估，尤其是在处理运行时错误方面。

Method: 1. 提出一个数据生成流程，利用在线论坛的种子样本，结合LLM的少样本提示（few-shot prompting）进行扩展。
2. 采用LLM-as-a-Judge验证框架和基于执行的检查来确保生成数据的正确性和语义保真度。
3. 提出一个上下文感知的基线技术，利用LLM来考虑有问题的公式和相关的电子表格上下文。
4. 使用执行为基础的指标在生成的数据集上评估了多种LLM。

Result: 1. 构建了一个包含618个高质量样本的Excel公式修复基准数据集，涵盖了常见的运行时错误。
2. 评估了GPT-4o、GPT-4.1、Phi-3和Mistral等LLM在数据集上的性能。
3. 通过手动标注和分析展示了数据集的质量，并提供了关于错误和函数分布的见解。
4. 证明了所提出的生成方法具有高度可扩展性，并可应用于其他低资源编程语言的代码修复任务。

Conclusion: 该研究成功地解决了Excel公式修复领域的数据集稀缺问题，构建了一个高质量的基准数据集，并提出了一个有效的LLM驱动的公式修复方法。所提出的数据集生成方法具有广泛的应用前景。

Abstract: Excel is a pervasive yet often complex tool, particularly for novice users, where runtime errors arising from logical mistakes or misinterpretations of functions pose a significant challenge. While large language models (LLMs) offer promising assistance by explaining formula errors, the automated correction of these semantic runtime errors remains an open problem. A primary challenge to advancing models for such scenarios is the severe lack of high-quality, comprehensive datasets for training and rigorous evaluation. This paper addresses this gap by introducing a novel approach for constructing a benchmark dataset specifically designed for Excel formula repair. We propose a data generation pipeline, which leverages a small set of curated seed samples from online forums to synthetically expand the dataset. Our pipeline integrates few-shot prompting with LLMs and employs a robust \textit{LLM-as-a-Judge} validation framework, combined with execution-based checks to ensure the correctness and semantic fidelity of the generated data. This process produced a benchmark dataset of 618 high-quality samples, covering common runtime errors. Furthermore, we propose a context-aware baseline technique for Excel formula repair that utilizes LLMs to leverage both the faulty formula, and relevant spreadsheet context. We evaluate the performance of various LLMs (GPT-4o, GPT-4.1, Phi-3, Mistral) on our newly generated benchmark using execution-based metrics. Our analysis demonstrates the dataset's quality through manual annotation and provides insights into error and function distributions. The proposed generation methodology is highly scalable and can be readily adapted to create evaluation benchmarks for similar code repair tasks in other low-resource programming languages.

</details>
