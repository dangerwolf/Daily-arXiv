<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution](https://arxiv.org/abs/2510.25726)
*Junlong Li,Wenshuo Zhao,Jian Zhao,Weihao Zeng,Haoze Wu,Xiaochen Wang,Rui Ge,Yuxuan Cao,Yuzhen Huang,Wei Liu,Junteng Liu,Zhaochen Su,Yiyang Guo,Fan Zhou,Lueyang Zhang,Juan Michelini,Xingyao Wang,Xiang Yue,Shuyan Zhou,Graham Neubig,Junxian He*

Main category: cs.CL

TL;DR: Toolathlon是一个新的基准测试，旨在评估语言智能体在真实世界中处理复杂、多步骤、跨应用工作流的能力。它包含32个应用、604个工具、真实的初始环境状态和108个多轮任务。当前最先进的模型在此基准上表现出显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有的语言智能体基准测试通常侧重于狭窄领域或简化任务，缺乏评估智能体真实世界性能所需的任务多样性、真实性和长周期复杂性。

Method: 我们引入了Tool Decathlon（Toolathlon），这是一个针对语言智能体的基准测试，具有以下特点：提供多样化的应用程序和工具（涵盖32种软件和604种工具），真实的初始环境设置（来自真实软件的初始状态），以及可靠的基于执行的评估（包含108个手动来源或精心设计的任务，平均需要大约20轮交互才能完成，并通过专用评估脚本严格验证）。

Result: 对当前最先进模型的全面评估显示出它们的显著不足：表现最好的模型Claude-4.5-Sonnet仅取得了38.6%的成功率，平均调用工具20.2次；而顶级的开源模型DeepSeek-V3.2-Exp则达到了20.1%。

Conclusion: 我们期望Toolathlon能够推动开发出更强大的语言智能体，以应对真实世界中的长周期任务执行。

Abstract: Real-world language agents must handle complex, multi-step workflows across diverse Apps. For instance, an agent may manage emails by coordinating with calendars and file systems, or monitor a production database to detect anomalies and generate reports following an operating manual. However, existing language agent benchmarks often focus on narrow domains or simplified tasks that lack the diversity, realism, and long-horizon complexity required to evaluate agents' real-world performance. To address this gap, we introduce the Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering diverse Apps and tools, realistic environment setup, and reliable execution-based evaluation. Toolathlon spans 32 software applications and 604 tools, ranging from everyday platforms such as Google Calendar and Notion to professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools are based on a high-quality set of Model Context Protocol (MCP) servers that we may have revised or implemented ourselves. Unlike prior works, which primarily ensure functional realism but offer limited environment state diversity, we provide realistic initial environment states from real software, such as Canvas courses with dozens of students or real financial spreadsheets. This benchmark includes 108 manually sourced or crafted tasks in total, requiring interacting with multiple Apps over around 20 turns on average to complete. Each task is strictly verifiable through dedicated evaluation scripts. Comprehensive evaluation of SOTA models highlights their significant shortcomings: the best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate with 20.2 tool calling turns on average, while the top open-weights model DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development of more capable language agents for real-world, long-horizon task execution.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Benchmark Dataset Generation and Evaluation for Excel Formula Repair with LLMs](https://arxiv.org/abs/2508.11715)
*Ananya Singha,Harshita Sahijwani,Walt Williams,Emmanuel Aboah Boateng,Nick Hausman,Miguel Di Luca,Keegan Choudhury,Chaya Binet,Vu Le,Tianwei Chen,Oryan Rokeah Chen,Sulaiman Vesal,Sadid Hasan*

Main category: cs.SE

TL;DR: 该论文提出了一种构建高质量 Excel 公式修复基准数据集的新方法，并提出了一个上下文感知的基线修复技术，评估了各种大型语言模型。


<details>
  <summary>Details</summary>
Motivation: Excel 新手用户常因逻辑错误或功能误解而遇到运行时错误，尽管大型语言模型可以解释错误，但自动修复这些语义运行时错误仍是一个开放问题，主要原因是缺乏高质量的训练和评估数据集。

Method: 论文提出了一种新颖的数据生成流程，通过利用少量在线论坛的种子样本，结合大型语言模型的少样本提示和“LLM-as-a-Judge”验证框架，以及基于执行的检查来合成扩展数据集。此外，还提出了一种利用大型语言模型和相关电子表格上下文的上下文感知 Excel 公式修复基线技术。

Result: 生成了一个包含 618 个高质量样本的基准数据集，涵盖了常见的运行时错误。通过基于执行的指标评估了 GPT-4o、GPT-4.1、Phi-3、Mistral 等多种大型语言模型在该数据集上的性能。分析通过手动标注展示了数据集的质量，并提供了错误和函数分布的见解。

Conclusion: 所提出的生成方法具有高度可扩展性，可以很容易地适用于为其他低资源编程语言中类似的 代码修复任务创建评估基准。

Abstract: Excel is a pervasive yet often complex tool, particularly for novice users, where runtime errors arising from logical mistakes or misinterpretations of functions pose a significant challenge. While large language models (LLMs) offer promising assistance by explaining formula errors, the automated correction of these semantic runtime errors remains an open problem. A primary challenge to advancing models for such scenarios is the severe lack of high-quality, comprehensive datasets for training and rigorous evaluation. This paper addresses this gap by introducing a novel approach for constructing a benchmark dataset specifically designed for Excel formula repair. We propose a data generation pipeline, which leverages a small set of curated seed samples from online forums to synthetically expand the dataset. Our pipeline integrates few-shot prompting with LLMs and employs a robust \textit{LLM-as-a-Judge} validation framework, combined with execution-based checks to ensure the correctness and semantic fidelity of the generated data. This process produced a benchmark dataset of 618 high-quality samples, covering common runtime errors. Furthermore, we propose a context-aware baseline technique for Excel formula repair that utilizes LLMs to leverage both the faulty formula, and relevant spreadsheet context. We evaluate the performance of various LLMs (GPT-4o, GPT-4.1, Phi-3, Mistral) on our newly generated benchmark using execution-based metrics. Our analysis demonstrates the dataset's quality through manual annotation and provides insights into error and function distributions. The proposed generation methodology is highly scalable and can be readily adapted to create evaluation benchmarks for similar code repair tasks in other low-resource programming languages.

</details>
