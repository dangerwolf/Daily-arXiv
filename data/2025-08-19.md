<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Benchmark Dataset Generation and Evaluation for Excel Formula Repair with LLMs](https://arxiv.org/abs/2508.11715)
*Ananya Singha,Harshita Sahijwani,Walt Williams,Emmanuel Aboah Boateng,Nick Hausman,Miguel Di Luca,Keegan Choudhury,Chaya Binet,Vu Le,Tianwei Chen,Oryan Rokeah Chen,Sulaiman Vesal,Sadid Hasan*

Main category: cs.SE

TL;DR: 针对Excel公式语义运行时错误缺乏高质量数据集的问题，本文提出了一种利用LLMs和验证框架来生成大型Excel公式修复基准数据集的方法，并在此基础上评估了多种LLMs的性能，为未来代码修复任务提供可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: Excel新手用户在处理公式时常遇到运行时错误，现有的大型语言模型（LLMs）虽能解释错误，但自动化修复语义运行时错误仍是一个开放问题。主要挑战在于缺乏高质量、全面的训练和评估数据集。

Method: 提出了一种构建Excel公式修复基准数据集的新方法，包括一个利用少量种子样本、LLM少样本提示、LLM作为法官验证框架和基于执行检查的数据生成流程。此外，还提出了一种利用LLM和电子表格上下文的上下文感知基线技术。

Result: 成功构建了一个包含618个高质量样本的基准数据集，涵盖常见的运行时错误。通过手动标注和执行指标，评估了数据集的质量以及不同LLMs（GPT-4o, GPT-4.1, Phi-3, Mistral）在该数据集上的性能，并提供了错误和函数分布的见解。

Conclusion: 本文提出的数据生成方法具有高度可扩展性，可轻松适应其他低资源编程语言中类似代码修复任务的评估基准。

Abstract: Excel is a pervasive yet often complex tool, particularly for novice users,
where runtime errors arising from logical mistakes or misinterpretations of
functions pose a significant challenge. While large language models (LLMs)
offer promising assistance by explaining formula errors, the automated
correction of these semantic runtime errors remains an open problem. A primary
challenge to advancing models for such scenarios is the severe lack of
high-quality, comprehensive datasets for training and rigorous evaluation. This
paper addresses this gap by introducing a novel approach for constructing a
benchmark dataset specifically designed for Excel formula repair. We propose a
data generation pipeline, which leverages a small set of curated seed samples
from online forums to synthetically expand the dataset. Our pipeline integrates
few-shot prompting with LLMs and employs a robust \textit{LLM-as-a-Judge}
validation framework, combined with execution-based checks to ensure the
correctness and semantic fidelity of the generated data. This process produced
a benchmark dataset of 618 high-quality samples, covering common runtime
errors. Furthermore, we propose a context-aware baseline technique for Excel
formula repair that utilizes LLMs to leverage both the faulty formula, and
relevant spreadsheet context. We evaluate the performance of various LLMs
(GPT-4o, GPT-4.1, Phi-3, Mistral) on our newly generated benchmark using
execution-based metrics. Our analysis demonstrates the dataset's quality
through manual annotation and provides insights into error and function
distributions. The proposed generation methodology is highly scalable and can
be readily adapted to create evaluation benchmarks for similar code repair
tasks in other low-resource programming languages.

</details>
