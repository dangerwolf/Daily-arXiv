<div id=toc></div>

# Table of Contents

- [cs.DL](#cs.DL) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [1] [BookReconciler: An Open-Source Tool for Metadata Enrichment and Work-Level Clustering](https://arxiv.org/abs/2512.10165)
*Matt Miller,Dan Sinykin,Melanie Walsh*

Main category: cs.DL

TL;DR: BookReconciler是一个开源工具，用于增强和聚类图书数据，自动添加ISBN等标识符，并将同一作品的不同版本（如翻译或版本）聚类，从而便于大规模分析。它作为OpenRefine的扩展，连接到各大书目服务，并优先考虑用户判断，允许用户手动评估匹配并定义作品的范围。该工具在评估中对美国图书表现出近乎完美的准确性，但对全球文学作品的表现较差，这反映了非英语和全球文学书目基础设施的结构性不足。总体而言，BookReconciler支持跨领域和应用的目录数据复用。


<details>
  <summary>Details</summary>
Motivation: 提供一个开源工具，用于增强和聚类图书数据，使用户能够处理仅包含书籍标题和作者等少量元数据的电子表格，并自动进行数据丰富（例如，添加ISBN）和聚类（例如，将同一作品的不同版本组合在一起），从而便于大规模分析和跨域数据重用。

Method: BookReconciler作为一个OpenRefine的扩展，连接到包括国会图书馆、VIAF、OCLC、HathiTrust、Google图书和Wikidata在内的主要书目服务。它通过交互式界面，允许用户手动评估匹配项并定义作品的范围（例如，是否包含翻译），优先考虑人类判断，然后自动添加持久标识符（如ISBN）并聚类相关的表达和体现（例如，同一作品的不同翻译或版本）。

Result: 在对美国获奖图书和当代世界小说数据集进行的评估中，BookReconciler在处理美国图书时达到了近乎完美的准确性，但在处理全球文学作品时准确性较低，这表明了非英语和全球文学书目基础设施存在结构性弱点。

Conclusion: BookReconciler是一个有价值的工具，可以增强和聚类图书数据，从而促进跨域和应用程序的目录数据重用，并为数字图书馆和数字人文领域的持续工作做出贡献。然而，该工具在处理全球文学作品方面表现出的准确性较低，凸显了改进非英语和全球文学书目基础设施的必要性。

Abstract: We present BookReconciler, an open-source tool for enhancing and clustering book data. BookReconciler allows users to take spreadsheets with minimal metadata, such as book title and author, and automatically 1) add authoritative, persistent identifiers like ISBNs 2) and cluster related Expressions and Manifestations of the same Work, e.g., different translations or editions. This enhancement makes it easier to combine related collections and analyze books at scale. The tool is currently designed as an extension for OpenRefine -- a popular software application -- and connects to major bibliographic services including the Library of Congress, VIAF, OCLC, HathiTrust, Google Books, and Wikidata. Our approach prioritizes human judgment. Through an interactive interface, users can manually evaluate matches and define the contours of a Work (e.g., to include translations or not). We evaluate reconciliation performance on datasets of U.S. prize-winning books and contemporary world fiction. BookReconciler achieves near-perfect accuracy for U.S. works but lower performance for global texts, reflecting structural weaknesses in bibliographic infrastructures for non-English and global literature. Overall, BookReconciler supports the reuse of bibliographic data across domains and applications, contributing to ongoing work in digital libraries and digital humanities.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [2] [The Invisible Mentor: Inferring User Actions from Screen Recordings to Recommend Better Workflows](https://arxiv.org/abs/2509.26557)
*Litao Yan,Andrew Head,Ken Milne,Vu Le,Sumit Gulwani,Chris Parnin,Emerson Murphy-Hill*

Main category: cs.HC

TL;DR: InvisibleMentor通过分析屏幕录像，在用户不知情的情况下，识别并提供更优的Excel操作建议。


<details>
  <summary>Details</summary>
Motivation: 现有AI助手需要用户主动描述问题，不够便捷，而用户常常难以发现更高效的操作方法。

Method: 通过两阶段模型：1. 视觉-语言模型重建操作和上下文；2. 语言模型生成结构化建议，直接分析屏幕录像而非依赖日志或API。

Result: 准确识别低效工作流，用户认为其建议比基于提示的助手更具可操作性、更个性化，并有助于学习和改进。

Conclusion: InvisibleMentor能够有效识别并提出改进建议，提升用户在Excel等工具中的操作效率和学习效果。

Abstract: Many users struggle to notice when a more efficient workflow exists in feature-rich tools like Excel. Existing AI assistants offer help only after users describe their goals or problems, which can be effortful and imprecise. We present InvisibleMentor, a system that turns screen recordings of task completion into vision-grounded reflections on tasks. It detects issues such as repetitive edits and recommends more efficient alternatives based on observed behavior. Unlike prior systems that rely on logs, APIs, or user prompts, InvisibleMentor operates directly on screen recordings. It uses a two-stage pipeline: a vision-language model reconstructs actions and context, and a language model generates structured, high-fidelity suggestions. In evaluation, InvisibleMentor accurately identified inefficient workflows, and participants found its suggestions more actionable, tailored, and more helpful for learning and improvement compared to a prompt-based spreadsheet assistant.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [AI Playing Business Games: Benchmarking Large Language Models on Managerial Decision-Making in Dynamic Simulations](https://arxiv.org/abs/2509.26331)
*Berdymyrat Ovezmyradov*

Main category: cs.AI

TL;DR: 研究提出了一个新的基于商业游戏的基准测试框架，用于评估大型语言模型（LLMs）在长期战略商业决策方面的能力，并测试了五种主流LLMs。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在长期战略决策方面存在不足，本研究旨在填补这一空白，并为LLM在商业领域的应用提供评估工具。

Method: 使用一个可复现的、开源的管理模拟器，让五个主流LLMs（Gemini、ChatGPT、Meta AI、Mistral AI、Grok）为一个模拟的零售公司做出为期十二个月的战略决策（定价、订单量、营销预算等），并收集定量指标（利润、收入、市场份额）和其他关键绩效指标。

Result: 评估了LLMs在战略一致性、市场适应性和决策理由方面的表现，并超越了简单的绩效指标。

Conclusion: 该研究提出了一个用于LLM长期决策能力评估的新颖框架，并对五种主流LLMs进行了实证分析，为该领域的研究和应用提供了基础。

Abstract: The rapid advancement of LLMs sparked significant interest in their potential to augment or automate managerial functions. One of the most recent trends in AI benchmarking is performance of Large Language Models (LLMs) over longer time horizons. While LLMs excel at tasks involving natural language and pattern recognition, their capabilities in multi-step, strategic business decision-making remain largely unexplored. Few studies demonstrated how results can be different from benchmarks in short-term tasks, as Vending-Bench revealed. Meanwhile, there is a shortage of alternative benchmarks for long-term coherence. This research analyses a novel benchmark using a business game for the decision making in business. The research contributes to the recent literature on AI by proposing a reproducible, open-access management simulator to the research community for LLM benchmarking. This novel framework is used for evaluating the performance of five leading LLMs available in free online interface: Gemini, ChatGPT, Meta AI, Mistral AI, and Grok. LLM makes decisions for a simulated retail company. A dynamic, month-by-month management simulation provides transparently in spreadsheet model as experimental environment. In each of twelve months, the LLMs are provided with a structured prompt containing a full business report from the previous period and are tasked with making key strategic decisions: pricing, order size, marketing budget, hiring, dismissal, loans, training expense, R&D expense, sales forecast, income forecast The methodology is designed to compare the LLMs on quantitative metrics: profit, revenue, and market share, and other KPIs. LLM decisions are analyzed in their strategic coherence, adaptability to market changes, and the rationale provided for their decisions. This approach allows to move beyond simple performance metrics for assessment of the long-term decision-making.

</details>
