<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 5]
- [cs.CL](#cs.CL) [Total: 13]
- [cs.IR](#cs.IR) [Total: 7]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.PL](#cs.PL) [Total: 9]
- [cs.DL](#cs.DL) [Total: 5]
- [cs.CY](#cs.CY) [Total: 12]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.SE](#cs.SE) [Total: 45]
- [cs.HC](#cs.HC) [Total: 32]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.DB](#cs.DB) [Total: 27]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.AI](#cs.AI) [Total: 16]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 8]
- [stat.OT](#stat.OT) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.OH](#cs.OH) [Total: 1]
- [math.HO](#math.HO) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.MS](#cs.MS) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Image deidentification in the XNAT ecosystem: use cases and solutions](https://arxiv.org/abs/2504.20657)
*Alex Michie,Simon J Doran*

Main category: cs.CV

TL;DR: XNAT 结合机器学习工具，实现了 DICOM 数据的去标识化，并在 MIDI-B 挑战赛中取得了显著的性能提升，但地址信息的处理和图像内嵌信息的去除仍需改进。


<details>
  <summary>Details</summary>
Motivation: 为了参与医学图像去标识化基准（MIDI-B）挑战赛，需要对现有的本地去标识化方法进行适应和改进，以满足数据隐私和安全的要求。

Method: 本文详细介绍了一个使用 XNAT 设施和 XNAT 生态系统中的独立工具对 DICOM 数据进行去标识化的工作流程。该方法基于规则，并结合了机器学习模型。

Result: 在 MIDI-B 挑战赛的测试阶段，初始得分为 97.91%，由于与 Synapse 平台的技能不兼容，无法在验证阶段获得反馈。通过额外的差异报告和 MIDI-B 连续基准测试设施，分数显著提高到 99.61%。基于规则的方法可以移除所有姓名相关信息，但地址信息移除不完全。机器学习方法在移除地址方面部分成功，但可能过度处理其他自由文本数据，导致总体性能下降至 99.54%。估计真正的去标识化失败率为 0.19%。

Conclusion: XNAT 的去标识化工作流程可以成功移除测试语料库中所有与姓名相关的信息，但处理地址数据时存在不足。基于机器学习的方法在处理地址方面取得部分成功，但可能过度处理其他类型的自由文本数据。未来的工作将集中于改进地址识别能力和去除图像像素中烧录的可识别数据。

Abstract: XNAT is a server-based data management platform widely used in academia for
curating large databases of DICOM images for research projects. We describe in
detail a deidentification workflow for DICOM data using facilities in XNAT,
together with independent tools in the XNAT "ecosystem". We list different
contexts in which deidentification might be needed, based on our prior
experience. The starting point for participation in the Medical Image
De-Identification Benchmark (MIDI-B) challenge was a set of pre-existing local
methodologies, which were adapted during the validation phase of the challenge.
Our result in the test phase was 97.91\%, considerably lower than our peers,
due largely to an arcane technical incompatibility of our methodology with the
challenge's Synapse platform, which prevented us receiving feedback during the
validation phase. Post-submission, additional discrepancy reports from the
organisers and via the MIDI-B Continuous Benchmarking facility, enabled us to
improve this score significantly to 99.61\%. An entirely rule-based approach
was shown to be capable of removing all name-related information in the test
corpus, but exhibited failures in dealing fully with address data. Initial
experiments using published machine-learning models to remove addresses were
partially successful but showed the models to be "over-aggressive" on other
types of free-text data, leading to a slight overall degradation in performance
to 99.54\%. Future development will therefore focus on improving
address-recognition capabilities, but also on better removal of identifiable
data burned into the image pixels. Several technical aspects relating to the
"answer key" are still under discussion with the challenge organisers, but we
estimate that our percentage of genuine deidentification failures on the MIDI-B
test corpus currently stands at 0.19\%. (Abridged from original for arXiv
submission)

</details>


### [2] [Vision Language Models for Spreadsheet Understanding: Challenges and Opportunities](https://arxiv.org/abs/2405.16234)
*Shiyu Xia,Junyu Xiong,Haoyu Dong,Jianbo Zhao,Yuzhang Tian,Mengyu Zhou,Yeye He,Shi Han,Dongmei Zhang*

Main category: cs.CV

TL;DR: 本研究旨在评估和提升视觉语言模型（VLMs）在电子表格理解方面的能力。研究者们设计了三种自监督挑战和多种电子表格到图像的转换设置，以测试VLMs的OCR、空间感知和格式识别能力。结果表明，VLMs在OCR方面有潜力，但在处理单元格遗漏、错位及空间格式识别方面仍有不足，需要进一步研究。


<details>
  <summary>Details</summary>
Motivation: 为了全面评估VLMs在电子表格理解方面的能力，并为未来提升VLMs的电子表格数据理解能力提供研究方向。

Method: 提出三个自监督挑战（OCR、空间感知、视觉格式识别）及评估指标，利用电子表格表格检测任务评估VLMs的整体性能，并通过列宽调整、样式更改、地址增强等三种电子表格到图像的设置来深入探究VLMs，提出不同提示词变体来处理这些任务，并在边界检测中解码表格四边界的单元格值以利用VLMs的文本理解优势。

Result: VLMs在OCR方面表现出有潜力的能力，但在单元格遗漏和错位方面存在问题，空间感知和格式识别能力不足。

Conclusion: VLMs在处理电子表格方面展现出有潜力的OCR能力，但在单元格遗漏、错位以及空间和格式识别方面表现不佳，需要进一步研究以提升其对电子表格数据的理解能力。

Abstract: This paper explores capabilities of Vision Language Models on spreadsheet
comprehension. We propose three self-supervised challenges with corresponding
evaluation metrics to comprehensively evaluate VLMs on Optical Character
Recognition (OCR), spatial perception, and visual format recognition.
Additionally, we utilize the spreadsheet table detection task to assess the
overall performance of VLMs by integrating these challenges. To probe VLMs more
finely, we propose three spreadsheet-to-image settings: column width
adjustment, style change, and address augmentation. We propose variants of
prompts to address the above tasks in different settings. Notably, to leverage
the strengths of VLMs in understanding text rather than two-dimensional
positioning, we propose to decode cell values on the four boundaries of the
table in spreadsheet boundary detection. Our findings reveal that VLMs
demonstrate promising OCR capabilities but produce unsatisfactory results due
to cell omission and misalignment, and they notably exhibit insufficient
spatial and format recognition skills, motivating future work to enhance VLMs'
spreadsheet data comprehension capabilities using our methods to generate
extensive spreadsheet-image pairs in various settings.

</details>


### [3] [High-Throughput Phenotyping using Computer Vision and Machine Learning](https://arxiv.org/abs/2407.06354)
*Vivaan Singhvi,Langalibalele Lunga,Pragya Nidhi,Chris Keum,Varrun Prakash*

Main category: cs.CV

TL;DR: 通过OCR和机器学习技术提高植物表型分析效率，但叶片大小和表型相关性评估因数据缺失受限。


<details>
  <summary>Details</summary>
Motivation: 为了提高高通量植物表型分析的效率和准确性，本研究旨在利用机器学习和计算机视觉技术，结合包含详细标签信息的真实世界数据集，克服现有研究中数据标签缺失的局限性。

Method: 本研究结合了光学字符识别（OCR）技术用于读取植物叶片上的标签信息，利用图像分割技术和机器学习算法进行形态学分类，并通过机器学习模型预测植物处理（对照或干旱）情况。此外，还分析了编码的EXIF标签以提取叶片大小信息并探究表型相关性。

Result: OCR模型在非空文本提取方面达到了94.31%的准确率，能够准确地将标签信息录入电子表格。形态学分类模型（识别叶片形状、颜色和褐斑程度）的平均准确率为62.82%，而预测植物处理情况的模型准确率为60.08%。研究还发现，EXIF标签中缺失关键信息，阻碍了对叶片大小的评估以及表型与条件之间相关性的分析。

Conclusion: 该研究表明，结合OCR、图像分割和机器学习的综合方法在植物表型分析方面具有潜力，但仍需改进以全面评估叶片大小和表型相关性。

Abstract: High-throughput phenotyping refers to the non-destructive and efficient
evaluation of plant phenotypes. In recent years, it has been coupled with
machine learning in order to improve the process of phenotyping plants by
increasing efficiency in handling large datasets and developing methods for the
extraction of specific traits. Previous studies have developed methods to
advance these challenges through the application of deep neural networks in
tandem with automated cameras; however, the datasets being studied often
excluded physical labels. In this study, we used a dataset provided by Oak
Ridge National Laboratory with 1,672 images of Populus Trichocarpa with white
labels displaying treatment (control or drought), block, row, position, and
genotype. Optical character recognition (OCR) was used to read these labels on
the plants, image segmentation techniques in conjunction with machine learning
algorithms were used for morphological classifications, machine learning models
were used to predict treatment based on those classifications, and analyzed
encoded EXIF tags were used for the purpose of finding leaf size and
correlations between phenotypes. We found that our OCR model had an accuracy of
94.31% for non-null text extractions, allowing for the information to be
accurately placed in a spreadsheet. Our classification models identified leaf
shape, color, and level of brown splotches with an average accuracy of 62.82%,
and plant treatment with an accuracy of 60.08%. Finally, we identified a few
crucial pieces of information absent from the EXIF tags that prevented the
assessment of the leaf size. There was also missing information that prevented
the assessment of correlations between phenotypes and conditions. However,
future studies could improve upon this to allow for the assessment of these
features.

</details>


### [4] [Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language](https://arxiv.org/abs/2204.00598)
*Andy Zeng,Maria Attarian,Brian Ichter,Krzysztof Choromanski,Adrian Wong,Stefan Welker,Federico Tombari,Aveek Purohit,Michael Ryoo,Vikas Sindhwani,Johnny Lee,Vincent Vanhoucke,Pete Florence*

Main category: cs.CV

TL;DR: Socratic Models (SMs) combine different pretrained models without finetuning to gain new multimodal abilities, achieving strong results in existing tasks and enabling new applications like egocentric video analysis and robot assistance.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the observation that large pretrained models, trained on different domains of data, store diverse forms of commonsense knowledge. The paper aims to leverage this diversity by enabling these models to exchange information and collaborate.

Method: The paper proposes Socratic Models (SMs), a modular framework that composes multiple pretrained models (e.g., visual-language models and large language models) using zero-shot, multimodal-informed prompting. This framework allows models to exchange information and acquire new multimodal capabilities without the need for finetuning.

Result: SMs demonstrate competitiveness with state-of-the-art models in zero-shot image captioning and video-to-text retrieval. Furthermore, they enable novel applications such as answering questions about egocentric video, facilitating multimodal assistive dialogue by interfacing with external APIs and databases, and advancing robot perception and planning.

Conclusion: Socratic Models (SMs) are a modular framework that composes multiple pretrained models zero-shot via multimodal-informed prompting, enabling them to exchange information and capture new multimodal capabilities without finetuning. SMs are competitive with state-of-the-art models in image captioning and video-to-text retrieval, and enable new applications like egocentric video question answering, multimodal assistive dialogue, and robot perception and planning.

Abstract: Large pretrained (e.g., "foundation") models exhibit distinct capabilities
depending on the domain of data they are trained on. While these domains are
generic, they may only barely overlap. For example, visual-language models
(VLMs) are trained on Internet-scale image captions, but large language models
(LMs) are further trained on Internet-scale text with no images (e.g.,
spreadsheets, SAT questions, code). As a result, these models store different
forms of commonsense knowledge across different domains. In this work, we show
that this diversity is symbiotic, and can be leveraged through Socratic Models
(SMs): a modular framework in which multiple pretrained models may be composed
zero-shot i.e., via multimodal-informed prompting, to exchange information with
each other and capture new multimodal capabilities, without requiring
finetuning. With minimal engineering, SMs are not only competitive with
state-of-the-art zero-shot image captioning and video-to-text retrieval, but
also enable new applications such as (i) answering free-form questions about
egocentric video, (ii) engaging in multimodal assistive dialogue with people
(e.g., for cooking recipes) by interfacing with external APIs and databases
(e.g., web search), and (iii) robot perception and planning.

</details>


### [5] [TableParser: Automatic Table Parsing with Weak Supervision from Spreadsheets](https://arxiv.org/abs/2201.01654)
*Susie Xi Rao,Johannes Rausch,Peter Egger,Ce Zhang*

Main category: cs.CV

TL;DR: 提出TableParser系统，用于解析PDF和扫描图像中的表格，并结合弱监督机制提升表格解析能力。


<details>
  <summary>Details</summary>
Motivation: 能够解析表格结构和提取表格内容在许多应用中都非常重要，尤其是在处理PDF、图像、电子表格和CSV等不同格式的表格数据时。

Method: 该研究提出了一种名为TableParser的系统，并结合了TableAnnotator和ExcelAnnotator，利用基于电子表格的弱监督机制和流水线来实现表格解析。

Result: 研究通过广泛的实验证明了域自适应在开发此类工具中的有效性，并创建了TableAnnotator和ExcelAnnotator等资源，以促进该领域的研究。

Conclusion: 该研究提出了一种名为TableParser的系统，能够高精度地解析原生PDF和扫描图像中的表格结构和内容。同时，研究还介绍了TableAnnotator和ExcelAnnotator，它们构成了一个基于电子表格的弱监督机制和表格解析流程。

Abstract: Tables have been an ever-existing structure to store data. There exist now
different approaches to store tabular data physically. PDFs, images,
spreadsheets, and CSVs are leading examples. Being able to parse table
structures and extract content bounded by these structures is of high
importance in many applications. In this paper, we devise TableParser, a system
capable of parsing tables in both native PDFs and scanned images with high
precision. We have conducted extensive experiments to show the efficacy of
domain adaptation in developing such a tool. Moreover, we create TableAnnotator
and ExcelAnnotator, which constitute a spreadsheet-based weak supervision
mechanism and a pipeline to enable table parsing. We share these resources with
the research community to facilitate further research in this interesting
direction.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [6] [An Empirical Study of Validating Synthetic Data for Formula Generation](https://arxiv.org/abs/2407.10657)
*Usneek Singh,José Cambronero,Sumit Gulwani,Aditya Kanade,Anirudh Khatry,Vu Le,Mukul Singh,Gust Verbruggen*

Main category: cs.CL

TL;DR: 电子表格公式的自然语言描述稀缺，本研究提出了一种验证方法，通过代理目标来评估自然语言描述的准确性，并证明了该方法能提升模型性能和解决问题的复杂度。


<details>
  <summary>Details</summary>
Motivation: 为了解决电子表格公式的自然语言描述资源稀缺的问题，从而提升预训练模型的性能和微调能力。

Method: 使用代理目标来验证模型生成的自然语言描述的准确性。

Result: 在两种开源和两种闭源模型上，验证都比原始数据带来了性能的提升。验证倾向于剔除更具挑战性的例子，但却能提高微调后模型能解决的问题的复杂度。

Conclusion: 验证可以提高模型性能，并且会增加模型在经过微调后可以解决的问题的复杂度。

Abstract: Large language models (LLMs) can be leveraged to help with writing formulas
in spreadsheets, but resources on these formulas are scarce, impacting both the
base performance of pre-trained models and limiting the ability to fine-tune
them. Given a corpus of formulas, we can use a(nother) model to generate
synthetic natural language utterances for fine-tuning. However, it is important
to validate whether the NL generated by the LLM is indeed accurate to be
beneficial for fine-tuning. In this paper, we provide empirical results on the
impact of validating these synthetic training examples with surrogate
objectives that evaluate the accuracy of the synthetic annotations. We
demonstrate that validation improves performance over raw data across four
models (2 open and 2 closed weight). Interestingly, we show that although
validation tends to prune more challenging examples, it increases the
complexity of problems that models can solve after being fine-tuned on
validated data.

</details>


### [7] [General Table Question Answering via Answer-Formula Joint Generation](https://arxiv.org/abs/2503.12345)
*Zhongyuan Wang,Richong Zhang,Zhijie Nie*

Main category: cs.CL

TL;DR: This paper introduces TabAF, a versatile TableQA framework that uses Spreadsheet Formulas for complex reasoning, achieving SOTA results on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing TableQA methods using LLMs lack versatility for specific question types or table structures. Spreadsheet Formula, a well-defined operation language, has not been thoroughly explored for TableQA.

Method: TabAF framework decodes answers and Formulas with a single LLM backbone. A new dataset, FormulaQA, was constructed from existing datasets for Formula-annotated TableQA.

Result: TabAF based on Llama3.1-70B achieves new state-of-the-art performance on WikiTableQuestion, HiTab, and TabFact.

Conclusion: The proposed TabAF framework, utilizing Spreadsheet Formula as the executable representation, achieves state-of-the-art performance on WikiTableQuestion, HiTab, and TabFact datasets, demonstrating versatility and generalization for TableQA.

Abstract: Advanced table question answering (TableQA) methods prompt large language
models (LLMs) to generate answer text, SQL query, Python code, or custom
operations, which impressively improve the complex reasoning problems in the
TableQA task. However, these methods lack the versatility to cope with specific
question types or table structures. In contrast, the Spreadsheet Formula, the
widely used and well-defined operation language for tabular data, has not been
thoroughly explored to solve TableQA. In this paper, we first attempt to use
the Formula as the executable representation for solving complex reasoning on
tables with different structures. Specifically, we construct
\texttt{FromulaQA}, a large Formula-annotated TableQA dataset from existing
datasets. In addition, we propose \texttt{TabAF}, a general table answering
framework to solve multiple types of tasks over multiple types of tables
simultaneously. Unlike existing methods, \texttt{TabAF} decodes answers and
Formulas with a single LLM backbone, demonstrating great versatility and
generalization. \texttt{TabAF} based on Llama3.1-70B achieves new
state-of-the-art performance on the WikiTableQuestion, HiTab, and TabFact.

</details>


### [8] [TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios](https://arxiv.org/abs/2403.19318)
*Xiaokang Zhang,Sijia Luo,Bohan Zhang,Zeyao Ma,Jing Zhang,Yang Li,Guanlin Li,Zijun Yao,Kangli Xu,Jinchang Zhou,Daniel Zhang-Li,Jifan Yu,Shu Zhao,Juanzi Li,Jie Tang*

Main category: cs.CL

TL;DR: TableLLM是一个80亿参数的大型语言模型，专门用于处理文档和电子表格中的表格数据，其训练方法提高了模型的推理和数据质量，并在评估中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决在现实办公场景中，LLM在处理文档或电子表格中的表格数据操作任务时遇到的挑战。

Method: 提出了一种包含推理过程扩展策略和交叉验证策略的远程监督训练方法，以提高LLM理解推理模式和数据质量的能力。

Result: TableLLM在针对文档和电子表格格式定制的基准测试中表现出优势，并已公开模型检查点、源代码、基准和Web应用程序。

Conclusion: TableLLM在处理文档和电子表格中的表格数据操作任务方面表现出色，优于现有的通用和表格数据专用LLM。

Abstract: We introduce TableLLM, a robust large language model (LLM) with 8 billion
parameters, purpose-built for proficiently handling tabular data manipulation
tasks, whether they are embedded within documents or spreadsheets, catering to
real-world office scenarios. We propose a distant supervision method for
training, which comprises a reasoning process extension strategy, aiding in
training LLMs to understand reasoning patterns more effectively as well as a
cross-way validation strategy, ensuring the quality of the automatically
generated data. To evaluate the performance of TableLLM, we have crafted
benchmarks tailored to address both document and spreadsheet formats as well as
constructed a well-organized evaluation pipeline capable of handling both
scenarios. Thorough evaluations underscore the advantages of TableLLM when
compared to various existing general-purpose and tabular data-focused LLMs. We
have publicly released the model checkpoint, source code, benchmarks, and a web
application for user interaction. Our codes and data are publicly available at
https://github.com/TableLLM/TableLLM.

</details>


### [9] [GiesKaNe: Bridging Past and Present in Grammatical Theory and Practical Application](https://arxiv.org/abs/2502.05113)
*Volker Emmrich*

Main category: cs.CL

TL;DR: GiesKaNe项目是一个参考性、历史性和句法深度注释的树状数据库，其构建方法结合了人工和机器辅助流程，并提出了新的文本分类和注释标准推导方法，最终证明了即使是大型项目也能有效利用现有基础设施和简单工具来完成。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨GiesKaNe项目（一个参考性、历史性和句法深度注释的树状数据库）的语料库构建要求，并提出一种创新的机器辅助文本分类方法和一种从现有注释中推导出现实注释标准的方法，以平衡标准化与创新。

Method: 本文介绍了GiesKaNe项目的语料库构建方法，该项目被定义为一个参考性、历史性和句法深度注释的树状数据库。该方法强调了在创新与遵守标准之间的平衡，并采用互补的人工专业知识和机器辅助流程来处理方法论的复杂性。讨论了分词、规范化、句子定义、标记、解析和注释员之间的一致性等基础主题，以及语法模型、注释模式和现有事实注释标准之间的比较，并提出了一种新颖的机器辅助文本分类方法，用于在概念口语和书面语的连续体上进行分类，以及一种从现有注释中推导出现实注释标准的方法。

Result: GiesKaNe项目展示了如何利用现有的研究基础设施和简单的电子表格，通过结合人工和机器辅助流程，有效地构建一个包含深度句法注释的树状数据库，即使是对于雄心勃勃的历史语言学项目也是如此。

Conclusion: GiesKaNe项目展示了如何利用现有的研究基础设施和简单的电子表格，通过结合人工和机器辅助流程，有效地构建一个包含深度句法注释的树状数据库，即使是对于雄心勃勃的历史语言学项目也是如此。

Abstract: This article explores the requirements for corpus compilation within the
GiesKaNe project (University of Giessen and Kassel, Syntactic Basic Structures
of New High German). The project is defined by three central characteristics:
it is a reference corpus, a historical corpus, and a syntactically deeply
annotated treebank. As a historical corpus, GiesKaNe aims to establish
connections with both historical and contemporary corpora, ensuring its
relevance across temporal and linguistic contexts. The compilation process
strikes the balance between innovation and adherence to standards, addressing
both internal project goals and the broader interests of the research
community. The methodological complexity of such a project is managed through a
complementary interplay of human expertise and machine-assisted processes. The
article discusses foundational topics such as tokenization, normalization,
sentence definition, tagging, parsing, and inter-annotator agreement, alongside
advanced considerations. These include comparisons between grammatical models,
annotation schemas, and established de facto annotation standards as well as
the integration of human and machine collaboration. Notably, a novel method for
machine-assisted classification of texts along the continuum of conceptual
orality and literacy is proposed, offering new perspectives on text selection.
Furthermore, the article introduces an approach to deriving de facto standard
annotations from existing ones, mediating between standardization and
innovation. In the course of describing the workflow the article demonstrates
that even ambitious projects like GiesKaNe can be effectively implemented using
existing research infrastructure, requiring no specialized annotation tools.
Instead, it is shown that the workflow can be based on the strategic use of a
simple spreadsheet and integrates the capabilities of the existing
infrastructure.

</details>


### [10] [MiMoTable: A Multi-scale Spreadsheet Benchmark with Meta Operations for Table Reasoning](https://arxiv.org/abs/2412.11711)
*Zheng Li,Yang Du,Mao Zheng,Mingyang Song*

Main category: cs.CL

TL;DR: MiMoTable是一个包含真实世界电子表格和新难度评估标准（元操作）的基准测试，用于改进LLM的表格推理能力。


<details>
  <summary>Details</summary>
Motivation: 填补LLM在真实世界表格推理任务中存在的复杂性和多样性与现有基准测试之间的差距。

Method: 提出了一种名为MiMoTable的多尺度电子表格基准测试，并定义了包含六类元操作的新标准来衡量问题难度。

Result: MiMoTable包含了七个领域、不同类型的真实世界电子表格。新标准能够衡量现有基准测试的难度，并证明了LLM性能随难度增加而下降。

Conclusion: LLMs在MiMoTable上的表现仍有提升空间，Claude-3.5-Sonnet准确率达到77.4%，为最佳。LLM在MiMoTable上的性能随基准测试难度增加而下降，证明了新标准标准的有效性。

Abstract: Extensive research has been conducted to explore the capability of Large
Language Models (LLMs) for table reasoning and has significantly improved the
performance on existing benchmarks. However, tables and user questions in
real-world applications are more complex and diverse, presenting an unignorable
gap compared to the existing benchmarks. To fill the gap, we propose a
\textbf{M}ult\textbf{i}-scale spreadsheet benchmark with \textbf{M}eta
\textbf{o}perations for \textbf{Table} reasoning, named as MiMoTable.
Specifically, MiMoTable incorporates two key features. First, the tables in
MiMoTable are all spreadsheets used in real-world scenarios, which cover seven
domains and contain different types. Second, we define a new criterion with six
categories of meta operations for measuring the difficulty of each question in
MiMoTable, simultaneously as a new perspective for measuring the difficulty of
the existing benchmarks. Experimental results show that Claude-3.5-Sonnet
achieves the best performance with 77.4\% accuracy, indicating that there is
still significant room to improve for LLMs on MiMoTable. Furthermore, we grade
the difficulty of existing benchmarks according to our new criteria.
Experiments have shown that the performance of LLMs decreases as the difficulty
of benchmarks increases, thereby proving the effectiveness of our proposed new
criterion.

</details>


### [11] [SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation](https://arxiv.org/abs/2406.14991)
*Zeyao Ma,Bohan Zhang,Jing Zhang,Jifan Yu,Xiaokang Zhang,Xiaohan Zhang,Sijia Luo,Xi Wang,Jie Tang*

Main category: cs.CL

TL;DR: 该研究提出了一个名为SpreadsheetBench的电子表格处理基准，包含真实世界的复杂问题和数据，并采用更可靠的评估方法。评估结果表明，现有的大型语言模型在处理电子表格任务方面仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 为了弥合大型语言模型在电子表格处理能力方面的不足，并为评估这些模型提供一个更贴近真实世界场景的基准。

Method: 提出了一个名为SpreadsheetBench的基准，该基准包含912个来自在线Excel论坛的真实问题和相应的电子表格文件，并采用类似在线评判平台的评估指标，通过为每个指令创建多个电子表格文件来评估模型的鲁棒性。

Result: 在单轮和多轮推理设置下对多种语言模型进行了评估，结果显示最先进模型在处理电子表格任务时，与人类的性能存在显著差距。

Conclusion: 现有的最先进模型在处理电子表格任务方面与人类的差距很大，这表明该基准的挑战性。

Abstract: We introduce SpreadsheetBench, a challenging spreadsheet manipulation
benchmark exclusively derived from real-world scenarios, designed to immerse
current large language models (LLMs) in the actual workflow of spreadsheet
users. Unlike existing benchmarks that rely on synthesized queries and
simplified spreadsheet files, SpreadsheetBench is built from 912 real questions
gathered from online Excel forums, which reflect the intricate needs of users.
The associated spreadsheets from the forums contain a variety of tabular data
such as multiple tables, non-standard relational tables, and abundant
non-textual elements. Furthermore, we propose a more reliable evaluation metric
akin to online judge platforms, where multiple spreadsheet files are created as
test cases for each instruction, ensuring the evaluation of robust solutions
capable of handling spreadsheets with varying values. Our comprehensive
evaluation of various LLMs under both single-round and multi-round inference
settings reveals a substantial gap between the state-of-the-art (SOTA) models
and human performance, highlighting the benchmark's difficulty.

</details>


### [12] [NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries](https://arxiv.org/abs/2402.14853)
*Wei Zhao,Zhitao Hou,Siyuan Wu,Yan Gao,Haoyu Dong,Yao Wan,Hongyu Zhang,Yulei Sui,Haidong Zhang*

Main category: cs.CL

TL;DR: 该研究介绍了NL2Formula任务和fCoder模型，旨在通过自然语言生成电子表格公式，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了减轻用户在电子表格中编写复杂公式的负担，该研究引入了NL2Formula任务。

Method: 通过构建包含70,799个成对的自然语言查询和对应的电子表格公式的数据集，并实现了一个名为fCoder的序列到序列模型。

Result: fCoder模型在NL2Formula任务上表现优于基线模型，并与GPT-3.5模型进行了比较。

Conclusion: 该研究提出了NL2Formula基准任务，并实现了一个名为fCoder的序列到序列模型来解决该任务，实验证明了fCoder的有效性。

Abstract: Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets,
is a widespread practice among users performing data analysis. However,
crafting formulas on spreadsheets remains a tedious and error-prone task for
many end-users, particularly when dealing with complex operations. To alleviate
the burden associated with writing spreadsheet formulas, this paper introduces
a novel benchmark task called NL2Formula, with the aim to generate executable
formulas that are grounded on a spreadsheet table, given a Natural Language
(NL) query as input. To accomplish this, we construct a comprehensive dataset
consisting of 70,799 paired NL queries and corresponding spreadsheet formulas,
covering 21,670 tables and 37 types of formula functions. We realize the
NL2Formula task by providing a sequence-to-sequence baseline implementation
called fCoder. Experimental results validate the effectiveness of fCoder,
demonstrating its superior performance compared to the baseline models.
Furthermore, we also compare fCoder with an initial GPT-3.5 model (i.e.,
text-davinci-003). Lastly, through in-depth error analysis, we identify
potential challenges in the NL2Formula task and advocate for further
investigation.

</details>


### [13] [InstructExcel: A Benchmark for Natural Language Instruction in Excel](https://arxiv.org/abs/2310.14495)
*Justin Payan,Swaroop Mishra,Mukul Singh,Carina Negreanu,Christian Poelitz,Chitta Baral,Subhro Roy,Rasika Chakravarthy,Benjamin Van Durme,Elnaz Nouri*

Main category: cs.CL

TL;DR: This paper introduces InstructExcel, a benchmark for evaluating LLMs' ability to generate Excel code from text. Current models struggle, but improvements are seen with better models, more examples, and dynamic prompting.


<details>
  <summary>Details</summary>
Motivation: The motivation of this work is to investigate the capability of Large Language Models (LLMs) in generating Excel-specific code (OfficeScripts) to fulfill natural language user instructions, given the increasing complexity of tasks that LLMs can handle across various domains, including spreadsheets.

Method: The study introduces InstructExcel, a large-scale benchmark consisting of over 10,000 samples covering 170+ Excel operations. This benchmark was created by automatically generating OfficeScripts from user actions using Excel's 'Automate' feature. Experiments were conducted in zero-shot and few-shot settings to evaluate the performance of LLMs on this benchmark.

Result: Experiments show that InstructExcel is a challenging benchmark for state-of-the-art models such as GPT-4. The study observed that utilizing GPT-4 over GPT-3.5, increasing the number of in-context examples, and implementing dynamic prompting can lead to performance improvements on this benchmark.

Conclusion: LLMs can be used to generate Excel OfficeScripts from natural language instructions, but it remains a challenging task for current state-of-the-art models like GPT-4, as demonstrated by the InstructExcel benchmark. Performance can be improved by using more advanced models (GPT-4 over GPT-3.5), providing more in-context examples, and employing dynamic prompting.

Abstract: With the evolution of Large Language Models (LLMs) we can solve increasingly
more complex NLP tasks across various domains, including spreadsheets. This
work investigates whether LLMs can generate code (Excel OfficeScripts, a
TypeScript API for executing many tasks in Excel) that solves Excel specific
tasks provided via natural language user instructions. To do so we introduce a
new large-scale benchmark, InstructExcel, created by leveraging the 'Automate'
feature in Excel to automatically generate OfficeScripts from users' actions.
Our benchmark includes over 10k samples covering 170+ Excel operations across
2,000 publicly available Excel spreadsheets. Experiments across various
zero-shot and few-shot settings show that InstructExcel is a hard benchmark for
state of the art models like GPT-4. We observe that (1) using GPT-4 over
GPT-3.5, (2) providing more in-context examples, and (3) dynamic prompting can
help improve performance on this benchmark.

</details>


### [14] [Active Learning with Tabular Language Models](https://arxiv.org/abs/2211.04128)
*Martin Ringsquandl,Aneta Koleva*

Main category: cs.CL

TL;DR: 该研究调查了用于单元格内命名实体识别的真实工业表格语言模型用例中的不同获取函数，发现内置多样性的细胞级获取函数可以显著降低标注工作量，而强制表多样性则是有害的。


<details>
  <summary>Details</summary>
Motivation: 尽管表格语言模型研究取得了最新进展，但实际应用仍然具有挑战性。在工业中，电子表格中存在大量表格，但获取大量标签成本高昂，因为只有专家才能标注通常高度技术性和领域特定的表格。主动学习有可能降低标注成本，然而，到目前为止，还没有关于主动学习与表格语言模型结合的研究。

Method: 研究了用于单元格内命名实体识别的真实工业表格语言模型用例中的不同获取函数。

Result: 细胞级获取函数内置多样性可以显著降低标注工作量，而强制表多样性则是有害的。此外，计算效率和人类注释者的视角方面存在一些公开的基本问题。

Conclusion: 细胞级获取函数内置多样性可以显著降低标注工作量，而强制表多样性则是有害的。

Abstract: Despite recent advancements in tabular language model research, real-world
applications are still challenging. In industry, there is an abundance of
tables found in spreadsheets, but acquisition of substantial amounts of labels
is expensive, since only experts can annotate the often highly technical and
domain-specific tables. Active learning could potentially reduce labeling
costs, however, so far there are no works related to active learning in
conjunction with tabular language models. In this paper we investigate
different acquisition functions in a real-world industrial tabular language
model use case for sub-cell named entity recognition. Our results show that
cell-level acquisition functions with built-in diversity can significantly
reduce the labeling effort, while enforced table diversity is detrimental. We
further see open fundamental questions concerning computational efficiency and
the perspective of human annotators.

</details>


### [15] [Table-To-Text generation and pre-training with TabT5](https://arxiv.org/abs/2210.09162)
*Ewa Andrejczuk,Julian Martin Eisenschlos,Francesco Piccinno,Syrine Krichene,Yasemin Altun*

Main category: cs.CL

TL;DR: TABT5是一个Encoder-Decoder模型，能够基于表格和文本生成自然语言，并在多个表格理解任务上取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有Encoder-only Transformer模型在表格理解任务中仅限于分类类任务（如单元格选择或蕴含检测）的限制。

Method: TABT5是一个结合了编码器和解码器组件的模型，利用了特定于表的嵌入和预训练来处理表格和文本输入，克服了仅编码器架构的局限性。

Result: TABT5在电子表格公式预测方面提高了15%的序列准确率，在问答方面提高了2.5%的序列准确率，在数据到文本生成方面提高了2.5%的BLEU分数。

Conclusion: TABT5模型在包括电子表格公式预测、问答和数据到文本生成在内的多个领域取得了新的最先进成果。

Abstract: Encoder-only transformer models have been successfully applied to different
table understanding tasks, as in TAPAS (Herzig et al., 2020). A major
limitation of these architectures is that they are constrained to
classification-like tasks such as cell selection or entailment detection. We
present TABT5, an encoder-decoder model that generates natural language text
based on tables and textual inputs. TABT5 overcomes the encoder-only limitation
by incorporating a decoder component and leverages the input structure with
table specific embeddings and pre-training. TABT5 achieves new state-of-the-art
results on several domains, including spreadsheet formula prediction with a 15%
increase in sequence accuracy, QA with a 2.5% increase in sequence accuracy and
data-to-text generation with a 2.5% increase in BLEU.

</details>


### [16] [Table Pre-training: A Survey on Model Architectures, Pre-training Objectives, and Downstream Tasks](https://arxiv.org/abs/2201.09745)
*Haoyu Dong,Zhoujun Cheng,Xinyi He,Mengyu Zhou,Anda Zhou,Fan Zhou,Ao Liu,Shi Han,Dongmei Zhang*

Main category: cs.CL

TL;DR: 表格预训练是处理表格数据的热门研究领域，本调查全面回顾了相关进展，并指出了未来的发展方向。


<details>
  <summary>Details</summary>
Motivation: 鉴于可以从网页、电子表格、PDF和其他各种文档类型中轻松收集大量表格，在文本和图像成功之后，人们提出了许多表格预训练框架，并在各种任务（如表格问答、表格类型识别、列关系分类、表格搜索、公式预测等）上取得了新的进展。

Method: 对表格预训练的不同模型设计、预训练目标和下游任务进行了全面的回顾。

Result: 表格预训练框架在表格问答、表格类型识别、列关系分类、表格搜索、公式预测等任务上取得了新的进展。

Conclusion: 该调查全面回顾了表格预训练的不同模型设计、预训练目标和下游任务，并对现有挑战和未来机遇进行了思考与展望。

Abstract: Since a vast number of tables can be easily collected from web pages,
spreadsheets, PDFs, and various other document types, a flurry of table
pre-training frameworks have been proposed following the success of text and
images, and they have achieved new state-of-the-arts on various tasks such as
table question answering, table type recognition, column relation
classification, table search, formula prediction, etc. To fully use the
supervision signals in unlabeled tables, a variety of pre-training objectives
have been designed and evaluated, for example, denoising cell values,
predicting numerical relationships, and implicitly executing SQLs. And to best
leverage the characteristics of (semi-)structured tables, various tabular
language models, particularly with specially-designed attention mechanisms,
have been explored. Since tables usually appear and interact with free-form
text, table pre-training usually takes the form of table-text joint
pre-training, which attracts significant research interests from multiple
domains. This survey aims to provide a comprehensive review of different model
designs, pre-training objectives, and downstream tasks for table pre-training,
and we further share our thoughts and vision on existing challenges and future
opportunities.

</details>


### [17] [TableQuery: Querying tabular data with natural language](https://arxiv.org/abs/2202.00454)
*Abhijith Neil Abraham,Fariz Rahman,Damanpreet Kaur*

Main category: cs.CL

TL;DR: TableQuery是一个创新的工具，它利用预训练的深度学习模型来处理自然语言查询，并将其转换为可执行的结构化查询，可以直接用于查询数据库或电子表格。该工具克服了传统方法的局限性，即需要将整个表格数据加载到内存中，特别适用于处理大规模和实时更新的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有表格数据问答的深度学习方法通常需要将整个表格作为输入，这对于包含大量数据或实时更新的实际应用来说是不切实际的。TableQuery旨在解决这些限制。

Method: TableQuery利用预训练的NLP模型（如HuggingFace Model Hub上的模型）来理解自然语言查询，并将其转换为SQL等结构化查询语言，然后直接在数据库或电子表格上执行。

Result: TableQuery成功地将自然语言查询转换为可执行的结构化查询，适用于大型或实时更新的表格数据，无需将全部数据加载到内存中。

Conclusion: TableQuery 使用预训练的文本问答深度学习模型将自然语言查询转换为结构化查询，可直接在数据库或电子表格上运行。该方法无需将整个表读入内存或序列化数据库，并支持即插即用，便于集成更新的模型。

Abstract: This paper presents TableQuery, a novel tool for querying tabular data using
deep learning models pre-trained to answer questions on free text. Existing
deep learning methods for question answering on tabular data have various
limitations, such as having to feed the entire table as input into a neural
network model, making them unsuitable for most real-world applications. Since
real-world data might contain millions of rows, it may not entirely fit into
the memory. Moreover, data could be stored in live databases, which are updated
in real-time, and it is impractical to serialize an entire database to a neural
network-friendly format each time it is updated. In TableQuery, we use deep
learning models pre-trained for question answering on free text to convert
natural language queries to structured queries, which can be run against a
database or a spreadsheet. This method eliminates the need for fitting the
entire data into memory as well as serializing databases. Furthermore, deep
learning models pre-trained for question answering on free text are readily
available on platforms such as HuggingFace Model Hub (7). TableQuery does not
require re-training; when a newly trained model for question answering with
better performance is available, it can replace the existing model in
TableQuery.

</details>


### [18] [The Impact of Text Presentation on Translator Performance](https://arxiv.org/abs/2011.05978)
*Samuel Läubli,Patrick Simianer,Joern Wuebker,Geza Kovacs,Rico Sennrich,Spence Green*

Main category: cs.CL

TL;DR: 逐句呈现和上下排列的源/目标视图可提高翻译速度，但未分段文本在修订时更准确高效。


<details>
  <summary>Details</summary>
Motivation: 评估计算机辅助翻译（CAT）工具的分句和并排视图设计选择对翻译者表现的影响。

Method: 本研究旨在通过三种实验性文本处理任务，对广泛使用的计算机辅助翻译（CAT）工具的设计选择（如分句和并排视图）进行控制性评估，从而衡量翻译者的速度和准确性。

Result: 与未分段文本相比，逐句呈现可实现更快的文本复制和句内错误识别；与并排视图相比，源句和目标句的上下排列可实现更快的文本复制。然而，在修订方面，未分段文本在准确性和时间效率方面表现最佳。

Conclusion: 本研究结果对设计计算机辅助翻译工具的最佳实践有直接影响。

Abstract: Widely used computer-aided translation (CAT) tools divide documents into
segments such as sentences and arrange them in a side-by-side, spreadsheet-like
view. We present the first controlled evaluation of these design choices on
translator performance, measuring speed and accuracy in three experimental text
processing tasks. We find significant evidence that sentence-by-sentence
presentation enables faster text reproduction and within-sentence error
identification compared to unsegmented text, and that a top-and-bottom
arrangement of source and target sentences enables faster text reproduction
compared to a side-by-side arrangement. For revision, on the other hand, our
results suggest that presenting unsegmented text results in the highest
accuracy and time efficiency. Our findings have direct implications for best
practices in designing CAT tools.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [19] [FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining](https://arxiv.org/abs/2109.07323)
*Zhoujun Cheng,Haoyu Dong,Ran Jia,Pengfei Wu,Shi Han,Fan Cheng,Dongmei Zhang*

Main category: cs.IR

TL;DR: FORTAP是一种新的预训练方法，它利用电子表格公式来提高表格的数值推理能力，并在下游任务中取得了最佳结果。


<details>
  <summary>Details</summary>
Motivation: 电子表格公式是对数值推理的有力监督，并且网上有大量的专家制作的公式电子表格可供获取。

Method: FORTAP是第一个利用大型电子表格公式语料库进行数值推理感知表预训练的方法。我们设计了两个公式预训练任务，以明确指导FORTAP学习半结构化表中的数值引用和计算。

Result: FORTAP在两个代表性下游任务（单元格类型分类和公式预测）上取得了最先进的结果。

Conclusion: FORTAP在两个代表性下游任务（单元格类型分类和公式预测）上取得了最先进的结果，显示了具有数值推理意识的预训练的巨大潜力。

Abstract: Tables store rich numerical data, but numerical reasoning over tables is
still a challenge. In this paper, we find that the spreadsheet formula, which
performs calculations on numerical values in tables, is naturally a strong
supervision of numerical reasoning. More importantly, large amounts of
spreadsheets with expert-made formulae are available on the web and can be
obtained easily. FORTAP is the first method for numerical-reasoning-aware table
pretraining by leveraging large corpus of spreadsheet formulae. We design two
formula pretraining tasks to explicitly guide FORTAP to learn numerical
reference and calculation in semi-structured tables. FORTAP achieves
state-of-the-art results on two representative downstream tasks, cell type
classification and formula prediction, showing great potential of
numerical-reasoning-aware pretraining.

</details>


### [20] [Detecting Layout Templates in Complex Multiregion Files](https://arxiv.org/abs/2109.06630)
*Gerardo Vitagliano,Lan Jiang,Felix Naumann*

Main category: cs.IR

TL;DR: Mondrian方法通过图像渲染、聚类和图表示，能够有效地识别电子表格中的多个区域及其跨文件的重复布局模式，解决了自动化数据分析中的一个关键挑战。


<details>
  <summary>Details</summary>
Motivation: 电子表格因其广泛使用而易于收集大量数据，但其灵活的画布结构使得在没有大量准备工作的情况下难以进行自动化分析。一个常见的问题是单个电子表格中存在多个独立的区域，它们可能被重复的空单元格分隔。本研究旨在自动识别跨多个文件的布局模板并系统地提取相应区域。

Method: Mondrian方法包含三个阶段：1. 将每个文件渲染成图像并识别潜在的区域元素；2. 使用聚类算法将识别出的元素分组形成区域；3. 将每个文件布局表示为图并进行比较以寻找布局模板。

Result: Mondrian方法在检测可靠区域边界和识别重复布局方面表现出最佳性能。

Conclusion: Mondrian方法在检测单个文件内的可靠区域边界以及跨文件识别重复布局方面表现优于现有技术，在两个真实企业电子表格语料库上的实验证明了其有效性。

Abstract: Spreadsheets are among the most commonly used file formats for data
management, distribution, and analysis. Their widespread employment makes it
easy to gather large collections of data, but their flexible canvas-based
structure makes automated analysis difficult without heavy preparation. One of
the common problems that practitioners face is the presence of multiple,
independent regions in a single spreadsheet, possibly separated by repeated
empty cells. We define such files as "multiregion" files. In collections of
various spreadsheets, we can observe that some share the same layout. We
present the Mondrian approach to automatically identify layout templates across
multiple files and systematically extract the corresponding regions. Our
approach is composed of three phases: first, each file is rendered as an image
and inspected for elements that could form regions; then, using a clustering
algorithm, the identified elements are grouped to form regions; finally, every
file layout is represented as a graph and compared with others to find layout
templates. We compare our method to state-of-the-art table recognition
algorithms on two corpora of real-world enterprise spreadsheets. Our approach
shows the best performances in detecting reliable region boundaries within each
file and can correctly identify recurring layouts across files.

</details>


### [21] [TUTA: Tree-based Transformers for Generally Structured Table Pre-training](https://arxiv.org/abs/2010.12537)
*Zhiruo Wang,Haoyu Dong,Ran Jia,Jia Li,Zhiyi Fu,Shi Han,Dongmei Zhang*

Main category: cs.IR

TL;DR: TUTA是一种新的预训练框架，通过其独特的结构感知机制，能够统一理解各种表格结构，并在多个表格理解任务上取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 目前的表格理解方法主要关注关系表格，而忽视了其他常见表格结构。本文旨在提出一个能理解通用结构表格的统一框架。

Method: TUTA是一个统一的预训练框架，通过结合三维坐标树、基于树的注意力和位置嵌入以及三种渐进式预训练目标（令牌、单元格和表格级别）来捕捉表格的空间、层级和语义信息。

Result: TUTA在细胞类型分类和表格类型分类两个任务上均达到了最先进的水平。

Conclusion: TUTA在细胞类型分类和表格类型分类两个关键任务上均达到了最先进的水平，并在五个广泛研究的数据集上取得了优异的成果。

Abstract: Tables are widely used with various structures to organize and present data.
Recent attempts on table understanding mainly focus on relational tables, yet
overlook to other common table structures. In this paper, we propose TUTA, a
unified pre-training architecture for understanding generally structured
tables. Noticing that understanding a table requires spatial, hierarchical, and
semantic information, we enhance transformers with three novel structure-aware
mechanisms. First, we devise a unified tree-based structure, called a
bi-dimensional coordinate tree, to describe both the spatial and hierarchical
information of generally structured tables. Upon this, we propose tree-based
attention and position embedding to better capture the spatial and hierarchical
information. Moreover, we devise three progressive pre-training objectives to
enable representations at the token, cell, and table levels. We pre-train TUTA
on a wide range of unlabeled web and spreadsheet tables and fine-tune it on two
critical tasks in the field of table structure understanding: cell type
classification and table type classification. Experiments show that TUTA is
highly effective, achieving state-of-the-art on five widely-studied datasets.

</details>


### [22] [TableSense: Spreadsheet Table Detection with Convolutional Neural Networks](https://arxiv.org/abs/2106.13500)
*Haoyu Dong,Shijie Liu,Shi Han,Zhouyu Fu,Dongmei Zhang*

Main category: cs.IR

TL;DR: TableSense 是一种新颖的端到端框架，用于电子表格表格检测。它利用有效的单元格特征化、增强的卷积神经网络和主动学习来提高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 电子表格表格检测对于电子表格数据智能至关重要，但表格结构和布局的多样性带来了挑战。

Method: 开发了一种端到端的框架 TableSense，包括有效的单元格特征化方案、用于表检测的增强卷积神经网络模型以及用于指导主动学习的有效不确定性度量。

Result: TableSense 在 EoB-2 度量中实现了 91.3% 的召回率和 86.5% 的准确率，显著优于现有方法。

Conclusion: TableSense 在 EoB-2 度量中实现了 91.3% 的召回率和 86.5% 的准确率，显著优于商品电子表格工具中使用的现有检测算法和计算机视觉领域最先进的卷积神经网络。

Abstract: Spreadsheet table detection is the task of detecting all tables on a given
sheet and locating their respective ranges. Automatic table detection is a key
enabling technique and an initial step in spreadsheet data intelligence.
However, the detection task is challenged by the diversity of table structures
and table layouts on the spreadsheet. Considering the analogy between a cell
matrix as spreadsheet and a pixel matrix as image, and encouraged by the
successful application of Convolutional Neural Networks (CNN) in computer
vision, we have developed TableSense, a novel end-to-end framework for
spreadsheet table detection. First, we devise an effective cell featurization
scheme to better leverage the rich information in each cell; second, we develop
an enhanced convolutional neural network model for table detection to meet the
domain-specific requirement on precise table boundary detection; third, we
propose an effective uncertainty metric to guide an active learning based smart
sampling algorithm, which enables the efficient build-up of a training dataset
with 22,176 tables on 10,220 sheets with broad coverage of diverse table
structures and layouts. Our evaluation shows that TableSense is highly
effective with 91.3\% recall and 86.5\% precision in EoB-2 metric, a
significant improvement over both the current detection algorithm that are used
in commodity spreadsheet tools and state-of-the-art convolutional neural
networks in computer vision.

</details>


### [23] [Recommending Related Tables](https://arxiv.org/abs/1907.03595)
*Shuo Zhang,Krisztian Balog*

Main category: cs.IR

TL;DR: 推荐相关表格：给定一个输入表格，识别并返回相关表格 的排名列表。


<details>
  <summary>Details</summary>
Motivation: 该任务 的一项可能 的应用场景 是向电子表格程序 的用户主动提供有关 Web 上相关结构化内容 的推荐。

Method: 提出了一种理论上合理 的表格匹配框架，该框架通过在多个语义空间中表示表格元素，并使用判别学习模型组合元素级相似性来执行表格匹配。

Result: 在维基百科表格 的专门构建 的测试集合上 ，所提出 的方法实现了最先进 的性能。

Conclusion: 所提出 的方法在维基百科表格 的专门构建 的测试集合上 实现了最先进 的性能。

Abstract: Tables are an extremely powerful visual and interactive tool for structuring
and manipulating data, making spreadsheet programs one of the most popular
computer applications. In this paper we introduce and address the task of
recommending related tables: given an input table, identifying and returning a
ranked list of relevant tables. One of the many possible application scenarios
for this task is to provide users of a spreadsheet program proactively with
recommendations for related structured content on the Web. At its core, the
related table recommendation task boils down to computing the similarity
between a pair of tables. We develop a theoretically sound framework for
performing table matching. Our approach hinges on the idea of representing
table elements in multiple semantic spaces, and then combining element-level
similarities using a discriminative learning model. Using a purpose-built test
collection from Wikipedia tables, we demonstrate that the proposed approach
delivers state-of-the-art performance.

</details>


### [24] [SmartTable: A Spreadsheet Program with Intelligent Assistance](https://arxiv.org/abs/1805.06353)
*Shuo Zhang,Vugar Abdul Zada,Krisztian Balog*

Main category: cs.IR

TL;DR: SmartTable是一个在线电子表格工具，可以帮助用户填充行和列。


<details>
  <summary>Details</summary>
Motivation: 为了提供带有智能辅助功能的在线电子表格应用程序，特别是针对关系表，提供填充实体和扩展属性方面的帮助。

Method: 介绍了SmartTable的实现细节，并已开源。

Result: SmartTable能够辅助用户填充表格中的实体（行）和扩展实体属性（列）。

Conclusion: SmartTable是一款带有智能辅助功能的在线电子表格应用程序，专注于关系表，提供填充实体（行）和扩展实体属性（列）方面的辅助。

Abstract: We introduce SmartTable, an online spreadsheet application that is equipped
with intelligent assistance capabilities. With a focus on relational tables,
describing entities along with their attributes, we offer assistance in two
flavors: (i) for populating the table with additional entities (rows) and (ii)
for extending it with additional entity attributes (columns). We provide
details of our implementation, which is also released as open source. The
application is available at http://smarttable.cc.

</details>


### [25] [EntiTables: Smart Assistance for Entity-Focused Tables](https://arxiv.org/abs/1708.08721)
*Shuo Zhang,Krisztian Balog*

Main category: cs.IR

TL;DR: We develop generative probabilistic models for populating rows with additional instances and columns with new headings in entity-focused tables. Our methods, which utilize a knowledge base and a large table corpus for model estimation, outperform existing approaches.


<details>
  <summary>Details</summary>
Motivation: Our motivation is to equip spreadsheet programs with smart assistance capabilities. We concentrate on one particular family of tables, namely, tables with an entity focus.

Method: We develop generative probabilistic models for both tasks. For estimating the components of these models, we consider a knowledge base as well as a large table corpus.

Result: Our experimental evaluation simulates the various stages of the user entering content into an actual table. A detailed analysis of the results shows that the models' components are complimentary and that our methods outperform existing approaches from the literature.

Conclusion: Our methods outperform existing approaches from the literature.

Abstract: Tables are among the most powerful and practical tools for organizing and
working with data. Our motivation is to equip spreadsheet programs with smart
assistance capabilities. We concentrate on one particular family of tables,
namely, tables with an entity focus. We introduce and focus on two specific
tasks: populating rows with additional instances (entities) and populating
columns with new headings. We develop generative probabilistic models for both
tasks. For estimating the components of these models, we consider a knowledge
base as well as a large table corpus. Our experimental evaluation simulates the
various stages of the user entering content into an actual table. A detailed
analysis of the results shows that the models' components are complimentary and
that our methods outperform existing approaches from the literature.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [26] [Dynamic Kernel Matching for Non-conforming Data: A Case Study of T-cell Receptor Datasets](https://arxiv.org/abs/2103.10472)
*Jared Ostmeyer,Scott Christley,Lindsay Cowell*

Main category: q-bio.QM

TL;DR: This paper presents Dynamic Kernel Matching (DKM) to analyze complex biological sequence data that doesn't fit standard formats. DKM successfully adapted classifiers for T-cell receptor data, showing good performance and identifying disease-related patterns consistent with other research.


<details>
  <summary>Details</summary>
Motivation: The motivation is to uncover patterns in non-standard data formats that do not fit traditional row-column structures, specifically in the context of biological data such as T-cell receptor sequences and repertoires, with the goal of diagnosing diseases.

Method: The paper introduces dynamic kernel matching (DKM) to modify existing statistical classifiers, enabling them to handle non-conforming data structures. This approach was applied to two datasets: T-cell receptor (TCR) sequences labeled by disease antigen and TCR repertoires labeled by patient cytomegalovirus (CMV) serostatus.

Result: Statistical classifiers augmented with DKM were successfully fitted to both TCR datasets. The performance was evaluated using standard metrics and metrics that accommodate indeterminate diagnoses. The identified patterns used by the classifiers for predictions align with findings from experimental studies.

Conclusion: The study successfully adapted statistical classifiers using dynamic kernel matching (DKM) to analyze non-conforming data, specifically T-cell receptor (TCR) sequences and repertoires, achieving good performance and identifying biologically relevant patterns.

Abstract: Most statistical classifiers are designed to find patterns in data where
numbers fit into rows and columns, like in a spreadsheet, but many kinds of
data do not conform to this structure. To uncover patterns in non-conforming
data, we describe an approach for modifying established statistical classifiers
to handle non-conforming data, which we call dynamic kernel matching (DKM). As
examples of non-conforming data, we consider (i) a dataset of T-cell receptor
(TCR) sequences labelled by disease antigen and (ii) a dataset of sequenced TCR
repertoires labelled by patient cytomegalovirus (CMV) serostatus, anticipating
that both datasets contain signatures for diagnosing disease. We successfully
fit statistical classifiers augmented with DKM to both datasets and report the
performance on holdout data using standard metrics and metrics allowing for
indeterminant diagnoses. Finally, we identify the patterns used by our
statistical classifiers to generate predictions and show that these patterns
agree with observations from experimental studies.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [27] [Is spreadsheet syntax better than numeric indexing for cell selection?](https://arxiv.org/abs/2505.23296)
*Philip Heltweg,Dirk Riehle,Georg-Daniel Schwarz*

Main category: cs.PL

TL;DR: Spreadsheet-style cell selection syntax is better than numeric indexing for data engineering tasks, improving speed and accuracy for users.


<details>
  <summary>Details</summary>
Motivation: The motivation is to compare different approaches for expressing cell selection in data engineering, specifically numeric indexing versus spreadsheet-style syntax, to understand their impact on user performance (speed and correctness) in reading and writing code.

Method: A large-scale controlled experiment was conducted with student participants, acting as proxies for data practitioners. The experiment compared numeric indexing with spreadsheet-style syntax for cell selection in terms of speed and correctness in both reading and writing code.

Result: The experiment found that participants made fewer mistakes when reading code using spreadsheet-style syntax. Furthermore, when writing code, participants made fewer mistakes and were faster using spreadsheet syntax compared to numeric syntax.

Conclusion: In a large-scale controlled experiment, spreadsheet-style syntax leads to fewer mistakes and faster code writing compared to numeric syntax when selecting cells. This suggests that domain-specific syntax like spreadsheet syntax is a promising alternative for data engineering tools, especially for practitioners without a software engineering background.

Abstract: Selecting a subset of cells is a common task in data engineering, for
example, to remove errors or select only specific parts of a table. Multiple
approaches to express this selection exist. One option is numeric indexing,
commonly found in general programming languages, where a tuple of numbers
identifies the cell. Alternatively, the separate dimensions can be referred to
using different enumeration schemes like "A1" for the first cell, commonly
found in software such as spreadsheet systems.
  In a large-scale controlled experiment with student participants as proxy for
data practitioners, we compare the two options with respect to speed and
correctness of reading and writing code.
  The results show that, when reading code, participants make less mistakes
using spreadsheet-style syntax. Additionally, when writing code, they make
fewer mistakes and are faster when using spreadsheet syntax compared to numeric
syntax.
  From this, a domain-specific syntax, such as spreadsheet syntax for data
engineering, appears to be a promising alternative to explore in future tools
to support practitioners without a software engineering background.

</details>


### [28] [Solving Data-centric Tasks using Large Language Models](https://arxiv.org/abs/2402.11734)
*Shraddha Barke,Christian Poelitz,Carina Suzana Negreanu,Benjamin Zorn,José Cambronero,Andrew D. Gordon,Vu Le,Elnaz Nouri,Nadia Polikarpova,Advait Sarkar,Brian Slininger,Neil Toronto,Jack Williams*

Main category: cs.PL

TL;DR: LLM处理表格数据时，选择代表性数据行作为提示比随机选择效果更好。


<details>
  <summary>Details</summary>
Motivation: 为了解决在提示LLM处理数据中心任务时，如何确定包含多少以及哪些数据的问题。

Method: 提出了一种cluster-then-select的提示技术，该技术将输入数据中最具代表性的行添加到LLM提示中。

Result: LLM性能确实对提示中传递的数据量敏感，并且对于输入表格具有大量语法变体的任务，所提出的cluster-then-select技术优于随机选择基线。

Conclusion: LLM在处理以表格数据为主的自然语言到代码任务方面，通过cluster-then-select方法，选择代表性数据行输入提示，能够比随机选择的方法表现更好。LLM性能对输入提示中的数据量敏感。

Abstract: Large language models (LLMs) are rapidly replacing help forums like
StackOverflow, and are especially helpful for non-professional programmers and
end users. These users are often interested in data-centric tasks, such as
spreadsheet manipulation and data wrangling, which are hard to solve if the
intent is only communicated using a natural-language description, without
including the data. But how do we decide how much data and which data to
include in the prompt? This paper makes two contributions towards answering
this question. First, we create a dataset of real-world NL-to-code tasks
manipulating tabular data, mined from StackOverflow posts. Second, we introduce
a cluster-then-select prompting technique, which adds the most representative
rows from the input data to the LLM prompt. Our experiments show that LLM
performance is indeed sensitive to the amount of data passed in the prompt, and
that for tasks with a lot of syntactic variation in the input table, our
cluster-then-select technique outperforms a random selection baseline.

</details>


### [29] [FLAME: A small language model for spreadsheet formulas](https://arxiv.org/abs/2301.13779)
*Harshit Joshi,Abishai Ebenezer,José Cambronero,Sumit Gulwani,Aditya Kanade,Vu Le,Ivan Radiček,Gust Verbruggen*

Main category: cs.PL

TL;DR: A smaller, specialized language model called FLAME is efficient for spreadsheet formula tasks, outperforming larger general models.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of using large language models for spreadsheet formula authoring assistance due to their size and cost, by developing a smaller, more efficient model.

Method: FLAME leverages domain insights with a transformer architecture, using a curated training dataset with sketch deduplication, an Excel-specific formula tokenizer, and domain-specific pre-training objectives (masked span prediction and noisy auto-encoding).

Result: FLAME demonstrates strong performance in formula repair, completion, and retrieval, outperforming significantly larger models in several settings.

Conclusion: FLAME, a 60M parameter transformer model trained on Excel formulas, achieves competitive performance and outperforms larger models like Davinci, Cushman, and CodeT5 in 10 of 14 evaluation settings for formula repair and completion. It also outperforms CodeT5, CodeBERT, and GraphCodeBERT in formula retrieval.

Abstract: Spreadsheets are a vital tool for end-user data management. Using large
language models for formula authoring assistance in these environments can be
difficult, as these models are expensive to train and challenging to deploy due
to their size (up to billions of parameters). We present FLAME, a
transformer-based model trained exclusively on Excel formulas that leverages
domain insights to achieve competitive performance while being substantially
smaller (60M parameters) and training on two orders of magnitude less data. We
curate a training dataset using sketch deduplication, introduce an
Excel-specific formula tokenizer, and use domain-specific versions of masked
span prediction and noisy auto-encoding as pre-training objectives. We evaluate
FLAME on formula repair, formula completion, and similarity-based formula
retrieval. FLAME can outperform much larger models, such as the Davinci (175B)
and Cushman (12B) variants of Codex and CodeT5 (220M), in 10 of 14 evaluation
settings for the repair and completion tasks. For formula retrieval, FLAME
outperforms CodeT5, CodeBERT, and GraphCodeBERT.

</details>


### [30] [Enhanced Spreadsheet Computing with Finite-Domain Constraint Satisfaction](https://arxiv.org/abs/2203.16346)
*Ezana N. Beyenne,Hai-Feng Guo*

Main category: cs.PL

TL;DR: 本研究扩展了电子表格的应用范围，通过引入约束求解和特定约束语言，使其能够处理约束满足问题，并简化了相关应用程序的开发。


<details>
  <summary>Details</summary>
Motivation: 电子表格由于其单向数据流的限制，主要局限于类似簿记的应用程序，本研究旨在通过扩展电子表格计算范式来打破这一限制，以解决约束满足问题。

Method: 提出了一种增强的电子表格系统，该系统在可视化环境中支持有限域约束求解，并构建了一种电子表格特定的约束语言，用于声明式和可扩展地指定约束。

Result: 新的电子表格系统显著简化了使用可视化表格界面开发许多基于约束的应用程序的过程，并举例说明了扩展后的电子表格范式的可用性和实用性。

Conclusion: 该研究通过增强电子表格系统，在可视化环境中支持有限域约束求解，并构建了特定于电子表格的约束语言，使用户能够以声明式和可扩展的方式指定数据单元之间的约束，从而有效简化了许多基于约束的应用程序的开发。

Abstract: The spreadsheet application is among the most widely used computing tools in
modern society. It provides excellent usability and usefulness, and it easily
enables a non-programmer to perform programming-like tasks in a visual tabular
"pen and paper" approach. However, spreadsheets are mostly limited to
bookkeeping-like applications due to their mono-directional data flow. This
paper shows how the spreadsheet computing paradigm is extended to break this
limitation for solving constraint satisfaction problems. We present an enhanced
spreadsheet system where finite-domain constraint solving is well supported in
a visual environment. Furthermore, a spreadsheet-specific constraint language
is constructed for general users to specify constraints among data cells in a
declarative and scalable way. The new spreadsheet system significantly
simplifies the development of many constraint-based applications using a visual
tabular interface. Examples are given to illustrate the usability and
usefulness of the extended spreadsheet paradigm.
  KEYWORDS: Spreadsheet computing, Finite-domain constraint satisfaction,
Constraint logic programming

</details>


### [31] [Typed Image-based Programming with Structure Editing](https://arxiv.org/abs/2110.08993)
*Jonathan Edwards,Tomas Petricek*

Main category: cs.PL

TL;DR: 图像编程通过静态类型和结构化编辑实现协作，解决了模式变更问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决图像编程系统（如Smalltalk、LISP）在协作和部署方面支持不足的问题，本研究旨在通过类型和结构化编辑来增强其协作能力，特别是针对持久化数据的模式变更问题。

Method: 本研究通过静态类型和结构化编辑来解决图像编程中的模式变更问题。静态类型能够表达和执行模式变更，而结构化编辑则能捕获类型定义中的变更，以便自动适应数据。研究还提出了一个实现协作的理论，支持对结构化编辑的版本控制。

Result: 研究提出了一个关于结构化编辑版本控制的理论，以支持图像编程中的协作。虽然目前侧重于类型编辑，但该方法有望扩展到整个编程体验，为图像编程协作提供新的途径。

Conclusion: 该研究提出了一个支持协作的图像编程新方法，通过静态类型和结构化编辑来解决模式变更问题，并为图像编程的协作提供了理论基础。

Abstract: Many beloved programming systems are image-based: self-contained worlds that
persist both code and data in a single file. Examples include Smalltalk, LISP,
HyperCard, Flash, and spreadsheets. Image-based programming avoids much of the
complexity of modern programming technology stacks and encourages more casual
and exploratory programming. However conventional file-based programming has
better support for collaboration and deployment. These problems have been
blamed for the limited commercial success of Smalltalk. We propose to enable
collaboration in image-based programming via types and structure editing.
  We focus on the problem of schema change on persistent data. We turn to
static types, which paradoxically require more schema change but also provide a
mechanism to express and execute those changes. To determine those changes we
turn to structure editing, so that we can capture changes in type definitions
with sufficient fidelity to automatically adapt the data to suit. We conjecture
that typical schema changes can be handled through structure editing of static
types.
  That positions us to tackle collaboration with what could be called version
control for structure editing. We present a theory realizing this idea, which
is our main technical contribution. While we focus here on editing types, if we
can extend the approach to cover the entire programming experience then it
would offer a new way to collaborate in image-based programming.

</details>


### [32] [ExceLint: Automatically Finding Spreadsheet Formula Errors](https://arxiv.org/abs/1901.11100)
*Daniel W. Barowy,Emery D. Berger,Benjamin Zorn*

Main category: cs.PL

TL;DR: ExceLint 是一种用于检测电子表格公式错误的静态分析工具，它利用电子表格的矩形特性和信息论方法，能够快速有效地找出错误。


<details>
  <summary>Details</summary>
Motivation: 电子表格被广泛应用于金融等领域，其中错误可能导致灾难性后果，因此需要一种专门用于查找电子表格公式错误的静态分析方法。

Method: 该分析方法利用电子表格的矩形特性，并采用信息论方法来识别对相邻矩形区域造成意外中断的公式。

Result: 在对 70 个电子表格进行的测试中，ExceLint 平均处理每个电子表格的中位时间为 5 秒，并且其检测效果显著优于现有的分析方法。

Conclusion: ExceLint 是一种用于 Microsoft Excel 的静态分析工具，它能快速有效地检测电子表格公式中的错误，并且性能优于现有的分析方法。

Abstract: Spreadsheets are one of the most widely used programming environments, and
are widely deployed in domains like finance where errors can have catastrophic
consequences. We present a static analysis specifically designed to find
spreadsheet formula errors. Our analysis directly leverages the rectangular
character of spreadsheets. It uses an information-theoretic approach to
identify formulas that are especially surprising disruptions to nearby
rectangular regions. We present ExceLint, an implementation of our static
analysis for Microsoft Excel. We demonstrate that ExceLint is fast and
effective: across a corpus of 70 spreadsheets, ExceLint takes a median of 5
seconds per spreadsheet, and it significantly outperforms the state of the art
analysis.

</details>


### [33] [Synthesizing Bijective Lenses](https://arxiv.org/abs/1710.03248)
*Anders Miltner,Kathleen Fisher,Benjamin C. Pierce,David Walker,Steve Zdancewic*

Main category: cs.PL

TL;DR: Optician 工具通过类型导向综合，利用正则表达式和示例自动生成双向字符串变换器，解决了手动编码的痛点，并能在实际应用中高效执行。


<details>
  <summary>Details</summary>
Motivation: 手动构建双向数据表示之间的变换（如序列化/反序列化、数据库视图）既繁琐又容易出错，通常需要编写两个互为逆操作的函数。为了解决这个问题，需要一种更优的方法来简化这个过程。

Method: Optician 通过类型导向综合来构建双向字符串变换器。用户提供代表两种数据格式的正则表达式和一些具体的示例，Optician 输出一个符合 Boomerang 语言（一种基于 Lens 理论的双向语言）的类型正确的程序。该方法解决了在具有复杂类型系统的领域特定语言中编程的困难，并通过一种等价于正则表达式的语言来导航搜索空间，最终生成 Lens 语言的项。

Result: Optician 能够根据用户提供的正则表达式和示例，高效地合成双向字符串变换器。其类型导向的综合方法在包含 39 个示例的基准测试中得到了验证，这些示例涵盖了微基准测试和来自 Flash Fill、Augeas 等数据管理系统的实际应用。实验证明，Optician 改变了综合问题的性质，使其能够获得高效的解决方案。

Conclusion: Optician 是一个用于类型导向合成双向字符串变换器的工具，它通过在具有丰富类型等价关系的语言（正则表达式理论）的上下文中操作，解决了手动构建双向变换的繁琐和易错问题。该工具能够高效地处理程序搜索空间，将原本棘手的综合问题转化为高效问题，并在真实世界的应用中得到了验证。

Abstract: Bidirectional transformations between different data representations occur
frequently in modern software systems. They appear as serializers and
deserializers, as database views and view updaters, and more. Manually building
bidirectional transformations---by writing two separate functions that are
intended to be inverses---is tedious and error prone. A better approach is to
use a domain-specific language in which both directions can be written as a
single expression. However, these domain-specific languages can be difficult to
program in, requiring programmers to manage fiddly details while working in a
complex type system.
  To solve this, we present Optician, a tool for type-directed synthesis of
bijective string transformers. The inputs to Optician are two ordinary regular
expressions representing two data formats and a few concrete examples for
disambiguation. The output is a well-typed program in Boomerang (a
bidirectional language based on the theory of lenses). The main technical
challenge involves navigating the vast program search space efficiently enough.
Unlike most prior work on type-directed synthesis, our system operates in the
context of a language with a rich equivalence relation on types (the theory of
regular expressions). We synthesize terms of a equivalent language and convert
those generated terms into our lens language. We prove the correctness of our
synthesis algorithm. We also demonstrate empirically that our new language
changes the synthesis problem from one that admits intractable solutions to one
that admits highly efficient solutions. We evaluate Optician on a benchmark
suite of 39 examples including both microbenchmarks and realistic examples
derived from other data management systems including Flash Fill, a tool for
synthesizing string transformations in spreadsheets, and Augeas, a tool for
bidirectional processing of Linux system configuration files.

</details>


### [34] [Active Learning of Input Grammars](https://arxiv.org/abs/1708.08731)
*Matthias Höschele,Alexander Kampmann,Andreas Zeller*

Main category: cs.PL

TL;DR: AUTOGRAM generates accurate and readable context-free grammars for program inputs using data flow analysis and membership queries, requiring only a few samples for automated testing.


<details>
  <summary>Details</summary>
Motivation: Knowing the precise format of a program's input is necessary for systematic testing.

Method: 1. Track data flow to aggregate input fragments with the same data flow into lexical and syntactic entities. 2. Assign names to these entities based on variable and function identifiers. 3. Systematically generalize production rules using membership queries.

Result: The method obtains human-readable context-free grammars that reflect valid input structure, as demonstrated by the AUTOGRAM prototype's evaluation on various input types like URLs, spreadsheets, and configuration files.

Conclusion: The AUTOGRAM prototype can obtain accurate and readable input grammars from a minimal set of sample inputs, which can be directly used for automated testing.

Abstract: Knowing the precise format of a program's input is a necessary prerequisite
for systematic testing. Given a program and a small set of sample inputs, we
(1) track the data flow of inputs to aggregate input fragments that share the
same data flow through program execution into lexical and syntactic entities;
(2) assign these entities names that are based on the associated variable and
function identifiers; and (3) systematically generalize production rules by
means of membership queries. As a result, we need only a minimal set of sample
inputs to obtain human-readable context-free grammars that reflect valid input
structure. In our evaluation on inputs like URLs, spreadsheets, or
configuration files, our AUTOGRAM prototype obtains input grammars that are
both accurate and very readable - and that can be directly fed into test
generators for comprehensive automated testing.

</details>


### [35] [Synthesis of Data Completion Scripts using Finite Tree Automata](https://arxiv.org/abs/1707.01469)
*Xinyu Wang,Isil Dillig,Rishabh Singh*

Main category: cs.PL

TL;DR: 通过编程by-example和DSL自动完成表格数据补全任务，使用基于FTA的版本空间学习算法，并在基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的数据补全任务通常需要编程专业知识，给最终用户和数据科学家带来困难。

Method: 提出了一种领域特定语言（DSL），结合了表格数据的空间和关系推理，并开发了一种新的合成算法来生成与输入输出示例一致的DSL程序。该算法的核心是基于有限树自动机（FTA）的新版本空间学习算法，这种算法能够更紧凑地表示程序，并增加了一致示例的程序间的共享。

Result: 在84个基准测试上评估了DACE工具，并与PROSE和SKETCH两个现有合成器进行了比较，证明了该方法的优势。

Conclusion: 该方法通过编程by-example（PBE）和轻量级方法来自动化数据补全任务，用户只需提供公式草图和少量输入输出示例，即可自动生成补全数据的程序。

Abstract: In application domains that store data in a tabular format, a common task is
to fill the values of some cells using values stored in other cells. For
instance, such data completion tasks arise in the context of missing value
imputation in data science and derived data computation in spreadsheets and
relational databases. Unfortunately, end-users and data scientists typically
struggle with many data completion tasks that require non-trivial programming
expertise. This paper presents a synthesis technique for automating data
completion tasks using programming-by-example (PBE) and a very lightweight
sketching approach. Given a formula sketch (e.g., AVG($?_1$, $?_2$)) and a few
input-output examples for each hole, our technique synthesizes a program to
automate the desired data completion task. Towards this goal, we propose a
domain-specific language (DSL) that combines spatial and relational reasoning
over tabular data and a novel synthesis algorithm that can generate DSL
programs that are consistent with the input-output examples. The key technical
novelty of our approach is a new version space learning algorithm that is based
on finite tree automata (FTA). The use of FTAs in the learning algorithm leads
to a more compact representation that allows more sharing between programs that
are consistent with the examples. We have implemented the proposed approach in
a tool called DACE and evaluate it on 84 benchmarks taken from online help
forums. We also illustrate the advantages of our approach by comparing our
technique against two existing synthesizers, namely PROSE and SKETCH.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [36] [Trapped in Transformative Agreements? A Multifaceted Analysis of >1,000 Contracts](https://arxiv.org/abs/2409.20224)
*Laura Rothfritz,W. Benedikt Schmal,Ulrich Herb*

Main category: cs.DL

TL;DR: ESAC倡议提供了1000多个合同的数据库，其中包含每个合同的详细信息。通过对这些数据进行网络抓取，并结合定性和定量方法，对合同特征和TA格局进行了分析。研究结果表明，学术界被困在变革性协议中，而非通往完全开放获取的途径。


<details>
  <summary>Details</summary>
Motivation: 在ESAC数据库的基础上，扩展了现有的TA概述，以深入分析TA格局和协议特征。

Method: 通过对ESAC数据库中的1000多个协议进行网络抓取，并结合定性和定量方法，对协议特征和TA格局进行了深入分析。

Result: 研究机构似乎被“困”在变革性协议中，阻碍了向完全开放获取的过渡，并使传统出版商获得了显著的市场力量。

Conclusion: 学术界似乎被“困”在变革性协议中，这会增加图书馆和大学的成本，同时增加进入壁垒和降低竞争。

Abstract: Transformative agreements between academic publishers and research
institutions are ubiquitous. The 'Efficiency and Standards for Article Charges'
(ESAC) Initiative lists more than 1,000 contracts in its database. We make use
of this unique dataset by web-scraping the details of every contract to
substantially expand the overview spreadsheet provided by the ESAC Initiative.
Based on that hitherto unused data source, we combine qualitative and
quantitative methods to conduct an in-depth analysis of the contract
characteristics and the TA landscape. Our analysis demonstrates that research
institutions seem to be 'trapped' in transformative agreements. Instead of
being a bridge towards a fully Open Access world, academia is stuck in the
hybrid system. This endows the legacy (non-Open Access) publishing houses with
substantial market power. It raises entry barriers, lowers competition, and
increases costs for libraries and universities.

</details>


### [37] [Ensuring Adherence to Standards in Experiment-Related Metadata Entered Via Spreadsheets](https://arxiv.org/abs/2409.08897)
*Martin J. O'Connor,Josef Hardi,Marcos Martínez-Romero,Sowmya Somasundaram,Brendan Honick,Stephen A. Fisher,Ajay Pillai,Mark A. Musen*

Main category: cs.DL

TL;DR: 该研究提出了一种支持基于电子表格的元数据输入的端到端方法，确保符合社区标准并提供质量控制，例如在 HuBMAP 生物医学联盟中的应用。


<details>
  <summary>Details</summary>
Motivation: 尽管有复杂的工具可用，但研究人员通常更喜欢使用电子表格来提供元数据，尽管电子表格在确保元数据一致性和符合正式规范方面存在局限性。

Method: 该方法结合了可定制模板（捕获元数据标准并为作者元数据提供电子表格）、受控术语和本体（用于定义可直接从电子表格访问的元数据值）以及交互式 Web 工具（用于快速识别和修复基于电子表格的元数据中的错误）。

Result: 该方法被部署在 HuBMAP 生物医学联盟中，用于定义和收集有关各种生物学检测的元数据。

Conclusion: 该方法通过可定制的模板、受控术语和本体以及交互式 Web 工具，支持基于电子表格的元数据输入，同时确保符合社区元数据标准和质量控制。

Abstract: Scientists increasingly recognize the importance of providing rich,
standards-adherent metadata to describe their experimental results. Despite the
availability of sophisticated tools to assist in the process of data
annotation, investigators generally seem to prefer to use spreadsheets when
supplying metadata, despite the limitations of spreadsheets in ensuring
metadata consistency and compliance with formal specifications. In this paper,
we describe an end-to-end approach that supports spreadsheet-based entry of
metadata, while ensuring rigorous adherence to community-based metadata
standards and providing quality control. Our methods employ several key
components, including customizable templates that capture metadata standards
and that can inform the spreadsheets that investigators use to author metadata,
controlled terminologies and ontologies for defining metadata values that can
be accessed directly from a spreadsheet, and an interactive Web-based tool that
allows users to rapidly identify and fix errors in their spreadsheet-based
metadata. We demonstrate how this approach is being deployed in a biomedical
consortium known as HuBMAP to define and collect metadata about a wide range of
biological assays.

</details>


### [38] [A Comprehensive Approach to Ensuring Quality in Spreadsheet-Based Metadata](https://arxiv.org/abs/2312.09107)
*Martin J. O'Connor,Marcos Martínez-Romero,Mete Ugur Akdogan,Josef Hardi,Mark A. Musen*

Main category: cs.DL

TL;DR: 尽管电子表格是提供元数据的首选工具，但它们在合规性和质量方面存在不足。本研究提出了一种端到端的解决方案，支持基于电子表格的元数据输入，并具有严格的合规性和质量控制。该解决方案使用可定制模板、受控术语和交互式 Web 工具来识别和修复错误。


<details>
  <summary>Details</summary>
Motivation: 尽管科学家们越来越认识到元数据在描述其数据方面的重要性，但电子表格仍然是提供这些信息的首选工具，尽管它们在确保合规性和质量方面存在局限性。已开发出各种工具来解决这些局限性，但它们存在陡峭的学习曲线和有限的定制性等缺点。

Method: 该方法采用多种关键策略，包括用于定义元数据的可定制模板、对使用受控术语定义这些模板提供集成支持，以及一个交互式的基于Web的工具，允许用户快速识别和修复他们提供的基于电子表格的元数据中的错误。

Result: 提供了一个端到端的解决方案，支持基于电子表格的元数据输入，同时提供严格的合规性和质量控制。

Conclusion: 该方法正在生物医学联盟中部署，用于定义和收集有关科学实验的元数据。

Abstract: While scientists increasingly recognize the importance of metadata in
describing their data, spreadsheets remain the preferred tool for supplying
this information despite their limitations in ensuring compliance and quality.
Various tools have been developed to address these limitations, but they suffer
from their own shortcomings, such as steep learning curves and limited
customization. In this paper, we describe an end-to-end approach that supports
spreadsheet-based entry of metadata while providing rigorous compliance and
quality control. Our approach employs several key strategies, including
customizable templates for defining metadata, integral support for the use of
controlled terminologies when defining these templates, and an interactive
Web-based tool that allows users to rapidly identify and fix errors in the
spreadsheet-based metadata they supply. We demonstrate how this approach is
being deployed in a biomedical consortium to define and collect metadata about
scientific experiments.

</details>


### [39] [Towards Semantic Interoperability in Historical Research: Documenting Research Data and Knowledge with Synthesis](https://arxiv.org/abs/2107.13957)
*Pavlos Fafalios,Konstantina Konsolaki,Lida Charami,Kostas Petrakis,Manos Paterakis,Dimitris Angelakis,Yannis Tzitzikas,Chrysoula Bekiari,Martin Doerr*

Main category: cs.DL

TL;DR: Synthesis系统通过基于Web的协作和语义互操作性，改进了历史文物数据的记录和研究方式。


<details>
  <summary>Details</summary>
Motivation: 解决当前历史文物研究中数据组织方式（如电子表格、简单关系数据库）存在的协作困难、细节信息缺失、数据结构扩展性差以及数据复用性低等问题。

Method: 介绍了一个名为Synthesis的Web系统，该系统利用CIDOC-CRM和RDF等标准，支持大规模协作式文物数据记录与研究。

Result: Synthesis系统已被大量历史学家用于艺术史研究项目，有效支持了数据的协作记录、详细表示、集成和长期复用。

Conclusion: 该研究描述了一个名为Synthesis的基于Web的协作系统，该系统使用CIDOC-CRM和RDF等现有标准，解决了历史文物数据记录和研究中的挑战，提高了数据的互操作性和长期有效性。

Abstract: A vast area of research in historical science concerns the documentation and
study of artefacts and related evidence. Current practice mostly uses
spreadsheets or simple relational databases to organise the information as rows
with multiple columns of related attributes. This form offers itself for data
analysis and scholarly interpretation, however it also poses problems including
i) the difficulty for collaborative but controlled documentation by a large
number of users, ii) the lack of representation of the details from which the
documented relations are inferred, iii) the difficulty to extend the underlying
data structures as well as to combine and integrate data from multiple and
diverse information sources, and iv) the limitation to reuse the data beyond
the context of a particular research activity. To support historians to cope
with these problems, in this paper we describe the Synthesis documentation
system and its use by a large number of historians in the context of an ongoing
research project in the field of History of Art. The system is Web-based and
collaborative, and makes use of existing standards for information
documentation and publication (CIDOC-CRM, RDF), focusing on semantic
interoperability and the production of data of high value and long-term
validity.

</details>


### [40] [FAST CAT: Collaborative Data Entry and Curation for Semantic Interoperability in Digital Humanities](https://arxiv.org/abs/2105.13733)
*Pavlos Fafalios,Kostas Petrakis,Georgios Samaritakis,Korina Doerr,Athina Kritsotaki,Yannis Tzitzikas,Martin Doerr*

Main category: cs.DL

TL;DR: FAST CAT is a new system to help researchers manage data in fields like history, improving on old spreadsheet methods by making data more reusable and verifiable, as shown in a project about Mediterranean steamboats.


<details>
  <summary>Details</summary>
Motivation: To cope with the limitations of current data management practices in descriptive and empirical sciences, such as the high dependency of data on initial research hypothesis, lack of representation of inferred relations, and difficulty in revisiting original data sources.

Method: The paper describes the challenges, the overall methodology for supporting semantic interoperability, and discusses the use of FAST CAT in a European (ERC) project of Maritime History called SeaLiT.

Result: FAST CAT, a collaborative system for assistive data entry and curation, is presented as a solution to improve data management in empirical research, with a case study in Maritime History.

Conclusion: The paper presents FAST CAT, a collaborative system for assistive data entry and curation in Digital Humanities and empirical research, addressing limitations of traditional spreadsheet and database systems.

Abstract: Descriptive and empirical sciences, such as History, are the sciences that
collect, observe and describe phenomena in order to explain them and draw
interpretative conclusions about influences, driving forces and impacts under
given circumstances. Spreadsheet software and relational database management
systems are still the dominant tools for quantitative analysis and overall data
management in these these sciences, allowing researchers to directly analyse
the gathered data and perform scholarly interpretation. However, this current
practice has a set of limitations, including the high dependency of the
collected data on the initial research hypothesis, usually useless for other
research, the lack of representation of the details from which the registered
relations are inferred, and the difficulty to revisit the original data sources
for verification, corrections or improvements. To cope with these problems, in
this paper we present FAST CAT, a collaborative system for assistive data entry
and curation in Digital Humanities and similar forms of empirical research. We
describe the related challenges, the overall methodology we follow for
supporting semantic interoperability, and discuss the use of FAST CAT in the
context of a European (ERC) project of Maritime History, called SeaLiT, which
examines economic, social and demographic impacts of the introduction of
steamboats in the Mediterranean area between the 1850s and the 1920s.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [41] [Excel: Automated Ledger or Analytics IDE?](https://arxiv.org/abs/2409.12976)
*Andrew Kumiega*

Main category: cs.CY

TL;DR: Excel已演变为分析IDE，需更新风险管理框架。


<details>
  <summary>Details</summary>
Motivation: 阐述了Excel已成为一个功能完备的分析IDE，但目前的风险管理框架并未跟上这一转变，需要进行扩展以管理由此产生的风险。

Method: 分析Excel从桌面应用程序到分析IDE的演变过程，并阐述了现有风险管理框架的不足之处。

Result: 明确了Excel作为分析IDE的重要性，并指出了扩展现有风险管理框架以应对相关风险的必要性。

Conclusion: Excel已从简单的账簿自动化工具转变为分析的集成开发环境（IDE），需要扩展现有的风险管理框架以应对其作为分析IDE带来的日益增长的风险。

Abstract: Since the inception of VisiCalc over four decades ago, spreadsheets have
undergone a gradual transformation, evolving from simple ledger automation
tools to the current state of Excel, which can be described as an Integrated
Development Environment (IDE) for analytics. The slow evolution of Excel from
an automation tool for ledgers to an IDE for analytics explains why many people
have not noticed that Excel includes a fully functional database, an OLAP
Engine, multiple statistical programming languages, multiple third-party
software libraries, dynamic charts, and real time data connectors. The
simplicity of accessing these multiple tools is a low-code framework controlled
from the Excel tool that is effectively an IDE. Once we acknowledge Excel's
shift from a desk top application to an IDE for analytics, the importance of
establishing a comprehensive risk framework for managing this distinctive
development environment becomes clear. In this paper we will explain how the
current risk framework for spreadsheets needs to be expanded to manage the
growing risks of using Excel as an IDE for analytics.

</details>


### [42] [Improving Feedback from Automated Reviews of Student Spreadsheets](https://arxiv.org/abs/2311.10728)
*Sören Aguirre Reid,Frank Kammer,Jonas-Ian Kuche,Pia-Doreen Ritzke,Markus Siepermann,Max Stephan,Armin Wagenknecht*

Main category: cs.CY

TL;DR: 开发了一个智能辅导系统（ITS），可以自动评估学生的Excel作业，并通过分层反馈帮助学生改进。


<details>
  <summary>Details</summary>
Motivation: 现有针对教学场景的电子表格作业评估的数字解决方案不足，因此需要开发一个自动评估和反馈的系统。

Method: 开发了一个包含值匹配、公式详细分析和质量评估等多种分析方式的智能辅导系统（ITS），并设计了分层反馈机制以适应不同的学习水平。

Result: 分层反馈机制能够提高正确作业的提交百分比，并且学生认为反馈清晰易懂且有帮助。

Conclusion: 该研究开发了一个智能辅导系统（ITS），用于自动评估学生的Excel作业并提供个性化反馈，提高了作业的正确率，并得到了学生的积极评价。

Abstract: Spreadsheets are one of the most widely used tools for end users. As a
result, spreadsheets such as Excel are now included in many curricula. However,
digital solutions for assessing spreadsheet assignments are still scarce in the
teaching context. Therefore, we have developed an Intelligent Tutoring System
(ITS) to review students' Excel submissions and provide individualized feedback
automatically. Although the lecturer only needs to provide one reference
solution, the students' submissions are analyzed automatically in several ways:
value matching, detailed analysis of the formulas, and quality assessment of
the solution. To take the students' learning level into account, we have
developed feedback levels for an ITS that provide gradually more information
about the error by using one of the different analyses. Feedback at a higher
level has been shown to lead to a higher percentage of correct submissions and
was also perceived as well understandable and helpful by the students.

</details>


### [43] [How Beaufort, Neumann and Gates met? Subject integration with spreadsheeting](https://arxiv.org/abs/2309.12353)
*Maria Csernoch,Julia Csernoch*

Main category: cs.CY

TL;DR: 计算思维应成为继阅读、写作和算术之后的第四项基本技能。本研究提出了一种新颖的方法，通过将 Beaufort 量表应用于传统的纸质问题和数据检索过程，以支持学科整合和数字图式的构建。研究发现，该方法能同时提升学生的内容知识和数字技能。


<details>
  <summary>Details</summary>
Motivation: 计算思维应该是继读、写、算（3R）之后的第四项基本技能。为了达到计算思维技能，尤其是数字问题解决方法拥有自己的图式的水平，还有很长的路要走。

Method: 本研究提出了一种新颖的方法，该方法在一个广为人知的 Beaufort 量表上，支持学科整合和数字图式的构建。研究提出了在八年级行动研究的框架内，将传统的纸质问题和数据检索过程进行转换。

Result: 研究发现，与传统的教科书和非情境化的数字环境相比，学生的内容知识和数字技能都得到了更有效的提升。

Conclusion: 该方法可以应用于任何基于纸质的问题，这些问题在数字环境中解决会更有效，并且可以在学科内容和信息学方面为构建图式提供各种形式。

Abstract: Computational thinking should be the fourth fundamental skill, along with
reading, writing, and arithmetic (3R). To reach the level where computational
thinking skills, especially digital problem solving have their own schemata,
there is a long way to go. In the present paper, a novel approach is detailed
to support subject integration and building digital schemata, on the well-known
Beaufort scale. The conversion of a traditional, paper-based problem and a data
retrieval process are presented within the frame of a Grade 8 action research
study. It is found that both students content knowledge and their digital
skills developed more efficiently than in traditional course book and
decontextualized digital environments. Furthermore, the method presented here
can be adapted to any paper-based problems whose solutions would be more
effective in a digital environment and which offer various forms for building
schemata both in the subject matter and informatics.

</details>


### [44] [A Simpler Method for Understanding Emergency Shelter Access Patterns](https://arxiv.org/abs/2210.13619)
*Geoffrey G. Messier*

Main category: cs.CY

TL;DR: SAM is a new, user-friendly metric for analyzing shelter client access patterns and vulnerability, requiring less data than traditional methods and offering real-time insights into external factors' impact.


<details>
  <summary>Details</summary>
Motivation: The goal of SAM is to provide shelter operators with an intuitive way to understand access patterns that can be implemented by non-technical staff using spreadsheet operations.

Method: The Simplified Access Metric (SAM) uses client data from a large North American shelter to characterize emergency shelter access patterns as a measure of shelter client vulnerability. SAM produces similar results to traditional transitional, episodic and chronic client cluster analysis but requires less data, allowing for real-time analysis of external factors' impact on shelter access. Timelines generated from nine years of data demonstrate the impact of Housing First programming and COVID-19 lockdown on shelter access.

Result: SAM produces similar results to traditional cluster analysis while requiring less data, enabling real-time analysis. It demonstrates the impact of Housing First programming and COVID-19 lockdown on shelter access and provides a direct measure of vulnerability.

Conclusion: "SAM allows shelter staff to move beyond assigning transitional, episodic and chronic labels and instead use the "soft" output of SAM directly as a measure of vulnerability."

Abstract: The Simplified Access Metric (SAM) is a new approach for characterizing
emergency shelter access patterns as a measure of shelter client vulnerability.
The goal of SAM is to provide shelter operators with an intuitive way to
understand access patterns that can be implemented by non-technical staff using
spreadsheet operations. Client data from a large North American shelter will be
used to demonstrate that SAM produces similar results to traditional
transitional, episodic and chronic client cluster analysis. Since SAM requires
less data than cluster analysis, it is also able to generate a real time
picture of how shelter access patterns are affected by external factors.
Timelines generated from nine years of shelter client data using SAM
demonstrate the impact of Housing First programming and the COVID-19 lockdown
on how people access shelter. Finally, SAM allows shelter staff to move beyond
assigning transitional, episodic and chronic labels and instead use the "soft"
output of SAM directly as a measure of vulnerability.

</details>


### [45] [Long-Term Mentoring for Computer Science Researchers](https://arxiv.org/abs/2208.04738)
*Emily Ruppel,Sihang Liu,Elba Garza,Sukyoung Ryu,Alexandra Silva,Talia Ringer*

Main category: cs.CY

TL;DR: PL和CA社区领导者建立了SIGPLAN-M和CALM两个长期指导计划，以解决社区内联系固化问题，并取得了显著成效，希望推动更广泛的计划。


<details>
  <summary>Details</summary>
Motivation: 解决社区内只能通过已有的联系形成新联系的问题，建立长期的指导关系

Method: 在PL和CA社区中建立了长期的指导计划SIGPLAN-M和CALM

Result: SIGPLAN-M覆盖328名受指导者和234名指导者，遍布41个国家，被誉为“改变人生”和“挽救职业”；CALM处于试点阶段，已有13名指导者和21名受指导者，遍布7个国家，反馈积极

Conclusion: 希望借此启动计算机科学领域更大范围的长期指导计划

Abstract: Early in the pandemic, we -- leaders in the research areas of programming
languages (PL) and computer architecture (CA) -- realized that we had a
problem: the only way to form new lasting connections in the community was to
already have lasting connections in the community. Both of our academic
communities had wonderful short-term mentoring programs to address this
problem, but it was clear that we needed long-term mentoring programs.
  Those of us in CA approached this scientifically, making an evidence-backed
case for community-wide long-term mentoring. In the meantime, one of us in PL
had impulsively launched an unofficial long-term mentoring program, founded on
chaos and spreadsheets. In January 2021, the latter grew to an official
cross-institutional long-term mentoring program called SIGPLAN-M; in January
2022, the former grew to Computer Architecture Long-term Mentoring (CALM).
  The impacts have been strong: SIGPLAN-M reaches 328 mentees and 234 mentors
across 41 countries, and mentees have described it as "life changing" and "a
career saver." And while CALM is in its pilot phase -- with 13 mentors and 21
mentees across 7 countries -- it has received very positive feedback. The
leaders of SIGPLAN-M and CALM shared our designs, impacts, and challenges along
the way. Now, we wish to share those with you. We hope this will kick-start a
larger long-term mentoring effort across all of computer science.

</details>


### [46] [Optimization Helps Scheduling Nursing Staff at the Long-Term Care Homes of the City of Toronto](https://arxiv.org/abs/2102.09461)
*Manion Anderson,Merve Bodur,Scott Rathwell,Vahid Sarhangian*

Main category: cs.CY

TL;DR: 多伦多长期护理服务部开发了一个优化的排班工具，用时更少，满意度更高。


<details>
  <summary>Details</summary>
Motivation: 为了应对多伦多市长期护理服务部在为养老院护理人员排班方面遇到的挑战，特别是要减少因排班困难和兼职护士缺勤率高企的问题。

Method: 开发了一个基于层次优化模型的电子表格排班工具，该模型能在满足工龄优先分配原则的前提下，最大化总偏好得分并满足尽可能多的需求。

Result: 该工具在多伦多一家391个床位的养老院实施后，将生成可行排班表的时间从手动方式下的数小时缩短至不到一小时。此外，排班表成功考虑了护理人员的偏好，平均有94%的排班任务被安排为最优先的班次。

Conclusion: 该排班工具能够高效生成满足复杂约束和个人偏好的可行排班表，显著减少了排班时间并提高了班次偏好满足率。

Abstract: The City of Toronto Long Term Care Homes & Services (LTCH&S) division is one
of the largest providers of long-term care in the Canadian province of Ontario,
providing care to 2,640 residents at 10 homes across Toronto. Our collaboration
with LTCH&S was initiated to facilitate the increasingly challenging task of
scheduling nursing staff and reduce high absenteeism rate observed among the
part-time nurses. We developed a spreadsheet-based scheduling tool to automate
the generation of schedules and incorporate nurses' preferences for different
shifts into the schedules. At the core of the scheduling tool is a hierarchical
optimization model that generates a feasible schedule with the highest total
preference score while satisfying the maximum possible demand. Feasible
schedules had to abide by a set of complex seniority requirements which
prioritized more senior nurses when allocating the available shifts. Our
scheduling tool was implemented in a 391-bed home in Toronto. The tool allowed
nursing managers to generate feasible schedules within a fraction of an hour,
in contrast to the status-quo manual approach which could took up to tens of
hours. In addition, the schedules successfully accounted for preferences with
on average above 94% of the allocated shifts ranked as most preferred.

</details>


### [47] [Developing Excel Thought Leadership](https://arxiv.org/abs/2006.04793)
*David Lyford-Smith*

Main category: cs.CY

TL;DR: ICAEW发布了三篇关于电子表格良好实践的思想领导力论文，这些论文总结了关键经验教训，并巩固了ICAEW在该领域的地位。


<details>
  <summary>Details</summary>
Motivation: 介绍ICAEW在过去五年中关于电子表格良好实践的思想领导力论文的发展历程。

Method: 对ICAEW五年间发布的关于电子表格良好实践的三篇思想领导力论文进行回顾，分析其关键经验教训，并讨论该过程如何帮助ICAEW在该领域发展其地位。

Result: ICAEW成功地围绕电子表格的良好实践制定了一系列思想领导力论文，并在此过程中发展了其在该领域的地位。

Conclusion: ICAEW通过发布关于电子表格良好实践的思想领导力论文，巩固了其在该领域的地位。

Abstract: Over a period of five years, the Institute of Chartered Accountants in
England and Wales (ICAEW) has developed a suite of three 'thought leadership'
papers surrounding good practice in spreadsheet use and spreadsheet work
environments. We will review the history of these three papers, the key lessons
which each has to teach, and discuss how the process of making them has helped
ICAEW to develop its position in the field.

</details>


### [48] [A Case Study of Spreadsheet Use within the Finance and Academic Registry units within a Higher Education Institution](https://arxiv.org/abs/1909.07462)
*Simon Thorne,Jamie Hancock*

Main category: cs.CY

TL;DR: 本研究考察了英国一所高等教育机构学术注册处和财务部门的电子表格使用情况，发现需要制定明确的电子表格开发指南，以提高数据完整性和效率。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨英国一所高等教育机构的电子表格使用情况，并考虑其对准确报告、内部数据完整性和效率的影响。

Method: 通过对英国一所高等教育机构的两个部门（学术注册处和财务部）的电子表格使用情况进行案例研究，探讨了电子表格在重要性、培训、经验、目的、部署的技术、创建的电子表格的大小和共享方面的使用情况。

Result: 研究结果显示，机构创建和使用的电子表格数量庞大，电子表格开发者的画像与其他电子表格使用研究中的典型情况一致。

Conclusion: 本研究结果表明，需要组织制定明确的电子表格模型开发原则和指南，以确保数据完整性、减少重复工作并优化电子表格的使用以实现机构目标。

Abstract: This paper presents the findings of a case study of spreadsheet use in a
higher education institution in the UK. The paper considers the use of
spreadsheets in two units of the organisation, academic registry and finance.
Spreadsheet use is explored in terms of importance, training, experience,
purpose, techniques deployed, size of spreadsheets created and sharing of
spreadsheets. The implications of the results are then considered in terms of
accurate reporting to external funding bodies such the funding councils,
internal data integrity and internal data efficiencies. The results show a
large volume of spreadsheets being created and used, that the profile of
spreadsheet developers is typical of other studies of spreadsheet use and the
need for the organisation to have clear principles and guidelines for the
development of spreadsheet models in the organisation to ensure data integrity,
reduce duplication of effort and to optimise the use of spreadsheets to meet
the institutions goals.

</details>


### [49] [Computer Simulation of Neural Networks Using Spreadsheets: The Dawn of the Age of Camelot](https://arxiv.org/abs/1807.00018)
*Serhiy O. Semerikov,Illia O. Teplytskyi,Yuliia V. Yechkalo,Arnold E. Kiv*

Main category: cs.CY

TL;DR: 本篇论文探讨了在电子表格环境中模拟训练神经网络的必要性和方法，回顾了现有技术，并强调了掌握基于历史和遗传方法的模型以获得相关能力的重要性，同时指出了有前景的几类模型。


<details>
  <summary>Details</summary>
Motivation: 证实了在电子表格环境中开发神经网络计算机模拟训练方法的必要性。

Method: 对神经网络在电子表格环境中的应用的系统性回顾，区分了解决网络计算机模拟训练问题的基本方法，包括电子表格和神经网络模拟工具的联合应用、使用电子表格的第三方插件、使用电子表格的嵌入式语言开发宏、使用标准电子表格插件进行非线性优化、以及在没有插件和宏的情况下在电子表格环境中创建神经网络。

Result: 在分析了1890-1950年的文献后，确定了科学期刊“数学生物物理学公报”、其创始人尼古拉斯·拉舍夫斯基以及围绕该期刊的科学界在创建和发展计算神经科学模型和方法方面所起的作用。确定了神经网络创建的心理物理学基础、神经计算的数学基础和神经工程方法（特别是图像识别）。讨论了沃尔特·皮茨在结合训练的描述性和定量理论方面的作用。

Conclusion: 掌握基于历史和遗传方法的模型，有助于在电子表格环境中获得神经模拟能力。

Abstract: The article substantiates the necessity to develop training methods of
computer simulation of neural networks in the spreadsheet environment. The
systematic review of their application to simulating artificial neural networks
is performed. The authors distinguish basic approaches to solving the problem
of network computer simulation training in the spreadsheet environment, joint
application of spreadsheets and tools of neural network simulation, application
of third-party add-ins to spreadsheets, development of macros using the
embedded languages of spreadsheets; use of standard spreadsheet add-ins for
non-linear optimization, creation of neural networks in the spreadsheet
environment without add-ins and macros. After analyzing a collection of
writings of 1890-1950, the research determines the role of the scientific
journal "Bulletin of Mathematical Biophysics", its founder Nicolas Rashevsky
and the scientific community around the journal in creating and developing
models and methods of computational neuroscience. There are identified
psychophysical basics of creating neural networks, mathematical foundations of
neural computing and methods of neuroengineering (image recognition, in
particular). The role of Walter Pitts in combining the descriptive and
quantitative theories of training is discussed. It is shown that to acquire
neural simulation competences in the spreadsheet environment, one should master
the models based on the historical and genetic approach. It is indicated that
there are three groups of models, which are promising in terms of developing
corresponding methods - the continuous two-factor model of Rashevsky, the
discrete model of McCulloch and Pitts, and the discrete-continuous models of
Householder and Landahl.

</details>


### [50] [Edu-Edition Spreadsheet Competency Framework](https://arxiv.org/abs/1802.00496)
*Maria Csernoch,Piroska Biró*

Main category: cs.CY

TL;DR: 该研究提出了E2SCF，一个旨在从早期教育开始培养学生电子表格能力的框架，并强调专家教师的支持和多种能力的融合。


<details>
  <summary>Details</summary>
Motivation: 在金融专业人士的电子表格能力框架的基础上，提出E2SCF，旨在从早期教育开始培养学生的电子表格能力，并认为专家教师的支持可以使这一过程更加有效。

Method: 提出Edu-Edition of the Spreadsheet Competency Framework (E2SCF)，该框架基于电子表格能力框架，强调从早期教育开始培养学生的能力，并融合了高数学能力、计算机支持的真实世界问题解决、双向知识转移、数据和错误分析以及编程等方面。

Result: E2SCF 具备高数学能力、计算机支持的真实世界问题解决能力、双向知识转移、数据和错误分析以及编程等特点，旨在帮助基础用户建立扎实的电子表格知识，并培养可转移的问题解决技能和能力。

Conclusion: 该研究提出了Edu-Edition of the Spreadsheet Competency Framework (E2SCF)，一个基于电子表格能力框架的教育版本，旨在从早期教育开始培养学生的电子表格能力，并强调专家教师的支持、高数学能力、计算机支持的真实世界问题解决、双向知识转移、数据和错误分析以及编程方面。

Abstract: Based on the Spreadsheet Competency Framework for finance professionals, in
the present paper we introduce the Edu-Edition of the Spreadsheet Competency
Framework (E2SCF). We claim that building spreadsheet competences should start
in education, as early as possible, and this process is a lot more effective if
support arrives from expert teachers. The main feature of E2SCF is high
mathability computer-supported real world problem solving. This approach is
based on - from the very beginning of training - a two-directional knowledge
transfer, data and error analysis and handling, and the programming aspect of
spreadsheets. Based on these features, E2SCF is set up for basic and general
users to build up firm spreadsheet knowledge and to develop transferable
problem solving skills and competences.

</details>


### [51] [The Future of Spreadsheets in the Big Data Era](https://arxiv.org/abs/1801.10231)
*David Birch,David Lyford-Smith,Yike Guo*

Main category: cs.CY

TL;DR: Spreadsheets remain ubiquitous but are facing changes due to Big Data, end-user computing, and mobile computing. A workshop explored these trends, discussed future directions, and identified research areas and user implications.


<details>
  <summary>Details</summary>
Motivation: To explore the future of spreadsheet technology by examining its success, trends driving change, and likely future directions.

Method: Workshop convening academia and industry to discuss the future of spreadsheet technology.

Result: The paper records participants' views on the reasons for spreadsheet success, trends driving change, and future directions, and sets out key research directions and implications for end-users.

Conclusion: This paper discusses the future of spreadsheet technology, driven by Big Data, end-user computing, and mobile computing, and outlines key research directions and implications for end-users.

Abstract: The humble spreadsheet is the most widely used data storage, manipulation and
modelling tool. Its ubiquity over the past 30 years has seen its successful
application in every area of life. Surprisingly the spreadsheet has remained
fundamentally unchanged over the past three decades. As spreadsheet technology
enters its 4th decade a number of drivers of change are beginning to impact
upon the spreadsheet. The rise of Big Data, increased end-user computing and
mobile computing will undoubtedly increasingly shape the evolution and use of
spreadsheet technology.
  To explore the future of spreadsheet technology a workshop was convened with
the aim of "bringing together academia and industry to examine the future
direction of spreadsheet technology and the consequences for users". This paper
records the views of the participants on the reasons for the success of the
spreadsheet, the trends driving change and the likely directions of change for
the spreadsheet. We then set out key directions for further research in the
evolution and use of spreadsheets. Finally we look at the implications of these
trends for the end users who after all are the reason for the remarkable
success of the spreadsheet.

</details>


### [52] [The Role of Spreadsheets in Clinical Decision Support: A Survey of the Medical Algorithms Company User Community](https://arxiv.org/abs/1801.07782)
*Simon Thorne*

Main category: cs.CY

TL;DR: This paper analyzes a survey of CDSS users, finding they are used in various clinical settings as operational, research, and reference tools, with logic often developed on spreadsheets.


<details>
  <summary>Details</summary>
Motivation: To contribute to the wider understanding of how CDSS impact on clinical practice.

Method: A small scoping survey of Clinical Decision Support System (CDSS) users from the Medical Algorithms Company website.

Result: The survey results show that CDSS provided by Medal are being used by clinical professionals in a variety of settings, both as an operational tool and as a research and reference tool. The initial logic for these tools is worked out on a spreadsheet, and the paper describes this process and examines some of the survey results.

Conclusion: CDSS provided by Medal are being used by clinical professionals in a variety of settings, both as an operational tool and as a research and reference tool.

Abstract: This paper presents and discusses the results of a small scoping survey of
Clinical Decision Support System (CDSS) users from the Medical Algorithms
Company website which hosts 24,000 different CDSS. These results are analysed,
discussed, and compared with other similar studies and contribute to the wider
understanding of how CDSS impact on clinical practice. The results show that
CDSS provided by Medal are being used by clinical professionals in a variety of
settings, both as an operational tool and as a research and reference tool.
Whilst these tools are implemented and executed in a database, the initial
logic is worked out on a spreadsheet. The paper describes that process and
examines some of the results of the survey.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [53] [Using a Catenary Trajectory to Reduce Wellbore Friction in Horizontal Extended Reach Drilling](https://arxiv.org/abs/2309.12317)
*Vu Nguyen*

Main category: cs.RO

TL;DR: 为了减少钻井成本，研究人员提出了一种名为“猫爬器”的自然悬链线轨迹设计方法，以替代传统的2D弧设计，并提供了易于使用的Excel工具来帮助进行设计。


<details>
  <summary>Details</summary>
Motivation: 钻井过程中的井筒摩擦是影响总成本的主要因素之一，因此需要新的方法来减少这种摩擦。

Method: 提出了一种猫爬器轨迹设计方法，并与传统的2D弧设计进行了比较。计算过程已包含在易于使用的MS Excel电子表格中。

Result: 与传统的2D弧设计相比，猫爬器轨迹设计有望在延伸井时有效减少扭矩和阻力，从而降低成本。

Conclusion: 该项目通过引入猫爬器概念并进行详细分析来解决井筒摩擦问题，旨在通过优化钻井轨迹来降低成本。

Abstract: Wellbore friction is one of the biggest concerns when drilling due to its
relation to the total cost. The catenary concept was introduced to reduce
wellbore friction, but it requires detailed analyses. This project would fill
this gap. A catenary shape is simply the natural shape of a rope, chain, or
drill string. The drill string will then hang freely inside the wellbore.
Perfectly, there should be no contact between the hole and the string, and thus
no friction. Torque and drag should be minimized this way. A case study is
introduced to examine the outcome between Catenary Trajectory Design and
traditional 2D Arc design. The calculation procedure of Catenary Trajectory and
2D Arc Design can be found in an MS Excel spreadsheet which is easy to use and
reliable for designing catenary well trajectories for extended-reach wells.

</details>


### [54] [Hazard Analysis of Collaborative Automation Systems: A Two-layer Approach based on Supervisory Control and Simulation](https://arxiv.org/abs/2209.12560)
*Tom P. Huck,Yuvaraj Selvaraj,Constantin Cronrath,Christoph Ledermann,Martin Fabian,Bengt Lennartson,Torsten Kröger*

Main category: cs.RO

TL;DR: 提出一种结合形式化方法和仿真的两层危害分析方法，以应对日益复杂的安全关键系统。


<details>
  <summary>Details</summary>
Motivation: 随着系统复杂性的增加，传统的基于人类推理和经验的危害分析方法越来越不适用，而基于测试的方法成本高昂或存在危险。需要一种新的危害分析方法。

Method: 提出了一种两层方法，首先使用基于监督控制理论的形式化模型综合不安全行为，然后将结果输入仿真以进行基于领域特定风险指标的详细分析。

Result: 该方法能够结合形式化方法和仿真的优点，对安全关键系统进行更全面、更详细的危害分析。

Conclusion: 该方法结合了形式化方法和仿真的优点，用于对安全关键系统进行危害分析，并以工业人机协作系统为例进行了演示。

Abstract: Safety critical systems are typically subjected to hazard analysis before
commissioning to identify and analyse potentially hazardous system states that
may arise during operation. Currently, hazard analysis is mainly based on human
reasoning, past experiences, and simple tools such as checklists and
spreadsheets. Increasing system complexity makes such approaches decreasingly
suitable. Furthermore, testing-based hazard analysis is often not suitable due
to high costs or dangers of physical faults. A remedy for this are model-based
hazard analysis methods, which either rely on formal models or on simulation
models, each with their own benefits and drawbacks. This paper proposes a
two-layer approach that combines the benefits of exhaustive analysis using
formal methods with detailed analysis using simulation. Unsafe behaviours that
lead to unsafe states are first synthesised from a formal model of the system
using Supervisory Control Theory. The result is then input to the simulation
where detailed analyses using domain-specific risk metrics are performed.
Though the presented approach is generally applicable, this paper demonstrates
the benefits of the approach on an industrial human-robot collaboration system.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [55] [Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE](https://arxiv.org/abs/2506.17330)
*Simon Thorne*

Main category: cs.SE

TL;DR: This paper benchmarks LLMs on spreadsheet tasks, finding they excel at simple operations but struggle with complex logic, indicating a need for enhanced reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: While LLMs show promise in many domains, their effectiveness in spreadsheet-related tasks is underexplored. This study aims to establish a benchmark to evaluate and understand LLM capabilities in this area.

Method: The study introduces a benchmark framework to evaluate LLM performance on spreadsheet tasks including function execution, formula generation, and data manipulation. It encompasses tasks from basic formula creation to complex, real-world scenarios. The FLARE benchmark is also introduced for evaluating LLM performance on spreadsheet logic, auditing, and reasoning.

Result: LLMs demonstrate proficiency in simple spreadsheet tasks but often fail in complex, multi-step operations, generating incorrect outputs. This highlights the need for improved logical reasoning in LLMs for such tasks.

Conclusion: LLMs currently struggle with complex, multi-step spreadsheet tasks, often producing plausible but incorrect outputs. Integrating symbolic reasoning capabilities into LLM architectures is necessary to address these limitations.

Abstract: Large Language Models (LLMs) have demonstrated some significant capabilities
across various domains; however, their effectiveness in spreadsheet related
tasks remains underexplored. This study introduces a foundation for a
comprehensive benchmark framework to evaluate the performance of leading LLMs
in executing spreadsheet functions, formula generation and data manipulation
tasks. The benchmark encompasses tasks ranging from basic formula creation to
complex, real world spreadsheet scenarios. Our findings reveal that while LLMs
exhibit proficiency in straightforward tasks, they often falter in complex,
multi step operations, frequently producing plausible yet incorrect outputs.
These results underscore the limitations of current LLMs in handling
spreadsheet tasks that require precise logical reasoning and highlight the need
for integrating symbolic reasoning capabilities into LLM architectures. To
support this, we introduce FLARE (Formula Logic, Auditing, Reasoning and
Evaluation) a new benchmark for evaluating LLM performance on real-world
spreadsheet logic, auditing, and reasoning tasks.

</details>


### [56] [TableTalk: Scaffolding Spreadsheet Development with a Language Agent](https://arxiv.org/abs/2502.09787)
*Jenny T. Liang,Aayush Kumar,Yasharth Bajpai,Sumit Gulwani,Vu Le,Chris Parnin,Arjun Radhakrishna,Ashish Tiwari,Emerson Murphy-Hill,Guastavo Soares*

Main category: cs.SE

TL;DR: TableTalk 是一个语言代理，可以帮助用户通过增量和分步方法以对话方式构建电子表格。


<details>
  <summary>Details</summary>
Motivation: 由于电子表格编程需要特定于电子表格的知识和解决问题的能力，因此具有挑战性。LLM 和语言代理可以帮助自动化此过程。

Method: TableTalk 是一个语言代理，通过生成分步计划和建议用户可以选择的三个后续步骤来构建电子表格。它还集成了增量式电子表格构建工具。

Result: 与基线代理相比，TableTalk 产生的电子表格更受青睐的可能性是其 2.3 倍，同时将认知负担和用于推理电子表格操作的时间减少了 12.6%。

Conclusion: TableTalk 的方法对人机协作有影响，包括为停止或撤销代理操作提供持久的直接操作界面，同时确保可以禁用接受操作的界面。

Abstract: Despite its ubiquity in the workforce, spreadsheet programming remains
challenging as programmers need both spreadsheet-specific knowledge (e.g., APIs
to write formulas) and problem-solving skills to create complex spreadsheets.
Large language models (LLMs) can help automate aspects of this process, and
recent advances in planning and reasoning have enabled language agents, which
dynamically plan, use tools, and take iterative actions to complete complex
tasks. These agents observe, plan, and act, making them well-suited to scaffold
spreadsheet programming by following expert processes.
  We present TableTalk, a language agent that helps programmers build
spreadsheets conversationally. Its design reifies three design principles --
scaffolding, flexibility, and incrementality -- which we derived from two
studies of seven programmers and 62 Excel templates. TableTalk structures
spreadsheet development by generating step-by-step plans and suggesting three
next steps users can choose from. It also integrates tools that enable
incremental spreadsheet construction. A user study with 20 programmers shows
that TableTalk produces spreadsheets 2.3 times more likely to be preferred over
a baseline agent, while reducing cognitive load and time spent reasoning about
spreadsheet actions by 12.6%. TableTalk's approach has implications for
human-agent collaboration. This includes providing persistent direct
manipulation interfaces for stopping or undoing agent actions, while ensuring
that such interfaces for accepting actions can be deactivated.

</details>


### [57] [Overcoming Vision Language Model Challenges in Diagram Understanding: A Proof-of-Concept with XML-Driven Large Language Models Solutions](https://arxiv.org/abs/2502.04389)
*Shue Shiinoki,Ryo Koshihara,Hayato Motegi,Masumi Morishige*

Main category: cs.SE

TL;DR: 本研究提出一种文本驱动方法，直接从 XLSX 等源文件提取图表元数据，利用 LLM 分析图表关系，无需 VLM 即可实现更准确的图表问答。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLM）在准确识别和提取图表结构与关系方面仍存在挑战。本研究旨在提出一种新的方法，利用可编辑源文件中的文本元数据，绕过 VLM 的视觉识别瓶颈，实现对图表的理解。

Method: 提出一种文本驱动的方法，利用可编辑源文件（如 XLSX）中的文本元数据来提取图表信息，并将提取的形状数据转换为文本输入，供大语言模型（LLM）进行分析和问答，无需依赖 VLM 的视觉识别能力。

Result: 实验结果表明，与基于 VLM 的方法相比，所提出的文本驱动框架在需要详细理解图表结构的问答任务中，能够提供更准确的答案。该方法不仅适用于 XLSX 文件，还可以扩展到其他包含源文件的文档格式，如 Office 的 PPTX 和 DOCX。

Conclusion: 该方法通过直接从源文件（如 XLSX、PPTX、DOCX）提取文本元数据，为大语言模型（LLM）提供了分析图表结构和关系的途径，有效规避了视觉语言模型（VLM）在图像识别方面的局限性，并在需要详细理解图表结构的问答任务中展现出更高的准确性。

Abstract: Diagrams play a crucial role in visually conveying complex relationships and
processes within business documentation. Despite recent advances in
Vision-Language Models (VLMs) for various image understanding tasks, accurately
identifying and extracting the structures and relationships depicted in
diagrams continues to pose significant challenges. This study addresses these
challenges by proposing a text-driven approach that bypasses reliance on VLMs'
visual recognition capabilities. Instead, it utilizes the editable source
files--such as xlsx, pptx or docx--where diagram elements (e.g., shapes, lines,
annotations) are preserved as textual metadata. In our proof-of-concept, we
extracted diagram information from xlsx-based system design documents and
transformed the extracted shape data into textual input for Large Language
Models (LLMs). This approach allowed the LLM to analyze relationships and
generate responses to business-oriented questions without the bottleneck of
image-based processing. Experimental comparisons with a VLM-based method
demonstrated that the proposed text-driven framework yielded more accurate
answers for questions requiring detailed comprehension of diagram
structures.The results obtained in this study are not limited to the tested
.xlsx files but can also be extended to diagrams in other documents with source
files, such as Office pptx and docx formats. These findings highlight the
feasibility of circumventing VLM constraints through direct textual extraction
from original source files. By enabling robust diagram understanding through
LLMs, our method offers a promising path toward enhanced workflow efficiency
and information analysis in real-world business scenarios.

</details>


### [58] [On the Workflows and Smells of Leaderboard Operations (LBOps): An Exploratory Study of Foundation Model Leaderboards](https://arxiv.org/abs/2407.04065)
*Zhimin Zhao,Abdul Ali Bangash,Filipe Roseiro Côgo,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本研究通过分析1045个FM排行榜，识别了五种工作流程模式和八种“排行榜异味”，旨在提高FM评估和选择的透明度和效率。


<details>
  <summary>Details</summary>
Motivation: 基础模型（FM）在软件工程（SE）任务中表现出卓越的适应性，使得FM排行榜成为SE团队必不可少的工具。然而，缺乏标准化的FM评估和比较指南威胁到FM排行榜的透明度，并限制了利益相关者进行有效FM选择的能力。

Method: 通过收集来自GitHub、Hugging Face Spaces、Papers With Code、电子表格和独立平台等五个不同来源的1045个FM排行榜，检查其文档并通过直接沟通了解排行榜运营者的工作流程。通过卡片分类和协商一致，我们确定了五种不同的工作流程模式，并开发了一个捕获这些工作流程中关键组成部分及其交互的领域模型。然后，我们识别了LBOps中八种独特的排行榜异味类型。

Result: 确定了五种不同的工作流程模式，并开发了一个领域模型，捕获了这些工作流程中的关键组成部分及其交互。此外，还识别了LBOps中八种独特的排行榜异味类型。

Conclusion: 通过识别和缓解“排行榜异味”，软件工程（SE）团队可以提高透明度、可问责性和协作性，从而为基础模型（FM）的比较和选择营造一个更强大、更负责任的生态系统。

Abstract: Foundation models (FM), such as large language models (LLMs), which are
large-scale machine learning (ML) models, have demonstrated remarkable
adaptability in various downstream software engineering (SE) tasks, such as
code completion, code understanding, and software development. As a result, FM
leaderboards have become essential tools for SE teams to compare and select the
best third-party FMs for their specific products and purposes. However, the
lack of standardized guidelines for FM evaluation and comparison threatens the
transparency of FM leaderboards and limits stakeholders' ability to perform
effective FM selection. As a first step towards addressing this challenge, our
research focuses on understanding how these FM leaderboards operate in
real-world scenarios ("leaderboard operations") and identifying potential
pitfalls and areas for improvement ("leaderboard smells"). In this regard, we
collect up to 1,045 FM leaderboards from five different sources: GitHub,
Hugging Face Spaces, Papers With Code, spreadsheet and independent platform, to
examine their documentation and engage in direct communication with leaderboard
operators to understand their workflows. Through card sorting and negotiated
agreement, we identify five distinct workflow patterns and develop a domain
model that captures the key components and their interactions within these
workflows. We then identify eight unique types of leaderboard smells in LBOps.
By mitigating these smells, SE teams can improve transparency, accountability,
and collaboration in current LBOps practices, fostering a more robust and
responsible ecosystem for FM comparison and selection.

</details>


### [59] [MaxMind: A Memory Loop Network to Enhance Software Productivity based on Large Language Models](https://arxiv.org/abs/2408.03841)
*Yuchen Dong,XiaoXiang Fang,Yuchen Hu,Renshuang Jiang,Zhe Jiang*

Main category: cs.SE

TL;DR: 本文提出MaxMind模型，通过记忆循环网络和增强的RAG机制，解决了AI在软件操作和工具生成（SOTG）中记忆和知识价值区分的问题，实验显示能显著提升成功率、效率并解决再训练问题。


<details>
  <summary>Details</summary>
Motivation: 当前研究在将实时任务经验转化为系统记忆以及区分现有知识的价值方面存在不足。本文旨在解决这些问题，以期提升AI在软件操作和工具生成（SOTG）领域的生产力。

Method: 本文提出了一种新的方法，将外部记忆模型演化为记忆循环网络（Memory-Loop Networks），实现了实时的记忆和经验参考。同时，通过知识精度分割增强了检索增强生成（RAG）机制，并设计了MaxMind模型用于SOTG。

Result: MaxMind4Sheet（一个电子表格处理系统）的实验结果表明，任务记忆的累积和回收能使任务成功率稳步提升约3%-6%（每轮），任务执行效率最高可提升25%，并能通过记忆迁移解决LLMs在处理专业任务时的再训练问题。

Conclusion: MaxMind通过整合记忆循环网络（Memory-Loop Networks）和精确的检索增强生成（RAG）机制，并结合MaxMind模型，显著提升了语言模型在软件操作和工具生成（SOTG）方面的能力和生产力。

Abstract: The application of large language models to facilitate automated software
operations and tool generation (SOTG), thus augmenting software productivity,
mirrors the early stages of human evolution when the ability to create and use
tools accelerated the progress of civilization. These complex tasks require AI
to continuously summarize and improve. Current research often overlooks the
importance of converting real-time task experiences into system memory and
differentiating the value of existing knowledge for future reference. This
paper addresses these issues by evolving external memory models into
Memory-Loop Networks for timely memorization and experience referencing. We
also enhance a RAG mechanism with knowledge precision segmentation to utilize
memory based on value differentiation, and design the MaxMind model for SOTG
accordingly.To demonstrate our approach, we developed MaxMind4Sheet, an
electronic spreadsheet processing system aligned with the MaxMind philosophy.
Comparative experiments with SheetCopilot have demonstrated that the
accumulation and recycling of task memories lead to a steady enhancement in
task success rate, with an improvement rate of approximately 3%-6% per round in
this implementation example. Note that as the memories continue to grow, this
cumulative improvement may be substantial. The inclusion of memory recycling
can also boost the system's task execution efficiency by up to 25%, and it can
address the retraining issue faced by LLMs when handling specialized tasks
through memories transfer.These suggest that MaxMind has significant potential
to enhance the capabilities and productivity of LLM systems in SOTG.

</details>


### [60] [Will Dynamic Arrays finally change the way Models are built?](https://arxiv.org/abs/2006.14706)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: 电子表格的动态数组在专业开发环境中很有前景，可以减少错误并取代传统技术。


<details>
  <summary>Details</summary>
Motivation: 探究电子表格的动态数组能否在专业开发环境中取代传统技术，以提高解决方案的完整性，并减少与手动更新相关的错误和风险。

Method: 通过检查电子表格中新引入的动态数组的采用，以及它们在更专业的开发环境中的应用，来检验其替代传统技术的有效性。

Result: 与传统的CSE数组公式相比，动态数组模型可以减少手动干预以保持更新，从而有可能减少错误和风险。

Conclusion: 电子表格的动态数组在需要解决方案完整性的专业开发环境中具有取代传统技术的潜力，因为它们需要更少的手动干预来保持更新，从而有可能减少错误和风险。

Abstract: Spreadsheets offer a supremely successful and intuitive means of processing
and exchanging numerical content. Its intuitive ad-hoc nature makes it hugely
popular for use in diverse areas including business and engineering, yet these
very same characteristics make it extraordinarily error-prone; many would
question whether it is suitable for serious analysis or modelling tasks. A
previous EuSpRIG paper examined the role of Names in increasing solution
transparency and providing a readable notation to forge links with the problem
domain. Extensive use was made of CSE array formulas, but it is acknowledged
that their use makes spreadsheet development a distinctly cumbersome task.
Since that time, the new dynamic arrays have been introduced and array
calculation is now the default mode of operation for Excel. This paper examines
the thesis that their adoption within a more professional development
environment could replace traditional techniques where solution integrity is
important. A major advantage of fully dynamic models is that they require less
manual intervention to keep them updated and so have the potential to reduce
the attendant errors and risk.

</details>


### [61] [A Structured Approach to the development of Solutions in Excel](https://arxiv.org/abs/1704.01142)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: Excel 缺乏可扩展的结构，但可以通过公式序列解决。


<details>
  <summary>Details</summary>
Motivation: Excel 普及了数据处理，但缺乏可扩展的结构。

Method: 使用有争议或较少使用的技术，通过类似于编程语言步骤的公式序列来创建连贯的解决方案策略。

Result: 通过公式序列解决了问题，类似于编程语言的步骤。 值得注意的是，Excel 默认设置似乎缺少通常的代码结构，这些结构可以防止错误随着规模的扩大而扩大。 此外，它还考虑了如何解决这个问题，通过一系列公式来解决，这类似于一个编程语言的步骤。

Conclusion: Excel 默认设置缺乏超越单元格公式的结构。

Abstract: Spreadsheets offer a supremely successful democratisation platform, placing
the manipulation and presentation of numbers within the grasp of users that
have little or no mathematical expertise or IT experience. What appears to be
almost completely lacking within a "normal" solution built using Excel default
settings is the deployment of any structure that extends beyond a single-cell
formula. The structural elements that allow conventional code to scale without
escalating errors appear to be absent. This paper considers the use of
controversial or lesser-used techniques to create a coherent solution strategy
in which the problem is solved by a sequence of formulas resembling the steps
of a programmed language.

</details>


### [62] [Spreadsheet-based Configuration of Families of Real-Time Specifications](https://arxiv.org/abs/2310.20395)
*José Proença,David Pereira,Giann Spilere Nandi,Sina Borrami,Jonas Melchert*

Main category: cs.SE

TL;DR: 本研究提出了一种利用Excel配置和自动处理来简化实时系统模型检查的方法，并扩展了对特征组合的分析。


<details>
  <summary>Details</summary>
Motivation: 实时系统的模型检查很复杂，需要在包含足够的信息以有用和避免状态爆炸之间进行权衡。本研究旨在通过利用形式化模型和检查需求的变异性来简化实时规范变体的模型检查。

Method: 本研究通过利用形式化模型和检查需求的变异性来促进实时规范变体的模型检查。通过使用具有特定结构和简单接口的MS Excel电子表格来配置形式化规范的变异性，并自动处理这些电子表格以生成实例并运行模型检查器。此外，本研究还将通过利用对有效特征组合的分析来扩展先前的工作，同时保留基于电子表格的简单接口。

Result: 本研究提出了一种利用分析有效特征组合的方法，该方法在保留基于电子表格的简单接口和模型检查器的同时，促进了实时规范变体的模型检查。

Conclusion: 该研究通过利用形式化模型和检查需求的变异性，促进了实时规范变体的模型检查，并提出了一个基于电子表格接口的模型检查方法。

Abstract: Model checking real-time systems is complex, and requires a careful trade-off
between including enough detail to be useful and not too much detail to avoid
state explosion. This work exploits variability of the formal model being
analysed and the requirements being checked, to facilitate the model-checking
of variations of real-time specifications. This work results from the
collaboration between academics and Alstom, a railway company with a concrete
use-case, in the context of the VALU3S European project. The configuration of
the variability of the formal specifications is described in MS Excel
spreadsheets with a particular structure, making it easy to use also by
developers. These spreadsheets are processed automatically by our prototype
tool that generates instances and runs the model checker. We propose the
extension of our previous work by exploiting analysis over valid combination of
features, while preserving the simplicity of a spreadsheet-based interface with
the model checker.

</details>


### [63] [SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models](https://arxiv.org/abs/2305.19308)
*Hongxin Li,Jingran Su,Yuntao Chen,Qing Li,Zhaoxiang Zhang*

Main category: cs.SE

TL;DR: SheetCopilot is an agent that uses LLMs to control spreadsheets via natural language, automating repetitive tasks and outperforming code generation baselines.


<details>
  <summary>Details</summary>
Motivation: End users spend billions of hours on repetitive and error-prone tasks like tabular data processing and project timeline scheduling, but lack the skills to automate them. LLMs offer a potential solution for natural language software control.

Method: SheetCopilot uses a state machine-based task planning framework for LLMs, interacting with spreadsheets through a set of atomic actions representing spreadsheet functionalities.

Result: SheetCopilot correctly completes 44.3% of tasks in a single generation, outperforming strong code generation baselines. A dataset of 221 spreadsheet control tasks and an automated evaluation pipeline were created for benchmarking.

Conclusion: SheetCopilot agents can complete 44.3% of tasks with a single generation, significantly outperforming code generation baselines in controlling spreadsheets via natural language.

Abstract: Computer end users have spent billions of hours completing daily tasks like
tabular data processing and project timeline scheduling. Most of these tasks
are repetitive and error-prone, yet most end users lack the skill to automate
these burdensome works. With the advent of large language models (LLMs),
directing software with natural language user requests become a reachable goal.
In this work, we propose a SheetCopilot agent that takes natural language task
and control spreadsheet to fulfill the requirements. We propose a set of atomic
actions as an abstraction of spreadsheet software functionalities. We further
design a state machine-based task planning framework for LLMs to robustly
interact with spreadsheets. We curate a representative dataset containing 221
spreadsheet control tasks and establish a fully automated evaluation pipeline
for rigorously benchmarking the ability of LLMs in software control tasks. Our
SheetCopilot correctly completes 44.3\% of tasks for a single generation,
outperforming the strong code generation baseline by a wide margin. Our project
page:https://sheetcopilot.github.io/.

</details>


### [64] [Excel as a Turing-complete Functional Programming Environment](https://arxiv.org/abs/2309.00115)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: Excel's Dynamic Arrays are changing spreadsheet development, making it more like formal programming. It's too early to know the full impact, but trends are emerging.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore how the introduction of Dynamic Arrays in Excel's calculation engine has led to significant changes in spreadsheet development, moving towards more formal programming paradigms and to discuss the potential impact and adoption of these new functionalities.

Method: This paper analyzes the ad-hoc end-user practices of traditional spreadsheets and contrasts them with new approaches enabled by Dynamic Arrays in Excel, discussing trends observed from pioneering work within the Excel community.

Result: The paper discusses emerging trends and the potential impact on risk as end-user computing practices in Excel evolve due to Dynamic Arrays, suggesting a shift towards more formal programming approaches.

Conclusion: This paper discusses emerging trends and the potential impact on risk as end-user computing practices in Excel evolve due to Dynamic Arrays, suggesting a shift towards more formal programming approaches.

Abstract: Since the calculation engine of Excel was the subject of a major upgrade to
accommodate Dynamic Arrays in 2018 there has been a series of seismic changes
to the art of building spreadsheet solutions. This paper will show the ad-hoc
end user practices of traditional spreadsheets can be replaced by radically
different approaches that have far more in common with formal programming. It
is too early to guess the extent to which the new functionality will be adopted
by the business and engineering communities and the impact that may have upon
risk. Nevertheless, some trends are emerging from pioneering work within the
Excel community which we will discuss here.

</details>


### [65] [A Use Case-Engineering Resources Taxonomy for Analytical Spreadsheet Models](https://arxiv.org/abs/2309.00104)
*Thomas A. Grossman,Vijay Mehrotra*

Main category: cs.SE

TL;DR: 论文提出了一个分析电子表格模型的分类法，该分类法有九种类型，并探讨了它们的性质、定义、相关性以及可能的起源。该分类法有助于指导电子表格开发、风险评估和理解其随时间的变化。


<details>
  <summary>Details</summary>
Motivation: 本篇论文提出了一个分析电子表格模型的分类法。

Method: 提出了一种分析电子表格模型的分类法，该模型扩展了以前的三种类型，以识别九种类型的电子表格模型。将不同的研究文献联系起来，以区分“分析解决方案”和“工业级分析电子表格模型”。

Result: 识别出九种类型的电子表格模型，它们涵盖了文献中可见的许多分析电子表格模型。

Conclusion: 该分类法有助于识别各种电子表格开发指南最有用之处，提供了一个审视电子表格错误和风险的视角，并为理解电子表格如何随时间变化提供了一个结构。此分类法为许多有趣的研究问题打开了大门，包括对其本身的改进。

Abstract: This paper presents a taxonomy for analytical spreadsheet models. It
considers both the use case that a spreadsheet is meant to serve, and the
engineering resources devoted to its development. We extend a previous
three-type taxonomy, to identify nine types of spreadsheet models, that
encompass the many analytical spreadsheet models seen in the literature. We
connect disparate research literature to distinguish between an "analytical
solution" and an "industrial-quality analytical spreadsheet model". We explore
the nature of each of the nine types, propose definitions for some, relate them
to the literature, and hypothesize on how they might arise. The taxonomy aids
in identifying where various spreadsheet development guidelines are most
useful, provides a lens for viewing spreadsheet errors and risk, and offers a
structure for understanding how spreadsheets change over time. This taxonomy
opens the door to many interesting research questions, including refinements to
itself.

</details>


### [66] [Experimenting with ChatGPT for Spreadsheet Formula Generation: Evidence of Risk in AI Generated Spreadsheets](https://arxiv.org/abs/2309.00095)
*Simon Thorne*

Main category: cs.SE

TL;DR: ChatGPT 在生成电子表格公式方面展现出潜力，但在信息有限或问题复杂时准确性会下降，并可能产生错误陈述。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的复杂性足以通过解释纯英文句子来创建复杂的计算机程序，并用 Python、Java Script、C++ 和 Spreadsheets 等多种现代语言实现它们。这些工具功能强大且相对准确，因此无论使用者背景或知识如何，都能广泛获得计算机编程的能力。

Method: 本论文提出了一系列使用 ChatGPT 的实验，以探索该工具在需要推测、推断和解决问题的场景中生成有效电子表格公式及相关计算输出的能力。

Result: 结果表明，在某些情况下，ChatGPT 能够通过正确的推理、推断和演绎生成正确的电子表格公式。

Conclusion: 当信息有限、不确定或问题过于复杂时，ChatGPT 的准确性及其推理、推断和演绎能力会下降。此外，这还会导致错误的陈述和“幻觉”，从而破坏创建电子表格公式的过程。

Abstract: Large Language Models (LLM) have become sophisticated enough that complex
computer programs can be created through interpretation of plain English
sentences and implemented in a variety of modern languages such as Python, Java
Script, C++ and Spreadsheets. These tools are powerful and relatively accurate
and therefore provide broad access to computer programming regardless of the
background or knowledge of the individual using them. This paper presents a
series of experiments with ChatGPT to explore the tool's ability to produce
valid spreadsheet formulae and related computational outputs in situations
where ChatGPT has to deduce, infer and problem solve the answer. The results
show that in certain circumstances, ChatGPT can produce correct spreadsheet
formulae with correct reasoning, deduction and inference. However, when
information is limited, uncertain or the problem is too complex, the accuracy
of ChatGPT breaks down as does its ability to reason, infer and deduce. This
can also result in false statements and "hallucinations" that all subvert the
process of creating spreadsheet formulae.

</details>


### [67] [Demonstration of CORNET: A System For Learning Spreadsheet Formatting Rules By Example](https://arxiv.org/abs/2308.07357)
*Mukul Singh,Jose Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Gust Verbruggen*

Main category: cs.SE

TL;DR: CORNET is an Excel add-in that learns spreadsheet formatting rules from examples, saving users manual effort.


<details>
  <summary>Details</summary>
Motivation: To address the manual effort required by users to write conditional formatting rules in spreadsheet software.

Method: CORNET combines symbolic rule enumeration (using semi-supervised clustering and iterative decision tree learning) with a neural ranker to synthesize conditional formatting rules.

Result: CORNET generates accurate conditional formatting rule suggestions based on user-provided examples, enhancing the user experience in spreadsheet applications.

Conclusion: CORNET can automatically learn conditional formatting rules from user examples, demonstrating its effectiveness as an Excel add-in.

Abstract: Data management and analysis tasks are often carried out using spreadsheet
software. A popular feature in most spreadsheet platforms is the ability to
define data-dependent formatting rules. These rules can express actions such as
"color red all entries in a column that are negative" or "bold all rows not
containing error or failure." Unfortunately, users who want to exercise this
functionality need to manually write these conditional formatting (CF) rules.
We introduce CORNET, a system that automatically learns such conditional
formatting rules from user examples. CORNET takes inspiration from inductive
program synthesis and combines symbolic rule enumeration, based on
semi-supervised clustering and iterative decision tree learning, with a neural
ranker to produce accurate conditional formatting rules. In this demonstration,
we show CORNET in action as a simple add-in to Microsoft Excel. After the user
provides one or two formatted cells as examples, CORNET generates formatting
rule suggestions for the user to apply to the spreadsheet.

</details>


### [68] [Excel Spreadsheet Analyzer](https://arxiv.org/abs/2211.06333)
*Amir Nassereldine,Patrick Chen,Jinjun Xiong*

Main category: cs.SE

TL;DR: 电子表格数据迁移工具


<details>
  <summary>Details</summary>
Motivation: 电子表格广泛用于大型数值分析，但使用python进行分析时会丢失公式和依赖关系等信息。

Method: 提出一种创建电子表格抽象中间表示（AIR）的工具，并构建了一个python库。

Result: 创建的AIR有助于将电子表格迁移到科学编程语言，同时保留数据间的相互依赖关系。构建的python库可在python中执行数据分析。

Conclusion: 使用python分析公司电子表格时，会丢失一些信息，例如单元格的公式和依赖关系。我们提出了一种创建电子表格抽象中间表示（AIR）的工具。此表示有助于将电子表格迁移到科学编程语言，同时保留有关数据的相互依赖信息。此外，我们还在该工具的基础上构建了一个python库，用于在python中执行一些数据分析。

Abstract: Spreadsheets are widely used in various fields to do large numerical
analysis. While several companies have relied on spreadsheets for decades, data
scientists are going in the direction of using scientific programming languages
such as python to do their data analysis due to the support, community, and
vast amount of libraries. While using python to analyze a company's
spreadsheets, some information such as the formulas and dependencies of a cell
are lost. We propose a tool that creates an abstract intermediate
representation (AIR) of a spreadsheet. This representation facilitates the
transfer from spreadsheets into scientific programming languages while
preserving inter-dependency information about data. In addition to that, we
build a python library on top of our tool to perform some data analysis in
python.

</details>


### [69] [Exploring Spreadsheet Use and Practices in a Technologically Constrained Setting](https://arxiv.org/abs/2201.07696)
*Khwima Mckinley Mkamanga,Simon Thorne*

Main category: cs.SE

TL;DR: 本研究探讨了电子表格对马拉维一家供水公共事业机构的影响，发现在促进业务自动化的同时，也带来了管理、技术和人为方面的风险，并指出了在政策和法规方面的改进空间。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨电子表格对马拉维一家供水公共事业机构业务运营的影响，该机构是撒哈拉以南非洲一个技术欠发达国家半政府机构的典型代表。

Method: 本研究聚焦于电子表格的使用范围和生命周期，以及组织政策和治理。

Result: 总体而言，研究结果表明，电子表格在该组织的普及为业务自动化提供了一个有利环境。然而，研究也强调了导致电子表格广泛使用伴随的高风险的管理、技术和人为因素问题。

Conclusion: 研究结果证实，在规范电子表格开发过程和采用的制定全面的政策和法规等许多方面都有很大的改进空间。

Abstract: This paper explores the impacts of spreadsheets on business operations in a
water utility parastatal in Malawi, Sub-Saharan Africa. The organisation is a
typical example of a semi-government body operating in a technologically
underdeveloped country. The study focused on spreadsheet scope of use and life
cycle as well as organisational policy and governance. The results will help
define future spreadsheet usage by influencing new approaches for managing
potential risks associated with spreadsheets in the organization. Generally,
findings indicate that the proliferation of spreadsheets in the organization
has provided an enabling environment for business automation. The paper also
highlights management, technological and human factor issues contributing to
high risks associated with the pervasive spreadsheet use. The conclusions drawn
from the research confirms that there is ample room for improvement in many
areas such as implementation of comprehensive policies and regulations
governing spreadsheet development processes and adoption.

</details>


### [70] [Methodology for Assessing the State of the Practice for Domain X](https://arxiv.org/abs/2110.11575)
*Spencer Smith,Jacques Carette,Peter Michalski,Ao Dong,Olu Owojaiye*

Main category: cs.SE

TL;DR: 本研究提出了一种评估研究软件开发实践的方法，通过对代码、文档、存储库数据和开发者访谈进行分析，并结合 AHP 排名，为特定研究软件领域提供全面的见解。完成一项评估大约需要 173 人时。


<details>
  <summary>Details</summary>
Motivation: 为了改进研究软件的软件开发方法和工具，首先需要了解该领域的实践现状。

Method: 该研究提出了一种评估研究软件领域软件开发实践状态的方法。该方法包括识别领域、筛选候选软件包、收集源代码和文档、收集存储库相关数据（如 stars、open issues、lines of code）、填写包含 108 个问题以评估 9 种质量（包括可安装性、可用性和可见性）的测量模板、进行包含 20 个问题的开发者访谈、使用分析层次过程（AHP）对软件进行排名，最后分析数据以回答预先设定的问题。该方法还强调了领域专家的参与，以确保信息的准确性和进行共性和变异性分析。完成一项评估大约需要 173 人时。

Result: 该研究提出了一种用于评估研究软件开发实践状态的方法，该方法通过对代码、文档、存储库数据和开发者访谈进行分析，并结合 AHP 排名，为特定研究软件领域提供全面的见解。

Conclusion: 该研究提出了一种评估研究软件领域软件开发实践状态的方法，该方法通过一系列步骤（包括识别领域、收集和分析代码/文档/存储库数据、进行开发者访谈以及使用分析层次过程进行排名）来回答有关工件、工具、流程、痛点和改进措施的问题。

Abstract: To improve software development methods and tools for research software, we
first need to understand the current state of the practice. Therefore, we have
developed a methodology for assessing the state of the software development
practices for a given research software domain. For each domain we wish to
answer questions such as: i) What artifacts (documents, code, test cases, etc.)
are present? ii) What tools are used? iii) What principles, process and
methodologies are used? iv) What are the pain points for developers? v) What
actions are used to improve qualities like maintainability and reproducibility?
To answer these questions, our methodology prescribes the following steps: i)
Identify the domain; ii) Identify a list of candidate software packages; iii)
Filter the list to a length of about 30 packages; iv) Gather source code and
documentation for each package; v) Collect repository related data on each
software package, like number of stars, number of open issues, number of lines
of code; vi) Fill in the measurement template (the template consists of 108
questions to assess 9 qualities (including the qualities of installability,
usability and visibility)); vii) Interview developers (the interview consists
of 20 questions and takes about an hour); viii) Rank the software using the
Analytic Hierarchy Process (AHP); and, ix) Analyze the data to answer the
questions posed above. A domain expert should be engaged throughout the
process, to ensure that implicit information about the domain is properly
represented and to assist with conducting an analysis of the commonalities and
variabilities between the 30 selected packages. Using our methodology,
spreadsheet templates and AHP tool, we estimate (based on our experience with
using the process) the time to complete an assessment for a given domain at 173
person hours.

</details>


### [71] [Agile Elicitation of Scalability Requirements for Open Systems: A Case Study](https://arxiv.org/abs/2108.00567)
*Gunnar Brataas,Antonio Martini,Geir Kjetil Hanssen,Georg Ræder*

Main category: cs.SE

TL;DR: 本研究提出了ScrumScale模型，一种用于敏捷开发的可扩展性需求引出工具，通过开放银行案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 敏捷软件开发中引出可扩展性需求的方法复杂且研究不足。

Method: 设计科学研究，结合了协调理论，并使用了一个名为ScrumScale的电子表格模型。

Result: 在开放银行案例研究中，ScrumScale模型被证明能系统地引出可扩展性需求，并为利益相关者对话提供了显著优势，用户花费了55小时。

Conclusion: ScrumScale模型为敏捷软件开发提供了简化的可扩展性需求引出方法，并在开放银行案例研究中得到了验证，提高了利益相关者的沟通效率。

Abstract: Eliciting scalability requirements during agile software development is
complicated and poorly described in previous research. This article presents a
lightweight artifact for eliciting scalability requirements during agile
software development: the ScrumScale model. The ScrumScale model is a simple
spreadsheet. The scalability concepts underlying the ScrumScale model are
clarified in this design science research, which also utilizes coordination
theory. This paper describes the open banking case study, where a legacy
banking system becomes open. This challenges the scalability of this legacy
system. The first step in understanding this challenge is to elicit the new
scalability requirements. In the open banking case study, key stakeholders from
TietoEVRY spent 55 hours eliciting TietoEVRY's open banking project's
scalability requirements. According to TietoEVRY, the ScrumScale model provided
a systematic way of producing scalability requirements. For TietoEVRY, the
scalability concepts behind the ScrumScale model also offered significant
advantages in dialogues with other stakeholders.

</details>


### [72] [SpreadsheetCoder: Formula Prediction from Semi-structured Context](https://arxiv.org/abs/2106.15339)
*Xinyun Chen,Petros Maniatis,Rishabh Singh,Charles Sutton,Hanjun Dai,Max Lin,Denny Zhou*

Main category: cs.SE

TL;DR: SpreadsheetCoder是一种基于BERT的模型，可以通过考虑表头和表格数据来合成电子表格公式，相比现有方法，其准确率更高，用户辅助率也更高。


<details>
  <summary>Details</summary>
Motivation: 先前的研究通常利用输入输出示例作为电子表格公式合成的规范，但这种方法未能充分捕捉真实电子表格中的丰富上下文，例如表格组织、行和列之间的依赖关系以及表头信息。

Method: 提出了一种名为SpreadsheetCoder的基于BERT的模型架构，该模型能够以基于行和基于列的格式来表示表格上下文。

Result: SpreadsheetCoder在包含表头和半结构化表格数据的电子表格公式合成方面取得了显著成果，其预测准确率达到了42.51%，显著优于未考虑丰富表格上下文的基线方法。

Conclusion: SpreadsheetCoder在Google表格上比基于规则的系统多帮助82%的用户编写公式。

Abstract: Spreadsheet formula prediction has been an important program synthesis
problem with many real-world applications. Previous works typically utilize
input-output examples as the specification for spreadsheet formula synthesis,
where each input-output pair simulates a separate row in the spreadsheet.
However, this formulation does not fully capture the rich context in real-world
spreadsheets. First, spreadsheet data entries are organized as tables, thus
rows and columns are not necessarily independent from each other. In addition,
many spreadsheet tables include headers, which provide high-level descriptions
of the cell data. However, previous synthesis approaches do not consider
headers as part of the specification. In this work, we present the first
approach for synthesizing spreadsheet formulas from tabular context, which
includes both headers and semi-structured tabular data. In particular, we
propose SpreadsheetCoder, a BERT-based model architecture to represent the
tabular context in both row-based and column-based formats. We train our model
on a large dataset of spreadsheets, and demonstrate that SpreadsheetCoder
achieves top-1 prediction accuracy of 42.51%, which is a considerable
improvement over baselines that do not employ rich tabular context. Compared to
the rule-based system, SpreadsheetCoder assists 82% more users in composing
formulas on Google Sheets.

</details>


### [73] [From webtables to datatables](https://arxiv.org/abs/2006.14694)
*Mária Csernoch*

Main category: cs.SE

TL;DR: Webtables在电子表格教学和计算思维培养方面具有重要价值，本文提供了将LOL数据转换为表格的实用方法。


<details>
  <summary>Details</summary>
Motivation: Webtables为电子表格教学、商业和专业组织中的知识转移、解决现实世界问题以及培养计算思维技能提供了绝佳的途径。

Method: 本文详细介绍了将LOL英雄联盟数据转换为表格的过程，并提供了两种实现方案：一种在文字处理器中，另一种纯粹在电子表格应用程序中。

Result: 提供了两种实现webtables转换的解决方案，为进一步讨论和创新留下了空间。

Conclusion: Webtables是用于教学和发展计算思维技能的宝贵资源。

Abstract: Webtables -- tables and table-like structures on webpages -- are excellent
sources for teaching spreadsheeting, in commercial and professional
organisations by utilizing and developing knowledge-transfer items, presenting
and handling various real-world problems and solutions, discussing and
debugging, and in general, developing and utilizing computational thinking
skills. In the present paper the conversion process of one of the LOL Boards
(League of Legends, Riot Games Inc. 2019) is detailed. After presenting the
algorithm of the conversion, two solutions are offered -- one in a word
processor, the other purely in a spreadsheet application -- leaving space for
discussions, inventing other solutions and combining them.

</details>


### [74] [Implementation Strategies for Multidimensional Spreadsheets](https://arxiv.org/abs/2006.05814)
*Paul Mireault*

Main category: cs.SE

TL;DR: Excel'de çok boyutlu değişkenleri yönetmek için projeksiyon ve veritabanı yaklaşımları kullanıldı; veritabanı yaklaşımı daha basit formüller sunar.


<details>
  <summary>Details</summary>
Motivation: Çok boyutlu değişkenlere sahip bir elektronik tabloyu uygulamak için stratejileri analiz etmek.

Method: Katılımcıların elektronik tabloları analiz edilerek farklı uygulama stratejileri belirlendi.

Result: Katılımcıların çoğu projeksiyon stratejisini kullanırken, az sayıda katılımcı daha basit formüller sağlayan bir veritabanı yaklaşımını tercih etti.

Conclusion: Excel geliştiricileri, çok boyutlu değişkenlere sahip bir elektronik tabloyu uygulamak için bir yarışmaya katıldılar. Katılımcıların çoğu, Excel'in iki boyutlu düzlemine üç veya dört boyutlu değişkenlerin projeksiyonunu kullandı. Daha az sayıda katılımcı ise, çok boyutlu değişkenlerin uygun birincil anahtarla bir veri kümesi tablosu biçiminde sunulduğu bir veritabanı yaklaşımı kullandı. Bu yaklaşım daha basit formüllere yol açıyor.

Abstract: Seasoned Excel developers were invited to participate in a challenge to
implement a spreadsheet with multi-dimensional variables. We analyzed their
spreadsheet to see the different implement strategies employed. We identified
two strategies: most participants used a projection of three or
four-dimensional variables on the two-dimensional plane used by Excel. A few
participants used a database approach where the multi-dimensional variables are
presented in the form of a dataset table with the appropriate primary key. This
approach leads to simpler formulas.

</details>


### [75] [Abstracting spreadsheet data flow through hypergraph redrawing](https://arxiv.org/abs/2006.04794)
*David Birch,Nicolai Stawinoga,Jack Binks,Bruno Nicoletti,Paul Kelly*

Main category: cs.SE

TL;DR: 本研究提出通过“重新绘制单元格边界”来提高电子表格的抽象级别，将电子表格转换为细粒度图，以揭示隐藏的链接结构并提高单元格的表达能力。


<details>
  <summary>Details</summary>
Motivation: 传统电子表格易出错是由于其低级抽象。最终用户程序员被迫从低级单元格构建数据模型，单元格仅限于包含标量值，且它们之间的链接隐藏。本研究旨在提高电子表格的抽象级别。

Method: 提出通过“重新绘制单元格边界”来提高电子表格的抽象级别。将电子表格转换为细粒度图，将运算符和值作为节点，单元格表示为超图边。通过识别常见子表达式和应用子树同构来检测向量（数组）运算。

Result: 通过将电子表格转换为细粒度图，并使用超图边表示单元格，可以揭示隐藏的链接结构。通过识别常见子表达式和子树同构，可以检测到向量（数组）运算。

Conclusion: 通过将电子表格重绘为包含运算符和值的节点，并使用超图边表示单元格（围绕一组运算符/数据节点绘制边界“墙”），可以提高电子表格的抽象级别。通过识别常见子表达式和应用子树同构来检测向量（数组）运算，可以进一步提高抽象级别。

Abstract: We believe the error prone nature of traditional spreadsheets is due to their
low level of abstraction. End user programmers are forced to construct their
data models from low level cells which we define as "a data container or
manipulator linked by user-intent to model their world and positioned to
reflect its structure". Spreadsheet cells are limited in what they may contain
(scalar values) and the links between them are inherently hidden. This paper
proposes a method of raising the level of abstraction of spreadsheets by
"redrawing the boundary" of the cell. To expose the hidden linkage structure we
transform spreadsheets into fine-grained graphs with operators and values as
nodes. "cells" are then represented as hypergraph edges by drawing a boundary
"wall" around a set of operator/data nodes. To extend what cells may contain
and to create a higher level model of the spreadsheet we propose that
researchers should seek techniques to redraw these boundaries to create higher
level "cells" which will more faithfully represent the end-user's real
world/mental model. We illustrate this approach via common sub-expression
identification and the application of sub-tree isomorphisms for the detection
of vector (array) operations.

</details>


### [76] [Comprehensive review for common types of errors using spreadsheets](https://arxiv.org/abs/1912.09209)
*Ali Aburas*

Main category: cs.SE

TL;DR: 本文对电子表格错误查找和修复方法进行了综述。


<details>
  <summary>Details</summary>
Motivation: 电子表格在组织和个人中被广泛使用，但其中包含错误的数量也很多，因此研究用于预防、检测和纠正电子表格错误的方法具有重要意义。

Method: 对目前已有的关于电子表格错误查找和修复的文献进行收集、梳理和分类，并对各类方法进行描述和分析。

Result: 对现有的电子表格错误查找和修复方法进行了分类和总结，并对用户常犯的错误类型进行了分析。

Conclusion: 对电子表格中常见的错误类型进行了分类，并对目前已有的错误查找和修复方法进行了全面的梳理和归纳。

Abstract: Thanks to their flexibility and capability to perform different tasks and
organize data in the best form and format, spreadsheets are widely used in
different organizations and by different end users. Many business organizations
rely on spreadsheets to fulfill their various tasks. On the other hand, the
number of spreadsheets that contain errors are very high, thus researchers have
developed different tools aimed at the prevention, detection, and correction of
errors in spreadsheets. This research work is a comprehensive review that
describes and classifies approaches on finding and fixing errors in
spreadsheets. The paper discusses up-to-date research work approaches in terms
of definition, how they work, and kinds of errors they can find in
spreadsheets. The paper looks also for the kinds of errors that end users
commonly make in spreadsheets.

</details>


### [77] [A Coding-free Software Framework of Developing Web Data Management Systems](https://arxiv.org/abs/1910.05685)
*Can Yang,Shiying Pan,Runmin Li,Yu Liu,Lizhang Peng*

Main category: cs.SE

TL;DR: 本研究提出了一种基于SaaS架构的无代码数据管理系统构建方法和平台，允许非程序员通过电子表格定义需求来快速开发定制化的系统。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的企业将数据管理系统部署到云端，而非程序员（非专业开发者）在开发此类系统时仍面临困难。SaaS（软件即服务）的发展为无代码软件开发带来了可行性。

Method: 通过抽象数据管理系统的通用特征，设计了一个通用的Web平台，并提出了一种使用特定需求表（电子表格）来开发数据管理系统的方法。该平台通过解析表格模型并在运行阶段实现目标系统来映射需求表。

Result: 研究实现并部署了提出的框架，并通过了实证结果的检验，证明了该无代码方法在开发Web数据管理系统方面的可行性和可用性。

Conclusion: 该研究提出了一个无代码构建数据管理系统的理论和方法，并通过一个实际的应用平台进行了验证，证明了该方法的可行性和可用性。

Abstract: More and more enterprises recently intend to deploy data management systems
in the cloud. Due to the professionalism of software development, it has still
been difficult for non-programmers to develop this kind of systems, even a
small one. However, the development of SaaS brings forth the more feasibility
of coding-free software development than before. Based on the SaaS
architecture, this paper presents a set of theory and method for coding-free
construction of a data management system, on which our contributions involve in
a practical application platform, a set of construction method and a set of
interface on data exchange. By abstracting the common features of data
management systems, we design a universal web platform to quickly generate and
publish customized system instances. Moreover, we propose a kind of method to
develop a data management system using a specific requirements table in
spreadsheet. The corresponding platform maps the requirements table into a
system instance through parsing the table model and implementing the objective
system in the running stage. Finally, we implement the proposed framework and
deploy it on web. The empirical result demonstrates the feasibility and
availability of the coding-free method in developing web data management
systems.

</details>


### [78] [Structured Spreadsheet Modelling and Implementation with Multiple Dimensions -- Part 2: Implementation](https://arxiv.org/abs/1909.00891)
*Paul Mireault*

Main category: cs.SE

TL;DR: This paper provides practical steps to create a maintainable spreadsheet for multi-dimensional problems, following a previously established conceptual model.


<details>
  <summary>Details</summary>
Motivation: To provide precise steps for implementing a multi-dimensional problem in a spreadsheet that is easy to maintain.

Method: The paper describes the steps to implement a multi-dimensional problem in a spreadsheet, building upon a conceptual model presented as a Formula Diagram and Formula List.

Result: The implementation results in a spreadsheet that is easy to maintain, based on a conceptual model with a Formula Diagram and Formula List.

Conclusion: The paper details the implementation of a multi-dimensional problem into a spreadsheet for easier maintenance.

Abstract: In Part 1, we showed how to develop a conceptual model of a problem involving
variables of multiple dimensions, like Products, Regions, Sectors and Months.
The conceptual model is presented as a Formula Diagram, giving a global view of
the interaction between all the variables, and a Formula List, giving a precise
view of the interaction between the variables. In this paper, we present
precise steps to implement a multi-dimensional problem in a way that will
produce a spreadsheet that is easy to maintain

</details>


### [79] [On the Refinement of Spreadsheet Smells by means of Structure Information](https://arxiv.org/abs/1810.04542)
*Patrick Koch,Birgit Hofer,Franz Wotawa*

Main category: cs.SE

TL;DR: 通过利用推断的结构信息改进电子表格气味检测，减少冗余报告并发现新问题。


<details>
  <summary>Details</summary>
Motivation: 目前的电子表格气味检测技术存在报告不正确或冗余的问题，例如对同一问题进行重复报告，这会使用户不堪重负。

Method: 提出了一种静态分析方法来推断相关单元格的簇和块，并在此基础上改进了现有的气味检测技术，同时提出了三种利用推断的电子表格结构的新型气味检测技术。

Result: 经验评估表明，所提出的改进措施成功减少了不正确和冗余的气味报告数量，并且新引入的气味揭示了新的缺陷。

Conclusion: 该研究通过利用推断的结构信息来改进电子表格气味检测技术，成功减少了不正确和冗余的气味报告，并揭示了新的缺陷。

Abstract: Spreadsheet users are often unaware of the risks imposed by poorly designed
spreadsheets. One way to assess spreadsheet quality is to detect smells which
attempt to identify parts of spreadsheets that are hard to comprehend or
maintain and which are more likely to be the root source of bugs.
Unfortunately, current spreadsheet smell detection techniques suffer from a
number of drawbacks that lead to incorrect or redundant smell reports. For
example, the same quality issue is often reported for every copy of a cell,
which may overwhelm users. To deal with these issues, we propose to refine
spreadsheet smells by exploiting inferred structural information for smell
detection. We therefore first provide a detailed description of our static
analysis approach to infer clusters and blocks of related cells. We then
elaborate on how to improve existing smells by providing three example
refinements of existing smells that incorporate information about cell groups
and computation blocks. Furthermore, we propose three novel smell detection
techniques that make use of the inferred spreadsheet structures. Empirical
evaluation of the proposed techniques suggests that the refinements
successfully reduce the number of incorrectly and redundantly reported smells,
and novel deficits are revealed by the newly introduced smells.

</details>


### [80] [Now You're Thinking With Structures: A Concept for Structure-based Interactions with Spreadsheets](https://arxiv.org/abs/1809.03435)
*Patrick Koch*

Main category: cs.SE

TL;DR: 本研究提出了一种结构感知的电子表格理解和交互方法，旨在解决复杂电子表格难以理解和适应的问题。通过推断和利用电子表格的结构信息，增强可视化效果，优化用户交互，并提供主动编辑功能，以提高电子表格的可维护性和用户生产力。


<details>
  <summary>Details</summary>
Motivation: 电子表格在达到一定复杂性后难以理解和适应，而高阶心智模型有助于理解复杂系统。

Method: 通过结构感知理解和交互来扩展电子表格，其中结构信息用于丰富可视化，响应式地增强传统用户操作，并提供主动更改整体电子表格构成的工具。

Result: 实现了一个用于结构推断和可视化的工具，并计划在此基础上引入主动和响应式交互机制，最终为常用的电子表格处理器提供一个加载项，实现结构感知功能。

Conclusion: 提供思考和交互式电子表格的工具将使电子表格用户在生产力和整体电子表格质量方面受益。

Abstract: Spreadsheets are the go-to tool for computerized calculation and modelling,
but are hard to comprehend and adapt after reaching a certain complexity. In
general, cognition of complex systems is facilitated by having a higher order
mental model of the system in question to work with. We therefore present a
concept for structure-aware understanding of and interaction with spreadsheets
that extends previous work on structure inference in the domain. Following this
concept, structural information is used to enrich visualizations, reactively
enhance traditional user actions, and provide tools to proactively alter the
overall spreadsheet makeup instead of individual cells The intended systems
should, in first approximation, not replace common spreadsheet tools, but
provide an additional layer of functionality alongside the established
interface. In ongoing work, we therefore implemented a tool for structure
inference and visualization along the common spreadsheet layout. Based on this
framework, we plan to introduce the envisioned proactive and reactive
interaction mechanics, and finally provide structure-aware unctionality as an
add-in for common spreadsheet processors. We believe that providing the tools
for thinking about and interacting with spreadsheets in this manner will
benefit users both in terms of productivity and overall spreadsheet quality.

</details>


### [81] [Typed Table Transformations](https://arxiv.org/abs/1809.02746)
*Martin Erwig*

Main category: cs.SE

TL;DR: 电子表格的类型转换


<details>
  <summary>Details</summary>
Motivation: 电子表格中的标签可以作为数据类型，表格可以视为由类型化数据构成，其中值在表格中的放置由行和列的类型控制。

Method: 基于行和列类型转换的电子表格转换

Result: 提出了电子表格转换的新方法，并阐述了未来研究中应解决的一系列研究问题。

Conclusion: 该方法为基于类型的表格构建和转换

Abstract: Spreadsheet tables are often labeled, and these labels effectively constitute
types for the data in the table. In such cases tables can be considered to be
built from typed data where the placement of values within the table is
controlled by the types used for rows and columns. We present a new approach to
the transformations of spreadsheet tables that is based on transformations of
row and column types. We illustrate the basic idea of type-based table
construction and transformation and lay out a series of research questions that
should be addressed in future work.

</details>


### [82] [Implementing WHERE and ORDER BY as spreadsheet formulas](https://arxiv.org/abs/1809.00025)
*Paul Mireault*

Main category: cs.SE

TL;DR: This paper introduces spreadsheet formulas that replicate the functionality of SQL's WHERE and ORDER BY clauses, addressing the lack of automatic updates in existing spreadsheet tools.


<details>
  <summary>Details</summary>
Motivation: The disadvantage of using spreadsheet tools like filter, sort, Query or Pivot Table is that they don't react automatically to changes in the calculated values of the spreadsheet.

Method: Develop spreadsheet formulas that implement SQL's WHERE and ORDER BY clauses.

Result: Spreadsheet formulas that implement SQL's WHERE and ORDER BY clauses.

Conclusion: We develop spreadsheet formulas that implement SQL's WHERE and ORDER BY clauses.

Abstract: The WHERE and ORDER BY clauses of the SQL SELECT statement select a subset of
rows in the result of a database query and present the result in the specified
order. In a spreadsheet program like Microsoft Excel, one could use the filter
and sort buttons, or use its Query or its Pivot Table tools to achieve a
similar effect. The disadvantage of using those tools is that they don't react
automatically to changes in the calculated values of the spreadsheet. In this
paper, we develop spreadsheet formulas that implement SQL's WHERE and ORDER BY
clauses.

</details>


### [83] [The use of Charts, Pivot Tables, and Array Formulas in two Popular Spreadsheet Corpora](https://arxiv.org/abs/1808.10642)
*Bas Jansen,Felienne Hermans*

Main category: cs.SE

TL;DR: Spreadsheets are widely used but error-prone. Current research mainly focuses on formulas, ignoring other crucial elements like charts and pivot tables. This paper analyzes the usage of these neglected elements in popular spreadsheet corpora to improve overall spreadsheet quality.


<details>
  <summary>Details</summary>
Motivation: Spreadsheets are error-prone, increasing the risk of incorrect decisions and financial loss. Most research focuses on formulas, neglecting other constructions like charts, pivot tables, and array formulas.

Method: Analyzing two popular spreadsheet corpora: Enron and EUSES on the use of charts, pivot tables, and array formulas.

Result: The paper analyzes the usage of charts, pivot tables, and array formulas in the Enron and EUSES spreadsheet corpora.

Conclusion: To improve spreadsheet quality it is important to understand how spreadsheets are used and to obtain a full understanding, the use of charts, pivot tables, and array formulas should be included in research.

Abstract: The use of spreadsheets in industry is widespread. Companies base decisions
on information coming from spreadsheets. Unfortunately, spreadsheets are
error-prone and this increases the risk that companies base their decisions on
inaccurate information, which can lead to incorrect decisions and loss of
money. In general, spreadsheet research is aimed to reduce the error-proneness
of spreadsheets. Most research is concentrated on the use of formulas. However,
there are other constructions in spreadsheets, like charts, pivot tables, and
array formulas, that are also used to present decision support information to
the user. There is almost no research about how these constructions are used.
To improve spreadsheet quality it is important to understand how spreadsheets
are used and to obtain a complete understanding, the use of charts, pivot
tables, and array formulas should be included in research. In this paper, we
analyze two popular spreadsheet corpora: Enron and EUSES on the use of the
aforementioned constructions.

</details>


### [84] [Asheetoxy: A Taxonomy for Classifying Negative Spreadsheet-related Phenomena](https://arxiv.org/abs/1808.10231)
*Daniel Kulesz,Stefan Wagner*

Main category: cs.SE

TL;DR: 提出了一种名为Asheetoxy的电子表格错误分类法，它简单、面向现象且易于使用，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 许多组织业务关键的电子表格中存在错误，导致了糟糕的决策。现有分类法存在术语模糊和需要详细背景知识的问题，限制了其在实际应用中的分类能力。

Method: 提出了一种名为Asheetoxy的分类法，该方法简单且面向现象，旨在解决现有电子表格错误分类法存在的歧义和应用限制问题。

Result: 一项包含7名参与者的初步研究表明，非电子表格研究人员也能使用Asheetoxy对现实世界中的电子表格现象进行分类。

Conclusion: Asheetoxy是一种简单且面向现象的分类法，它完全避开了“错误”这个有问题的术语。初步研究表明，即使是非电子表格研究人员，也能使用Asheetoxy对现实世界中的电子表格现象进行分类。

Abstract: Spreadsheets (sometimes also called Excel programs) are powerful tools which
play a business-critical role in many organizations. However, due to faulty
spreadsheets many bad decisions have been taken in recent years. Since then, a
number of researchers have been studying spreadsheet errors. However, one issue
that hinders discussion among researchers and professionals is the lack of a
commonly accepted taxonomy.
  Albeit a number of taxonomies for spreadsheet errors have been proposed in
previous work, a major issue is that they use the term error that itself is
already ambiguous. Furthermore, to apply most existing taxonomies, detailed
knowledge about the underlying process and knowledge about the "brain state" of
the acting spreadsheet users is required. Due to these limitations, known
error-like phenomena in freely available spreadsheet corpora cannot be
classified with these taxonomies.
  We propose Asheetoxy, a simple and phenomenon-oriented taxonomy that avoids
the problematic term error altogether. An initial study with 7 participants
indicates that even non-spreadsheet researchers similarly classify real-world
spreadsheet phenomena using Asheetoxy.

</details>


### [85] [Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (SEMS'18)](https://arxiv.org/abs/1808.09174)
*Birgit Hofer,Jorge Mendes*

Main category: cs.SE

TL;DR: This paper is an introduction to a workshop and does not contain a TLDR.


<details>
  <summary>Details</summary>
Motivation: This paper is an introduction to a workshop and does not contain motivation.

Method: This paper does not contain a method.

Result: This paper does not contain results.

Conclusion: This paper does not contain a conclusion.

Abstract: Proceedings of the 5th International Workshop on Software Engineering Methods
in Spreadsheets (SEMS'18), held on October 1st, 2018, in Lisbon, Portugal, and
co-located with the 2018 IEEE Symposium on Visual Languages and Human-Centric
Computing (VL/HCC).

</details>


### [86] [Combining Spreadsheet Smells for Improved Fault Prediction](https://arxiv.org/abs/1805.10493)
*Patrick Koch,Konstantin Schekotihin,Dietmar Jannach,Birgit Hofer,Franz Wotawa*

Main category: cs.SE

TL;DR: This paper uses machine learning (AdaBoost) to improve spreadsheet fault prediction by combining multiple code smells, outperforming individual smell predictions.


<details>
  <summary>Details</summary>
Motivation: Spreadsheets are widely used for business calculations, but faults can have significant business impacts. Existing approaches using code smells have shown limited predictive power for individual smells.

Method: The paper applies an AdaBoost ensemble classifier to combine the predictions of individual spreadsheet smells for fault prediction.

Result: Experiments on two public datasets with real-world spreadsheet faults show significant improvements in fault prediction accuracy using the proposed AdaBoost ensemble method.

Conclusion: The study proposes a machine learning approach using an AdaBoost ensemble classifier to combine predictions from individual spreadsheet smells, demonstrating significant improvements in fault prediction accuracy on real-world datasets.

Abstract: Spreadsheets are commonly used in organizations as a programming tool for
business-related calculations and decision making. Since faults in spreadsheets
can have severe business impacts, a number of approaches from general software
engineering have been applied to spreadsheets in recent years, among them the
concept of code smells. Smells can in particular be used for the task of fault
prediction. An analysis of existing spreadsheet smells, however, revealed that
the predictive power of individual smells can be limited. In this work we
therefore propose a machine learning based approach which combines the
predictions of individual smells by using an AdaBoost ensemble classifier.
Experiments on two public datasets containing real-world spreadsheet faults
show significant improvements in terms of fault prediction accuracy.

</details>


### [87] [An Easy & Collaborative RDF Data Entry Method using the Spreadsheet Metaphor](https://arxiv.org/abs/1804.04175)
*Markus Schröder,Christian Jilek,Jörn Hees,Sven Hertling,Andreas Dengel*

Main category: cs.SE

TL;DR: 该研究提出了一种简便的Web版电子表格编辑器，可将电子表格数据直接转换为RDF语句，用户研究显示其效率和质量均优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 鉴于电子表格在知识工作者（尤其是在工业领域）中的广泛使用及其易用性，为了方便用户（特别是RDF新手）创建语义数据。

Method: 提出了一种易于使用的、零配置的、基于Web的电子表格编辑器，该编辑器可同时将电子表格条目转换为RDF语句。

Result: 用户研究表明，与采用其他方法相比，参与者能够在更短的时间内创建更多的RDF语句，并且质量相当或更高。

Conclusion: 该研究提出了一种基于Web的电子表格编辑器，可以将电子表格条目同时转换为RDF语句，使得各种用户（无论是否为RDF专家）都能轻松创建语义数据。

Abstract: Spreadsheets are widely used by knowledge workers, especially in the
industrial sector. Their methodology enables a well understood, easy and fast
possibility to enter data. As filling out a spreadsheet is more accessible to
common knowledge workers than defining RDF statements, in this paper, we
propose an easy-to-use, zero-configuration, web-based spreadsheet editor that
simultaneously transfers spreadsheet entries into RDF statements. It enables
various kinds of users to easily create semantic data whether they are RDF
experts or novices. The typical scenario we address focuses on creating
instance data starting with an empty knowledge base that is filled
incrementally. In a user study, participants were able to create more
statements in shorter time, having similar or even significantly outperforming
quality, compared to other approaches.

</details>


### [88] [Structured Spreadsheet Modelling and Implementation with Multiple Dimensions - Part 1: Modelling](https://arxiv.org/abs/1801.09777)
*Paul Mireault*

Main category: cs.SE

TL;DR: The abstract discusses the use of dimensions in models, like time in financial spreadsheets, and the common practice of using repeated formula blocks or multiple worksheets to represent a second dimension.


<details>
  <summary>Details</summary>
Motivation: The motivation appears to be the common use of dimensions, particularly time, in everyday models and the challenges of representing a second dimension in spreadsheets.

Method: The provided text is an abstract and does not detail the methodology used in the paper.

Result: The abstract does not provide specific results from the paper.

Conclusion: Abstract analysis is not possible with the provided information. The abstract does not contain details about the paper's method, results, or conclusion.

Abstract: Dimensions are an integral part of many models we use every day. Without
thinking about it, we frequently use the time dimension: many financial and
accounting spreadsheets have columns representing months or years. Representing
a second dimension is often done by repeating blocs of formulas in a worksheet
or creating multiple worksheets with the same structure.

</details>


### [89] [Mitigating Spreadsheet Risk in Complex Multi-Dimensional Models in Excel](https://arxiv.org/abs/1802.01640)
*Steve Litt*

Main category: cs.SE

TL;DR: Microsoft Excel is great but prone to error. This paper proposes a 


<details>
  <summary>Details</summary>
Motivation: Microsoft Excel is the most ubiquitous analytical tool ever built. Companies around the world leverage it for its power, flexibility and ease of use. However, spreadsheets are manually intensive and prone to error, making it difficult for companies to control spreadsheet risk.

Method: The solution is defined as a 

Result: The proposed solution aims to mitigate spreadsheet risk for complex multi-dimensional models.

Conclusion: The proposed solution, 

Abstract: Microsoft Excel is the most ubiquitous analytical tool ever built. Companies
around the world leverage it for its power, flexibility and ease of use.
However, spreadsheets are manually intensive and prone to error, making it
difficult for companies to control spreadsheet risk. The following solution is
designed to mitigate spreadsheet risk for a set of problems commonly addressed
in a spreadsheet defined as "complex multi-dimensional models". "Complex"
referring to certain types of applications that require functionality such as
sophisticated algorithms, challenging hierarchies and database write-back (i.e.
planning, forecasting, etc.) and "multi-dimensional" referring to providing
capabilities such as reporting, data input forms and ad hoc analysis on the
different attributes associated with the resulting model. The solution is
defined as a "PivotModel" because it works similarly to a PivotTable but is
designed to leverage the robust capabilities of the Microsoft Excel platform.

</details>


### [90] [Proposed Spreadsheet Transparency Definition and Measures](https://arxiv.org/abs/1802.01628)
*Craig Hatmaker*

Main category: cs.SE

TL;DR: 缺乏对金融模型透明化的明确定义和度量标准，阻碍了审计师和建模人员的工作。本研究提出了一个具体的电子表格建模透明化定义，以支持度量和自动化工具的开发，并帮助建模人员客观地评估和选择方法。


<details>
  <summary>Details</summary>
Motivation: 审计师要求金融模型透明化，但目前对透明化的确切含义尚无共识。缺乏明确的建模透明化定义，导致无法确定模型何时“透明”。金融建模界在争论哪些方法更透明，但目前缺乏透明化度量标准，这阻碍了建模人员客观评估方法并选择能够提高模型透明度的方法。

Method: 提出了一个具体的电子表格建模透明化定义，并阐述了该定义如何支持度量和自动化工具的创建，以及如何帮助建模人员客观地比较和选择建模方法。

Result: 提出了一个具体的电子表格建模透明化定义，该定义能够支持度量和自动化工具的创建，从而使审计师能够评估模型的透明化程度。

Conclusion: 该论文提出了一个针对电子表格建模透明化的定义，该定义足够具体，可以创建度量和自动化工具，以便审计师能够确定模型是否满足透明化要求。该定义还使建模人员能够客观地比较电子表格建模方法，以选择最符合其目标的方法。

Abstract: Auditors demand financial models be transparent yet no consensus exists on
what that means precisely. Without a clear modeling transparency definition we
cannot know when our models are "transparent". The financial modeling community
debates which methods are more or less transparent as though transparency is a
quantifiable entity yet no measures exist. Without a transparency measure
modelers cannot objectively evaluate methods and know which improves model
transparency.
  This paper proposes a definition for spreadsheet modeling transparency that
is specific enough to create measures and automation tools for auditors to
determine if a model meets transparency requirements. The definition also
provides modelers the ability to objectively compare spreadsheet modeling
methods to select which best meets their goals.

</details>


### [91] [Alternative Spreadsheet Model Designs for an Operations Management Model Embedded in a Periodic Business Process](https://arxiv.org/abs/1802.00484)
*Thomas A. Grossman,Vijay Mehrotra,Mouwafac Sidaoui*

Main category: cs.SE

TL;DR: Evaluated three spreadsheet designs for an operations management model, finding the technical design superior for modification and reuse.


<details>
  <summary>Details</summary>
Motivation: To address the need for model modification and reuse in periodic operations management and supply chain planning, and to evaluate different spreadsheet implementation designs.

Method: Evaluation of three spreadsheet implementations (data-driven, canonical, and technical) based on accuracy, modification, analysis, transfer, training, and technical sophistication.

Result: The data-driven design reveals poor practices by naive modelers. The technical design allows modification without formula editing, speeding up the process and reducing errors, and shows potential for use with other model classes.

Conclusion: The novel technical design is most suitable for modification, analysis, and transfer, with potential for broader application.

Abstract: We present a widely-used operations management model used in supply and
distribution planning, that is typically embedded in a periodic business
process that necessitates model modification and reuse. We consider three
alternative spreadsheet implementations, a data-driven design, a canonical
(textbook) design, and a novel (table-driven) technical design. We evaluate
each regarding suitability for accuracy, modification, analysis, and transfer.
We consider the degree of training and technical sophistication required to
utilize each design. The data-driven design provides insight into poor
spreadsheet practices by na\"ive modelers. The technical design can be modified
for new data and new structural elements without manual writing or editing of
cell formulas, thus speeding modification and reducing risk of error. The
technical design has potential for use with other classes of models. We
identify opportunities for future research.

</details>


### [92] [Mitigating Spreadsheet Model Risk with Python Open Source Infrastructure](https://arxiv.org/abs/1801.09771)
*Oliver Beavers*

Main category: cs.SE

TL;DR: 本文提出了一个使用Python进行电子表格模型审计的方法，以提高准确性。


<details>
  <summary>Details</summary>
Motivation: 电子表格模型缺乏传统软件工程工具和协议，导致错误率较高。

Method: 本文介绍了一种使用Python的开源包为电子表格模型开发可审计工具的方法。

Result: 旨在使电子表格建模专业人员能够开发可重复审计的工具，并创建模型“预言家”来测试和审计电子表格计算。

Conclusion: 通过使用Python进行可审计性开发，可以实现电子表格模型的可重复审计。

Abstract: Across an aggregation of EuSpRIG presentation papers, two maxims hold true:
spreadsheets models are akin to software, yet spreadsheet developers are not
software engineers. As such, the lack of traditional software engineering tools
and protocols invites a higher rate of error in the end result. This paper lays
ground work for spreadsheet modelling professionals to develop reproducible
audit tools using freely available, open source packages built with the Python
programming language, enabling stakeholders to develop clearly defined model
"oracles" with which to test and audit spreadsheet calculations against.

</details>


### [93] [Structuring Spreadsheets with the "Lish" Data Model](https://arxiv.org/abs/1801.08603)
*Alan Hall,Michel Wermelinger,Tony Hirst,Santi Phithakkitnukoon*

Main category: cs.SE

TL;DR: lish是一种新的数据模型，通过嵌套列表和模板来改进电子表格，以更好地处理结构化数据并减少错误。


<details>
  <summary>Details</summary>
Motivation: 电子表格的单元格缺乏对其所属更大结构的认知，这会影响理解并增加公式复制的风险。

Method: 通过将单元格组织成嵌套列表，并允许用户使用模板来原型化重复结构，来探索一种新的数据模型（lish），以作为传统网格的替代方案。

Result: lish数据模型能够捕捉更高层次的结构，同时保留电子表格的简洁性。一个小型演示应用程序展示了lish模型的使用。

Conclusion: 电子表格的单元格缺乏对其所属更大结构的认知，这会影响理解并增加公式复制的风险。lish数据模型通过将单元格组织成嵌套列表，并允许用户使用模板来原型化重复结构，旨在捕捉这些更高层次的结构，同时保留电子表格的简洁性。

Abstract: A spreadsheet is remarkably flexible in representing various forms of
structured data, but the individual cells have no knowledge of the larger
structures of which they may form a part. This can hamper comprehension and
increase formula replication, increasing the risk of error on both scores. We
explore a novel data model (called the "lish") that could form an alternative
to the traditional grid in a spreadsheet-like environment. Its aim is to
capture some of these higher structures while preserving the simplicity that
makes a spreadsheet so attractive. It is based on cells organised into nested
lists, in each of which the user may optionally employ a template to prototype
repeating structures. These template elements can be likened to the marginal
"cells" in the borders of a traditional worksheet, but are proper members of
the sheet and may themselves contain internal structure. A small demonstration
application shows the "lish" in operation.

</details>


### [94] [Automated Refactoring of Nested-IF Formulae in Spreadsheets](https://arxiv.org/abs/1712.09797)
*Jie Zhang,Shi Han,Dan Hao,Lu Zhang,Dongmei Zhang*

Main category: cs.SE

TL;DR: 这项研究提出了一种自动化方法，通过重构电子表格中的嵌套IF公式来提高其可读性并降低维护成本，实验结果表明该方法非常有效。


<details>
  <summary>Details</summary>
Motivation: 嵌套IF公式（nest-IF expressions）是电子表格中一种常见的代码坏味（code smell），其可读性差、认知成本高，且在复用和维护时容易出错。然而，终端用户通常缺乏必要的编程知识来解决或认识到这个问题。

Method: 提出了一种基于抽象语法树（AST）的自动化方法，通过检测和移除逻辑冗余，识别并重组被碎片化的更高级别语义，使用简洁的内置函数来重构嵌套IF公式。

Result: 对包含2700万个嵌套IF公式的68000多个真实世界电子表格进行的评估显示，该方法能够重构超过99%的嵌套IF公式。超过50%的重构将嵌套深度减少了一半以上。用户调查也表明，大多数参与者偏好重构后的公式，并认为这种自动化重构方法是必要且有用的。

Conclusion: 该研究提出了一种基于AST的自动化方法来重构嵌套IF公式，旨在提高可读性并降低维护成本。该方法通过检测和移除冗余逻辑，以及识别和重组高级语义来简化公式。

Abstract: Spreadsheets are the most popular end-user programming software, where
formulae act like programs and also have smells. One well recognized common
smell of spreadsheet formulae is nest-IF expressions, which have low
readability and high cognitive cost for users, and are error-prone during reuse
or maintenance. However, end users usually lack essential programming language
knowledge and skills to tackle or even realize the problem. The previous
research work has made very initial attempts in this aspect, while no effective
and automated approach is currently available.
  This paper firstly proposes an AST-based automated approach to systematically
refactoring nest-IF formulae. The general idea is two-fold. First, we detect
and remove logic redundancy on the AST. Second, we identify higher-level
semantics that have been fragmented and scattered, and reassemble the syntax
using concise built-in functions. A comprehensive evaluation has been conducted
against a real-world spreadsheet corpus, which is collected in a leading IT
company for research purpose. The results with over 68,000 spreadsheets with 27
million nest-IF formulae reveal that our approach is able to relieve the smell
of over 99\% of nest-IF formulae. Over 50% of the refactorings have reduced
nesting levels of the nest-IFs by more than a half. In addition, a survey
involving 49 participants indicates that for most cases the participants prefer
the refactored formulae, and agree on that such automated refactoring approach
is necessary and helpful.

</details>


### [95] [Spreadsheet Guardian: An Approach to Protecting Semantic Correctness throughout the Evolution of Spreadsheets](https://arxiv.org/abs/1612.03813)
*Daniel Kulesz,Verena Käfer,Stefan Wagner*

Main category: cs.SE

TL;DR: Spreadsheet Guardian helps ensure the quality of spreadsheets by separating test rule specification from execution, detecting semantic faults, and protecting users during maintenance. It's easy to use and increases user awareness of spreadsheet correctness.


<details>
  <summary>Details</summary>
Motivation: Spreadsheets play a business-critical role but often contain faults, leading to bad decisions. There is a lack of support for ensuring spreadsheet correctness during collaboration and maintenance.

Method: Developed an approach named Spreadsheet Guardian which separates the specification of spreadsheet test rules from their execution. Implemented a representative testing technique as an add-in for Microsoft Excel.

Result: Evaluated in two empirical studies with 29 end-users and 42 computer science students. Results indicate the technique is easy to learn and apply. Participants using Spreadsheet Guardian were more realistic about spreadsheet correctness after maintenance compared to those using only static analysis techniques.

Conclusion: Spreadsheet Guardian can be of use for business-critical spreadsheets, as it is easy to learn and apply, and helps users be more realistic about the correctness of their spreadsheets after maintenance.

Abstract: Spreadsheets are powerful tools which play a business-critical role in many
organizations. However, many bad decisions taken due to faulty spreadsheets
show that these tools need serious quality assurance. Furthermore, while
collaboration on spreadsheets for maintenance tasks is common, there has been
almost no support for ensuring that the spreadsheets remain correct during this
process.
  We have developed an approach named Spreadsheet Guardian which separates the
specification of spreadsheet test rules from their execution. By automatically
executing user-defined test rules, our approach is able to detect semantic
faults. It also protects all collaborating spreadsheet users from introducing
faults during maintenance, even if only few end-users specify test rules. To
evaluate Spreadsheet Guardian, we implemented a representative testing
technique as an add-in for Microsoft Excel.
  We evaluated the testing technique in two empirical evaluations with 29
end-users and 42 computer science students. The results indicate that the
technique is easy to learn and to apply. Furthermore, after finishing
maintenance, participants with spreadsheets "protected" by the technique are
more realistic about the correctness of their spreadsheets than participants
who employ only "classic", non-interactive test rules based on static analysis
techniques. Hence, we believe Spreadsheet Guardian can be of use for
business-critical spreadsheets.

</details>


### [96] [On Evidence-based Risk Management in Requirements Engineering](https://arxiv.org/abs/1707.00144)
*Daniel Méndez Fernández,Michaela Tießler,Marcos Kalinowski,Michael Felderer,Marco Kuhrmann*

Main category: cs.SE

TL;DR: 本研究提出并验证了一种基于证据的方法，用于通过关于问题、原因和影响的跨公司数据来评估需求工程（RE）中的风险。


<details>
  <summary>Details</summary>
Motivation: 需求工程（RE）对上下文的敏感性使得难以有效控制其中存在的问题，从而阻碍了有效的风险管理，而有效的风险管理能够及早采取纠正甚至预防措施。目前关于特定环境下的需求工程现象的经验知识仍然很少，而这对于需求工程中有效的特定环境风险管理是必不可少的。

Method: 通过对来自228家公司的调查数据进行分析，构建了一个概率网络，用于预测特定环境下的需求工程现象。该方法使用电子表格实现，以支持轻量级的风险评估。

Result: 初步在6家公司进行的验证结果增强了我们对该方法能够提高需求工程中各个风险因素的认识的信心，并且反馈进一步允许将该方法传播到实践中。

Conclusion: 该方法通过增加对需求工程中各个风险因素的认识来提高风险管理意识，并且反馈允许将该方法传播到实践中。

Abstract: Background: The sensitivity of Requirements Engineering (RE) to the context
makes it difficult to efficiently control problems therein, thus, hampering an
effective risk management devoted to allow for early corrective or even
preventive measures. Problem: There is still little empirical knowledge about
context-specific RE phenomena which would be necessary for an effective
context- sensitive risk management in RE. Goal: We propose and validate an
evidence-based approach to assess risks in RE using cross-company data about
problems, causes and effects. Research Method: We use survey data from 228
companies and build a probabilistic network that supports the forecast of
context-specific RE phenomena. We implement this approach using spreadsheets to
support a light-weight risk assessment. Results: Our results from an initial
validation in 6 companies strengthen our confidence that the approach increases
the awareness for individual risk factors in RE, and the feedback further
allows for disseminating our approach into practice.

</details>


### [97] [Tabula: A Language to Model Spreadsheet Tables](https://arxiv.org/abs/1707.02833)
*Jorge Mendes,João Saraiva*

Main category: cs.SE

TL;DR: Tabula is a new, more expressive, and extensible modeling language for spreadsheets that includes a synchronization engine, addressing limitations of previous models.


<details>
  <summary>Details</summary>
Motivation: Spreadsheets are flexible and easy to use but error-prone. Previous model languages for spreadsheets have limited expressiveness and cannot model several features present in real-world spreadsheets.

Method: Introduce the modeling language Tabula, which extends previous spreadsheet models with features like type constraints and nested classes with repetitions. Include a bidirectional transformation engine that guarantees synchronization after an update either in the model or spreadsheet.

Result: Tabula is more expressive than other models, can be extended with more features, and includes a bidirectional transformation engine for synchronization.

Conclusion: Spreadsheets are prone to errors due to their flexibility, but existing modeling languages have limited expressiveness. Tabula, a new modeling language, addresses this by extending previous models with features like type constraints and nested classes, offering greater expressiveness and extensibility. It also includes a bidirectional transformation engine for synchronization.

Abstract: Spreadsheets provide a flexible and easy to use software development
environment, but that leads to error proneness. Work has been done to prevent
errors in spreadsheets, including using models to specify distinct parts of a
spreadsheet as it is done with model-driven software development. Previous
model languages for spreadsheets offer a limited expressiveness, and cannot
model several features present in most real world spreadsheets.
  In this paper, the modeling language Tabula is introduced. It extends
previous spreadsheet models with features like type constraints and nested
classes with repetitions. Tabula is not only more expressive than other models
but it can also be extended with more features. Moreover, Tabula includes a
bidirectional transformation engine that guarantees synchronization after an
update either in the model or spreadsheet.

</details>


### [98] [SpreadCluster: Recovering Versioned Spreadsheets through Similarity-Based Clustering](https://arxiv.org/abs/1704.08476)
*Liang Xu,Wensheng Dou,Chushu Gao,Jie Wang,Jun Wei,Hua Zhong,Tao Huang*

Main category: cs.SE

TL;DR: SpreadCluster 是一种新的电子表格版本聚类算法，通过学习表格特征而非依赖文件名或邮件，提高了聚类准确性，并能处理更大规模的数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过聚类相似电子表格来恢复版本信息，但由于文件名或邮件对话等必要信息通常缺失，导致其适用性和准确性受限。因此，需要一种新的方法来解决电子表格版本信息缺失的问题。

Method: 提出了一种名为 SpreadCluster 的自动聚类算法，该算法通过学习电子表格的版本演化特征（如相似的表头和工作表名称）来将具有相似特征的电子表格自动聚类到同一个演化组中。

Result: SpreadCluster 在 Enron 语料库上的评估结果显示，其聚类精确率和召回率均高于 VEnron 中使用的基于文件名的聚类方法。该算法在 FUSE 和 EUSES 两个语料库上也取得了高精确率的聚类结果。

Conclusion: SpreadCluster 算法能够有效地对版本化的电子表格进行聚类，提高了聚类的精确率和召回率，优于基于文件名的聚类方法。此外，基于 SpreadCluster 的聚类结果创建了一个更大的新语料库 VEnron2，并且该算法在 FUSE 和 EUSES 语料库上也表现出了高精确率。

Abstract: Version information plays an important role in spreadsheet understanding,
maintaining and quality improving. However, end users rarely use version
control tools to document spreadsheet version information. Thus, the
spreadsheet version information is missing, and different versions of a
spreadsheet coexist as individual and similar spreadsheets. Existing approaches
try to recover spreadsheet version information through clustering these similar
spreadsheets based on spreadsheet filenames or related email conversation.
However, the applicability and accuracy of existing clustering approaches are
limited due to the necessary information (e.g., filenames and email
conversation) is usually missing. We inspected the versioned spreadsheets in
VEnron, which is extracted from the Enron Corporation. In VEnron, the different
versions of a spreadsheet are clustered into an evolution group. We observed
that the versioned spreadsheets in each evolution group exhibit certain common
features (e.g., similar table headers and worksheet names). Based on this
observation, we proposed an automatic clustering algorithm, SpreadCluster.
SpreadCluster learns the criteria of features from the versioned spreadsheets
in VEnron, and then automatically clusters spreadsheets with the similar
features into the same evolution group. We applied SpreadCluster on all
spreadsheets in the Enron corpus. The evaluation result shows that
SpreadCluster could cluster spreadsheets with higher precision and recall rate
than the filename-based approach used by VEnron. Based on the clustering result
by SpreadCluster, we further created a new versioned spreadsheet corpus
VEnron2, which is much bigger than VEnron. We also applied SpreadCluster on the
other two spreadsheet corpora FUSE and EUSES. The results show that
SpreadCluster can cluster the versioned spreadsheets in these two corpora with
high precision.

</details>


### [99] [A Conceptual Model for Measuring the Complexity of Spreadsheets](https://arxiv.org/abs/1704.01147)
*Thomas Reschenhofer,Bernhard Waltl,Klym Shumaiev,Florian Matthes*

Main category: cs.SE

TL;DR: 电子表格广泛应用于工业领域，其错误风险与其复杂性密切相关。本研究提出一个概念模型，整合了所有已识别的电子表格复杂性驱动因素，为定义和评估复杂性指标提供了基础，增强了结果的可重复性，并促进了跨领域指标的识别。


<details>
  <summary>Details</summary>
Motivation: 解决目前缺乏对电子表格潜在复杂性驱动因素的共同理解的研究空白，因为电子表格的错误风险与其复杂性密切相关。

Method: 提出一个概念模型，整合了相关文献中识别出的所有潜在的电子表格复杂性驱动因素。

Result: 为结构化定义复杂性指标和增强其结果的可重复性奠定了基础，并有助于识别其他领域的复杂性指标。

Conclusion: 该研究提出了一个整合了所有被文献识别为潜在驱动因素的电子表格复杂性方面的概念模型，为结构化定义复杂性指标奠定了基础，并增强了其结果的可重复性。该模型还有助于识别来自其他科学领域的其他适用复杂性指标。

Abstract: Spreadsheets are widely used in industry, even for critical business
processes. This implies the need for proper risk assessment in spreadsheets to
evaluate the reliability and validity of the spreadsheet's outcome. As related
research has shown, the risk of spreadsheet errors is strongly related to the
spreadsheet's complexity. Therefore, spreadsheet researchers proposed various
metrics for quantifying different aspects of a spreadsheet in order to assess
its complexity. However, until now there is no shared understanding of
potential complexity drivers for spreadsheets. The present work addresses this
research gap by proposing a conceptual model integrating all aspects which are
identified by related literature as potential drivers to spreadsheet
complexity. In this sense, this model forms the foundation for a structured
definition of complexity metrics, and thus enhances the reproducibility of
their results. At the same time, it forms the foundation for identifying
further applicable complexity metrics from other scientific domains.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [100] [The Paradox of Spreadsheet Self-Efficacy: Social Incentives for Informal Knowledge Sharing in End-User Programming](https://arxiv.org/abs/2408.08068)
*Qing,Xia,Advait Sarkar,Duncan P. Brumby,Anna Cox*

Main category: cs.HC

TL;DR: 个人（自我效能感）、社会（声誉收益）和软件相关（编纂工作量）变量会影响电子表格知识共享意愿。


<details>
  <summary>Details</summary>
Motivation: 非正式知识共享（KS）对于最终用户程序员获得专业知识至关重要。

Method: 通过对来自行政和财务部门的电子表格用户（n=100）的调查数据进行多元回归分析，以更好地理解个人（自我效能感）、社会（声誉收益、同事间的信任）和软件相关（编纂工作量）变量如何影响电子表格知识共享意愿。

Result: 高水平的电子表格自我效能感和认为分享会带来声誉收益的看法可以预测更高的知识共享意愿，但那些认为知识编纂工作量大的人的知识共享意愿较低。此外，无论职业如何，用户倾向于报告他们在一般电子表格熟练程度方面的自我效能感较低，尽管他们在工作相关的背景下使用电子表格的自我效能感较高。

Conclusion: 承认并设计这些社会和个人变量，可以帮助避免有经验的个人不必要地回避分享，这对电子表格的设计有影响。

Abstract: Informal Knowledge Sharing (KS) is vital for end-user programmers to gain
expertise. To better understand how personal (self-efficacy), social
(reputational gains, trust between colleagues), and software-related
(codification effort) variables influence spreadsheet KS intention, we
conducted a multiple regressions analysis based on survey data from spreadsheet
users (n=100) in administrative and finance roles. We found that high levels of
spreadsheet self-efficacy and a perception that sharing would result in
reputational gains predicted higher KS intention, but individuals who found
knowledge codification effortful showed lower KS intention. We also observed
that regardless of occupation, users tended to report a lower sense of
self-efficacy in their general spreadsheet proficiency, despite also reporting
high self-efficacy in spreadsheet use for job-related contexts. Our findings
suggest that acknowledging and designing for these social and personal
variables can help avoid situations where experienced individuals refrain
unnecessarily from sharing, with implications for spreadsheet design.

</details>


### [101] [Buckaroo: A Direct Manipulation Visual Data Wrangler](https://arxiv.org/abs/2507.16073)
*Annabelle Warner,Andrew McNutt,Paul Rosen,El Kindi Rezig*

Main category: cs.HC

TL;DR: Data wrangling is time-consuming and error-prone. Buckaroo is a visualization system that helps by finding data anomalies, suggesting repairs, and allowing direct visual manipulation, making the process more efficient and accurate.


<details>
  <summary>Details</summary>
Motivation: Data wrangling, a critical phase in data science, consumes up to 80% of project time and is prone to errors using traditional manual coding or spreadsheet methods. These errors can impact downstream tasks.

Method: Buckaroo is a visualization system that automatically finds data groups with anomalies, recommends repairs, and allows users to visually manipulate data with undo/redo capabilities.

Result: Buckaroo addresses challenges in data wrangling by providing a visualization system that highlights discrepancies, suggests repairs, and supports iterative data manipulation.

Conclusion: Buckaroo highlights discrepancies in data and enables on-the-spot corrections through direct manipulations of visual objects, addressing the laborious and error-prone nature of traditional data wrangling methods.

Abstract: Preparing datasets -- a critical phase known as data wrangling -- constitutes
the dominant phase of data science development, consuming upwards of 80% of the
total project time. This phase encompasses a myriad of tasks: parsing data,
restructuring it for analysis, repairing inaccuracies, merging sources,
eliminating duplicates, and ensuring overall data integrity. Traditional
approaches, typically through manual coding in languages such as Python or
using spreadsheets, are not only laborious but also error-prone. These issues
range from missing entries and formatting inconsistencies to data type
inaccuracies, all of which can affect the quality of downstream tasks if not
properly corrected. To address these challenges, we present Buckaroo, a
visualization system to highlight discrepancies in data and enable on-the-spot
corrections through direct manipulations of visual objects. Buckaroo (1)
automatically finds "interesting" data groups that exhibit anomalies compared
to the rest of the groups and recommends them for inspection; (2) suggests
wrangling actions that the user can choose to repair the anomalies; and (3)
allows users to visually manipulate their data by displaying the effects of
their wrangling actions and offering the ability to undo or redo these actions,
which supports the iterative nature of data wrangling. A video companion is
available at https://youtu.be/iXdCYbvpQVE

</details>


### [102] [SheetMind: An End-to-End LLM-Powered Multi-Agent Framework for Spreadsheet Automation](https://arxiv.org/abs/2506.12339)
*Ruiyan Zhu,Xi Cheng,Ke Liu,Brian Zhu,Daniel Jin,Neeraj Parihar,Zhoutian Xu,Oliver Gao*

Main category: cs.HC

TL;DR: SheetMind是一个利用LLMs和多智能体（管理、动作、反思）的框架，通过自然语言自动化Google Sheets操作，成功率达70%-80%。


<details>
  <summary>Details</summary>
Motivation: 旨在通过自然语言指令实现电子表格自动化，解决用户无需脚本或公式知识即可操作电子表格的需求。

Method: SheetMind是一个由LLMs驱动的模块化多智能体框架，通过自然语言指令实现电子表格自动化。该系统包含三个智能体：管理智能体（分解用户指令）、动作智能体（将指令翻译成BNF语法命令）和反思智能体（验证动作与用户意图的对齐）。通过Google Sheets插件集成，SheetMind支持实时交互，无需脚本或公式知识。

Result: 在基准数据集上的实验表明，SheetMind在单步任务上的成功率为80%，在多步指令上的成功率约为70%，优于其他模型。

Conclusion: SheetMind通过多智能体分解和基于语法的执行有效地实现了自然语言与电子表格功能的结合。

Abstract: We present SheetMind, a modular multi-agent framework powered by large
language models (LLMs) for spreadsheet automation via natural language
instructions. The system comprises three specialized agents: a Manager Agent
that decomposes complex user instructions into subtasks; an Action Agent that
translates these into structured commands using a Backus Naur Form (BNF)
grammar; and a Reflection Agent that validates alignment between generated
actions and the user's original intent. Integrated into Google Sheets via a
Workspace extension, SheetMind supports real-time interaction without requiring
scripting or formula knowledge. Experiments on benchmark datasets demonstrate
an 80 percent success rate on single step tasks and approximately 70 percent on
multi step instructions, outperforming ablated and baseline variants. Our
results highlight the effectiveness of multi agent decomposition and grammar
based execution for bridging natural language and spreadsheet functionalities.

</details>


### [103] ["How do you even know that stuff?": Barriers to expertise sharing among spreadsheet users](https://arxiv.org/abs/2506.09216)
*Qing,Xia,Advait Sarkar,Duncan Brumby,Anna Cox*

Main category: cs.HC

TL;DR: Spreadsheet experts struggle to share their knowledge due to social factors and software design that prioritizes initial usability over long-term learning, creating challenges for collaborative learning and expertise retention in organizations.


<details>
  <summary>Details</summary>
Motivation: The research aims to understand why spreadsheet experts often fail to disseminate their knowledge to colleagues, despite the benefits of expertise sharing for organizational skill retention. It hypothesizes that social norms and beliefs about spreadsheet use value significantly impact user engagement in sharing behaviors.

Method: The study employed 31 semi-structured interviews with professional spreadsheet users from two distinct samples to investigate the influence of social norms and beliefs on knowledge sharing behaviors.

Result: Spreadsheet providers encounter difficulties in tailoring personalized strategies to subjective standards and determining the opportune social timing for sharing. Furthermore, conflicting self-assessments of expertise, negative perceptions of the knowledge's value, and worries about collaborative disruptions hinder knowledge dissemination. These factors suggest challenges in long-term learning within software emphasizing initial ease of use.

Conclusion: This research demonstrates that the complex interplay between technological design and social dynamics influences collaborative learning behaviors in feature-rich software like spreadsheets. It highlights challenges in knowledge sharing among spreadsheet experts, including adapting personalized strategies, subjective standards, social timing, self-evaluation conflicts, dismissive beliefs about knowledge value, and collaboration disruption concerns. These issues are linked to the design of software prioritizing initial learnability over long-term learning.

Abstract: Spreadsheet collaboration provides valuable opportunities for learning and
expertise sharing between colleagues. Sharing expertise is essential for the
retention of important technical skillsets within organisations, but previous
studies suggest that spreadsheet experts often fail to disseminate their
knowledge to others. We suggest that social norms and beliefs surrounding the
value of spreadsheet use significantly influence user engagement in sharing
behaviours. To explore this, we conducted 31 semi-structured interviews with
professional spreadsheet users from two separate samples. We found that
spreadsheet providers face challenges in adapting highly personalised
strategies to often subjective standards and evaluating the appropriate social
timing of sharing. In addition, conflicted self-evaluations of one's
spreadsheet expertise, dismissive normative beliefs about the value of this
knowledge, and concerns about the potential disruptions associated with
collaboration can further deter sharing. We suggest these observations reflect
the challenges of long-term learning in feature-rich software designed
primarily with initial learnability in mind. We therefore provide implications
for design to navigate this tension. Overall, our findings demonstrate how the
complex interaction between technology design and social dynamics can shape
collaborative learning behaviours in the context of feature-rich software.

</details>


### [104] [Prompting in the Dark: Assessing Human Performance in Prompt Engineering for Data Labeling When Gold Labels Are Absent](https://arxiv.org/abs/2502.11267)
*Zeyu He,Saniya Naphade,Ting-Hao 'Kenneth' Huang*

Main category: cs.HC

TL;DR: 本研究通过 PromptingSheet 插件探讨了在没有黄金标准标签的情况下进行 LLM 数据标记的“暗提示”方法。研究发现，该方法可靠性低，大多数用户在多次迭代后标记准确性没有提高，并强调了黄金标准标签和自动化支持的重要性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨在没有黄金标准标签的情况下，用户在提示工程方面的能力，以及他们通过多轮提示是否能更接近期望的输出。这对于 LLM 数据标记至关重要。

Method: 本研究调查了 LLM 驱动的数据标记场景，即“暗提示”，用户在没有手动标记基准的情况下迭代提示 LLMs 来标记数据。研究开发了 PromptingSheet，这是一个 Google 表格插件，用户可以通过电子表格编写、修改和迭代标记数据。

Result: 通过对 20 名参与者的研究发现，“暗提示”的可靠性很低，只有 9 名参与者在四次或更多次迭代后提高了标记准确性。

Conclusion: 在没有黄金标准标签的情况下，自动提示优化工具（如 DSPy）在标记准确性方面表现不佳。研究强调了黄金标准标签的重要性，以及在未来工具设计中自动化支持的必要性和风险。

Abstract: Millions of users prompt large language models (LLMs) for various tasks, but
how good are people at prompt engineering? Do users actually get closer to
their desired outcome over multiple iterations of their prompts? These
questions are crucial when no gold-standard labels are available to measure
progress. This paper investigates a scenario in LLM-powered data labeling,
"prompting in the dark," where users iteratively prompt LLMs to label data
without using manually-labeled benchmarks. We developed PromptingSheet, a
Google Sheets add-on that enables users to compose, revise, and iteratively
label data through spreadsheets. Through a study with 20 participants, we found
that prompting in the dark was highly unreliable-only 9 participants improved
labeling accuracy after four or more iterations. Automated prompt optimization
tools like DSPy also struggled when few gold labels were available. Our
findings highlight the importance of gold labels and the needs, as well as the
risks, of automated support in human prompt engineering, providing insights for
future tool design.

</details>


### [105] [When Copilot Becomes Autopilot: Generative AI's Critical Risk to Knowledge Work and a Critical Solution](https://arxiv.org/abs/2412.15030)
*Advait Sarkar,Xiaotong,Xu,Neil Toronto,Ian Drosos,Christian Poelitz*

Main category: cs.HC

TL;DR: 生成式AI在提高工作效率的同时，可能削弱用户的批判性思维。本文提出了一种通过AI生成“挑衅”来辅助批判性思维的原型系统，并探讨了相关研究方向。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在为知识工作带来机遇的同时，也带来了风险。特别是在电子表格工作流程中，AI幻觉并非最大风险，更大的风险在于随着越来越多工作可以安全地委托给AI，人类批判性思维能力可能会被削弱。因此，需要设计能够培养批判性思维的生成式AI系统。

Method: 提出了一种用于电子表格中关键筛选活动的生成式AI原型系统。该系统利用生成式AI提出筛选标准，并将这些标准应用于对电子表格中的行进行排序。此外，它还生成“挑衅”文本，用以批评AI生成的标准，并强调潜在的风险、缺点和替代方案。

Result: 该原型系统为现代AI辅助知识工作探索了一个丰富且全新的批判性思维工具设计空间。研究还为将AI作为批评者或挑衅者提出了研究议程，探讨了挑衅出现的时机、形式、内容以及潜在的设计权衡。

Conclusion: 生成式AI可能通过引入错误来危害知识工作，但它也为用户提供了前所未有的机会，可以学习和应用高级软件功能，并极大地扩展他们能够成功完成的任务的范围和复杂性。关键在于设计生成式AI系统的界面，以培养和鼓励知识工作中的批判性思维，而不是仅仅关注AI幻觉问题。

Abstract: Generative AI, with its tendency to "hallucinate" incorrect results, may pose
a risk to knowledge work by introducing errors. On the other hand, it may also
provide unprecedented opportunities for users, particularly non-experts, to
learn and apply advanced software features and greatly increase the scope and
complexity of tasks they can successfully achieve.
  As an example of a complex knowledge workflow that is subject to risks and
opportunities from generative AI, we consider the spreadsheet. AI
hallucinations are an important challenge, but they are not the greatest risk
posed by generative AI to spreadsheet workflows. Rather, as more work can be
safely delegated to AI, the risk is that human critical thinking -- the ability
to holistically and rigorously evaluate a problem and its solutions -- is
degraded in the process. The solution is to design the interfaces of generative
AI systems deliberately to foster and encourage critical thinking in knowledge
work, building primarily on a long history of research on critical thinking
tools for education.
  We discuss a prototype system for the activity of critical shortlisting in
spreadsheets. The system uses generative AI to suggest shortlisting criteria
and applies these criteria to sort rows in a spreadsheet. It also generates
"provocations": short text snippets that critique the AI-generated criteria,
highlighting risks, shortcomings, and alternatives. Our prototype opens up a
rich and completely unexplored design space of critical thinking tools for
modern AI-assisted knowledge work. We outline a research agenda for AI as a
critic or provocateur, including questions about where and when provocations
should appear, their form and content, and potential design trade-offs.

</details>


### [106] [Understanding and Evaluating Trust in Generative AI and Large Language Models for Spreadsheets](https://arxiv.org/abs/2412.14062)
*Simon Thorne*

Main category: cs.HC

TL;DR: 生成式AI可用于创建电子表格公式，但其输出可能不准确或不可靠。本研究提出了一个框架，通过评估公式的透明度和可依赖性来解决这些问题，并探讨了导致这些问题的因素以及不信任技术的后果。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和大型语言模型（LLMs）在自动化电子表格公式创建方面展现出潜力，但由于幻觉、偏见和用户技能差异，其输出的准确性和可信度无法保证。因此，有必要研究一个框架来评估和提高生成公式的可信度。

Method: 本研究提出一个可信度框架，通过评估公式的可解释性、可见性、可靠性和合乎道德的考量（偏见和公平性）来解决生成式AI在电子表格公式创建中的问题。同时，研究还考察了导致这些问题（如幻觉、训练数据偏见和不正确的提示）的驱动因素，并探讨了对技术不信任的例子及其后果。

Result: 本研究提出的可信度框架，通过评估公式的透明度和可依赖性，为解决生成式AI在电子表格公式创建中的问题提供了方法。透明度通过可解释性和可见性来衡量，可依赖性则通过可靠性和道德考量来评估。

Conclusion: Generative AI在电子表格公式创建方面具有自动化潜力，但其输出的准确性和可信度仍面临挑战。本研究提出的可信度框架通过评估公式的可解释性、可见性、可靠性和合乎道德的考量，解决了这些问题。

Abstract: Generative AI and Large Language Models (LLMs) hold promise for automating
spreadsheet formula creation. However, due to hallucinations, bias and variable
user skill, outputs obtained from generative AI cannot be assumed to be
accurate or trustworthy. To address these challenges, a trustworthiness
framework is proposed based on evaluating the transparency and dependability of
the formula. The transparency of the formula is explored through explainability
(understanding the formula's reasoning) and visibility (inspecting the
underlying algorithms). The dependability of the generated formula is evaluated
in terms of reliability (consistency and accuracy) and ethical considerations
(bias and fairness). The paper also examines the drivers to these metrics in
the form of hallucinations, training data bias and poorly constructed prompts.
Finally, examples of mistrust in technology are considered and the consequences
explored.

</details>


### [107] [Dynamic Prompt Middleware: Contextual Prompt Refinement Controls for Comprehension Tasks](https://arxiv.org/abs/2412.02357)
*Ian Drosos,Jack Williams,Advait Sarkar,Nicholas Wilson*

Main category: cs.HC

TL;DR: 本研究旨在解决用户在引导生成式AI（特别是理解任务）时遇到的提示难题。我们比较了两种提示中间件方法：Dynamic PRC（动态生成上下文相关的控制选项）和Static PRC（提供预设的通用选项）。用户研究表明，Dynamic PRC提供了更强的用户控制感，降低了提供上下文的门槛，但用户在理解控制效果方面仍有挑战。总体而言，动态提示中间件有望改善用户体验。


<details>
  <summary>Details</summary>
Motivation: 许多用户在有效引导生成式AI方面存在困难，尤其是在为理解任务（如解释电子表格公式、Python代码和文本段落）提供上下文时。虽然提示中间件旨在帮助构建提示，但用户在表达足够的控制力以获得符合其偏好的AI响应方面仍存在障碍。

Method: 通过一项包含38名参与者的形成性调查，研究用户在理解任务中控制AI生成解释的需求，揭示了标准化但可预测的支持与定制化但不可预测的支持之间的权衡。在此基础上，实现了动态提示优化控制（Dynamic PRC）和静态提示优化控制（Static PRC）两种提示中间件方法。Dynamic PRC生成特定于上下文的UI元素，根据用户提示和用户对AI的需求提供提示优化；Static PRC提供一组预设的通用优化选项。最后，通过包含16名参与者的用户研究评估了这两种方法，以评估它们对用户控制AI响应以生成更好解释的影响。

Result: 研究结果显示，用户倾向于使用Dynamic PRC方法，因为它提供了更多的控制力，降低了提供上下文的门槛，并鼓励了对任务的探索和反思。然而，推断不同生成控制对最终输出的影响仍然具有挑战性。

Conclusion: 动态提示中间件可以提升生成式AI工作流的用户体验，提供更大的控制力，并引导用户获得更好的AI响应。

Abstract: Effective prompting of generative AI is challenging for many users,
particularly in expressing context for comprehension tasks such as explaining
spreadsheet formulas, Python code, and text passages. Prompt middleware aims to
address this barrier by assisting in prompt construction, but barriers remain
for users in expressing adequate control so that they can receive AI-responses
that match their preferences.
  We conduct a formative survey (n=38) investigating user needs for control
over AI-generated explanations in comprehension tasks, which uncovers a
trade-off between standardized but predictable support for prompting, and
adaptive but unpredictable support tailored to the user and task. To explore
this trade-off, we implement two prompt middleware approaches: Dynamic Prompt
Refinement Control (Dynamic PRC) and Static Prompt Refinement Control (Static
PRC). The Dynamic PRC approach generates context-specific UI elements that
provide prompt refinements based on the user's prompt and user needs from the
AI, while the Static PRC approach offers a preset list of generally applicable
refinements.
  We evaluate these two approaches with a controlled user study (n=16) to
assess the impact of these approaches on user control of AI responses for
crafting better explanations. Results show a preference for the Dynamic PRC
approach as it afforded more control, lowered barriers to providing context,
and encouraged exploration and reflection of the tasks, but that reasoning
about the effects of different generated controls on the final output remains
challenging. Drawing on participant feedback, we discuss design implications
for future Dynamic PRC systems that enhance user control of AI responses. Our
findings suggest that dynamic prompt middleware can improve the user experience
of generative AI workflows by affording greater control and guide users to a
better AI response.

</details>


### [108] [Subject integration with spreadsheets -- Ignoring education is the greatest risk ever](https://arxiv.org/abs/2409.12975)
*Mária Csernoch,Ádám Gulácsi,Júlia Csernoch*

Main category: cs.HC

TL;DR: 在电子表格中解决三年级任务，利用技术、教学和内容知识的框架，培养师生的技能和计算机科学知识，并为学生的未来职业做好准备。


<details>
  <summary>Details</summary>
Motivation: 技术、教学和内容知识的框架为学校提供了一种有意义的数字化和数字化的解决方案，即主题整合。本研究旨在探索这种方法，并确定其对师生在技能、能力和计算机科学知识方面的影响。

Method: 本研究探讨了在技术、教学和内容知识的框架内，如何将主题整合作为学校实现有意义的数字化和数字化的解决方案。研究详细介绍了如何用电子表格解决三个传统的三年级任务，以及教师和学生的技能、能力和计算机科学知识的发展。

Result: 通过在电子表格中解决三年级任务，可以培养师生的技能、能力和计算机科学知识。研究强调，分析、理解、规划和讨论任务的过程与在电子表格中进行活动同等重要，因为这个过程对学生未来的职业发展至关重要。

Conclusion: 该研究表明，在电子表格中解决三年级传统任务可以培养师生的相关技能、能力和计算机科学知识。此外，分析、理解、规划和讨论任务与在电子表格中进行活动同样重要，这有助于为学生未来的职业生涯做好准备。

Abstract: Within the framework of Technological Pedagogical and Content Knowledge,
subject integration is one possible solution for the introduction of meaningful
digitalization and digitization in schools. This process incorporates that any
school subject can be taught with digital support, informatics (computer)
classes can be contextualized, and the gap between 'serious informatics' and
'digital literacy' can be minimized. The present paper details how three
traditional Grade 3 tasks can be solved in spreadsheets, what skills,
competencies, and computer science knowledge of both teachers and students can
be developed. The solutions also reveal that analysing, understanding,
planning, and discussing tasks is as important as the activity in the
spreadsheets, which process plays a crucial role in the preparation of students
for their future jobs.

</details>


### [109] [Exploring Higher Education Competencies through Spreadsheet Self-Assessment and Time](https://arxiv.org/abs/2409.12974)
*Maria Csernoch,Judit T. Kiss,Viktor Takács,Domicián Máté*

Main category: cs.HC

TL;DR: 大学生对电子表格能力的自我评估不准确，在数字环境中学习效率低于纸质环境，这挑战了“数字原住民”的固有能力假设。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探究大学生在电子表格能力和可靠性方面的表现，并探讨自我评估与实际操作之间的差异，以及数字环境对学习效率的影响。

Method: 本研究通过对大学生进行自我评估和实际问题解决练习，旨在探究其电子表格能力和可靠性。

Result: 研究结果显示，学生倾向于高估自己在电子制作表格方面的能力，并且在数字环境中完成任务所需的时间是纸质环境的两倍以上。尽管有“数字原住民”的假设，但学生的表现并未证实这一点。

Conclusion: 本研究结果表明，大学生往往不能准确评估自己在电子表格方面的能力，并且在数字环境中完成任务需要更长的时间。研究还发现，所谓的“数字原住民”学生在电子表格任务上的表现并不比纸质任务好，这挑战了他们天生具备计算机技能的普遍假设。

Abstract: The present paper aims to explore higher education students' spreadsheet
competencies and reliability through self-assessment and real-world
problem-solving practices. Digital natives alleged skills and competences
allowed us to hypothesize that students perform better in Excel than on paper,
but the findings cannot confirm this hypothesis. However, our results indicate
that students tend to inaccurately assess their spreadsheet competencies
compared to their actual performance in both paper-based and Excel tasks. It
has also be found that students need at least twice as much time to achieve the
same high scores in the digital environment as they do on paper. The results
violated the widely accepted assumption that digital native students do not
need computer science education, since they are born with it. This study
highlights the importance of accurate self-assessment in digital skill
development and time management within higher education contexts, particularly
in technology-driven disciplines.

</details>


### [110] [Data Guards: Challenges and Solutions for Fostering Trust in Data](https://arxiv.org/abs/2407.14042)
*Nicole Sultanum,Dennis Bromley,Michael Correll*

Main category: cs.HC

TL;DR: 访谈表明，数据验证和核查具有普遍需求，但现有标准不足。因此，我们提出了一套数据卫士：用于培养数据制品信任度的方法和工具。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的决策面临着从脏数据到故意欺骗的多种威胁，因此利用数据（尤其是新的或不熟悉的数据）需要一定程度的信任或验证。

Method: 通过对数据制品（如图表、仪表盘等数据生态系统产出物）的生产者和消费者进行一系列访谈，理解建立数据信任的策略和障碍。

Result: 访谈显示，数据验证和核查具有普遍需求，但在现有标准中缺乏，尤其是在数据消费者中。

Conclusion: 需要建立一套数据卫士：用于培养数据制品信任度的方法和工具。

Abstract: From dirty data to intentional deception, there are many threats to the
validity of data-driven decisions. Making use of data, especially new or
unfamiliar data, therefore requires a degree of trust or verification. How is
this trust established? In this paper, we present the results of a series of
interviews with both producers and consumers of data artifacts (outputs of data
ecosystems like spreadsheets, charts, and dashboards) aimed at understanding
strategies and obstacles to building trust in data. We find a recurring need,
but lack of existing standards, for data validation and verification,
especially among data consumers. We therefore propose a set of data guards:
methods and tools for fostering trust in data artifacts.

</details>


### [111] [Smart Portable Computer](https://arxiv.org/abs/2405.05292)
*Niladri Das*

Main category: cs.HC

TL;DR: 疫情期间学生难以购买电脑，本文提出便携式智能计算机，性能媲美台式机，且小巧、节能、价廉，支持编程。


<details>
  <summary>Details</summary>
Motivation: 在COVID-19大流行期间，许多学生难以获得价格在15,000印度卢比左右但配置不足的个人电脑，这促使了便携式智能计算机的开发。

Method: 本文提出了一种名为“便携式智能计算机”的创新设备，旨在解决学生在疫情期间难以购买到配置合适的个人电脑的问题。

Result: 该设备体积小巧、能耗低、成本效益高，支持文档编辑、多标签浏览、电子表格管理、演示文稿创建以及Python、C、C++等编程语言和Keil、Xilinx等编译器的使用。

Conclusion: 该便携式智能计算机设备提供了与传统台式机相当的速度和性能，同时体积小巧、能耗低且成本效益高，能够提供无缝的桌面体验，满足学生和程序员的需求。

Abstract: Amidst the COVID-19 pandemic, with many organizations, schools, colleges, and
universities transitioning to virtual platforms, students encountered
difficulties in acquiring PCs such as desktops or laptops. The starting prices,
around 15,000 INR, often failed to offer adequate system specifications, posing
a challenge for consumers. Additionally, those reliant on laptops for work
found the conventional approach cumbersome. Enter the "Portable Smart
Computer," a leap into the future of computing. This innovative device boasts
speed and performance comparable to traditional desktops but in a compact,
energy-efficient, and cost-effective package. It delivers a seamless desktop
experience, whether one is editing documents, browsing multiple tabs, managing
spreadsheets, or creating presentations. Moreover, it supports programming
languages like Python, C, C++, as well as compilers such as Keil and Xilinx,
catering to the needs of programmers.

</details>


### [112] ["My toxic trait is thinking I'll remember this": gaps in the learner experience of video tutorials for feature-rich software](https://arxiv.org/abs/2404.07114)
*Ian Drosos,Advait Sarkar,Andrew D. Gordon*

Main category: cs.HC

TL;DR: This paper analyzes "gaps" that hinder learning in video tutorials, especially for software like Excel. It categorizes these gaps using viewer comments and creator interviews, and proposes design solutions to improve the learning experience.


<details>
  <summary>Details</summary>
Motivation: Video tutorials are a popular learning medium, but learners encounter 'gaps' that hinder their learning, especially with complex software like spreadsheets. This research aims to understand these gaps, their impact on learning, and how to address them.

Method: The study analyzes 360 viewer comments from 90 Microsoft Excel video tutorials and conducts contextual interviews with 8 influential tutorial creators. It also presents and gathers feedback on two design prototypes aimed at addressing identified gaps.

Result: The study develops a theory and taxonomy of learning gaps encountered in video tutorials for feature-rich software. It provides insights into creators' processes and frustrations and gathers feedback on design solutions to mitigate these gaps.

Conclusion: The study identifies and categorizes learning barriers (

Abstract: Video tutorials are a popular medium for informal and formal learning.
However, when learners attempt to view and follow along with these tutorials,
they encounter what we call gaps, that is, issues that can prevent learning. We
examine the gaps encountered by users of video tutorials for feature-rich
software, such as spreadsheets. We develop a theory and taxonomy of such gaps,
identifying how they act as barriers to learning, by collecting and analyzing
360 viewer comments from 90 Microsoft Excel video tutorials published by 43
creators across YouTube, TikTok, and Instagram. We conducted contextual
interviews with 8 highly influential tutorial creators to investigate the gaps
their viewers experience and how they address them. Further, we obtain insights
into their creative process and frustrations when creating video tutorials.
Finally, we present creators with two designs that aim to address gaps
identified in the comment analysis for feedback and alternative design ideas.

</details>


### [113] [Supporting Annotators with Affordances for Efficiently Labeling Conversational Data](https://arxiv.org/abs/2403.07762)
*Austin Z. Henley,David Piorkowski*

Main category: cs.HC

TL;DR: CAL 是一种新颖的数据标注接口，旨在通过防止错误标签、提供指导、整合文档和方便地查看历史标签来提高效率和用户体验，用户研究结果显示其优于传统电子表格。


<details>
  <summary>Details</summary>
Motivation: 为了解决手动数据标注耗时且成本高昂的问题，需要一种更有效的数据标注方法。

Method: 设计并实现了一个名为 CAL 的新颖接口，以辅助数据标注。CAL 的主要设计包括防止选择不合适的标签、在用户需要时指导用户选择合适的标签、将标注文档集成到接口中，并提供一种查看先前标签的有效方法。通过用户研究将 CAL 与标准电子表格进行了比较。

Result: 用户研究表明，使用 CAL 的用户报告的认知负荷较低，任务时间没有增加，并且认为 CAL 比电子表格更容易使用。

Conclusion: 用户发现 CAL 比标准电子表格更容易使用，并且更喜欢 CAL，同时认知负荷更低，任务时间没有增加。

Abstract: Without well-labeled ground truth data, machine learning-based systems would
not be as ubiquitous as they are today, but these systems rely on substantial
amounts of correctly labeled data. Unfortunately, crowdsourced labeling is time
consuming and expensive. To address the concerns of effort and tedium, we
designed CAL, a novel interface to aid in data labeling. We made several key
design decisions for CAL, which include preventing inapt labels from being
selected, guiding users in selecting an appropriate label when they need
assistance, incorporating labeling documentation into the interface, and
providing an efficient means to view previous labels. We implemented a
production-quality implementation of CAL and report a user-study evaluation
that compares CAL to a standard spreadsheet. Key findings of our study include
users using CAL reported lower cognitive load, did not increase task time,
users rated CAL to be easier to use, and users preferred CAL over the
spreadsheet.

</details>


### [114] [Prompting for Discovery: Flexible Sense-Making for AI Art-Making with Dreamsheets](https://arxiv.org/abs/2310.09985)
*Shm Garanganao Almeda,J. D. Zamfirescu-Pereira,Kyu Won Kim,Pradeep Mani Rathnam,Bjoern Hartmann*

Main category: cs.HC

TL;DR: DreamSheets 通过电子表格界面和 LLM 驱动的功能，帮助用户探索和控制文本到图像模型，通过两项研究发现了支持 TTI 设计空间探索的策略和界面需求。


<details>
  <summary>Details</summary>
Motivation: 探索如何在人工智能生成图像（TTI）模型的设计空间中，通过界面支持最终用户可靠地将提示词空间的探索引导至有趣的结果，因为提示词的微小变化可能导致生成图像的巨大差异。

Method: 设计了一个名为 DreamSheets 的探针，它在一个电子表格界面中集成了基于大语言模型的函数，用于辅助提示词构建和同时显示生成结果，以支持用户探索。

Result: 通过一项初步实验室研究和一项包含五位艺术家纵向研究，揭示了用户为应对TTI设计空间探索挑战所采用的一系列策略，以及支持这些策略所需的界面功能，并提出了一个UI模型来指导未来的界面设计。

Conclusion: 该研究提炼了用户在文本到图像（TTI）设计空间探索中遇到的挑战，并提出了支持这些挑战的界面功能，例如使用文本生成来定义探索的局部“轴”。

Abstract: Design space exploration (DSE) for Text-to-Image (TTI) models entails
navigating a vast, opaque space of possible image outputs, through a
commensurately vast input space of hyperparameters and prompt text. Minor
adjustments to prompt input can surface unexpectedly disparate images. How can
interfaces support end-users in reliably steering prompt-space explorations
towards interesting results? Our design probe, DreamSheets, supports
exploration strategies with LLM-based functions for assisted prompt
construction and simultaneous display of generated results, hosted in a
spreadsheet interface. The flexible layout and novel generative functions
enable experimentation with user-defined workflows. Two studies, a preliminary
lab study and a longitudinal study with five expert artists, revealed a set of
strategies participants use to tackle the challenges of TTI design space
exploration, and the interface features required to support them - like using
text-generation to define local "axes" of exploration. We distill these
insights into a UI mockup to guide future interfaces.

</details>


### [115] [Does Using ChatGPT Result in Human Cognitive Augmentation?](https://arxiv.org/abs/2401.11042)
*Ron Fulbright,Miranda Morrison*

Main category: cs.HC

TL;DR: 使用 ChatGPT 并不总能增强认知能力，它可能误导用户，导致认知能力下降，并且不能取代人类的判断。


<details>
  <summary>Details</summary>
Motivation: 探讨了使用 ChatGPT 增强人类认知能力。

Method: 通过两个实验对比了使用 ChatGPT 和不使用 ChatGPT 创建的响应结果。

Result: 研究发现，使用 ChatGPT 并不总能增强认知能力，也不能取代人类的判断、辨别和评估。在某些情况下，ChatGPT 甚至会误导用户，导致认知能力下降。

Conclusion: 使用 ChatGPT 并不总能增强认知能力，在某些任务类型中，它还不能替代人类的判断、辨别和评估。事实上，ChatGPT 可能会误导用户，导致认知能力下降。

Abstract: Human cognitive performance is enhanced by the use of tools. For example, a
human can produce a much greater, and more accurate, volume of mathematical
calculation in a unit of time using a calculator or a spreadsheet application
on a computer. Such tools have taken over the burden of lower level cognitive
grunt work but the human still serves the role of the expert performing higher
level thinking and reasoning. Recently, however, unsupervised, deep, machine
learning has produced cognitive systems able to outperform humans in several
domains. When humans use these tools in a human cog ensemble, the cognitive
ability of the human is augmented. In some cases, even non experts can achieve,
and even exceed, the performance of experts in a particular domain, synthetic
expertise. A new cognitive system, ChatGPT, has burst onto the scene during the
past year. This paper investigates human cognitive augmentation due to using
ChatGPT by presenting the results of two experiments comparing responses
created using ChatGPT with results created not using ChatGPT. We find using
ChatGPT does not always result in cognitive augmentation and does not yet
replace human judgement, discernment, and evaluation in certain types of tasks.
In fact, ChatGPT was observed to result in misleading users resulting in
negative cognitive augmentation.

</details>


### [116] [Co-audit: tools to help humans double-check AI-generated content](https://arxiv.org/abs/2310.01297)
*Andrew D. Gordon,Carina Negreanu,José Cambronero,Rasika Chakravarthy,Ian Drosos,Hao Fang,Bhaskar Mitra,Hannah Richardson,Advait Sarkar,Stephanie Simmons,Jack Williams,Ben Zorn*

Main category: cs.HC

TL;DR: AI-generated content is getting complex, making it hard to check. Co-audit tools help users double-check AI output, especially for critical tasks like spreadsheet calculations. This paper discusses the importance, principles, and challenges of these tools.


<details>
  <summary>Details</summary>
Motivation: Users need to check AI-generated content for correctness, but as AI output becomes more complex (e.g., summaries, tables, code), it's harder for users to audit its quality or correctness. This has led to the emergence of tool-assisted experiences, known as co-audit tools, to help users double-check AI content.

Method: This paper describes research on co-audit tools for spreadsheet computations powered by generative models, proposes a preliminary list of principles for co-audit, and outlines research challenges.

Result: Co-audit tools complement prompt engineering by assisting users in both constructing input prompts and checking output responses. The paper focuses on co-audit tools for spreadsheet computations, highlighting their essential nature for generative AI applications where errors have significant consequences.

Conclusion: As AI-generated content becomes more complex and harder to audit, tool-assisted co-audit experiences are essential for applications where quality and correctness are critical, such as spreadsheet computations. This paper outlines principles and research challenges for developing effective co-audit tools.

Abstract: Users are increasingly being warned to check AI-generated content for
correctness. Still, as LLMs (and other generative models) generate more complex
output, such as summaries, tables, or code, it becomes harder for the user to
audit or evaluate the output for quality or correctness. Hence, we are seeing
the emergence of tool-assisted experiences to help the user double-check a
piece of AI-generated content. We refer to these as co-audit tools. Co-audit
tools complement prompt engineering techniques: one helps the user construct
the input prompt, while the other helps them check the output response. As a
specific example, this paper describes recent research on co-audit tools for
spreadsheet computations powered by generative models. We explain why co-audit
experiences are essential for any application of generative AI where quality is
important and errors are consequential (as is common in spreadsheet
computations). We propose a preliminary list of principles for co-audit, and
outline research challenges.

</details>


### [117] ["What It Wants Me To Say": Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models](https://arxiv.org/abs/2304.06597)
*Michael Xieyang Liu,Advait Sarkar,Carina Negreanu,Ben Zorn,Jack Williams,Neil Toronto,Andrew D. Gordon*

Main category: cs.HC

TL;DR: 该研究提出了一种“接地”的方法，通过将代码转换回自然语言来弥合用户意图和AI理解之间的差距，以改进代码生成大语言模型的用户体验。


<details>
  <summary>Details</summary>
Motivation: 在代码生成大语言模型中，只有一小部分自然语言能有效指导代码生成，对于非专业用户来说，学习这一点是“对齐”的挑战。

Method: 通过一个将用户自然语言查询转换为Python代码，执行代码并显示结果的系统，来研究数据分析中“对齐”自然语言和代码生成之间的挑战。

Result: 研究发现，该“接地”方法能提升终端用户对代码生成模型范围和能力的理解，以及有效使用该模型所需的语言。

Conclusion: 该研究表明，基于生成式AI的编程工具可以通过“对齐”用户意图和AI理解来改进用户体验，因为用户可以通过“接地”来更好地理解AI的范围和能力。

Abstract: Code-generating large language models translate natural language into code.
However, only a small portion of the infinite space of naturalistic utterances
is effective at guiding code generation. For non-expert end-user programmers,
learning this is the challenge of abstraction matching. We examine this
challenge in the specific context of data analysis in spreadsheets, in a system
that maps the users natural language query to Python code using the Codex
generator, executes the code, and shows the result. We propose grounded
abstraction matching, which bridges the abstraction gap by translating the code
back into a systematic and predictable naturalistic utterance. In a
between-subjects, think-aloud study (n=24), we compare grounded abstraction
matching to an ungrounded alternative based on previously established query
framing principles. We find that the grounded approach improves end-users'
understanding of the scope and capabilities of the code-generating model, and
the kind of language needed to use it effectively.

</details>


### [118] [Team OS's System for Dialogue Robot Competition 2022](https://arxiv.org/abs/2210.09928)
*Yuki Kubo,Ryo Yanagimoto,Hayato Futase,Mikio Nakano,Zhaojie Luo,Kazunori Komatani*

Main category: cs.HC

TL;DR: A dialogue robot system (OSbot) for a competition, using state transitions, keyword extraction, and sentiment analysis, managed via a spreadsheet and logging function. It placed third in the preliminary round.


<details>
  <summary>Details</summary>
Motivation: Development of a dialogue robot system for the Dialogue Robot Competition 2022.

Method: The dialogue flow is based on state transitions described manually. Transition conditions use keyword extraction (based on named entity extraction and a predefined keyword set) and sentiment analysis (text-based SVM trained with the Hazumi corpus). Dialogue flow is managed on a spreadsheet for easy viewing and editing. A logging function was used for quick checking and editing.

Result: The system achieved third place in the competition's preliminary round.

Conclusion: The system achieved third place in the competition

Abstract: This paper describes our dialogue robot system, OSbot, developed for Dialogue
Robot Competition 2022. The dialogue flow is based on state transitions
described manually and the transition conditions use the results of keyword
extraction and sentiment analysis. The transitions can be easily viewed and
edited by managing them on a spreadsheet. The keyword extraction is based on
named entity extraction and our predefined keyword set. The sentiment analysis
is text-based and uses SVM, which was trained with the multimodal dialogue
corpus Hazumi. We quickly checked and edited a dialogue flow by using a logging
function. In the competition's preliminary round, our system ended up in third
place.

</details>


### [119] [What is it like to program with artificial intelligence?](https://arxiv.org/abs/2208.06213)
*Advait Sarkar,Andrew D. Gordon,Carina Negreanu,Christian Poelitz,Sruti Srinivasa Ragavan,Ben Zorn*

Main category: cs.HC

TL;DR: LLM-assisted programming is a new paradigm with unique challenges, especially for non-expert users in spreadsheets.


<details>
  <summary>Details</summary>
Motivation: To explore how LLM-assisted programming is similar to and differs from prior conceptualizations of programmer assistance, and to discuss the issues and challenges in applying LLMs to end-user programming.

Method: The paper explores LLM-assisted programming by drawing upon publicly available experience reports of LLM-assisted programming and prior usability and design studies. It also includes observations from a user study where non-expert end-user programmers used LLM-assisted tools for data tasks in spreadsheets.

Result: LLM-assisted programming shares some properties with compilation, pair programming, and programming via search and reuse, but has fundamental differences in technical possibilities and practical experience. Applying LLMs to end-user programming, especially for non-experts, raises specific issues and open research challenges.

Conclusion: LLM-assisted programming is a new way of programming with its own distinct properties and challenges, and presents unique issues and open research challenges when applied to end-user programming, especially for users with limited programming expertise.

Abstract: Large language models, such as OpenAI's codex and Deepmind's AlphaCode, can
generate code to solve a variety of problems expressed in natural language.
This technology has already been commercialised in at least one widely-used
programming editor extension: GitHub Copilot.
  In this paper, we explore how programming with large language models
(LLM-assisted programming) is similar to, and differs from, prior
conceptualisations of programmer assistance. We draw upon publicly available
experience reports of LLM-assisted programming, as well as prior usability and
design studies. We find that while LLM-assisted programming shares some
properties of compilation, pair programming, and programming via search and
reuse, there are fundamental differences both in the technical possibilities as
well as the practical experience. Thus, LLM-assisted programming ought to be
viewed as a new way of programming with its own distinct properties and
challenges.
  Finally, we draw upon observations from a user study in which non-expert end
user programmers use LLM-assisted tools for solving data tasks in spreadsheets.
We discuss the issues that might arise, and open research challenges, in
applying large language models to end-user programming, particularly with users
who have little or no programming expertise.

</details>


### [120] [MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization](https://arxiv.org/abs/2209.05739)
*Lu Ying,Xinhuan Shu,Dazhen Deng,Yuchen Yang,Tan Tang,Lingyun Yu,Yingcai Wu*

Main category: cs.HC

TL;DR: MetaGlyph通过自动选择隐喻图像和使用蒙特卡洛树搜索算法从电子表格生成图形化可视化，简化了设计过程。


<details>
  <summary>Details</summary>
Motivation: 创建隐喻图形化可视化（MGV）需要深厚的 
数据理解和专业的设计技能，并非易事。因此，需要开发一个能够自动生成MGV的系统，以降低其创作门槛。

Method: 该研究提出了一种名为MetaGlyph的自动系统，用于从电子表格生成隐喻图形化可视化（MGV）。研究首先进行了定性分析，从隐喻体现和图形设计两个角度理解当前的MGV。在此基础上，提出了一种新颖的框架，通过隐喻图像选择和MGV构建来生成MGV。具体而言，MetaGlyph根据输入数据的语义自动选择带有相应图像的隐喻，并集成蒙特卡洛树搜索算法，依据数据重要性、语义相关性和图形不重叠性来探索视觉元素与数据维度的关联，从而设计MGV。此外，该系统还提供编辑反馈，允许用户根据自己的设计偏好定制MGV。

Result: MetaGlyph系统能够根据输入数据自动选择隐喻图像，并通过蒙特卡洛树搜索算法优化图形设计，考虑了数据重要性、语义相关性和图形不重叠性，并允许用户进行编辑定制。

Conclusion: MetaGlyph是一个自动生成隐喻图形化可视化（MGV）的系统，它能从电子表格中提取数据，通过在线资源搜索隐喻图像，并利用蒙特卡洛树搜索算法来设计图形，同时允许用户自定义编辑。通过示例、使用场景和专家访谈证明了其有效性。

Abstract: Glyph-based visualization achieves an impressive graphic design when
associated with comprehensive visual metaphors, which help audiences
effectively grasp the conveyed information through revealing data semantics.
However, creating such metaphoric glyph-based visualization (MGV) is not an
easy task, as it requires not only a deep understanding of data but also
professional design skills. This paper proposes MetaGlyph, an automatic system
for generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct
a qualitative analysis to understand the design of current MGVs from the
perspectives of metaphor embodiment and glyph design. Based on the results, we
introduce a novel framework for generating MGVs by metaphoric image selection
and an MGV construction. Specifically, MetaGlyph automatically selects
metaphors with corresponding images from online resources based on the input
data semantics. We then integrate a Monte Carlo tree search algorithm that
explores the design of an MGV by associating visual elements with data
dimensions given the data importance, semantic relevance, and glyph
non-overlap. The system also provides editing feedback that allows users to
customize the MGVs according to their design preferences. We demonstrate the
use of MetaGlyph through a set of examples, one usage scenario, and validate
its effectiveness through a series of expert interviews.

</details>


### [121] [PoVRPoint: Authoring Presentations in Mobile Virtual Reality](https://arxiv.org/abs/2201.06337)
*Verena Biener,Travis Gesslein,Daniel Schneider,Felix Kawala,Alexander Otte,Per Ola Kristensson,Michel Pahud,Eyal Ofek,Cuauhtli Campos,Matjaž Kljun,Klen Čopič Pucihar,Jens Grubert*

Main category: cs.HC

TL;DR: 本研究提出了PoVRPoint，一种结合了移动设备笔触控编辑和VR交互的演示文稿创作工具，旨在增强移动知识工作者的工作效率。


<details>
  <summary>Details</summary>
Motivation: 探索用于在移动环境中创建演示文稿的VR设计空间，以期利用VR提供的三维输出空间和空间输入来增强传统输入设备的局限性。

Method: 提出了一种名为PoVRPoint的工具集，该工具集将移动设备（如平板电脑）上的笔和平板触控编辑与VR的交互能力相结合，用于在移动环境中创建演示文稿。

Result: 研究结果表明，与仅使用平板电脑的界面相比，VR的广阔视野可以显著提高目标幻灯片的识别速度；此外，与两个基线界面相比，VR的三维视图可以显著加快处理遮挡对象的重新排序速度。

Conclusion: 用户研究证实了该交互技术可用且有趣，并表明VR的广阔视野和三维视图在特定场景下（如目标幻灯片识别和带遮挡对象的重新排序）相比传统界面具有显著优势。

Abstract: Virtual Reality (VR) has the potential to support mobile knowledge workers by
complementing traditional input devices with a large three-dimensional output
space and spatial input. Previous research on supporting VR knowledge work
explored domains such as text entry using physical keyboards and spreadsheet
interaction using combined pen and touch input. Inspired by such work, this
paper probes the VR design space for authoring presentations in mobile
settings. We propose PoVRPoint -- a set of tools coupling pen- and touch-based
editing of presentations on mobile devices, such as tablets, with the
interaction capabilities afforded by VR. We study the utility of extended
display space to, for example, assist users in identifying target slides,
supporting spatial manipulation of objects on a slide, creating animations, and
facilitating arrangements of multiple, possibly occluded, shapes. Among other
things, our results indicate that 1) the wide field of view afforded by VR
results in significantly faster target slide identification times compared to a
tablet-only interface for visually salient targets; and 2) the
three-dimensional view in VR enables significantly faster object reordering in
the presence of occlusion compared to two baseline interfaces. A user study
further confirmed that the interaction techniques were found to be usable and
enjoyable.

</details>


### [122] [Untidy Data: The Unreasonable Effectiveness of Tables](https://arxiv.org/abs/2106.15005)
*Lyn Bartram,Michael Correll,Melanie Tory*

Main category: cs.HC

TL;DR: 数据表不仅仅是数据清理工具，用户在整个分析过程中都需要与数据进行交互，以进行重塑和丰富。交互式表格本身就是一种重要的可视化方法，能够丰富用户对数据的理解。


<details>
  <summary>Details</summary>
Motivation: 数据表不仅仅是在分析流程初期进行数据清理的准备步骤，在整个分析过程中，用户都希望能够看到并“接触”底层数据，以便通过重塑和增加数据来支持信息整合。

Method: 对数据工作者如何与数据交互和推理进行了定性研究。

Result: 研究表明，用户希望在整个分析过程中都能操作数据，并在此基础上进行重塑和丰富。他们会在基本数据的上下文中重新组织、标记数据、增加细节层级以及衍生替代方案。这些直接的交互和可读的数据表格表示形式，是理解数据含义和数据应用的关键认知组成部分。

Conclusion: 交互式表格是一种重要的可视化方法，它们提供的直接数据交互为可视化分析提供了丰富的实践设计，并且比当前可视化分析工具所支持的更灵活的人机交互能够丰富信息的整合过程。

Abstract: Working with data in table form is usually considered a preparatory and
tedious step in the sensemaking pipeline; a way of getting the data ready for
more sophisticated visualization and analytical tools. But for many people,
spreadsheets -- the quintessential table tool -- remain a critical part of
their information ecosystem, allowing them to interact with their data in ways
that are hidden or abstracted in more complex tools. This is particularly true
for data workers: people who work with data as part of their job but do not
identify as professional analysts or data scientists. We report on a
qualitative study of how these workers interact with and reason about their
data. Our findings show that data tables serve a broader purpose beyond data
cleanup at the initial stage of a linear analytic flow: users want to see and
"get their hands on" the underlying data throughout the analytics process,
reshaping and augmenting it to support sensemaking. They reorganize, mark up,
layer on levels of detail, and spawn alternatives within the context of the
base data. These direct interactions and human-readable table representations
form a rich and cognitively important part of building understanding of what
the data mean and what they can do with it. We argue that interactive tables
are an important visualization idiom in their own right; that the direct data
interaction they afford offers a fertile design space for visual analytics; and
that sense making can be enriched by more flexible human-data interaction than
is currently supported in visual analytics tools.

</details>


### [123] [Defining and Adopting an End User Computing Policy: A Case Study](https://arxiv.org/abs/1909.00855)
*Roger Turner*

Main category: cs.HC

TL;DR: This paper details the successful update of an End User Computing policy at Wesleyan Assurance Society using a risk-based approach and a new assessment application to mitigate risks effectively.


<details>
  <summary>Details</summary>
Motivation: To address the significant risks associated with End User Computing and demonstrate a practical approach to policy revision and risk management.

Method: Case study of policy update, development of a risk assessment application based on Complexity, Materiality, and Control, and implementation of a risk-based approach for mitigation.

Result: The paper outlines the plan, challenges, and solutions for updating the policy, including the development and application of a risk rating system that prioritizes high-risk areas for quicker benefit.

Conclusion: The paper presents a case study of updating an End User Computing policy, highlighting the successful implementation of a risk-based approach and a custom-developed Risk Assessment Application to manage associated risks.

Abstract: End User Computing carries significant risks if not well controlled. This
paper is a case study of the introduction of an updated End User Computing
policy at the Wesleyan Assurance Society. The paper outlines the plan and
identifies various challenges. The paper explains how these challenges were
overcome. We wrote an End User Computing Risk Assessment Application which
calculates a risk rating band based on the Complexity, Materiality and Control
(or lack of it) pertaining to any given application and the basis of assessment
is given in this paper. The policy uses a risk based approach for assessing and
mitigating against the highest risks first and obtaining the quickest benefit.

</details>


### [124] [Calliope: Automatic Visual Data Story Generation from a Spreadsheet](https://arxiv.org/abs/2010.09975)
*Danqing Shi,Xinyue Xu,Fuling Sun,Yang Shi,Nan Cao*

Main category: cs.HC

TL;DR: Calliope是一个创新的可视化数据故事生成系统，它能自动从电子表格创建数据故事，并允许用户通过在线编辑器进行修改。它使用一种新颖的蒙特卡洛树搜索算法来发现和组织数据事实，并通过信息论来衡量其重要性。实验证明该系统能有效提高数据故事的创作效率。


<details>
  <summary>Details</summary>
Motivation: 现有的可视化数据故事创作工具依赖用户的技能和经验，效率低下且难以使用，而Calliope系统旨在解决这一问题。

Method: Calliope系统通过一种新的面向逻辑的蒙特卡洛树搜索算法来探索输入电子表格给定的数据空间，以渐进地生成故事片段（数据事实）并以逻辑顺序组织它们。数据事实的重要性基于信息论进行度量，每个数据事实都通过图表进行可视化，并配有自动生成描述的字幕。

Result: 通过三个示例故事、两次对照实验以及与10位领域专家的访谈评估表明，Calliope系统有助于高效生成可视化数据故事。

Conclusion: Calliope系统能够高效地生成可视化数据故事，并支持在线故事编辑器进行修改。

Abstract: Visual data stories shown in the form of narrative visualizations such as a
poster or a data video, are frequently used in data-oriented storytelling to
facilitate the understanding and memorization of the story content. Although
useful, technique barriers, such as data analysis, visualization, and
scripting, make the generation of a visual data story difficult. Existing
authoring tools rely on users' skills and experiences, which are usually
inefficient and still difficult. In this paper, we introduce a novel visual
data story generating system, Calliope, which creates visual data stories from
an input spreadsheet through an automatic process and facilities the easy
revision of the generated story based on an online story editor. Particularly,
Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm
that explores the data space given by the input spreadsheet to progressively
generate story pieces (i.e., data facts) and organize them in a logical order.
The importance of data facts is measured based on information theory, and each
data fact is visualized in a chart and captioned by an automatically generated
description. We evaluate the proposed technique through three example stories,
two controlled experiments, and a series of interviews with 10 domain experts.
Our evaluation shows that Calliope is beneficial to efficient visual data story
generation.

</details>


### [125] [Pen-based Interaction with Spreadsheets in Mobile Virtual Reality](https://arxiv.org/abs/2008.04543)
*Travis Gesslein,Verena Biener,Philipp Gagel,Daniel Schneider,Per Ola Kristensson,Eyal Ofek,Michel Pahud,Jens Grubert*

Main category: cs.HC

TL;DR: 该研究提出了一种结合VR和笔式输入的工具集，用于改善移动设备上的电子表格交互体验，并评估了其可行性。


<details>
  <summary>Details</summary>
Motivation: 电子表格在移动设备上的交互存在挑战，尤其是在VR环境中，其交互方式尚未得到深入探索。VR提供的沉浸式大显示空间与移动办公有限的交互空间之间存在矛盾，本研究旨在弥合这一差距。

Method: 本研究通过视频在线调查和专家评估相结合的方式，研究了所提出工具集的可行性，并评估了其在提升人类潜在性能方面的作用。

Result: 该研究提出的工具集通过结合VR和笔式输入，为增强电子表格交互提供了多种可能性，有望提高生产力。初步研究表明，该工具集具有可行性，并展示了其在提升用户交互效率方面的潜力。

Conclusion: 该研究提出了一个结合VR头戴设备和平板电脑的工具集，用于增强移动知识工作者在平板设备上的电子表格交互体验。通过利用VR提供的沉浸式大显示空间以及精确的笔式输入，该工具集能够扩展电子表格的可视化范围，便于调试，并提供更高效的函数创建和编辑方式，如屏幕外菜单、单元格依赖关系可视化和基于注视与触摸的标签切换。

Abstract: Virtual Reality (VR) can enhance the display and interaction of mobile
knowledge work and in particular, spreadsheet applications. While spreadsheets
are widely used yet are challenging to interact with, especially on mobile
devices, using them in VR has not been explored in depth. A special uniqueness
of the domain is the contrast between the immersive and large display space
afforded by VR, contrasted by the very limited interaction space that may be
afforded for the information worker on the go, such as an airplane seat or a
small work-space. To close this gap, we present a tool-set for enhancing
spreadsheet interaction on tablets using immersive VR headsets and pen-based
input. This combination opens up many possibilities for enhancing the
productivity for spreadsheet interaction. We propose to use the space around
and in front of the tablet for enhanced visualization of spreadsheet data and
meta-data. For example, extending sheet display beyond the bounds of the
physical screen, or easier debugging by uncovering hidden dependencies between
sheet's cells. Combining the precise on-screen input of a pen with spatial
sensing around the tablet, we propose tools for the efficient creation and
editing of spreadsheets functions such as off-the-screen layered menus,
visualization of sheets dependencies, and gaze-and-touch-based switching
between spreadsheet tabs. We study the feasibility of the proposed tool-set
using a video-based online survey and an expert-based assessment of indicative
human performance potential.

</details>


### [126] [EQUS -- helping to see formulae](https://arxiv.org/abs/2007.00003)
*Chris Roast*

Main category: cs.HC

TL;DR: An interactive visualisation tool called EQUS was developed for spreadsheet formulae, which helps users understand complex data. It was designed with teenagers in mind but found to be useful for a wider audience and is now an Excel plug-in.


<details>
  <summary>Details</summary>
Motivation: Spreadsheet formulae are widely used for numerical processing and modeling, yet they can be easily misunderstood.

Method: The design, development, and evaluation of an interactive visualisation for spreadsheet formulae (EQUS) using iterative refinement engaging an initial target audience of mid-teen learners, involving re-design and formative evaluation.

Result: EQUS has since been developed as a fully integrated plug-in for MS Excel.

Conclusion: The developed visualisation techniques are broadly relevant to spreadsheet users beyond the initial target audience.

Abstract: Visualisation is often presented as a means of simplifying information and
helping people understand complex data. In this paper we describe the design,
development and evaluation of an interactive visualisation for spreadsheet
formulae (EQUS). The work is justified on the grounds that these are widely
used tools for significant numerical processing and modeling, yet the formula
developed can be easily misunderstood. The development process was one of
iterative refinement engaging an initial target audience of mid-teen learners,
involving re-design and formative evaluation. The resulting visualisation
techniques have been found to be broadly relevant to spreadsheet users beyond
the initial target audience. EQUS has since been developed as fully integrated
plug-in for MS Excel.

</details>


### [127] [Taggle: Combining Overview and Details in Tabular Data Visualizations](https://arxiv.org/abs/1712.05944)
*Katarina Furmanova,Samuel Gratzl,Holger Stitz,Thomas Zichner,Miroslava Jaresova,Alexander Lex,Marc Streit*

Main category: cs.HC

TL;DR: Taggle是一种用于大型表格的可视化工具，可对单个项目进行探索和呈现。


<details>
  <summary>Details</summary>
Motivation: 许多实际分析任务关注对感兴趣的单个项目的调查，并且将项目与潜在的大型表格的其余部分相关联很重要。

Method: Taggle通过为单元格使用视觉编码来单独可视化源数据中的每一行，并引入数据驱动的聚合，以及用于排序、选择和过滤的交互方法。

Result: Taggle在药物发现的复杂基因组数据分析的案例研究中得到了证明。

Conclusion: Taggle是一种用于探索和呈现大型复杂表格的可视化技术，它采用以项目为中心、类似电子表格的方法，并结合数据驱动的聚合和交互方法。

Abstract: Most tabular data visualization techniques focus on overviews, yet many
practical analysis tasks are concerned with investigating individual items of
interest. At the same time, relating an item to the rest of a potentially large
table is important. In this work we present Taggle, a tabular visualization
technique for exploring and presenting large and complex tables. Taggle takes
an item-centric, spreadsheet-like approach, visualizing each row in the source
data individually using visual encodings for the cells. At the same time,
Taggle introduces data-driven aggregation of data subsets. The aggregation
strategy is complemented by interaction methods tailored to answer specific
analysis questions, such as sorting based on multiple columns and rich data
selection and filtering capabilities. We demonstrate Taggle using a case study
conducted by a domain expert on complex genomics data analysis for the purpose
of drug discovery.

</details>


### [128] [Are digital natives spreadsheet natives?](https://arxiv.org/abs/1909.00865)
*Maria Csernoch,Piroska Biró*

Main category: cs.HC

TL;DR: 对信息学专业大一新生进行的电子表格技能测试显示，尽管他们是“数字原住民”，但仍存在显著困难。基于算法的公式比特定函数效果更好，且所有学生都需要专业的算法培训。


<details>
  <summary>Details</summary>
Motivation: 研究这些学生的算法技能和知识转移能力，以了解他们是否存在对电子表格问题的困难。

Method: 测试了信息学专业大一新生的算法技能和在电子表格环境中的知识转移能力。

Result: 尽管被认为是“数字原住民”，但学生在解决电子表格问题时遇到严重困难。使用基于算法的多层数组公式的学生表现优于使用特定、不相关内置函数 Thus, students regardless of their birth date and digital generation assigned to them are in great need of official, high-mathability, algorithm-based training with expert teachers。

Conclusion: 学生需要接受基于算法的、高数能力要求的、由专家教师授课的正规培训。

Abstract: The present paper reports the results of testing first year students of
Informatics on their algorithmic skills and knowledge transfer abilities in
spreadsheet environments. The selection of students plays a crucial role in the
project. On the one hand, they have officially finished their spreadsheet
training - they know everything - while on the other hand, they do not need any
training, since they are digital natives, to whom digital skills are assigned
by birth. However, we found that the students had serious difficulties in
solving the spreadsheet problems presented: so low were their results that it
allowed us to form broad tendencies. Considering computational thinking,
algorithmic skills, and knowledge transfer abilities, it is clear that those
students performed better who used algorithm-based, multilevel array formulas
instead of problem specific, unconnected built-in functions. Furthermore, we
can conclude that students, regardless of their birth date and digital
generation assigned to them, are in great need of official, high-mathability,
algorithm-based training with expert teachers.

</details>


### [129] [Somewhere Around That Number: An Interview Study of How Spreadsheet Users Manage Uncertainty](https://arxiv.org/abs/1905.13072)
*Judith Borghouts,Andrew D. Gordon,Advait Sarkar,Kenton P. O'Hara,Neil Toronto*

Main category: cs.HC

TL;DR: 电子表格用户处理不确定性的方式受其使用方式和目标影响，但现有工具支持不足。


<details>
  <summary>Details</summary>
Motivation: 人们在处理数据不确定性时常常依赖启发式方法，导致错误的结论，并且倾向于忽略或简化不确定性。因此，需要理解用户在电子表格中如何遇到和处理不确定性。

Method: 通过对11位来自不同领域的电子表格用户进行访谈研究，了解他们如何处理电子表格中的不确定性。

Result: 用户的目标（计算、比较、理解不确定性、简化呈现）以及电子表格的角色（数据库、模板、计算工具、记事本、探索工具）共同影响着用户处理不确定性的方式。目前的电子表格工具对这些需求的支持不足，用户存在变通做法。

Conclusion: 用户处理电子表格中不确定性的方式受到电子表格的角色和用户目标的影响。虽然用户有计算和比较不同场景、理解不确定性性质以及将复杂的不确定性数据简化呈现给决策者的需求，但目前的电子表格工具对此支持有限，用户不得不采用各种变通方法。

Abstract: Spreadsheet users regularly deal with uncertainty in their data, for example
due to errors and estimates. While an insight into data uncertainty can help in
making better informed decisions, prior research suggests that people often use
informal heuristics to reason with probabilities, which leads to incorrect
conclusions. Moreover, people often ignore or simplify uncertainty. To
understand how people currently encounter and deal with uncertainty in
spreadsheets, we conducted an interview study with 11 spreadsheet users from a
range of domains. We found that how people deal with uncertainty is influenced
by the role the spreadsheet plays in people's work and the user's aims.
Spreadsheets are used as a database, template, calculation tool, notepad and
exploration tool. In doing so, participants' aims were to compute and compare
different scenarios, understand something about the nature of the uncertainty
in their situation, and translate the complexity of data uncertainty into
simplified presentations to other people, usually decision-makers. Spreadsheets
currently provide limited tools to support these aims, and participants had
various workarounds.

</details>


### [130] [Characterizing Scalability Issues in Spreadsheet Software using Online Forums](https://arxiv.org/abs/1801.03829)
*Kelly Mack,John Lee,Kevin Chang,Karrie Karahalios,Aditya Parameswaran*

Main category: cs.HC

TL;DR: 本研究利用Reddit论坛数据分析Excel用户问题，为改进电子表格软件提供设计启示。


<details>
  <summary>Details</summary>
Motivation: 传统可用性研究成本高昂，本研究旨在探索利用在线论坛数据来大规模、低成本地了解用户在电子表格使用中遇到的挑战，以指导下一代电子表格软件的设计。

Method: 本研究采用抓取Reddit论坛帖子数据的方式，对用户在使用Excel表格时遇到的问题进行分析和表征。

Result: 研究分析了用户在使用电子表格软件时遇到的问题，特别是与处理大量数据相关的挑战，并讨论了这些发现对下一代电子表格软件设计的启示。

Conclusion: 本研究通过抓取Reddit论坛上的帖子，分析了用户在使用Excel表格时遇到的问题，特别是与处理大量数据相关的挑战。研究结果将为设计下一代电子表格软件提供参考。

Abstract: In traditional usability studies, researchers talk to users of tools to
understand their needs and challenges. Insights gained via such interviews
offer context, detail, and background. Due to costs in time and money, we are
beginning to see a new form of tool interrogation that prioritizes scale, cost,
and breadth by utilizing existing data from online forums. In this case study,
we set out to apply this method of using online forum data to a specific
issue---challenges that users face with Excel spreadsheets. Spreadsheets are a
versatile and powerful processing tool if used properly. However, with
versatility and power come errors, from both users and the software, which make
using spreadsheets less effective. By scraping posts from the website Reddit,
we collected a dataset of questions and complaints about Excel. Specifically,
we explored and characterized the issues users were facing with spreadsheet
software in general, and in particular, as resulting from a large amount of
data in their spreadsheets. We discuss the implications of our findings on the
design of next-generation spreadsheet software.

</details>


### [131] [The Reification of an Incorrect and Inappropriate Spreadsheet Model](https://arxiv.org/abs/1801.10249)
*Grenville J. Croll*

Main category: cs.HC

TL;DR: 电子表格使信息具体化，即使信息是错误的。


<details>
  <summary>Details</summary>
Motivation: 信息载入电子表格后会获得一些它可能不具备的属性，导致信息变得具体化。

Method: 通过一个案例研究，在近距离观察了一个小型非营利组织中一个明显不正确和不恰当的电子表格模型的具体化过程。

Result: 在一家小型非营利组织中，观察到了一个明显不正确和不恰当的电子表格模型具体化的过程。

Conclusion: 信息一旦载入电子表格，就会获得一些它可能不具备的属性，例如可信度、正确性、适当性、具体性、完整性、实在性、客观性和权威性。信息变得具体化了。

Abstract: Once information is loaded into a spreadsheet, it acquires properties that it
may not deserve. These properties include believability, correctness,
appropriateness, concreteness, integrity, tangibility, objectivity and
authority. The information becomes reified. We describe a case study through
which we were able to observe at close hand the reification of a demonstrably
incorrect and inappropriate spreadsheet model within a small non profit
organisation.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [132] [Radif Corpus: A Symbolic Dataset for Non-Metric Iranian Classical Music](https://arxiv.org/abs/2507.10456)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.SD

TL;DR: 该研究创建了一个伊朗古典音乐radif的数字语料库，包含281分钟的MIDI文件和详细数据，支持计算音乐学研究。


<details>
  <summary>Details</summary>
Motivation: 为了弥合伊朗古典音乐（特别是其核心的radif）在计算研究方面的空白，并为相关领域的研究者提供一个丰富的数据资源。

Method: 构建了一个包含228首音乐的数字语料库，提供了MIDI文件（总计约281分钟）以及描述音符、音长、音程和层次结构的數據，并包含支持性统计数据以及语料库的复杂性和相似性度量。

Result: 成功构建了一个全面的非量化radif数字语料库，忠实地表示了包括四分之一音符在内的音调和非量化特性，并提供了详细的统计数据和分析，为后续研究奠定了基础。

Conclusion: 该研究介绍了首个完整的非量化伊朗古典音乐radif数字语料库，包含13个组成部分，并提供MIDI文件和详细数据，为伊朗古典音乐的计算研究提供了平台。

Abstract: Non-metric music forms the core of the repertoire in Iranian classical music.
Dastgahi music serves as the underlying theoretical system for both Iranian art
music and certain folk traditions. At the heart of Iranian classical music lies
the radif, a foundational repertoire that organizes melodic material central to
performance and pedagogy.
  In this study, we introduce the first digital corpus representing the
complete non-metrical radif repertoire, covering all 13 existing components of
this repertoire. We provide MIDI files (about 281 minutes in total) and data
spreadsheets describing notes, note durations, intervals, and hierarchical
structures for 228 pieces of music. We faithfully represent the tonality
including quarter-tones, and the non-metric aspect. Furthermore, we provide
supporting basic statistics, and measures of complexity and similarity over the
corpus.
  Our corpus provides a platform for computational studies of Iranian classical
music. Researchers might employ it in studying melodic patterns, investigating
improvisational styles, or for other tasks in music information retrieval,
music theory, and computational (ethno)musicology.

</details>


### [133] [Tonal consonance parameters link microscopic and macroscopic properties of music exposing a hidden order in melody](https://arxiv.org/abs/1610.04551)
*Jorge Useche,Rafael Hurtado*

Main category: cs.SD

TL;DR: This paper uses tonal consonance parameters to study musical complexity, applying it to melody and finding that melodic lines can be described by interval properties and an entropy principle, linking perceived consonance to creative complexity via sound physics.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore tonal consonance, its relation to the perception of pleasantness in sound combinations, and its presence in various aspects of music, using a complexity perspective and applying it to the study of music complexity.

Method: The study applies a formalism based on tonal consonance parameters for complex tones to melody, describing melodic lines in terms of physical properties of melodic intervals and an entropy extremalization principle subject to psychoacoustic constraints.

Result: Melodic lines in musical pieces can be described using the physical properties of melodic intervals and an entropy extremalization principle subject to psychoacoustic constraints with musical meaning, connecting consonance perception with creative complexity through physical properties of the musical stimulus.

Conclusion: The study connects human perception of consonance with the complexity of human creativity in music through the physical properties of the musical stimulus.

Abstract: Consonance is related to the perception of pleasantness arising from a
combination of sounds and has been approached quantitatively using mathematical
relations, physics, information theory, and psychoacoustics. Tonal consonance
is present in timbre, musical tuning, harmony, and melody, and it is used for
conveying sensations, perceptions, and emotions in music. It involves the
physical properties of sound waves and is used to study melody and harmony
through musical intervals and chords. From the perspective of complexity, the
macroscopic properties of a system with many parts frequently rely on the
statistical properties of its constituent elements. Here we show how the tonal
consonance parameters for complex tones can be used to study complexity in
music. We apply this formalism to melody, showing that melodic lines in musical
pieces can be described in terms of the physical properties of melodic
intervals and the existence of an entropy extremalization principle subject to
psychoacoustic macroscopic constraints with musical meaning. This result
connects the human perception of consonance with the complexity of human
creativity in music through the physical properties of the musical stimulus.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [134] [Data-Semantics-Aware Recommendation of Diverse Pivot Tables](https://arxiv.org/abs/2507.06171)
*Whanhee Cho,Anna Fariha*

Main category: cs.DB

TL;DR: SAGE是一个数据语义感知系统，可以推荐多样化的透视表，解决了现有方法的不足，并在高维数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 识别有用的透视表属性组合仍然是一个挑战，尤其是在高维数据集的情况下。因此，我们提出了一个自动化推荐有见地且可解释的透视表的问题，以消除繁琐的手动过程。在推荐一组透视表时，多样性至关重要，但传统方法未能充分解决表多样性问题。

Method: SAGE 是一个数据语义感知系统，用于推荐 k 个预算内多样化的透视表。SAGE 通过以下两种技术贡献来实现这一点：(1) 一个数据语义感知模型，用于衡量单个透视表的效用和一组透视表的多样性；(2) 一个可扩展的贪婪算法，通过利用数据语义显著减少组合搜索空间，从而有效地选择一组具有高实用性的多样化透视表。

Result: SAGE 能够确保每个透视表都有见地、可解释且能适应用户的操作和偏好，同时还保证了透视表集合的多样性，提供了多样化的推荐。

Conclusion: SAGE 在三个真实世界的数据集上进行了广泛的实验，结果表明 SAGE 的性能优于其他方法，并且能够有效地扩展以适应高维数据集。此外，我们还展示了几个案例研究，以突出 SAGE 相对于商业软件和大型语言模型 (LLM) 的定性优势。

Abstract: Data summarization is essential to discover insights from large datasets. In
a spreadsheets, pivot tables offer a convenient way to summarize tabular data
by computing aggregates over some attributes, grouped by others. However,
identifying attribute combinations that will result in useful pivot tables
remains a challenge, especially for high-dimensional datasets. We formalize the
problem of automatically recommending insightful and interpretable pivot
tables, eliminating the tedious manual process. A crucial aspect of
recommending a set of pivot tables is to diversify them. Traditional works
inadequately address the table-diversification problem, which leads us to
consider the problem of pivot table diversification.
  We present SAGE, a data-semantics-aware system for recommending k-budgeted
diverse pivot tables, overcoming the shortcomings of prior work for top-k
recommendations that cause redundancy. SAGE ensures that each pivot table is
insightful, interpretable, and adaptive to the user's actions and preferences,
while also guaranteeing that the set of pivot tables are different from each
other, offering a diverse recommendation. We make two key technical
contributions: (1) a data-semantics-aware model to measure the utility of a
single pivot table and the diversity of a set of pivot tables, and (2) a
scalable greedy algorithm that can efficiently select a set of diverse pivot
tables of high utility, by leveraging data semantics to significantly reduce
the combinatorial search space. Our extensive experiments on three real-world
datasets show that SAGE outperforms alternative approaches, and efficiently
scales to accommodate high-dimensional datasets. Additionally, we present
several case studies to highlight SAGE's qualitative effectiveness over
commercial software and Large Language Models (LLMs).

</details>


### [135] [A System and Benchmark for LLM-based Q&A on Heterogeneous Data](https://arxiv.org/abs/2409.05735)
*Achille Fokoue,Srideepika Jayaraman,Elham Khabiri,Jeffrey O. Kephart,Yingjie Li,Dhruv Shah,Youssef Drissi,Fenno F. Heath III,Anu Bhamidipaty,Fateh A. Tipu,Robert J. Baseman*

Main category: cs.DB

TL;DR: Siwarex platform allows natural language queries across diverse data sources like databases and APIs, overcoming limitations of existing Text-to-SQL tools in industrial settings.


<details>
  <summary>Details</summary>
Motivation: Users in industrial settings often struggle to identify, access, and combine data from multiple, heterogeneous sources (databases, APIs, etc.) to answer questions. Existing Text-to-SQL applications are impractical due to their inability to cope with this heterogeneity.

Method: Introduced the siwarex platform to enable seamless natural language access to databases and APIs. Extended the Spider dataset and benchmark by replacing some tables with data retrieval APIs.

Result: Siwarex performs well in handling data source heterogeneity, as demonstrated by experiments on a modified Spider benchmark.

Conclusion: Siwarex can effectively handle data source heterogeneity, enabling natural language access to both databases and APIs.

Abstract: In many industrial settings, users wish to ask questions whose answers may be
found in structured data sources such as a spreadsheets, databases, APIs, or
combinations thereof. Often, the user doesn't know how to identify or access
the right data source. This problem is compounded even further if multiple (and
potentially siloed) data sources must be assembled to derive the answer.
Recently, various Text-to-SQL applications that leverage Large Language Models
(LLMs) have addressed some of these problems by enabling users to ask questions
in natural language. However, these applications remain impractical in
realistic industrial settings because they fail to cope with the data source
heterogeneity that typifies such environments. In this paper, we address
heterogeneity by introducing the siwarex platform, which enables seamless
natural language access to both databases and APIs. To demonstrate the
effectiveness of siwarex, we extend the popular Spider dataset and benchmark by
replacing some of its tables by data retrieval APIs. We find that siwarex does
a good job of coping with data source heterogeneity. Our modified Spider
benchmark will soon be available to the research community

</details>


### [136] [Auditable and reusable crosswalks for fast, scaled integration of scattered tabular data](https://arxiv.org/abs/2409.01517)
*Gavin Chait*

Main category: cs.DB

TL;DR: 一个开源的策展工具包，用于将分散的数据转换为结构良好、可互操作的数据。


<details>
  <summary>Details</summary>
Motivation: 提出一个开源的策展工具包，用于生成结构良好且可互操作的数据。

Method: 该研究将策展分为离散的组件，并采用以模式为中心的方法，对复杂且分散的表格式数据进行可审核的重组，以符合目标模式。任务分离允许在没有源数据的情况下进行软件开发和分析。转换被捕获为描述模式到模式映射的高级顺序脚本。

Result: 该工具包可以作为 Python 包或无代码的 Web 应用程序使用，并提供了一个处理来自数百个地方议会的零散源数据的可视化示例。

Conclusion: 该工具包旨在实现任何符合模式定义的数据的重组，为数据策展提供了一个开源解决方案。

Abstract: This paper presents an open-source curatorial toolkit intended to produce
well-structured and interoperable data. Curation is divided into discrete
components, with a schema-centric focus for auditable restructuring of complex
and scattered tabular data to conform to a destination schema. Task separation
allows development of software and analysis without source data being present.
Transformations are captured as high-level sequential scripts describing
schema-to-schema mappings, reducing complexity and resource requirements.
Ultimately, data are transformed, but the objective is that any data meeting a
schema definition can be restructured using a crosswalk. The toolkit is
available both as a Python package, and as a 'no-code' visual web application.
A visual example is presented, derived from a longitudinal study where
scattered source data from hundreds of local councils are integrated into a
single database.

</details>


### [137] [Auto-Formula: Recommend Formulas in Spreadsheets using Contrastive Learning for Table Representations](https://arxiv.org/abs/2404.12608)
*Sibei Chen,Yeye He,Weiwei Cui,Ju Fan,Song Ge,Haidong Zhang,Dongmei Zhang,Surajit Chaudhuri*

Main category: cs.DB

TL;DR: Auto-Formula 通过学习相似电子表格中的现有公式来帮助用户编写复杂的电子表格公式。


<details>
  <summary>Details</summary>
Motivation: 电子表格是广泛使用的工具，但对于非技术用户来说，编写复杂公式仍然很困难，因为他们需要查找和理解不直观的公式语法。

Method: 提出了一种名为 Auto-Formula 的系统，该系统利用对比学习技术，通过学习和适应相似电子表格中已有的公式来预测目标单元格中用户想要的公式。

Result: 在从真实企业电子表格中提取的 2K 多个测试公式上进行的广泛评估表明，Auto-Formula 在有效性方面优于其他方法。

Conclusion: 通过利用组织内相似的电子表格，Auto-Formula 系统能够通过学习和适应现有公式来准确预测用户想要编写的公式，从而解决了复杂公式创作的痛点。

Abstract: Spreadsheets are widely recognized as the most popular end-user programming
tools, which blend the power of formula-based computation, with an intuitive
table-based interface. Today, spreadsheets are used by billions of users to
manipulate tables, most of whom are neither database experts nor professional
programmers.
  Despite the success of spreadsheets, authoring complex formulas remains
challenging, as non-technical users need to look up and understand non-trivial
formula syntax. To address this pain point, we leverage the observation that
there is often an abundance of similar-looking spreadsheets in the same
organization, which not only have similar data, but also share similar
computation logic encoded as formulas. We develop an Auto-Formula system that
can accurately predict formulas that users want to author in a target
spreadsheet cell, by learning and adapting formulas that already exist in
similar spreadsheets, using contrastive-learning techniques inspired by
"similar-face recognition" from compute vision.
  Extensive evaluations on over 2K test formulas extracted from real enterprise
spreadsheets show the effectiveness of Auto-Formula over alternatives. Our
benchmark data is available at https://github.com/microsoft/Auto-Formula to
facilitate future research.

</details>


### [138] [Facilitating Digital Agriculture with Simple Databases](https://arxiv.org/abs/2312.06517)
*Dennis Buckmaster,Sami Basir,Hanae Sakata*

Main category: cs.DB

TL;DR: 为农业生产者提供易于使用的 Airtable 数据库模板，以改善数据管理和分析。


<details>
  <summary>Details</summary>
Motivation: 为农业生产者提供易于使用的数据库解决方案，以简化数据管理、物流、元数据提供和企业分析，特别关注那些电子表格技能有限的用户。

Method: 提供一系列结构化的、开源的、基于 Airtable 的私有数据库模板，并辅以易于使用的数据验证表单。

Result: 成功创建了面向农民的 Airtable 数据库模板，这些模板使用户能够生成易于处理、机器可读、可编辑和导出的数据，从而支持农业企业的运营分析和数字化转型。

Conclusion: 这些基于 Airtable 的数据库模板是开源的，旨在帮助农业生产者（尤其是那些电子表格技能有限的人）更好地管理和分析数据。它们提供了一个用户友好的界面，可以生成整洁、机器可读、易于编辑和导出的数据，从而简化物流、提供元数据并改进企业分析。此外，还提供了一个解释如何构建活动记录数据库的研讨会视频，以支持数字农业原则的推广。

Abstract: As an on-ramp to databases, we offer several well-structured private database
templates as open source resources for agriculturalists, particularly those
with modest spreadsheet skills. These farmer-oriented Air table databases use
simple data-validated forms, with the look and feel of a customized app, to
yield operational data that is tidy, machine- and human-readable, editable, and
exportable for analysis in other software. Such data can facilitate logistics,
provide contextual metadata, and improve enterprise analysis. A recorded
workshop explaining how to build a database for activity records is presented.
These resources may facilitate infusion of digital agriculture principles
through Extension and structured educational programming.

</details>


### [139] [Streamlining Knowledge Graph Construction with a façade: The SPARQL Anything project](https://arxiv.org/abs/2310.16700)
*Luigi Asprino,Enrico Daga,Justin Dowdy,Paul Mulholland,Aldo Gangemi,Marco Ratta*

Main category: cs.DB

TL;DR: SPARQL Anything is a data integration framework that lets knowledge engineers query diverse data formats (CSV, JSON, XML, Markdown, etc.) and web APIs using SPARQL, similar to querying RDF data.


<details>
  <summary>Details</summary>
Motivation: To propose a data integration framework for knowledge engineers by presenting the SPARQL Anything system, which allows querying diverse data sources as if they were in RDF using SPARQL 1.1.

Method: The paper discusses the application of a facade pattern to SPARQL Anything, a system that queries heterogeneous resources using SPARQL 1.1 by overloading the SERVICE clause. It supports various file formats, Web APIs, and complex pipelines. The design rationale and architecture are detailed, with references to real-world scenarios. User value is reported through a community survey and industry field report, comparing it to alternative solutions.

Result: The paper presents the SPARQL Anything system, detailing its design and architecture, and provides evidence of its value to users through a community survey and industry field report, highlighting its support for various file formats and flexible querying capabilities.

Conclusion: The paper describes the design rationale and software architecture of the SPARQL Anything system, a data integration framework for knowledge engineers that allows querying heterogeneous resources as-if they were in RDF using SPARQL 1.1.

Abstract: What should a data integration framework for knowledge engineers look like?
Recent research on Knowledge Graph construction proposes the design of a
fa\c{c}ade, a notion borrowed from object-oriented software engineering. This
idea is applied to SPARQL Anything, a system that allows querying heterogeneous
resources as-if they were in RDF, in plain SPARQL 1.1, by overloading the
SERVICE clause. SPARQL Anything supports a wide variety of file formats, from
popular ones (CSV, JSON, XML, Spreadsheets) to others that are not supported by
alternative solutions (Markdown, YAML, DOCx, Bibtex). Features include querying
Web APIs with high flexibility, parametrised queries, and chaining multiple
transformations into complex pipelines. In this paper, we describe the design
rationale and software architecture of the SPARQL Anything system. We provide
references to an extensive set of reusable, real-world scenarios from various
application domains. We report on the value-to-users of the founding
assumptions of its design, compared to alternative solutions through a
community survey and a field report from the industry.

</details>


### [140] [DataVinci: Learning Syntactic and Semantic String Repairs](https://arxiv.org/abs/2308.10922)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Gust Verbruggen*

Main category: cs.DB

TL;DR: DataVinci is an unsupervised system that detects and repairs errors in string data by learning patterns and leveraging LLMs and program execution information, outperforming existing baselines.


<details>
  <summary>Details</summary>
Motivation: Prior work on string data cleaning has often been limited to error detection or requires user annotations, examples, or constraints. These systems have also focused independently on syntactic or semantic errors, ignoring that strings often contain both.

Method: DataVinci learns regular-expression-based patterns that cover a majority of values in a column and reports values that do not satisfy such patterns as data errors. It automatically derives edits to the data error based on the majority patterns and constraints learned over other columns without the need for further user interaction. DataVinci uses an LLM to abstract (and re-concretize) portions of strings that are semantic prior to learning majority patterns and deriving edits. It also leverages execution information from an existing program to identify and correct data repairs that would not otherwise be identified.

Result: DataVinci outperforms 7 baselines on both error detection and repair when evaluated on 4 existing and new benchmarks.

Conclusion: DataVinci outperforms 7 baselines on both error detection and repair when evaluated on 4 existing and new benchmarks.

Abstract: String data is common in real-world datasets: 67.6% of values in a sample of
1.8 million real Excel spreadsheets from the web were represented as text.
Systems that successfully clean such string data can have a significant impact
on real users. While prior work has explored errors in string data, proposed
approaches have often been limited to error detection or require that the user
provide annotations, examples, or constraints to fix the errors. Furthermore,
these systems have focused independently on syntactic errors or semantic errors
in strings, but ignore that strings often contain both syntactic and semantic
substrings. We introduce DataVinci, a fully unsupervised string data error
detection and repair system. DataVinci learns regular-expression-based patterns
that cover a majority of values in a column and reports values that do not
satisfy such patterns as data errors. DataVinci can automatically derive edits
to the data error based on the majority patterns and constraints learned over
other columns without the need for further user interaction. To handle strings
with both syntactic and semantic substrings, DataVinci uses an LLM to abstract
(and re-concretize) portions of strings that are semantic prior to learning
majority patterns and deriving edits. Because not all data can result in
majority patterns, DataVinci leverages execution information from an existing
program (which reads the target data) to identify and correct data repairs that
would not otherwise be identified. DataVinci outperforms 7 baselines on both
error detection and repair when evaluated on 4 existing and new benchmarks.

</details>


### [141] [Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples](https://arxiv.org/abs/2307.14565)
*Peng Li,Yeye He,Cong Yan,Yue Wang,Surajit Chaudhuri*

Main category: cs.DB

TL;DR: Auto-Tables 系統能自動轉換不正規化的表格，提高數據準備效率。


<details>
  <summary>Details</summary>
Motivation: 在處理現實世界中的電子表格和網頁表格時，超過 30% 的表格不符合正規化標準，需要進行複雜的表格重構轉換才能使用 SQL 等分析工具輕鬆查詢。目前手動進行這些轉換的過程很困難，對技術和非技術用戶來說都是一個痛點。

Method: 開發了一個名為 Auto-Tables 的系統，能夠自動合成包含多步驟轉換（以 Python 或其他語言編寫）的管道，將非正規化表格轉換為標準正規化表格，以用於下游分析，無需用戶手動編寫轉換程式。

Result: Auto-Tables 系統可以為超過 70% 的測試案例成功合成轉換，處理速度快，且無需用戶輸入，證明了其作為數據準備工具的有效性。

Conclusion: Auto-Tables 系統能夠自動生成多步驟轉換管道，將非正規化表格轉換為標準正規化表格，以用於下游分析，提高了 70% 的測試案例的成功率，並且能在互動速度下完成，無需用戶介入，對技術和非技術用戶來說都是一個有效的数据准备工具。

Abstract: Relational tables, where each row corresponds to an entity and each column
corresponds to an attribute, have been the standard for tables in relational
databases. However, such a standard cannot be taken for granted when dealing
with tables "in the wild". Our survey of real spreadsheet-tables and web-tables
shows that over 30% of such tables do not conform to the relational standard,
for which complex table-restructuring transformations are needed before these
tables can be queried easily using SQL-based analytics tools. Unfortunately,
the required transformations are non-trivial to program, which has become a
substantial pain point for technical and non-technical users alike, as
evidenced by large numbers of forum questions in places like StackOverflow and
Excel/Power-BI/Tableau forums.
  We develop an Auto-Tables system that can automatically synthesize pipelines
with multi-step transformations (in Python or other languages), to transform
non-relational tables into standard relational forms for downstream analytics,
obviating the need for users to manually program transformations. We compile an
extensive benchmark for this new task, by collecting 244 real test cases from
user spreadsheets and online forums. Our evaluation suggests that Auto-Tables
can successfully synthesize transformations for over 70% of test cases at
interactive speeds, without requiring any input from users, making this an
effective tool for both technical and non-technical users to prepare data for
analytics.

</details>


### [142] [Efficient and Compact Spreadsheet Formula Graphs](https://arxiv.org/abs/2302.05482)
*Dixin Tang,Fanchao Chen,Christopher De Leon,Tana Wattanawaroon,Jeaseok Yun,Srinivasan Seshadri,Aditya G. Parameswaran*

Main category: cs.DB

TL;DR: TACO框架通过压缩电子表格的公式图来提高查询和维护效率。


<details>
  <summary>Details</summary>
Motivation: 电子表格的公式图通常庞大且复杂，导致查询和维护耗时，影响用户交互体验。

Method: TACO框架通过识别和利用四种基于表格局部性的模式来压缩公式图，并支持在压缩状态下直接进行查询和增量更新。

Result: TACO框架能够显著减小公式图的大小，在查询性能方面，相比基线实现和商业电子表格系统，分别实现了最高34,972倍和632倍的加速。

Conclusion: TACO框架通过利用表格局部性来压缩公式图，显著提高了查询和维护效率，在实际应用中展现出强大的性能优势。

Abstract: Spreadsheets are one of the most popular data analysis tools, wherein users
can express computation as formulae alongside data. The ensuing dependencies
are tracked as formula graphs. Efficiently querying and maintaining these
formula graphs is critical for interactivity across multiple settings.
Unfortunately, formula graphs are often large and complex such that querying
and maintaining them is time-consuming, reducing interactivity. We propose
TACO, a framework for efficiently compressing formula graphs, thereby reducing
the time for querying and maintenance. The efficiency of TACO stems from a key
spreadsheet property: tabular locality, which means that cells close to each
other are likely to have similar formula structures. We leverage four such
tabular locality-based patterns and develop algorithms for compressing formula
graphs using these patterns, directly querying the compressed graph without
decompression, and incrementally maintaining the graph during updates. We
integrate TACO into an open-source spreadsheet system and show that TACO can
significantly reduce formula graph sizes. For querying formula graphs, the
speedups of TACO over a baseline implemented in our framework and a commercial
spreadsheet system are up to 34,972x and 632x, respectively.

</details>


### [143] [Consensus-Free Spreadsheet Integration](https://arxiv.org/abs/2209.14457)
*Brandon Baylor,Eric Daimler,James Hansen,Esteban Montero,Ryan Wisnesky*

Main category: cs.DB

TL;DR: 我们提出了一种将多个电子表格合并为一个表，以及/或在表之间交换数据的方法。该方法使用范畴论的概念来整合由不同工程师创建的电子表格，即使在没有共识的情况下也能保持语义。我们通过一个油气计算的案例研究进行了说明，并讨论了该方法在工程实践中的应用前景。


<details>
  <summary>Details</summary>
Motivation: 在合并工程模型时，寻找不需要模型作者之间达成共识（一致）的方法，而我们的方法通过理论和模型态射保持语义不变，从而满足了这一条件。

Method: 通过将每个工作表的公式表达为代数（方程）理论，将每个工作表的值表达为其理论的模型，将工作表之间的重叠表达为理论和模型态射，然后执行范畴论的余极限、提升和Kan扩展构造，来计算一个规范的、通用的集成理论和模型，并将其表达为电子表格。

Result: 对一家主要能源公司的一个实际油气计算案例进行了该方法论的案例研究，描述了在整合由两位未相互作用的工程师构建的两个不同井筒压力测试（MASP）计算电子表格时出现的理论和模型。还描述了与验证重叠映射的语义保持性以及验证生成的集成表的保守性/一致性相关的自动定理证明的负担。

Conclusion: 将该方法论应用于企业范围的工程项目，以期扩大工程规模。

Abstract: We describe a method for merging multiple spreadsheets into one sheet, and/or
exchanging data among the sheets, by expressing each sheet's formulae as an
algebraic (equational) theory and each sheet's values as a model of its theory,
expressing the overlap between the sheets as theory and model morphisms, and
then performing colimit, lifting, and Kan-extension constructions from category
theory to compute a canonically universal integrated theory and model, which
can then be expressed as a spreadsheet. Our motivation is to find methods of
merging engineering models that do not require consensus (agreement) among the
authors of the models being merged, a condition fulfilled by our method because
theory and model morphisms are semantics-preserving. We describe a case study
of this methodology on a real-world oil and gas calculation at a major energy
company, describing the theories and models that arise when integrating two
different casing pressure test (MASP) calculation spreadsheets constructed by
two non-interacting engineers. We also describe the automated theorem proving
burden associated with both verifying the semantics preservation of the overlap
mappings as well as verifying the conservativity/consistency of the resulting
integrated sheet. We conclude with thoughts on how to apply the methodology to
scale engineering efforts across the enterprise.

</details>


### [144] [Sigma Workbook: A Spreadsheet for Cloud Data Warehouses](https://arxiv.org/abs/2204.03128)
*James Gale,Max Seiden,Deepanshu Utkarsh,Jason Frantz,Rob Woollen,Çağatay Demiralp*

Main category: cs.DB

TL;DR: Sigma Workbook是一个交互式系统，使业务用户能够轻松地对云数据仓库中的大规模数据进行可视化分析。


<details>
  <summary>Details</summary>
Motivation: 现有的CDW数据分析工具要么在即席转换方面功能有限，要么对业务用户来说难以使用。

Method: Sigma Workbook通过提供类似电子表格的直观界面，允许用户通过直接操作进行交互式数据分析。系统能够根据用户交互动态构建SQL查询，并在CDW上执行这些查询，从而利用CDW的可扩展性和新一代特性。

Result: 通过对同期群分析、会话化和数据增强这三个真实用例的演示，证明了Sigma Workbook的易用性、可扩展性和表现力。

Conclusion: Sigma Workbook系统易于使用、可扩展且具有表现力，适用于在云数据仓库（CDW）中对大规模数据进行可视化分析。

Abstract: Cloud data warehouses (CDWs) bring large-scale data and compute power closer
to users in enterprises. However, existing tools for analyzing data in CDWs are
either limited in ad-hoc transformations or difficult to use for business
users. Here we introduce Sigma Workbook, a new interactive system that enables
business users to easily perform a visual analysis of data in CDWs at scale.
For this, Sigma Workbook provides an accessible spreadsheet-like interface for
analysis through direct manipulation. Sigma Workbook dynamically constructs
matching SQL queries from user interactions, building on the versatility and
expressivity of SQL. Constructed queries are directly executed on CDWs,
leveraging the superior characteristics of the new generation CDWs, including
scalability. We demonstrate Sigma Workbook through 3 real-life use cases --
cohort analysis, sessionization, and data augmentation -- and underline
Workbook's ease of use, scalability, and expressivity.

</details>


### [145] [Efficient Specialized Spreadsheet Parsing for Data Science](https://arxiv.org/abs/2202.13189)
*Felix Henze,Haralampos Gavriilidis,Eleni Tzirita Zacharatou,Volker Markl*

Main category: cs.DB

TL;DR: 一种新的电子表格解析方法，可加快加载速度并大幅减少内存使用。


<details>
  <summary>Details</summary>
Motivation: 为了在商品系统上实现实用的电子表格加载，以解决现有方法在运行时或内存使用方面存在的问题。

Method: 通过紧密结合解压和解析来最小化内存使用，并采用优化的特定于电子表格的解析例程和并行处理来减少运行时间。

Result: 该方法比最先进的方法快 3 倍，内存消耗减少 40 倍。

Conclusion: 该研究实现了一个用于将 Excel 表格加载到 R 环境中的原型，并在内存和速度方面均优于现有方法。

Abstract: Spreadsheets are widely used for data exploration. Since spreadsheet systems
have limited capabilities, users often need to load spreadsheets to other data
science environments to perform advanced analytics. However, current approaches
for spreadsheet loading suffer from either high runtime or memory usage, which
hinders data exploration on commodity systems. To make spreasheet loading
practical on commodity systems, we introduce a novel parser that minimizes
memory usage by tightly coupling decompression and parsing. Furthermore, to
reduce the runtime, we introduce optimized spreadsheet-specific parsing
routines and employ parallelism. To evaluate our approach, we implement a
prototype for loading Excel spreadsheets into R environments. Our evaluation
shows that our novel approach is up to 3x faster while consuming up to 40x less
memory than state-of-the-art approaches.

</details>


### [146] [Spread2RML: Constructing Knowledge Graphs by Predicting RML Mappings on Messy Spreadsheets](https://arxiv.org/abs/2110.12829)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: Spread2RML自动预测混乱电子表格的RML映射，减少手动工作。


<details>
  <summary>Details</summary>
Motivation: 电子表格具有复杂的数据模型且容易混乱，导致映射创建过程耗时。Spread2RML旨在通过自动化预测RML映射来减少这一工作量。

Method: Spread2RML利用一套可扩展的RML对象映射模板，根据启发式规则为电子表格的每一列预测RML映射。

Result: 在包含合成数据和来自data.gov的真实数据的三个不同数据集上的评估表明，Spread2RML在处理混乱数据方面表现出有前景的结果，并且其全自动化的特性是一个显著优势。

Conclusion: Spread2RML通过基于启发式的可扩展RML对象映射模板，能够自动预测并生成RML映射，尤其在处理混乱的电子表格数据方面取得了初步但有希望的结果。

Abstract: The RDF Mapping Language (RML) allows to map semi-structured data to RDF
knowledge graphs. Besides CSV, JSON and XML, this also includes the mapping of
spreadsheet tables. Since spreadsheets have a complex data model and can become
rather messy, their mapping creation tends to be very time consuming. In order
to reduce such efforts, this paper presents Spread2RML which predicts RML
mappings on messy spreadsheets. This is done with an extensible set of RML
object map templates which are applied for each column based on heuristics. In
our evaluation, three datasets are used ranging from very messy synthetic data
to spreadsheets from data.gov which are less messy. We obtained first promising
results especially with regard to our approach being fully automatic and
dealing with rather messy data.

</details>


### [147] [Supercomputing Enabled Deployable Analytics for Disaster Response](https://arxiv.org/abs/2108.11525)
*Kaira Samuel,Jeremy Kepner,Michael Jones,Lauren Milechin,Vijay Gadepally,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Michael Houle,Anna Klein,Victor Lopez,Julie Mullen,Andrew Prout,Albert Reuther,Antonio Rosa,Sid Samsi,Charles Yee,Peter Michaleas*

Main category: cs.DB

TL;DR: 由于网络限制，该研究提出了一种预计算分析数据并将其打包为Excel和Google Earth文件的解决方案，以支持一线应急人员。该方法通过MIT SuperCloud处理地理空间人口普查数据，生成了可供离线使用的文件，旨在增强应急准备能力。


<details>
  <summary>Details</summary>
Motivation: 为了应对COVID-19大流行，需要一种解决方案，使一线应急人员能够在网络访问受限和软件安全要求严格的情况下，利用先进的分析能力。标准云基础微服务分析平台由于这些限制而无法使用。因此，需要一种预先计算分析数据并将其打包为无需网络访问或额外软件即可使用的文件的方法，以便在各种遗留硬件上运行。

Method: 该方法将地理空间人口普查数据（包括总人口、15岁以下人口、65岁以上人口和年龄中位数）按县进行排序，并通过Microsoft Excel电子表格（xlsx文件）和Google Earth地图（kml文件）进行展示。Excel界面支持用户在不同的经纬度坐标单位之间进行转换。对于Google Earth文件，探索了多种绝对和相对颜色映射的人口密度，以提供直观有意义的界面。

Result: 通过MIT SuperCloud处理人口普查数据，生成了数千个Google Earth和Microsoft Excel文件，代表了多种高级分析。该方法能够快速映射人口普查数据，为应急响应人员提供了一个潜在的强大工具。

Conclusion: 通过使用MIT SuperCloud预先计算地理空间人口普查数据，并将其以Excel和Google Earth文件的形式提供，可以为应急响应人员提供一个强大的工具，以改善应急准备工作。该方法能够在几分钟内生成新的分析数据，并具有将不同坐标单位进行转换的功能。

Abstract: First responders and other forward deployed essential workers can benefit
from advanced analytics. Limited network access and software security
requirements prevent the usage of standard cloud based microservice analytic
platforms that are typically used in industry. One solution is to precompute a
wide range of analytics as files that can be used with standard preinstalled
software that does not require network access or additional software and can
run on a wide range of legacy hardware. In response to the COVID-19 pandemic,
this approach was tested for providing geo-spatial census data to allow quick
analysis of demographic data for better responding to emergencies. These data
were processed using the MIT SuperCloud to create several thousand Google Earth
and Microsoft Excel files representative of many advanced analytics. The fast
mapping of census data using Google Earth and Microsoft Excel has the potential
to give emergency responders a powerful tool to improve emergency preparedness.
Our approach displays relevant census data (total population, population under
15, population over 65, median age) per census block, sorted by county, through
a Microsoft Excel spreadsheet (xlsx file) and Google Earth map (kml file). The
spreadsheet interface includes features that allow users to convert between
different longitude and latitude coordinate units. For the Google Earth files,
a variety of absolute and relative colors maps of population density have been
explored to provide an intuitive and meaningful interface. Using several
hundred cores on the MIT SuperCloud, new analytics can be generated in a few
minutes.

</details>


### [148] [Table2Charts: Recommending Charts by Learning Shared Table Representations](https://arxiv.org/abs/2008.11015)
*Mengyu Zhou,Qingtao Li,Xinyi He,Yuejiang Li,Yibo Liu,Wei Ji,Shi Han,Yining Chen,Daxin Jiang,Dongmei Zhang*

Main category: cs.DB

TL;DR: Table2Charts framework addresses challenges in chart recommendation by learning patterns from a large dataset of (table, charts) pairs using deep Q-learning and heuristic searching, outperforming existing systems.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of efficiency, imbalanced data, and table context in recommending commonly composed charts for multi-dimensional datasets.

Method: Table2Charts framework uses deep Q-learning with copying mechanism and heuristic searching to perform table-to-sequence generation, where each sequence follows a chart template.

Result: Table2Charts learns a shared representation of table fields, enabling mutual enhancement of recommendation tasks across different chart types, and achieves doubled recall numbers (R@3=0.61, R@1=0.43) compared to other systems.

Conclusion: Table2Charts outperforms other chart recommendation systems in both multi-type task and human evaluations.

Abstract: It is common for people to create different types of charts to explore a
multi-dimensional dataset (table). However, to recommend commonly composed
charts in real world, one should take the challenges of efficiency, imbalanced
data and table context into consideration. In this paper, we propose
Table2Charts framework which learns common patterns from a large corpus of
(table, charts) pairs. Based on deep Q-learning with copying mechanism and
heuristic searching, Table2Charts does table-to-sequence generation, where each
sequence follows a chart template. On a large spreadsheet corpus with 165k
tables and 266k charts, we show that Table2Charts could learn a shared
representation of table fields so that recommendation tasks on different chart
types could mutually enhance each other. Table2Charts outperforms other chart
recommendation systems in both multi-type task (with doubled recall numbers
R@3=0.61 and R@1=0.43) and human evaluations.

</details>


### [149] [Sigma Worksheet: Interactive Construction of OLAP Queries](https://arxiv.org/abs/2012.00697)
*James Gale,Max Seiden,Gretchen Atwood,Jason Frantz,Rob Woollen,Çağatay Demiralp*

Main category: cs.DB

TL;DR: Sigma Worksheet 是一款新的交互式系统，使业务用户能够轻松地对云数据仓库中的数据进行即席可视化分析。


<details>
  <summary>Details</summary>
Motivation: 现有的云数据仓库分析工具要么在即席转换方面有限，要么对企业中最大的用户群体——业务用户来说难以使用。

Method: Sigma Worksheet 提供了一个易于访问的、类似电子表格的数据分析界面，通过直接操作。Sigma Worksheet 根据用户在该界面的交互动态地构建匹配的 SQL 查询，并利用 SQL 的多功能性和表现力。Sigma Worksheet 直接在云数据仓库上执行构建的查询，利用了云数据仓库的优越特性，包括可扩展性。

Result: Sigma Worksheet 的查询性能与基准测试的参考查询相当。

Conclusion: Sigma Worksheet 易于使用和学习，提高了用户生产力。研究还表明，Sigma Worksheet 可通过在数据分析的各个阶段提供指导来进一步改善用户体验。

Abstract: The new generation of cloud data warehouses (CDWs) brings large amounts of
data and compute power closer to users in enterprises. The ability to directly
access the warehouse data, interactively analyze and explore it at scale can
empower users to improve their decision making cycles. However, existing tools
for analyzing data in CDWs are either limited in ad-hoc transformations or
difficult to use for business users, the largest user segment in enterprises.
Here we introduce Sigma Worksheet, a new interactive system that enables users
to easily perform ad-hoc visual analysis of data in CDWs at scale. For this,
Sigma Worksheet provides an accessible spreadsheet-like interface for data
analysis through direct manipulation. Sigma Worksheet dynamically constructs
matching SQL queries from user interactions on this familiar interface,
building on the versatility and expressivity of SQL. Sigma Worksheet executes
constructed queries directly on CDWs, leveraging the superior characteristics
of the new generation CDWs, including scalability. To evaluate Sigma Worksheet,
we first demonstrate its expressivity through two real life use cases, cohort
analysis and sessionization. We then measure the performance of the Worksheet
generated queries with a set of experiments using the TPC-H benchmark. Results
show the performance of our compiled SQL queries is comparable to that of the
reference queries of the benchmark. Finally, to assess the usefulness of Sigma
Worksheet in deployment, we elicit feedback through a 100-person survey
followed by a semi-structured interview study with 70 participants. We find
that Sigma Worksheet is easier to use and learn, improving the productivity of
users. Our findings also suggest Sigma Worksheet can further improve user
experience by providing guidance to users at various steps of data analysis.

</details>


### [150] [Mapping Spreadsheets to RDF: Supporting Excel in RML](https://arxiv.org/abs/2104.13600)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: RML现在支持Excel文件了！


<details>
  <summary>Details</summary>
Motivation: 现有的RDF映射语言（RML）规范和工具未能充分支持广泛使用的电子表格格式（如Microsoft Excel），导致在将电子表格数据转换为RDF图时存在不足。

Method: 扩展RML Mapper工具以支持Microsoft Excel文件，并允许在RML映射中访问单元格元数据。

Result: 成功扩展了RML Mapper以处理Excel文件，实现了对单元格元数据的访问，并通过在线演示验证了其能力，相关代码已开源。

Conclusion: 该研究通过扩展RML Mapper工具支持Excel文件，解决了RML在处理电子表格数据方面的不足，并提供了在线演示和开源代码。

Abstract: The RDF Mapping Language (RML) enables, among other formats, the mapping of
tabular data as Comma-Separated Values (CSV) files to RDF graphs.
Unfortunately, the widely used spreadsheet format is currently neglected by its
specification and well-known implementations. Therefore, we extended one of the
tools which is RML Mapper to support Microsoft Excel spreadsheet files and
demonstrate its capabilities in an interactive online demo. Our approach allows
to access various meta data of spreadsheet cells in typical RML maps. Some
experimental features for more specific use cases are also provided. The
implementation code is publicly available in a GitHub fork.

</details>


### [151] [Dataset Generation Patterns for Evaluating Knowledge Graph Construction](https://arxiv.org/abs/2104.13576)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: This paper presents Data Sprout, a synthetic data generator that imitates real-world spreadsheet patterns to create authentic-looking datasets for industrial knowledge graph construction, overcoming confidentiality issues that prevent the use of real data.


<details>
  <summary>Details</summary>
Motivation: Confidentiality prevents the publication of real, labeled datasets of personal and enterprise data, which are valuable for evaluating knowledge graph construction approaches in industrial settings. To address this, the paper aims to synthetically generate authentic-looking data by imitating the habits of knowledge workers.

Method: The study identifies 11 distinct patterns in real spreadsheets from industry and develops a generator called Data Sprout to reproduce these patterns. The paper details the generator's process and the effects of its implemented patterns.

Result: The paper demonstrates that Data Sprout can generate synthetic spreadsheets by reproducing 11 identified patterns found in real industrial spreadsheets, offering a solution for creating realistic datasets in the absence of real ones.

Conclusion: The paper introduces Data Sprout, a generator capable of reproducing 11 distinct patterns found in real spreadsheets, thereby enabling the creation of synthetic datasets that mimic authentic industrial data.

Abstract: Confidentiality hinders the publication of authentic, labeled datasets of
personal and enterprise data, although they could be useful for evaluating
knowledge graph construction approaches in industrial scenarios. Therefore, our
plan is to synthetically generate such data in a way that it appears as
authentic as possible. Based on our assumption that knowledge workers have
certain habits when they produce or manage data, generation patterns could be
discovered which can be utilized by data generators to imitate real datasets.
In this paper, we initially derived 11 distinct patterns found in real
spreadsheets from industry and demonstrate a suitable generator called Data
Sprout that is able to reproduce them. We describe how the generator produces
spreadsheets in general and what altering effects the implemented patterns
have.

</details>


### [152] [Interactively Constructing Knowledge Graphs from Messy User-Generated Spreadsheets](https://arxiv.org/abs/2103.03537)
*Markus Schröder,Christian Jilek,Michael Schulze,Andreas Dengel*

Main category: cs.DB

TL;DR: 当电子表格内容混乱时，知识图谱的构建会变得困难。本文提出了一种交互式方法，通过图形用户界面允许用户注释电子表格单元格，从而构建知识图谱，并在与 RML 的比较中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 知识工作者自由填写的电子表格可能包含非结构化内容，给机器和人类带来理解上的困难，因此需要将电子表格转换为更明确、正式和结构化的形式，例如知识图谱。

Method: 提出了一种交互式方法，包括一个图形用户界面，允许知识工程师批量注释提取信息的电子表格单元格。

Result: 在评估中，使用来自工业场景的五个电子表格构建了一个包含 25,000 个三元组的图谱。

Conclusion: 基于单元格注释，最终形成知识图谱，与最先进的 RDF 映射语言 (RML) 方法相比，我们的方法具有优势。

Abstract: When spreadsheets are filled freely by knowledge workers, they can contain
rather unstructured content. For humans and especially machines it becomes
difficult to interpret such data properly. Therefore, spreadsheets are often
converted to a more explicit, formal and structured form, for example, to a
knowledge graph. However, if a data maintenance strategy has been missing and
user-generated data becomes "messy", the construction of knowledge graphs will
be a challenging task. In this paper, we catalog several of those challenges
and propose an interactive approach to solve them. Our approach includes a
graphical user interface which enables knowledge engineers to bulk-annotate
spreadsheet cells with extracted information. Based on the cells' annotations a
knowledge graph is ultimately formed. Using five spreadsheets from an
industrial scenario, we built a 25k-triple graph during our evaluation. We
compared our method with the state-of-the-art RDF Mapping Language (RML)
attempt. The comparison highlights contributions of our approach.

</details>


### [153] [Leam: An Interactive System for In-situ Visual Text Analysis](https://arxiv.org/abs/2009.03520)
*Sajjadur Rahman,Peter Griggs,Çağatay Demiralp*

Main category: cs.DB

TL;DR: Leam 是一个旨在解决现有文本分析系统局限性的系统，通过整合多种工具的优点，提供了一个统一的平台来处理复杂的文本分析任务。


<details>
  <summary>Details</summary>
Motivation: 现有文本分析系统通常只包含部分功能，并且难以处理数据异构性、数据来源、工作流可重用性和可再现性以及与既有实践的兼容性等挑战。

Method: 提出了一种名为 Leam 的系统，该系统具有交互式用户界面，用于运行文本分析工作流；新的数据模型，用于管理多种原子和复合数据类型；以及一种表达性代数，用于捕获代表文本分析各个阶段的操作，并实现数据、代码和可视化等不同系统组件之间的协调。

Result: 报告了 Leam 系统的开发进展，并通过使用示例展示了其有效性。

Conclusion: Leam 系统通过整合计算笔记本、电子表格和可视化工具的优点，将文本分析过程视为一个连续的整体，以应对数据异构性、数据来源、工作流可重用性和可再现性以及与既有实践的兼容性等挑战。

Abstract: With the increase in scale and availability of digital text generated on the
web, enterprises such as online retailers and aggregators often use text
analytics to mine and analyze the data to improve their services and products
alike. Text data analysis is an iterative, non-linear process with diverse
workflows spanning multiple stages, from data cleaning to visualization.
Existing text analytics systems usually accommodate a subset of these stages
and often fail to address challenges related to data heterogeneity, provenance,
workflow reusability and reproducibility, and compatibility with established
practices. Based on a set of design considerations we derive from these
challenges, we propose Leam, a system that treats the text analysis process as
a single continuum by combining advantages of computational notebooks,
spreadsheets, and visualization tools. Leam features an interactive user
interface for running text analysis workflows, a new data model for managing
multiple atomic and composite data types, and an expressive algebra that
captures diverse sets of operations representing various stages of text
analysis and enables coordination among different components of the system,
including data, code, and visualizations. We report our current progress in
Leam development while demonstrating its usefulness with usage examples.
Finally, we outline a number of enhancements to Leam and identify several
research directions for developing an interactive visual text analysis system.

</details>


### [154] [ObjTables: structured spreadsheets that promote data quality, reuse, and integration](https://arxiv.org/abs/2005.05227)
*Jonathan R. Karr,Wolfram Liebermeister,Arthur P. Goldberg,John A. P. Sekar,Bilal Shaikh*

Main category: cs.DB

TL;DR: ObjTables是一个工具包，通过为电子表格添加模式，使其更易于人类和机器阅读和分析，从而促进科学数据的重用和整合。


<details>
  <summary>Details</summary>
Motivation: 科学中的核心挑战是从复杂网络中理解系统行为的出现，这通常需要聚合、重用和集成异构信息。作为文章的补充，电子表格是一个关键的数据来源。然而，电子表格通常难以重新分析，因为它们是临时创建的，没有定义其所代表的对象、关系和属性的模式。

Method: 开发了一个名为ObjTables的工具包，该工具包包含用于模式的格式、用于指示每个电子表格和列所代表的类和属性的标记、多种科学信息数据类型以及用于使用模式读取、写入、验证、比较、合并、修订和分析电子表格的高级软件。

Result: ObjTables通过使电子表格更易于重用，能够实现前所未有的二次元分析。通过轻松构建新格式和关联软件以支持新型数据，ObjTables还可以加速新兴的科学领域。

Conclusion: ObjTables工具包通过结合模式和对象关系映射系统，使电子表格人类可读且机器可读，从而使研究人员能够重用和组合电子表格。ObjTables可以实现前所未有的二次元分析，并能加速新兴的科学领域。

Abstract: A central challenge in science is to understand how systems behaviors emerge
from complex networks. This often requires aggregating, reusing, and
integrating heterogeneous information. Supplementary spreadsheets to articles
are a key data source. Spreadsheets are popular because they are easy to read
and write. However, spreadsheets are often difficult to reanalyze because they
capture data ad hoc without schemas that define the objects, relationships, and
attributes that they represent. To help researchers reuse and compose
spreadsheets, we developed ObjTables, a toolkit that makes spreadsheets human-
and machine-readable by combining spreadsheets with schemas and an
object-relational mapping system. ObjTables includes a format for schemas;
markup for indicating the class and attribute represented by each spreadsheet
and column; numerous data types for scientific information; and high-level
software for using schemas to read, write, validate, compare, merge, revision,
and analyze spreadsheets. By making spreadsheets easier to reuse, ObjTables
could enable unprecedented secondary meta-analyses. By making it easy to build
new formats and associated software for new types of data, ObjTables can also
accelerate emerging scientific fields.

</details>


### [155] [Needles in the 'Sheet'stack: Augmented Analytics to get Insights from Spreadsheets](https://arxiv.org/abs/2006.08224)
*Medha Atre,Anand Deshpande,Reshma Godse,Pooja Deokar,Sandip Moharir,Dhruva Ray,Akshay Chitlangia,Trupti Phadnis,Yugansh Goyal*

Main category: cs.DB

TL;DR: A demo provides insights from spreadsheets for users unfamiliar with databases, needing no schema knowledge.


<details>
  <summary>Details</summary>
Motivation: To provide insights for users without familiarity with database schema or resources for contemporary analytics.

Method: Examining periodic spreadsheets of different reports (views) without prior knowledge of the schema or data information.

Result: The solution provides ready insights or visual query explorations from periodic spreadsheets.

Conclusion: The demo focuses on providing insights from periodic spreadsheets without prior knowledge of schema or data, targeting users unfamiliar with database analytics.

Abstract: Business intelligence (BI) tools for database analytics have come a long way
and nowadays also provide ready insights or visual query explorations, e.g.
QuickInsights by Microsoft Power BI, SpotIQ by ThoughtSpot, Zenvisage, etc. In
this demo, we focus on providing insights by examining periodic spreadsheets of
different reports (aka views), without prior knowledge of the schema of the
database or reports, or data information. Such a solution is targeted at users
without the familiarity with the database schema or resources to conduct
analytics in the contemporary way.

</details>


### [156] [A Billion Updates per Second Using 30,000 Hierarchical In-Memory D4M Databases](https://arxiv.org/abs/1902.00846)
*Jeremy Kepner,Vijay Gadepally,Lauren Milechin,Siddharth Samsi,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Micheal Houle,Micheal Jones,Anne Klein,Peter Michaleas,Julie Mullen,Andrew Prout,Antonio Rosa,Charles Yee,Albert Reuther*

Main category: cs.DB

TL;DR: D4M 库使用层次化关联数组，在 MIT SuperCloud 上实现了大规模流式网络数据的快速更新。


<details>
  <summary>Details</summary>
Motivation: 对大规模网络进行分析需要高性能的流式更新图表示。

Method: 使用 D4M 库的 D4M 关联数组的层次化实现，在 1,100 个服务器节点上运行 34,000 个实例。

Result: 在 MIT SuperCloud 上实现了每秒 1,900,000,000 次更新的持续更新速率。

Conclusion: MIT SuperCloud 上的 D4M 库能够以每秒 1,900,000,000 次更新的速率处理超大规模流式网络数据。

Abstract: Analyzing large scale networks requires high performance streaming updates of
graph representations of these data. Associative arrays are mathematical
objects combining properties of spreadsheets, databases, matrices, and graphs,
and are well-suited for representing and analyzing streaming network data. The
Dynamic Distributed Dimensional Data Model (D4M) library implements associative
arrays in a variety of languages (Python, Julia, and Matlab/Octave) and
provides a lightweight in-memory database. Associative arrays are designed for
block updates. Streaming updates to a large associative array requires a
hierarchical implementation to optimize the performance of the memory
hierarchy. Running 34,000 instances of a hierarchical D4M associative arrays on
1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of
1,900,000,000 updates per second. This capability allows the MIT SuperCloud to
analyze extremely large streaming network data sets.

</details>


### [157] [Towards Semantically Enhanced Data Understanding](https://arxiv.org/abs/1806.04952)
*Markus Schröder,Christian Jilek,Jörn Hees,Andreas Dengel*

Main category: cs.DB

TL;DR: A new method links data with its documentation using a semantic model to reduce lookup overhead and improve data utilization.


<details>
  <summary>Details</summary>
Motivation: Current methods for data understanding involve separate documentation, causing significant lookup overhead and making it difficult for other applications to utilize unstructured data.

Method: A single semantic model that interlinks data with its documentation.

Result: A methodology that allows data scientists to directly look up connected information about the data by following links and enables browsing of documentation that always refers to the data. The model can also be used by other approaches for searching, comparing, integrating, or visualizing data. An early prototype is demonstrated.

Conclusion: We propose a methodology that uses a single semantic model to interlink data with its documentation, allowing data scientists to directly access connected information and enabling other applications to utilize the data.

Abstract: In the field of machine learning, data understanding is the practice of
getting initial insights in unknown datasets. Such knowledge-intensive tasks
require a lot of documentation, which is necessary for data scientists to grasp
the meaning of the data. Usually, documentation is separate from the data in
various external documents, diagrams, spreadsheets and tools which causes
considerable look up overhead. Moreover, other supporting applications are not
able to consume and utilize such unstructured data. That is why we propose a
methodology that uses a single semantic model that interlinks data with its
documentation. Hence, data scientists are able to directly look up the
connected information about the data by simply following links. Equally, they
can browse the documentation which always refers to the data. Furthermore, the
model can be used by other approaches providing additional support, like
searching, comparing, integrating or visualizing data. To showcase our approach
we also demonstrate an early prototype.

</details>


### [158] [WebRelate: Integrating Web Data with Spreadsheets using Examples](https://arxiv.org/abs/1711.05787)
*Jeevana Priya Inala,Rishabh Singh*

Main category: cs.DB

TL;DR: WebRelate simplifies joining web data with relational data by learning URL generation and data extraction programs from examples, overcoming challenges faced by users with limited programming expertise.


<details>
  <summary>Details</summary>
Motivation: Existing methods for joining web data with relational data are challenging for data scientists and spreadsheet users due to the difficulty of formulating logic to access webpages and writing complex scripts for data extraction, often requiring expertise beyond their programming skills.

Method: WebRelate decomposes the web data integration task into URL learning and input-dependent web extraction. It learns string transformation programs for URL generation and data extraction programs using input-output examples, utilizing domain-specific languages and efficient synthesis algorithms.

Result: WebRelate successfully learned the desired programs for the majority of the 88 evaluated real-world web data integration tasks within seconds, using only a single example.

Conclusion: WebRelate can learn the desired programs within few seconds using only 1 example for the majority of the tasks, as evaluated on 88 real-world web data integration tasks.

Abstract: Data integration between web sources and relational data is a key challenge
faced by data scientists and spreadsheet users. There are two main challenges
in programmatically joining web data with relational data. First, most websites
do not expose a direct interface to obtain tabular data, so the user needs to
formulate a logic to get to different webpages for each input row in the
relational table. Second, after reaching the desired webpage, the user needs to
write complex scripts to extract the relevant data, which is often conditioned
on the input data. Since many data scientists and end-users come from diverse
backgrounds, writing such complex regular-expression based logical scripts to
perform data integration tasks is unfortunately often beyond their programming
expertise.
  We present WebRelate, a system that allows users to join semi-structured web
data with relational data in spreadsheets using input-output examples.
WebRelate decomposes the web data integration task into two sub-tasks of i) URL
learning and ii) input-dependent web extraction. The first sub-task generates
the URLs for the webpages containing the desired data for all rows in the
relational table. WebRelate achieves this by learning a string transformation
program using a few example URLs. The second sub-task uses examples of desired
data to be extracted from the corresponding webpages and learns a program to
extract the data for the other rows. We design expressive domain-specific
languages for URL generation and web data extraction, and present efficient
synthesis algorithms for learning programs in these DSLs from few input-output
examples. We evaluate WebRelate on 88 real-world web data integration tasks
taken from online help forums and Excel product team, and show that WebRelate
can learn the desired programs within few seconds using only 1 example for the
majority of the tasks.

</details>


### [159] [Towards a Holistic Integration of Spreadsheets with Databases: A Scalable Storage Engine for Presentational Data Management](https://arxiv.org/abs/1708.06712)
*Mangesh Bendre,Vipul Venkataraman,Xinyan Zhou,Kevin Chang,Aditya Parameswaran*

Main category: cs.DB

TL;DR: 本研究提出了一种名为DataSpread的系统，旨在将电子表格的前端交互性与数据库的后端可扩展性相结合，以实现展示数据管理（PDM）。研究者开发了一种新的存储引擎，可以灵活地表示电子表格数据，并支持高效的按位置访问，从而提高了存储效率和公式计算速度。


<details>
  <summary>Details</summary>
Motivation: 电子表格软件是交互式临时数据管理的首选工具，被数十亿用户采用。然而，电子表格不像数据库系统那样具有可扩展性。另一方面，数据库系统虽然高度可扩展，但不将交互性作为一等原语。因此，我们正在开发DataSpread，旨在将电子表格作为前端接口与数据库作为后端数据存储进行整体集成，为电子表格提供可扩展性，为数据库提供交互性，这种集成被称为展示数据管理（PDM）。

Method: 本论文提出了一种用于展示数据管理（PDM）的存储引擎，研究了如何在数据库中灵活地表示电子表格数据，以及如何支持和维护按位置访问。首先，我们进行了广泛的电子表格使用调查，以确定PDM存储引擎的功能需求。然后，我们开发了一套灵活表示电子表格数据的机制，并证明了识别最佳表示方法是NP难的。然而，我们提出了一种有效的方法来识别一类重要的、直观的表示方法中的最佳表示。此外，我们还扩展了我们的机制，加入了按位置访问的机制，避免了级联更新问题，实现了常数时间访问和修改性能。

Result: 我们开发了一套灵活表示电子表格数据的机制，并证明了识别最佳表示方法是NP难的；但是，我们提出了一种有效的方法来识别一类重要的、直观的表示方法中的最佳表示。我们将我们的机制扩展到按位置访问机制，这些机制不会遭受级联更新问题，从而实现常数时间的访问和修改性能。我们在典型电子表格和电子表格操作的工作负载上评估了这些表示，提供了高达20%的存储减少和高达50%的公式评估时间减少。

Conclusion: 通过研究典型电子表格和电子表格操作的工作负载，我们证明了所提出的表示方法可以减少最多20%的存储，并减少最多50%的公式评估时间。

Abstract: Spreadsheet software is the tool of choice for interactive ad-hoc data
management, with adoption by billions of users. However, spreadsheets are not
scalable, unlike database systems. On the other hand, database systems, while
highly scalable, do not support interactivity as a first-class primitive. We
are developing DataSpread, to holistically integrate spreadsheets as a
front-end interface with databases as a back-end datastore, providing
scalability to spreadsheets, and interactivity to databases, an integration we
term presentational data management (PDM). In this paper, we make a first step
towards this vision: developing a storage engine for PDM, studying how to
flexibly represent spreadsheet data within a database and how to support and
maintain access by position. We first conduct an extensive survey of
spreadsheet use to motivate our functional requirements for a storage engine
for PDM. We develop a natural set of mechanisms for flexibly representing
spreadsheet data and demonstrate that identifying the optimal representation is
NP-Hard; however, we develop an efficient approach to identify the optimal
representation from an important and intuitive subclass of representations. We
extend our mechanisms with positional access mechanisms that don't suffer from
cascading update issues, leading to constant time access and modification
performance. We evaluate these representations on a workload of typical
spreadsheets and spreadsheet operations, providing up to 20% reduction in
storage, and up to 50% reduction in formula evaluation time.

</details>


### [160] [Synthesizing Mapping Relationships Using Table Corpus](https://arxiv.org/abs/1705.09276)
*Yue Wang,Yeye He*

Main category: cs.DB

TL;DR: 提出了一种利用表格共现统计信息和函数依赖等约束来合成映射关系的方法，以解决现有映射表存储库的不足，并证明了该方法在提高数据清理和集成效率方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的映射表存储库无法满足智能应用的需求，而映射表在数据清理和数据集成等领域具有广泛的应用前景

Method: 利用表格的共现统计信息和函数依赖等约束来合成映射关系

Result: 实验结果表明该方法能够生成高质量的映射表

Conclusion: 该方法可以生成高质量的映射表

Abstract: Mapping relationships, such as (country, country-code) or (company,
stock-ticker), are versatile data assets for an array of applications in data
cleaning and data integration like auto-correction and auto-join. However,
today there are no good repositories of mapping tables that can enable these
intelligent applications.
  Given a corpus of tables such as web tables or spreadsheet tables, we observe
that values of these mappings often exist in pairs of columns in same tables.
Motivated by their broad applicability, we study the problem of synthesizing
mapping relationships using a large table corpus. Our synthesis process
leverages compatibility of tables based on co-occurrence statistics, as well as
constraints such as functional dependency. Experiment results using web tables
and enterprise spreadsheets suggest that the proposed approach can produce high
quality mappings.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [161] [Polynomial-Time Proactive Synthesis of Tree-to-String Functions from Examples](https://arxiv.org/abs/1701.04288)
*Mikaël Mayer,Jad Hamza,Viktor Kuncak*

Main category: cs.FL

TL;DR: 本文提出了一种用于合成代数数据类型到字符串的递归函数的方法。该方法将问题形式化为学习树到字符串变换器，并在特定条件下（闭包条件）证明了其NP-完全性和多项式时间可解性。此外，还提出了一种主动学习方法，以减少用户提供示例的工作量。


<details>
  <summary>Details</summary>
Motivation: 合成了用于pretty printing和序列化代数数据类型到字符串的函数。

Method: 本文将问题形式化为学习具有单个状态的确定性顺序自顶向下树到字符串变换器。我们证明了从输入/输出示例学习树到字符串变换器在一般情况下是NP-完全的，但在每个子树也包含在输入/输出示例集中的（非常实用的）闭包条件下，可以在多项式时间内解决。

Result: 在闭包条件下，合成问题可以在多项式时间内解决。在主动学习场景中，该算法可以查询代数数据类型的定义大小的线性数量来确定唯一的变换器。

Conclusion: 本文为一类由结构递归定义的函数提出了一种完整的合成算法，该函数将代数数据类型映射到字符串，并可用于程序和数据的打印和序列化。

Abstract: Synthesis from examples enables non-expert users to generate programs by
specifying examples of their behavior. A domain-specific form of such synthesis
has been recently deployed in a widely used spreadsheet software product. In
this paper we contribute to foundations of such techniques and present a
complete algorithm for synthesis of a class of recursive functions defined by
structural recursion over a given algebraic data type definition. The functions
we consider map an algebraic data type to a string; they are useful for, e.g.,
pretty printing and serialization of programs and data. We formalize our
problem as learning deterministic sequential top-down tree-to-string
transducers with a single state.
  The first problem we consider is learning a tree-to-string transducer from
any set of input/output examples provided by the user. We show that this
problem is NP-complete in general, but can be solved in polynomial time under a
(practically useful) closure condition that each subtree of a tree in the
input/output example set is also part of the input/output examples.
  Because coming up with relevant input/output examples may be difficult for
the user while creating hard constraint problems for the synthesizer, we also
study a more automated active learning scenario in which the algorithm chooses
the inputs for which the user provides the outputs. Our algorithm asks a
worst-case linear number of queries as a function of the size of the algebraic
data type definition to determine a unique transducer.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [162] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: 人工智能应更多地关注建模现实世界中的事物及其关系，而非仅仅是像素和词语。关系学习是实现这一目标的关键，但其发展仍需克服挑战以太的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能系统主要关注像素和词语等感知层面的建模，而忽视了由具有属性和关系的事物组成的现实世界。最有价值的数据往往以关系形式存在于数据库和电子表格中，这与传统的机器学习研究范畴不同。

Method: 本文解释了关系学习未能主导世界的原因，并提出了实现其应有地位的必要措施。

Result: 关系学习在某些受限关系的情况下取得了一定的成功，但总体而言尚未占据主导地位。

Conclusion: 关系学习需要进一步发展才能在人工智能领域占据主导地位，尽管它在某些特定情况下已经取得了成功。

Abstract: AI seems to be taking over the world with systems that model pixels, words,
and phonemes. The world is arguably made up, not of pixels, words, and phonemes
but of entities (objects, things, including events) with properties and
relations among them. Surely we should model these, not the perception or
description of them. You might suspect that concentrating on modeling words and
pixels is because all of the (valuable) data in the world is in terms of text
and images. If you look into almost any company you will find their most
valuable data is in spreadsheets, databases and other relational formats. These
are not the form that are studied in introductory machine learning, but are
full of product numbers, student numbers, transaction numbers and other
identifiers that can't be interpreted naively as numbers. The field that
studies this sort of data has various names including relational learning,
statistical relational AI, and many others. This paper explains why relational
learning is not taking over the world -- except in a few cases with restricted
relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


### [163] [MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.05587)
*Junjie Xing,Yeye He,Mengyu Zhou,Haoyu Dong,Shi Han,Lingjiao Chen,Dongmei Zhang,Surajit Chaudhuri,H. V. Jagadish*

Main category: cs.AI

TL;DR: MMTU 是一个评估模型处理真实世界表格任务能力的大型基准测试，结果显示现有先进模型仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有针对表格任务的基准测试存在局限性，主要集中在自然语言到 SQL 和表格问答任务，忽视了专业用户面临的更广泛的真实世界任务，这限制了对该领域模型能力和进展的理解。

Method: MMTU 基准测试包含 30K 多个问题，涵盖 25 个真实世界的表格任务，这些任务源于计算机科学领域对表格数据的数十年研究，重点关注专业用户面临的复杂表格任务。

Result: MMTU 基准测试表明，当前包括 OpenAI o4-mini 和 DeepSeek R1 在内的先进模型在处理这些任务时，得分仅在 60% 左右，显示出在表格理解、推理和编码能力方面有很大的提升空间。

Conclusion: MMTU 是一个大规模的基准，包含 30K 多个跨越 25 个真实世界表格任务的问题，旨在全面评估模型在专家级别上理解、推理和操作真实表格的能力。研究表明，MMTU 需要结合表格理解、推理和编码等多种技能，这对于当今的先进模型来说仍然是一个挑战，即使是像 OpenAI o4-mini 和 DeepSeek R1 这样的前沿推理模型也只能获得 60% 左右的分数，表明在这一领域还有很大的改进空间。

Abstract: Tables and table-based use cases play a crucial role in many important
real-world applications, such as spreadsheets, databases, and computational
notebooks, which traditionally require expert-level users like data engineers,
data analysts, and database administrators to operate. Although LLMs have shown
remarkable progress in working with tables (e.g., in spreadsheet and database
copilot scenarios), comprehensive benchmarking of such capabilities remains
limited. In contrast to an extensive and growing list of NLP benchmarks,
evaluations of table-related tasks are scarce, and narrowly focus on tasks like
NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks
that professional users face. This gap limits our understanding and model
progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 30K
questions across 25 real-world table tasks, designed to comprehensively
evaluate models ability to understand, reason, and manipulate real tables at
the expert-level. These tasks are drawn from decades' worth of computer science
research on tabular data, with a focus on complex table tasks faced by
professional users. We show that MMTU require a combination of skills --
including table understanding, reasoning, and coding -- that remain challenging
for today's frontier models, where even frontier reasoning models like OpenAI
o4-mini and DeepSeek R1 score only around 60%, suggesting significant room for
improvement. We highlight key findings in our evaluation using MMTU and hope
that this benchmark drives further advances in understanding and developing
foundation models for structured data processing and analysis. Our code and
data are available at https://github.com/MMTU-Benchmark/MMTU and
https://huggingface.co/datasets/MMTU-benchmark/MMTU.

</details>


### [164] [Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models](https://arxiv.org/abs/2505.23667)
*Lang Cao,Jingxian Xu,Hanbing Liu,Jinyu Wang,Mengyu Zhou,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 提出了一种名为Formula Tuning (Fortune)的强化学习框架，用于训练语言模型生成电子表格公式以进行表格问答。该框架通过奖励模型生成正确答案，从而学习公式推导，并在多项基准测试中取得了显著效果，甚至超越了OpenAI o1。


<details>
  <summary>Details</summary>
Motivation: 表格是组织和分析数据的基本结构，而有效的表格理解对于智能系统至关重要。尽管大型语言模型在一般推理能力方面表现出色，但在处理表格数据（尤其是在复杂场景下）的数值或符号推理方面仍然存在挑战。电子表格公式是一种强大的、可执行的符号操作表示，蕴含着丰富的推理模式，但目前尚未得到充分利用。

Method: Formula Tuning (Fortune) 是一个强化学习框架，通过使用二进制答案正确性作为奖励信号来训练语言模型生成可执行的电子表格公式。

Result: Formula Tuning 框架通过减少对监督公式注释的依赖，并使用二进制答案正确性作为奖励信号，有效提升了语言模型在表格理解任务上的性能，尤其是在多步数值和符号推理任务上。实验表明，一个7B模型通过Fortune框架的表现优于OpenAI o1。

Conclusion: Formula Tuning (Fortune)框架通过使用二进制答案正确性作为奖励信号，利用强化学习训练语言模型生成可执行的电子表格公式，以解决表格数据问答问题。该方法减少了对监督公式注释的依赖，并通过推理引导模型学习公式推导。实验证明，Fortune显著提升了语言模型在表格理解任务上的性能，尤其是在多步数值和符号推理任务上，一个7B模型在该任务上表现优于OpenAI o1。

Abstract: Tables are a fundamental structure for organizing and analyzing data, making
effective table understanding a critical capability for intelligent systems.
While large language models (LMs) demonstrate strong general reasoning
abilities, they continue to struggle with accurate numerical or symbolic
reasoning over tabular data, especially in complex scenarios. Spreadsheet
formulas provide a powerful and expressive medium for representing executable
symbolic operations, encoding rich reasoning patterns that remain largely
underutilized. In this paper, we propose Formula Tuning (Fortune), a
reinforcement learning (RL) framework that trains LMs to generate executable
spreadsheet formulas for question answering over general tabular data. Formula
Tuning reduces the reliance on supervised formula annotations by using binary
answer correctness as a reward signal, guiding the model to learn formula
derivation through reasoning. We provide a theoretical analysis of its
advantages and demonstrate its effectiveness through extensive experiments on
seven table reasoning benchmarks. Formula Tuning substantially enhances LM
performance, particularly on multi-step numerical and symbolic reasoning tasks,
enabling a 7B model to outperform OpenAI o1 on table understanding. This
highlights the potential of formula-driven RL to advance symbolic table
reasoning in LMs.

</details>


### [165] [The AI Risk Repository: A Comprehensive Meta-Review, Database, and Taxonomy of Risks From Artificial Intelligence](https://arxiv.org/abs/2408.12622)
*Peter Slattery,Alexander K. Saeri,Emily A. C. Grundy,Jess Graham,Michael Noetel,Risto Uuk,James Dao,Soroush Pour,Stephen Casper,Neil Thompson*

Main category: cs.AI

TL;DR: 本研究创建了一个包含777个AI风险的数据库，并提供了两种分类方法，以促进对AI风险的更好理解和管理。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对AI风险的共同理解，阻碍了对其进行全面讨论、研究和应对。本研究旨在通过创建一个AI风险知识库来解决这一差距。

Method: 通过系统性回顾43种AI风险分类法，并进行专家咨询，构建了一个包含777个AI风险的数据库。利用“最佳匹配框架综合”方法开发了两个核心分类法：因果分类法（区分风险的实体、意图和时机）和领域分类法（将风险归入七个主要领域和23个子领域）。

Result: 创建了一个包含777个AI风险的AI风险知识库，并开发了因果分类法和领域分类法。该知识库是首个公开的、全面的、可扩展的AI风险数据库，为管理AI风险奠定了基础。

Conclusion: 该研究通过创建一个包含777个AI风险的AI风险知识库，并对其进行分类和整理，为应对AI风险提供了共同的参考框架，旨在促进对AI风险的协调、连贯和完整的讨论、研究和管理。

Abstract: The risks posed by Artificial Intelligence (AI) are of considerable concern
to academics, auditors, policymakers, AI companies, and the public. However, a
lack of shared understanding of AI risks can impede our ability to
comprehensively discuss, research, and react to them. This paper addresses this
gap by creating an AI Risk Repository to serve as a common frame of reference.
This comprises a living database of 777 risks extracted from 43 taxonomies,
which can be filtered based on two overarching taxonomies and easily accessed,
modified, and updated via our website and online spreadsheets. We construct our
Repository with a systematic review of taxonomies and other structured
classifications of AI risk followed by an expert consultation. We develop our
taxonomies of AI risk using a best-fit framework synthesis. Our high-level
Causal Taxonomy of AI Risks classifies each risk by its causal factors (1)
Entity: Human, AI; (2) Intentionality: Intentional, Unintentional; and (3)
Timing: Pre-deployment; Post-deployment. Our mid-level Domain Taxonomy of AI
Risks classifies risks into seven AI risk domains: (1) Discrimination &
toxicity, (2) Privacy & security, (3) Misinformation, (4) Malicious actors &
misuse, (5) Human-computer interaction, (6) Socioeconomic & environmental, and
(7) AI system safety, failures, & limitations. These are further divided into
23 subdomains. The AI Risk Repository is, to our knowledge, the first attempt
to rigorously curate, analyze, and extract AI risk frameworks into a publicly
accessible, comprehensive, extensible, and categorized risk database. This
creates a foundation for a more coordinated, coherent, and complete approach to
defining, auditing, and managing the risks posed by AI systems.

</details>


### [166] [SpreadsheetLLM: Encoding Spreadsheets for Large Language Models](https://arxiv.org/abs/2407.09025)
*Haoyu Dong,Jianbo Zhao,Yuzhang Tian,Junyu Xiong,Shiyu Xia,Mengyu Zhou,Yun Lin,José Cambronero,Yeye He,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: SpreadsheetLLM是一种新的编码方法，可以优化LLMs在电子表格上的理解和推理能力，通过SheetCompressor框架显著提高了性能，并在电子表格理解和问答任务中取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 电子表格的二维网格、灵活布局和多样化格式对大型语言模型（LLMs）构成重大挑战，需要一种有效的方法来优化LLMs在电子表格上的理解和推理能力。

Method: 提出了一种名为SpreadsheetLLM的编码方法，包括香草序列化方法和SheetCompressor框架（包含结构锚压缩、逆索引转换和数据格式感知聚合三个模块），以及用于下游任务的链式电子表格。

Result: SheetCompressor在电子表格表格检测任务中表现出显著的性能提升，在GPT4的上下文学习设置中比香草方法提高了25.6%。经过SheetCompressor微调的LLM实现了25倍的平均压缩率，并取得了78.9%的F1分数，优于现有最佳模型12.3%。

Conclusion: SpreadsheetLLM在各种电子表格任务中都非常有效，通过链式电子表格进行下游任务的理解和验证，并在新的、要求严苛的电子表格问答任务中进行了验证。

Abstract: Spreadsheets are characterized by their extensive two-dimensional grids,
flexible layouts, and varied formatting options, which pose significant
challenges for large language models (LLMs). In response, we introduce
SpreadsheetLLM, pioneering an efficient encoding method designed to unleash and
optimize LLMs' powerful understanding and reasoning capability on spreadsheets.
Initially, we propose a vanilla serialization approach that incorporates cell
addresses, values, and formats. However, this approach was limited by LLMs'
token constraints, making it impractical for most applications. To tackle this
challenge, we develop SheetCompressor, an innovative encoding framework that
compresses spreadsheets effectively for LLMs. It comprises three modules:
structural-anchor-based compression, inverse index translation, and
data-format-aware aggregation. It significantly improves performance in the
spreadsheet table detection task, outperforming the vanilla approach by 25.6%
in GPT4's in-context learning setting. Moreover, fine-tuned LLM with
SheetCompressor has an average compression ratio of 25 times, and achieves a
state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%.
Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet
understanding and validate it in a new and demanding spreadsheet QA task. We
methodically leverage the inherent layout and structure of spreadsheets,
demonstrating that SpreadsheetLLM is highly effective across a variety of
spreadsheet tasks.

</details>


### [167] [SheetAgent: Towards A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models](https://arxiv.org/abs/2403.03636)
*Yibin Chen,Yifu Yuan,Zeyu Zhang,Yan Zheng,Jinyi Liu,Fei Ni,Jianye Hao,Hangyu Mao,Fuzheng Zhang*

Main category: cs.AI

TL;DR: SheetAgent是一个利用LLM的自主代理，通过其规划、信息提供和检索模块，解决了电子表格操作中的复杂推理问题，并在多个基准测试中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了弥合当前大型语言模型在复杂、现实世界的电子表格操作任务（尤其是涉及长周期、多步骤推理和模糊需求的任务）中的不足，本研究引入了一个名为SheetRM的新基准，该基准包含具有推理依赖性的长周期、多类别电子表格操作任务，以应对现实世界的挑战。

Method: 提出了一种名为SheetAgent的新型自主代理，该代理利用大型语言模型（LLM）的能力，并包含三个协作模块：规划器、信息提供者和检索器。该代理通过迭代式任务推理和反思，实现了对电子表格的高级推理和精确操作，无需人工干预。

Result: SheetAgent在多个基准测试中，相较于现有方法，在电子表格操作的准确性方面提高了20%-40%，并展示了其在表格推理方面的优越能力。

Conclusion: SheetAgent在电子表格操作和表格推理方面表现出卓越的能力，通过其创新的三模块协作设计（规划、信息提供和检索），成功解决了长周期、多步骤和模糊需求等复杂现实世界的挑战，并在多个基准测试中带来了20%-40%的改进。

Abstract: Spreadsheets are ubiquitous across the World Wide Web, playing a critical
role in enhancing work efficiency across various domains. Large language model
(LLM) has been recently attempted for automatic spreadsheet manipulation but
has not yet been investigated in complicated and realistic tasks where
reasoning challenges exist (e.g., long horizon manipulation with multi-step
reasoning and ambiguous requirements). To bridge the gap with the real-world
requirements, we introduce SheetRM, a benchmark featuring long-horizon and
multi-category tasks with reasoning-dependent manipulation caused by real-life
challenges. To mitigate the above challenges, we further propose SheetAgent, a
novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of
three collaborative modules: Planner, Informer, and Retriever, achieving both
advanced reasoning and accurate manipulation over spreadsheets without human
interaction through iterative task reasoning and reflection. Extensive
experiments demonstrate that SheetAgent delivers 20--40\% pass rate
improvements on multiple benchmarks over baselines, achieving enhanced
precision in spreadsheet manipulation and demonstrating superior table
reasoning abilities. More details and visualizations are available at the
project website: https://sheetagent.github.io/. The datasets and source code
are available at https://anonymous.4open.science/r/SheetAgent.

</details>


### [168] [Large Language Model for Table Processing: A Survey](https://arxiv.org/abs/2402.05121)
*Weizheng Lu,Jing Zhang,Ju Fan,Zihao Fu,Yueguo Chen,Xiaoyong Du*

Main category: cs.AI

TL;DR: LLMs和VLMs在处理表格数据方面有很大潜力，但仍面临一些挑战。


<details>
  <summary>Details</summary>
Motivation: 自动化表格处理任务（如数据库查询、电子表格操作、网络表格问答和图像表格信息提取）具有重要的公共利益，引起了学术界和工业界的广泛关注。

Method: 对表格相关任务进行全面调查，涵盖用户场景、技术细节、模型训练技术、提示工程（特别是LLM驱动的代理）以及当前面临的挑战。

Result: 对表格处理的LLM和VLM的训练技术进行了总结，并讨论了用于各种表格相关任务的提示工程，特别是LLM驱动的代理。

Conclusion: LLMs and VLMs在处理表格数据方面展现出巨大潜力，但也面临用户输入多样化和思维链推理效率等挑战。

Abstract: Tables, typically two-dimensional and structured to store large amounts of
data, are essential in daily activities like database queries, spreadsheet
manipulations, web table question answering, and image table information
extraction. Automating these table-centric tasks with Large Language Models
(LLMs) or Visual Language Models (VLMs) offers significant public benefits,
garnering interest from academia and industry. This survey provides a
comprehensive overview of table-related tasks, examining both user scenarios
and technical aspects. It covers traditional tasks like table question
answering as well as emerging fields such as spreadsheet manipulation and table
data analysis. We summarize the training techniques for LLMs and VLMs tailored
for table processing. Additionally, we discuss prompt engineering, particularly
the use of LLM-powered agents, for various table-related tasks. Finally, we
highlight several challenges, including diverse user input when serving and
slow thinking using chain-of-thought.

</details>


### [169] [FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language](https://arxiv.org/abs/2310.17306)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Elnaz Nouri,Mohammad Raza,Gust Verbruggen*

Main category: cs.AI

TL;DR: FormaT5通过学习预测占位符来处理未指明的自然语言描述，从而生成电子表格的条件格式规则，并在广泛的基准测试中取得了领先地位。


<details>
  <summary>Details</summary>
Motivation: 电子表格软件允许用户通过编写数据依赖的条件格式（CF）规则来自动格式化表格，但用户在编写这些规则时常常遇到困难，因为这需要他们理解和实现潜在的逻辑。

Method: FormaT5是一个基于Transformer的模型，它能够根据目标表格和描述期望格式化逻辑的自然语言描述来生成条件格式（CF）规则。为了解决用户描述中常见的未指明或模糊问题，FormaT5通过弃权目标来预测占位符，这些占位符可以由第二个模型或通过编程由示例（PBE）系统来填充。

Result: FormaT5在包含来自四个不同来源的真实世界描述的1053个CF任务的基准测试中，其弃权和填充能力优于8种不同的神经方法，并且在有无示例的情况下均表现出色。

Conclusion: FormaT5通过引入占位符和使用弃权目标，优于8种不同的神经方法，并在有无示例的情况下均表现出色。该研究结果强调了构建领域特定学习系统的价值。

Abstract: Formatting is an important property in tables for visualization,
presentation, and analysis. Spreadsheet software allows users to automatically
format their tables by writing data-dependent conditional formatting (CF)
rules. Writing such rules is often challenging for users as it requires them to
understand and implement the underlying logic. We present FormaT5, a
transformer-based model that can generate a CF rule given the target table and
a natural language description of the desired formatting logic. We find that
user descriptions for these tasks are often under-specified or ambiguous,
making it harder for code generation systems to accurately learn the desired
rule in a single step. To tackle this problem of under-specification and
minimise argument errors, FormaT5 learns to predict placeholders though an
abstention objective. These placeholders can then be filled by a second model
or, when examples of rows that should be formatted are available, by a
programming-by-example system. To evaluate FormaT5 on diverse and real
scenarios, we create an extensive benchmark of 1053 CF tasks, containing
real-world descriptions collected from four different sources. We release our
benchmarks to encourage research in this area. Abstention and filling allow
FormaT5 to outperform 8 different neural approaches on our benchmarks, both
with and without examples. Our results illustrate the value of building
domain-specific learning systems.

</details>


### [170] [Don't Treat the Symptom, Find the Cause! Efficient Artificial-Intelligence Methods for (Interactive) Debugging](https://arxiv.org/abs/2306.12850)
*Patrick Rodler*

Main category: cs.AI

TL;DR: Model-based diagnosis is essential for troubleshooting complex modern systems due to our high dependency on them. This thesis introduces the topic, discusses challenges, and presents research approaches to address them, utilizing various techniques like knowledge representation, automated reasoning, and machine learning.


<details>
  <summary>Details</summary>
Motivation: Modern systems are highly sophisticated and essential for everyday life, leading to a high likelihood of failures that can have significant negative effects. Minimizing downtime and repair costs is vital, which is where model-based diagnosis is crucial.

Method: The thesis will discuss approaches addressing challenges in model-based diagnosis, including knowledge representation, automated reasoning, heuristic problem solving, intelligent search, optimization, stochastics, statistics, decision making under uncertainty, machine learning, calculus, combinatorics, and set theory.

Result: The thesis provides an introduction to model-based diagnosis, highlights major challenges, and discusses various research approaches to address these issues.

Conclusion: model-based diagnosis is a principled, domain-independent approach that can be generally applied to troubleshoot systems of a wide variety of types.

Abstract: In the modern world, we are permanently using, leveraging, interacting with,
and relying upon systems of ever higher sophistication, ranging from our cars,
recommender systems in e-commerce, and networks when we go online, to
integrated circuits when using our PCs and smartphones, the power grid to
ensure our energy supply, security-critical software when accessing our bank
accounts, and spreadsheets for financial planning and decision making. The
complexity of these systems coupled with our high dependency on them implies
both a non-negligible likelihood of system failures, and a high potential that
such failures have significant negative effects on our everyday life. For that
reason, it is a vital requirement to keep the harm of emerging failures to a
minimum, which means minimizing the system downtime as well as the cost of
system repair. This is where model-based diagnosis comes into play.
  Model-based diagnosis is a principled, domain-independent approach that can
be generally applied to troubleshoot systems of a wide variety of types,
including all the ones mentioned above, and many more. It exploits and
orchestrates i.a. techniques for knowledge representation, automated reasoning,
heuristic problem solving, intelligent search, optimization, stochastics,
statistics, decision making under uncertainty, machine learning, as well as
calculus, combinatorics and set theory to detect, localize, and fix faults in
abnormally behaving systems.
  In this thesis, we will give an introduction to the topic of model-based
diagnosis, point out the major challenges in the field, and discuss a selection
of approaches from our research addressing these issues.

</details>


### [171] [CORNET: Learning Table Formatting Rules By Example](https://arxiv.org/abs/2208.06032)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Mohammad Raza,Gust Verbruggen*

Main category: cs.AI

TL;DR: CORNET automates learning spreadsheet table formatting rules from examples, outperforming baselines and finding more concise rules than users write.


<details>
  <summary>Details</summary>
Motivation: Writing spreadsheet table formatting rules is challenging for users due to the need for knowledge of rule languages and data logic. CORNET aims to automate this process by learning rules from examples.

Method: CORNET combines symbolic rule enumeration with a neural ranker to learn conditional formatting rules from user-provided examples (formatted cells).

Result: CORNET accurately learns rules across varying evaluation setups. It finds shorter rules than user-written rules and discovers rules in spreadsheets that users have manually formatted. The system was evaluated on over 450K unique formatting rules extracted from 1.8M worksheets.

Conclusion: CORNET can accurately learn formatting rules across different evaluation scenarios, discovers shorter rules than user-written ones, and identifies rules in manually formatted spreadsheets.

Abstract: Spreadsheets are widely used for table manipulation and presentation.
Stylistic formatting of these tables is an important property for both
presentation and analysis. As a result, popular spreadsheet software, such as
Excel, supports automatically formatting tables based on rules. Unfortunately,
writing such formatting rules can be challenging for users as it requires
knowledge of the underlying rule language and data logic. We present CORNET, a
system that tackles the novel problem of automatically learning such formatting
rules from user examples in the form of formatted cells. CORNET takes
inspiration from advances in inductive programming and combines symbolic rule
enumeration with a neural ranker to learn conditional formatting rules. To
motivate and evaluate our approach, we extracted tables with over 450K unique
formatting rules from a corpus of over 1.8M real worksheets. Since we are the
first to introduce conditional formatting, we compare CORNET to a wide range of
symbolic and neural baselines adapted from related domains. Our results show
that CORNET accurately learns rules across varying evaluation setups.
Additionally, we show that CORNET finds shorter rules than those that a user
has written and discovers rules in spreadsheets that users have manually
formatted.

</details>


### [172] [Named Entity Recognition in Industrial Tables using Tabular Language Models](https://arxiv.org/abs/2209.14812)
*Aneta Koleva,Martin Ringsquandl,Mark Buckley,Rakebul Hasan,Volker Tresp*

Main category: cs.AI

TL;DR: Table transformers work well for industry NER on spreadsheets, especially with data augmentation for low-resource needs. Their structure helps models converge.


<details>
  <summary>Details</summary>
Motivation: To address the lack of applications of table transformers in industry despite their growing academic interest, specifically for Named Entity Recognition (NER) in tabular-structured spreadsheets, facing challenges like the technical nature of spreadsheets and limited labeled data.

Method: We developed a data augmentation strategy using knowledge graphs to improve performance in low-resource settings and compared the performance of table transformers against other baselines, analyzing the benefits of tabular structure as inductive bias versus linearized sequences.

Result: Our table transformer model outperformed other baselines, and the data augmentation strategy significantly boosted performance in the low-resource scenario, confirming the importance of tabular inductive bias for convergence.

Conclusion: Transformer-based models show strong performance on NER tasks with tabular data, and their tabular inductive bias is crucial for convergence, especially in low-resource scenarios.

Abstract: Specialized transformer-based models for encoding tabular data have gained
interest in academia. Although tabular data is omnipresent in industry,
applications of table transformers are still missing. In this paper, we study
how these models can be applied to an industrial Named Entity Recognition (NER)
problem where the entities are mentioned in tabular-structured spreadsheets.
The highly technical nature of spreadsheets as well as the lack of labeled data
present major challenges for fine-tuning transformer-based models. Therefore,
we develop a dedicated table data augmentation strategy based on available
domain-specific knowledge graphs. We show that this boosts performance in our
low-resource scenario considerably. Further, we investigate the benefits of
tabular structure as inductive bias compared to tables as linearized sequences.
Our experiments confirm that a table transformer outperforms other baselines
and that its tabular inductive bias is vital for convergence of
transformer-based models.

</details>


### [173] [Spreadsheet computing with Finite Domain Constraint Enhancements](https://arxiv.org/abs/2203.10944)
*Ezana N. Beyenne*

Main category: cs.AI

TL;DR: 电子表格计算的局限性在于其单向数据流，限制了其在解决约束满足问题方面的应用。本论文提出了一个将有限约束求解器与电子表格计算范例相结合的框架，以克服这一限制。


<details>
  <summary>Details</summary>
Motivation: 电子表格应用程序因其易用性和实用性而受到欢迎，但由于其单向数据流的限制，它们仅限于类似簿记的任务。本论文旨在克服这一限制，以解决约束满足问题。

Method: 本论文提出了一种将有限约束求解器与电子表格计算范例无缝结合的框架，允许用户将有限域或指定单元格之间关系的约束附加到电子表格的单元格上。

Result: 该框架通过提供一组特定于电子表格的约束来控制大型电子表格应用程序实现的规模，并提供了约束求解的接口，展示了扩展的电子表格范例的可用性和实用性。

Conclusion: 本论文提出了一个将有限约束求解器与电子表格计算范例相结合的框架，以解决约束满足问题。

Abstract: Spreadsheet computing is one of the more popular computing methodologies in
today's modern society. The spreadsheet application's ease of use and
usefulness has enabled non-programmers to perform programming-like tasks in a
familiar setting modeled after the tabular "pen and paper" approach. However,
spreadsheet applications are limited to bookkeeping-like tasks due to their
single-direction data flow. This thesis demonstrates an extension of the
spreadsheet computing paradigm in overcoming this limitation to solve
constraint satisfaction problems. We present a framework seamlessly
incorporating a finite constraint solver with the spreadsheet computing
paradigm. This framework allows the individual cells in the spreadsheet to be
attached to either a finite domain or a constraint specifying the relationship
among the cells. The framework provides an interface for constraint solving and
further enhances the spreadsheet computing paradigm by providing a set of
spreadsheet-specific constraints that will aid in controlling the scalability
of large spreadsheet applications implementations. Finally, we provide examples
to demonstrate the usability and usefulness of the extended spreadsheet
paradigm.
  Keywords: Spreadsheet computing, Constraint Logic Programming, Constraint
satisfaction, Domain-Specific language, Excel, SWI Prolog, C#

</details>


### [174] [Human-Machine Collaboration for Democratizing Data Science](https://arxiv.org/abs/2004.11113)
*Clément Gautrais,Yann Dauxais,Stefano Teso,Samuel Kolb,Gust Verbruggen,Luc De Raedt*

Main category: cs.AI

TL;DR: VisualSynth is a framework for human-machine collaboration in data science that allows users to analyze data using spreadsheet software and colored sketches.


<details>
  <summary>Details</summary>
Motivation: Many people want to analyze their data but lack the data science expertise.

Method: VisualSynth relies on users providing colored sketches to partially specify data science tasks, which are then determined and executed using AI techniques.

Result: VisualSynth allows users to perform and automate data analysis tasks such as data wrangling, data selection, clustering, constraint learning, predictive modeling, and auto-completion through a human-machine collaboration framework.

Conclusion: VisualSynth can democratize data science by allowing users to interact with spreadsheet software to perform and automate various data analysis tasks.

Abstract: Everybody wants to analyse their data, but only few posses the data science
expertise to to this. Motivated by this observation we introduce a novel
framework and system \textsc{VisualSynth} for human-machine collaboration in
data science.
  It wants to democratize data science by allowing users to interact with
standard spreadsheet software in order to perform and automate various data
analysis tasks ranging from data wrangling, data selection, clustering,
constraint learning, predictive modeling and auto-completion.
\textsc{VisualSynth} relies on the user providing colored sketches, i.e.,
coloring parts of the spreadsheet, to partially specify data science tasks,
which are then determined and executed using artificial intelligence
techniques.

</details>


### [175] [Automated Discovery of Data Transformations for Robotic Process Automation](https://arxiv.org/abs/2001.01007)
*Volodymyr Leno,Marlon Dumas,Marcello La Rosa,Fabrizio Maria Maggi,Artem Polyvyanyy*

Main category: cs.AI

TL;DR: 该研究提出了一种通过UI日志发现可RPA的数据传输例程的方法，并进行了优化以提高效率。


<details>
  <summary>Details</summary>
Motivation: 为了充分利用RPA提供的机会，公司需要发现哪些特定例程可以被自动化以及如何自动化，特别是涉及在电子表格或Web表单之间传输数据 的例程。

Method: 将数据传输例程的发现问题映射到通过示例发现数据转换问题，并提出两种利用UI日志信息和跨应用程序数据传输通常独立复制字母和数字标记的事实的优化方法。

Result: 提出的方法和优化方案通过复制真实数据传输例程的UI日志进行了评估，证明了其有效性。

Conclusion: 该研究提出了一种通过分析用户界面(UI)日志来发现可机器人过程自动化(RPA)的数据传输例程的方法，并针对现有技术效率低下的问题提出了两种优化方案。

Abstract: Robotic Process Automation (RPA) is a technology for automating repetitive
routines consisting of sequences of user interactions with one or more
applications. In order to fully exploit the opportunities opened by RPA,
companies need to discover which specific routines may be automated, and how.
In this setting, this paper addresses the problem of analyzing User Interaction
(UI) logs in order to discover routines where a user transfers data from one
spreadsheet or (Web) form to another. The paper maps this problem to that of
discovering data transformations by example - a problem for which several
techniques are available. The paper shows that a naive application of a
state-of-the-art technique for data transformation discovery is computationally
inefficient. Accordingly, the paper proposes two optimizations that take
advantage of the information in the UI log and the fact that data transfers
across applications typically involve copying alphabetic and numeric tokens
separately. The proposed approach and its optimizations are evaluated using UI
logs that replicate a real-life repetitive data transfer routine.

</details>


### [176] [Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples](https://arxiv.org/abs/1804.01186)
*Ashwin Kalyan,Abhishek Mohta,Oleksandr Polozov,Dhruv Batra,Prateek Jain,Sumit Gulwani*

Main category: cs.AI

TL;DR: 提出了一种名为NGDS的混合合成技术，结合了符号逻辑和神经网络，能够根据少量示例快速、准确地生成程序。


<details>
  <summary>Details</summary>
Motivation: 解决现有合成系统依赖于手动工程化的演绎逻辑技术或需要海量数据的纯统计模型的问题，这些系统在实际应用中通常无法提供实时合成。

Method: 提出了一种名为神经引导归纳搜索（NGDS）的混合合成技术，该技术结合了符号逻辑技术和统计模型。通过将归纳搜索框架与神经网络相结合，将学习问题简化为监督学习，利用了强大的循环神经网络编码器。

Result: NGDS能够生成满足规范且能很好地泛化到未见示例的程序，在实际客户场景中表现出高达12倍于现有最先进系统的速度提升。

Conclusion: 该方法通过结合符号逻辑技术和统计模型，能够根据少量的输入输出示例合成用户意图的程序，并在实际客户场景中实现了高达12倍的速度提升。

Abstract: Synthesizing user-intended programs from a small number of input-output
examples is a challenging problem with several important applications like
spreadsheet manipulation, data wrangling and code refactoring. Existing
synthesis systems either completely rely on deductive logic techniques that are
extensively hand-engineered or on purely statistical models that need massive
amounts of data, and in general fail to provide real-time synthesis on
challenging benchmarks. In this work, we propose Neural Guided Deductive Search
(NGDS), a hybrid synthesis technique that combines the best of both symbolic
logic techniques and statistical models. Thus, it produces programs that
satisfy the provided specifications by construction and generalize well on
unseen examples, similar to data-driven systems. Our technique effectively
utilizes the deductive search framework to reduce the learning problem of the
neural component to a simple supervised learning setup. Further, this allows us
to both train on sparingly available real-world data and still leverage
powerful recurrent neural network encoders. We demonstrate the effectiveness of
our method by evaluating on real-world customer scenarios by synthesizing
accurate programs with up to 12x speed-up compared to state-of-the-art systems.

</details>


### [177] [Contextual Data Collection for Smart Cities](https://arxiv.org/abs/1704.01802)
*Henrique Santos,Vasco Furtado,Paulo Pinheiro,Deborah L. McGuinness*

Main category: cs.AI

TL;DR: 本文提出了一种新的元数据模式和基于HASNetO的架构，以解决开放政府数据中城市传感器测量数据的复用和理解问题。


<details>
  <summary>Details</summary>
Motivation: 开放政府数据倡议要求政府更加开放地共享数据，包括城市传感器网络产生的测量数据。然而，现有数据发布方式（如电子表格、CSV、PDF等）给数据消费者带来了数据复用和理解的困难，例如文件格式不兼容和数据含义不明确等问题。

Method: 本文提出了一种新的元数据模式，用于识别和组织与传感器测量数据相关的元数据，并利用HASNetO（Human-Aware Sensor Network Ontology）构建了一个数据管理架构。

Result: 通过提出新的元数据模式和基于HASNetO的架构，可以更好地管理和复用城市传感器测量数据，解决了数据格式和语义理解方面的挑战。以福塔雷萨市为例，展示了该架构在实际应用中的潜力。

Conclusion: 本文提出了一种利用HASNetO（Human-Aware Sensor Network Ontology）来管理城市环境中传感器测量数据的架构，旨在解决开放政府数据中数据复用和理解的挑战，并以巴西福塔雷萨市为例进行了讨论。

Abstract: As part of Smart Cities initiatives, national, regional and local governments
all over the globe are under the mandate of being more open regarding how they
share their data. Under this mandate, many of these governments are publishing
data under the umbrella of open government data, which includes measurement
data from city-wide sensor networks. Furthermore, many of these data are
published in so-called data portals as documents that may be spreadsheets,
comma-separated value (CSV) data files, or plain documents in PDF or Word
documents. The sharing of these documents may be a convenient way for the data
provider to convey and publish data but it is not the ideal way for data
consumers to reuse the data. For example, the problems of reusing the data may
range from difficulty opening a document that is provided in any format that is
not plain text, to the actual problem of understanding the meaning of each
piece of knowledge inside of the document. Our proposal tackles those
challenges by identifying metadata that has been regarded to be relevant for
measurement data and providing a schema for this metadata. We further leverage
the Human-Aware Sensor Network Ontology (HASNetO) to build an architecture for
data collected in urban environments. We discuss the use of HASNetO and the
supporting infrastructure to manage both data and metadata in support of the
City of Fortaleza, a large metropolitan area in Brazil.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [178] [An Email Attachment is Worth a Thousand Words, or Is It?](https://arxiv.org/abs/1709.00362)
*Gregory Tsipenyuk,Jon Crowcroft*

Main category: cs.SI

TL;DR: 忽略电子邮件附件会浪费大量数据，附件可以作为连接的替代方法。


<details>
  <summary>Details</summary>
Motivation: 传统上，基于电子邮件存档的社交网络分析侧重于通信日志（发件人、收件人、抄送、密送字段）。然而，这种方法忽略了附件，而附件占电子邮件存档磁盘空间的大部分（平均 80-90%）。作者认为，附件可能更能体现人际关系的“亲密度”或强度。因此，本研究的动机是探索一种利用共享附件作为网络连接的方法，以期比传统方法获得更深入的社交结构洞察。

Method: 提出了一种新的社交网络分析方法，其中将共享附件作为节点之间的连接，而不是传统上使用的通信日志。从 Enron 电子邮件语料库中提取了通信网络和共享附件网络。使用度数、中间中心性、紧密度和特征向量中心性等中心性度量来分析这两个网络。此外，还使用最近邻算法根据共享附件生成了相似性群组，并与公司的组织结构图进行了比较以验证该方法。

Result: 共享附件网络提供了比传统通信网络更深入的社交结构洞察。通过最近邻算法生成的相似性群组与 Enron 员工的组织结构图一致，验证了该方法的有效性。

Conclusion: 通过使用共享附件作为节点之间连接的替代方法，可以获得对电子邮件存档的社交结构的更深入的见解。

Abstract: There is an extensive body of research on Social Network Analysis (SNA) based
on the email archive. The network used in the analysis is generally extracted
either by capturing the email communication in From, To, Cc and Bcc email
header fields or by the entities contained in the email message. In the latter
case, the entities could be, for instance, the bag of words, url's, names,
phones, etc. It could also include the textual content of attachments, for
instance Microsoft Word documents, excel spreadsheets, or Adobe pdfs. The nodes
in this network represent users and entities. The edges represent communication
between users and relations to the entities. We suggest taking a different
approach to the network extraction and use attachments shared between users as
the edges. The motivation for this is two-fold. First, attachments represent
the "intimacy" manifestation of the relation's strength. Second, the
statistical analysis of private email archives that we collected and Enron
email corpus shows that the attachments contribute in average around 80-90% to
the archive's disk-space usage, which means that most of the data is presently
ignored in the SNA of email archives. Consequently, we hypothesize that this
approach might provide more insight into the social structure of the email
archive. We extract the communication and shared attachments networks from
Enron email corpus. We further analyze degree, betweenness, closeness, and
eigenvector centrality measures in both networks and review the differences and
what can be learned from them. We use nearest neighbor algorithm to generate
similarity groups for five Enron employees. The groups are consistent with
Enron's organizational chart, which validates our approach.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [179] [Reducing Aleatoric and Epistemic Uncertainty through Multi-modal Data Acquisition](https://arxiv.org/abs/2501.18268)
*Arthur Hoarau,Benjamin Quost,Sébastien Destercke,Willem Waegeman*

Main category: cs.LG

TL;DR: 本研究提出了一种新的多模态数据采集框架，通过调整样本量和数据模态来分离和减少不确定性，从而做出更优决策。


<details>
  <summary>Details</summary>
Motivation: 为了生成准确可靠的预测，现代人工智能系统需要结合来自文本、图像、音频、电子表格和时间序列等多种模态的数据。然而，多模态数据为分离不确定性带来了新的机遇和挑战，现有的关于认知不确定性和随机不确定性的假设在多模态场景下受到挑战。

Method: 本研究提出了一种新的数据采集框架，结合了主动学习、主动特征采集和不确定性量化等思想，实现了不确定性分离并指导可操作决策。该框架允许在样本量和数据模态两个方向上进行采样。

Result: 通过在两个多模态数据集上进行概念验证，展示了我们的数据采集框架。框架结合了主动学习、主动特征采集和不确定性量化等思想，实现了不确定性分离并指导可操作决策。

Conclusion: 本研究提出了一个创新的数据采集框架，通过在样本量和数据模态两个方向上进行采样，实现不确定性分离并指导可操作决策。实验证明，增加模态数量可以减少不确定性，而增加观测数量可以减少认知不确定性。

Abstract: To generate accurate and reliable predictions, modern AI systems need to
combine data from multiple modalities, such as text, images, audio,
spreadsheets, and time series. Multi-modal data introduces new opportunities
and challenges for disentangling uncertainty: it is commonly assumed in the
machine learning community that epistemic uncertainty can be reduced by
collecting more data, while aleatoric uncertainty is irreducible. However, this
assumption is challenged in modern AI systems when information is obtained from
different modalities. This paper introduces an innovative data acquisition
framework where uncertainty disentanglement leads to actionable decisions,
allowing sampling in two directions: sample size and data modality. The main
hypothesis is that aleatoric uncertainty decreases as the number of modalities
increases, while epistemic uncertainty decreases by collecting more
observations. We provide proof-of-concept implementations on two multi-modal
datasets to showcase our data acquisition framework, which combines ideas from
active learning, active feature acquisition and uncertainty quantification.

</details>


### [180] [Large Scale Transfer Learning for Tabular Data via Language Modeling](https://arxiv.org/abs/2406.12031)
*Josh Gardner,Juan C. Perdomo,Ludwig Schmidt*

Main category: cs.LG

TL;DR: 开发了一个名为 TabuLa-8B 的表格预测语言模型，在表格数据处理方面取得了优于现有 SOTA 模型（如 XGBoost 和 TabPFN）的性能，并且开源了模型、代码和数据。


<details>
  <summary>Details</summary>
Motivation: 旨在缩小在表格数据领域，特别是表格预测任务上，基础模型应用与语言和视觉领域之间存在的差距。

Method: 通过从 TabLib 语料库中提取包含超过 21 亿行和 400 万个唯一表的表格数据，并采用新颖的打包和注意力机制对 Llama 3-8B 模型进行微调，从而构建了 TabuLa-8B 模型。

Result: TabuLa-8B 在 329 个数据集的测试中，零样本准确率比随机猜测高出 15 个百分点，并且在少样本（1-32 个样本）情况下，比经过相同或更多数据训练的 XGBoost 和 TabPFN 模型准确率高出 5-15 个百分点。

Conclusion: TabuLa-8B 在表格预测任务上展现出显著的优势，其零样本准确率比随机猜测高出 15 个百分点，并且在少样本场景下优于 XGBoost 和 TabPFN 等现有模型。

Abstract: Tabular data -- structured, heterogeneous, spreadsheet-style data with rows
and columns -- is widely used in practice across many domains. However, while
recent foundation models have reduced the need for developing task-specific
datasets and predictors in domains such as language modeling and computer
vision, this transfer learning paradigm has not had similar impact in the
tabular domain. In this work, we seek to narrow this gap and present TabuLa-8B,
a language model for tabular prediction. We define a process for extracting a
large, high-quality training dataset from the TabLib corpus, proposing methods
for tabular data filtering and quality control. Using the resulting dataset,
which comprises over 2.1B rows from over 4M unique tables, we fine-tune a Llama
3-8B large language model (LLM) for tabular data prediction (classification and
binned regression) using a novel packing and attention scheme for tabular
prediction. Through evaluation across a test suite of 329 datasets, we find
that TabuLa-8B has zero-shot accuracy on unseen tables that is over 15
percentage points (pp) higher than random guessing, a feat that is not possible
with existing state-of-the-art tabular prediction models (e.g. XGBoost,
TabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the
target datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN
models that are explicitly trained on equal, or even up to 16x more data. We
release our model, code, and data along with the publication of this paper.

</details>


### [181] [Generating tabular datasets under differential privacy](https://arxiv.org/abs/2308.14784)
*Gianluca Truda*

Main category: cs.LG

TL;DR: 该研究提出了一种名为 TableDiffusion 的差分隐私扩散模型，用于合成表格数据，解决了现有 DP GANs 的不足，提高了数据质量和隐私效率。


<details>
  <summary>Details</summary>
Motivation: 当前的生成模型容易记住并重复训练数据，从而破坏了隐私目标。虽然差分隐私（DP）可以解决这个问题，但会在数据质量和隐私之间产生权衡。DP GANs 在 DP 表格数据合成方面存在不稳定的对抗性训练和模式崩溃问题。

Method: 通过利用注意力机制学习可逆表格表示，并引入了第一个用于表格数据合成的差分隐私扩散模型 TableDiffusion。

Result: TableDiffusion 生成更高保真度的合成数据集，避免了模式崩溃问题，并在私有化表格数据合成方面取得了最先进的性能。

Conclusion: DP 扩散模型在合成表格数据方面比 DP GANs 更具数据和隐私效率，实现了最先进的性能。

Abstract: Machine Learning (ML) is accelerating progress across fields and industries,
but relies on accessible and high-quality training data. Some of the most
important datasets are found in biomedical and financial domains in the form of
spreadsheets and relational databases. But this tabular data is often sensitive
in nature. Synthetic data generation offers the potential to unlock sensitive
data, but generative models tend to memorise and regurgitate training data,
which undermines the privacy goal. To remedy this, researchers have
incorporated the mathematical framework of Differential Privacy (DP) into the
training process of deep neural networks. But this creates a trade-off between
the quality and privacy of the resulting data. Generative Adversarial Networks
(GANs) are the dominant paradigm for synthesising tabular data under DP, but
suffer from unstable adversarial training and mode collapse, which are
exacerbated by the privacy constraints and challenging tabular data modality.
This work optimises the quality-privacy trade-off of generative models,
producing higher quality tabular datasets with the same privacy guarantees. We
implement novel end-to-end models that leverage attention mechanisms to learn
reversible tabular representations. We also introduce TableDiffusion, the first
differentially-private diffusion model for tabular data synthesis. Our
experiments show that TableDiffusion produces higher-fidelity synthetic
datasets, avoids the mode collapse problem, and achieves state-of-the-art
performance on privatised tabular data synthesis. By implementing
TableDiffusion to predict the added noise, we enabled it to bypass the
challenges of reconstructing mixed-type tabular data. Overall, the diffusion
paradigm proves vastly more data and privacy efficient than the adversarial
paradigm, due to augmented re-use of each data batch and a smoother iterative
training process.

</details>


### [182] [Smart Metro: Deep Learning Approaches to Forecasting the MRT Line 3 Ridership](https://arxiv.org/abs/2304.07303)
*Jayrald Empino,Jean Allyson Junsay,Mary Grace Verzon,Mideth Abisado,Shekinah Lor Huyo-a,Gabriel Avelino Sampedro*

Main category: cs.LG

TL;DR: This paper analyzes the daily ridership of Metro Rail Transit Line 3 (MRT3) in the Philippines, which experiences fluctuations due to various factors. It proposes a time series prediction method to forecast daily traffic and passenger counts, aiding both commuters in planning and the Department of Transportation (DOTr) in data analysis.


<details>
  <summary>Details</summary>
Motivation: The MRT3's daily ridership fluctuates owing to variables such as holidays, working days, and other unexpected issues, making it challenging for commuters to plan efficient itineraries and for the DOTr to analyze historical data.

Method: Time series prediction

Result: The study aims to anticipate future attendance at a particular station on specific days.

Conclusion: This study presents a time series prediction of daily traffic to anticipate future attendance at a particular station on specific days.

Abstract: Since its establishment in 1999, the Metro Rail Transit Line 3 (MRT3) has
served as a transportation option for numerous passengers in Metro Manila,
Philippines. The Philippine government's transportation department records more
than a thousand people using the MRT3 daily and forecasting the daily passenger
count may be rather challenging. The MRT3's daily ridership fluctuates owing to
variables such as holidays, working days, and other unexpected issues.
Commuters do not know how many other commuters are on their route on a given
day, which may hinder their ability to plan an efficient itinerary. Currently,
the DOTr depends on spreadsheets containing historical data, which might be
challenging to examine. This study presents a time series prediction of daily
traffic to anticipate future attendance at a particular station on specific
days.

</details>


### [183] [Adversarial Networks and Machine Learning for File Classification](https://arxiv.org/abs/2301.11964)
*Ken St. Germain,Josh Angichiodo*

Main category: cs.LG

TL;DR: 该研究提出了一种基于SGAN的文件类型识别方法，即使在文件信息被混淆的情况下也能达到97.6%的准确率，并且在样本量少的情况下表现更优。


<details>
  <summary>Details</summary>
Motivation: 在取证调查中，准确识别文件类型至关重要，因为文件类型可以揭示其包含的内容。然而，有时系统所有者会试图隐藏文件类型或使其难以访问。

Method: 使用半监督生成对抗网络（SGAN）来训练一个机器学习模型，以识别文件的真实类型，即使文件扩展名或文件头被故意混淆。

Result: SGAN在文件类型分类任务中取得了97.6%的准确率，并且在只有少量监督样本的情况下，其表现优于传统的神经网络和其他机器学习算法。

Conclusion: 通过使用SGAN（一种半监督生成对抗网络），可以准确地识别文件类型，即使文件扩展名或文件头被混淆。该模型在11种不同文件类型的分类任务中达到了97.6%的准确率。

Abstract: Correctly identifying the type of file under examination is a critical part
of a forensic investigation. The file type alone suggests the embedded content,
such as a picture, video, manuscript, spreadsheet, etc. In cases where a system
owner might desire to keep their files inaccessible or file type concealed, we
propose using an adversarially-trained machine learning neural network to
determine a file's true type even if the extension or file header is obfuscated
to complicate its discovery. Our semi-supervised generative adversarial network
(SGAN) achieved 97.6% accuracy in classifying files across 11 different types.
We also compared our network against a traditional standalone neural network
and three other machine learning algorithms. The adversarially-trained network
proved to be the most precise file classifier especially in scenarios with few
supervised samples available. Our implementation of a file classifier using an
SGAN is implemented on GitHub (https://ksaintg.github.io/SGAN-File-Classier).

</details>


### [184] [TabularNet: A Neural Network Architecture for Understanding Semantic Structures of Tabular Data](https://arxiv.org/abs/2106.03096)
*Lun Du,Fei Gao,Xu Chen,Ran Jia,Junshan Wang,Jiang Zhang,Shi Han,Dongmei Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为TabularNet的新型神经网络架构，该架构结合了空间和关系信息来理解表格数据，并在实验中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了同时提取表格的空间和关系信息，解决现有研究在挖掘表格数据时忽略单元格之间多样化的关系信息（如层次和并列关系）的问题。

Method: 提出了一种新颖的神经网络架构TabularNet。空间编码器利用行/列池化和双向门控循环单元（Bi-GRU）分别捕获统计信息和局部位置相关性。关系编码器基于WordNet树设计了一种新的图构建方法，并采用图卷积网络（GCN）来关注单元格之间的层次和并列关系。

Result: TabularNet在三个分类任务和两个真实世界电子表格数据集上进行了广泛的实验，其有效性优于现有技术基线。

Conclusion: 所提出的TabularNet在三个分类任务和两个真实世界电子表格数据集上进行了广泛的实验，并且结果证明了其相对于现有技术基线的有效性。

Abstract: Tabular data are ubiquitous for the widespread applications of tables and
hence have attracted the attention of researchers to extract underlying
information. One of the critical problems in mining tabular data is how to
understand their inherent semantic structures automatically. Existing studies
typically adopt Convolutional Neural Network (CNN) to model the spatial
information of tabular structures yet ignore more diverse relational
information between cells, such as the hierarchical and paratactic
relationships. To simultaneously extract spatial and relational information
from tables, we propose a novel neural network architecture, TabularNet. The
spatial encoder of TabularNet utilizes the row/column-level Pooling and the
Bidirectional Gated Recurrent Unit (Bi-GRU) to capture statistical information
and local positional correlation, respectively. For relational information, we
design a new graph construction method based on the WordNet tree and adopt a
Graph Convolutional Network (GCN) based encoder that focuses on the
hierarchical and paratactic relationships between cells. Our neural network
architecture can be a unified neural backbone for different understanding tasks
and utilized in a multitask scenario. We conduct extensive experiments on three
classification tasks with two real-world spreadsheet data sets, and the results
demonstrate the effectiveness of our proposed TabularNet over state-of-the-art
baselines.

</details>


### [185] [Visual Backpropagation](https://arxiv.org/abs/1906.04011)
*Roy S. Freedman*

Main category: cs.LG

TL;DR: Backpropagation is implemented visually in spreadsheets using declarative functional programming, avoiding complex code.


<details>
  <summary>Details</summary>
Motivation: To demonstrate a visual and transparent implementation of backpropagation within spreadsheets using a declarative functional programming specification.

Method: Visual Backpropagation, a method that implements backpropagation within spreadsheets using declarative functional programming, array worksheet formulas, and a sequential computation order. It avoids macros, user-defined functions, loops, and external procedural programs.

Result: A Visual Backpropagation implementation that leverages array worksheet formulas and manual calculation, with a computation order resembling a systolic array. This implementation avoids hidden macros, user-defined functions, loops, and links to conventional procedural programs.

Conclusion: The declarative functional programming specification of backpropagation can be implemented visually and transparently within spreadsheets using array worksheet formulas and a sequential computation order similar to systolic arrays, without relying on macros, user-defined functions, loops, or external procedural programs.

Abstract: We show how a declarative functional programming specification of
backpropagation yields a visual and transparent implementation within
spreadsheets. We call our method Visual Backpropagation. This backpropagation
implementation exploits array worksheet formulas, manual calculation, and has a
sequential order of computation similar to the processing of a systolic array.
The implementation uses no hidden macros nor user-defined functions; there are
no loops, assignment statements, or links to any procedural programs written in
conventional languages. As an illustration, we compare a Visual Backpropagation
solution to a Tensorflow (Python) solution on a standard regression problem.

</details>


### [186] [MOLTE: a Modular Optimal Learning Testing Environment](https://arxiv.org/abs/1709.04553)
*Yingfei Wang,Warren Powell*

Main category: cs.LG

TL;DR: MOLTE 是一个用于贝叶斯排名和选择、随机多臂老虎机和顺序实验设计问题的 Matlab 模拟器，可促进对学习算法的经验测试。


<details>
  <summary>Details</summary>
Motivation: 为了解决学习算法（任何类型）的经验测试相对缺乏的问题。

Method: 介绍了一个名为 MOLTE（Modular Optimal Learning Testing Environment）的 Matlab 基础模拟器，该模拟器允许在各种问题（如贝叶斯排名和选择、随机多臂老虎机或顺序实验设计）的背景下比较多种学习策略。MOLTE 具有模块化设计，易于添加新算法和测试问题，并支持并行计算。

Result: MOLTE 允许对学习策略进行全面的测试，并包含一个入门级的测试问题库，用于演示其功能。此外，它还解决了最优学习文献中被忽视的调整和构建先验的问题。

Conclusion: MOLTE 提供了一个易于使用的工具，可促进对学习算法进行更全面的测试，并解决了最优学习文献中被忽视的先验问题。

Abstract: We address the relative paucity of empirical testing of learning algorithms
(of any type) by introducing a new public-domain, Modular, Optimal Learning
Testing Environment (MOLTE) for Bayesian ranking and selection problem,
stochastic bandits or sequential experimental design problems. The Matlab-based
simulator allows the comparison of a number of learning policies (represented
as a series of .m modules) in the context of a wide range of problems (each
represented in its own .m module) which makes it easy to add new algorithms and
new test problems. State-of-the-art policies and various problem classes are
provided in the package. The choice of problems and policies is guided through
a spreadsheet-based interface. Different graphical metrics are included. MOLTE
is designed to be compatible with parallel computing to scale up from local
desktop to clusters and clouds. We offer MOLTE as an easy-to-use tool for the
research community that will make it possible to perform much more
comprehensive testing, spanning a broader selection of algorithms and test
problems. We demonstrate the capabilities of MOLTE through a series of
comparisons of policies on a starter library of test problems. We also address
the problem of tuning and constructing priors that have been largely overlooked
in optimal learning literature. We envision MOLTE as a modest spur to provide
researchers an easy environment to study interesting questions involved in
optimal learning.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [187] [Pivoting the paradigm: the role of spreadsheets in K-12 data science](https://arxiv.org/abs/2506.03232)
*Oren Tirschwell,Nicholas Jon Horton*

Main category: stat.OT

TL;DR: 电子表格是K-12教育中用于数据收集、组织、可视化和交互的宝贵工具，有助于培养学生的数据和计算技能。本文回顾了相关框架，提出了学习成果，并讨论了推广策略和面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 电子表格工具在K-12学生和教师中广泛普及，在数据收集、组织、可视化和交互方面发挥着重要作用，有助于培养学生的数据和计算技能。

Method: 1. 回顾了K-12数据工具的现有框架；2. 提出了将电子表格纳入课程可以实现的数据驱动学习成果；3. 讨论了电子表格如何帮助培养数据敏锐度和计算能力。

Result: 提供了课堂活动示例，指出了推广中面临的挑战和障碍，提出了旨在降低师生学习门槛的教学方法，并强调了促进电子表格在数据科学和STEM学科中应用的专业发展需求。

Conclusion: 电子表格通过提供数据可视化和交互功能，可以帮助培养K-12学生的计算能力和数据素养。

Abstract: Spreadsheet tools are widely accessible to and commonly used by K-12 students
and teachers. They have an important role in data collection and organization.
Beyond data organization, spreadsheets also make data visible and easy to
interact with, facilitating student engagement in data exploration and
analysis. Though not suitable for all circumstances, spreadsheets can and do
help foster data and computing skills for K-12 students. This paper 1) reviews
prior frameworks on K-12 data tools; 2) proposes data-driven learning outcomes
that can be accomplished by incorporating spreadsheets into the curriculum; and
3) discusses how spreadsheets can help develop data acumen and computational
fluency. We provide example class activities, identify challenges and barriers
to adoption, suggest pedagogical approaches to ease the learning curve for
instructors and students, and discuss the need for professional development to
facilitate deeper use of spreadsheets for data science and STEM disciplines.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [188] [Billion-files File Systems (BfFS): A Comparison](https://arxiv.org/abs/2408.01805)
*Sohail Shaikh*

Main category: cs.PF

TL;DR: Analyzed EXT4, XFS, BtrFS, ZFS, and F2FS by creating, storing, and reading one billion files to understand their performance and limitations, focusing on throughput, storage usage, and degradation. Investigated side effects of handling numerous files and folders on filesystem performance.


<details>
  <summary>Details</summary>
Motivation: The increasing volume and need for quick processing of data necessitate data availability close to compute devices to reduce transfer latency, leading to a need to understand the workings, performance, and limitations of local filesystems.

Method: Analyzed popular Linux filesystems (EXT4, XFS, BtrFS, ZFS, F2FS) by creating, storing, and reading back one billion files. Captured and analyzed metrics like read/write throughput, storage blocks usage, disk space utilization, and overheads. Explored side effects like performance degradation during and after large file/folder creation.

Result: The study captured and analyzed various performance metrics and explored the side effects of handling a large number of files and folders on filesystem performance.

Conclusion: EXT4, XFS, BtrFS, ZFS, and F2FS are analyzed for their performance and limitations when handling one billion files, with a focus on read/write throughput, storage usage, and performance degradation.

Abstract: As the volume of data being produced is increasing at an exponential rate
that needs to be processed quickly, it is reasonable that the data needs to be
available very close to the compute devices to reduce transfer latency. Due to
this need, local filesystems are getting close attention to understand their
inner workings, performance, and more importantly their limitations. This study
analyzes few popular Linux filesystems: EXT4, XFS, BtrFS, ZFS, and F2FS by
creating, storing, and then reading back one billion files from the local
filesystem. The study also captured and analyzed read/write throughput, storage
blocks usage, disk space utilization and overheads, and other metrics useful
for system designers and integrators. Furthermore, the study explored other
side effects such as filesystem performance degradation during and after these
large numbers of files and folders are created.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [189] [Hillview: A trillion-cell spreadsheet for big data](https://arxiv.org/abs/1907.04827)
*Mihai Budiu,Parikshit Gopalan,Lalith Suresh,Udi Wieder,Han Kruiger,Marcos K. Aguilera*

Main category: cs.DC

TL;DR: Hillview 是一个分布式电子表格，通过 vizketches 技术高效处理大型数据集，实现快速数据探索。


<details>
  <summary>Details</summary>
Motivation: 为了使电子表格能够处理无法由单台计算机处理的大型数据集，并提供高交互性以快速探索信息。

Method: Hillview 使用 vizketches（一种数据摘要的算法技术与计算机图形学原理相结合）来生成紧凑的数据可视化，从而提供高交互性。

Result: Hillview 运行在八台服务器上，能够导航和可视化包含数千亿行和数万亿个单元格的数据集，其性能超越了同类系统。

Conclusion: Hillview 扩展了电子表格的功能，使其能够处理无法由单台计算机处理的大型分布式数据集。通过使用 vizketches（一种数据摘要的算法技术与计算机图形学原理相结合），Hillview 实现了高交互性，允许数据分析师快速探索信息。Vizketches 通过并行计算、减少通信、提供渐进式可视化和保证精度，有效地扩展了电子表格。

Abstract: Hillview is a distributed spreadsheet for browsing very large datasets that
cannot be handled by a single machine. As a spreadsheet, Hillview provides a
high degree of interactivity that permits data analysts to explore information
quickly along many dimensions while switching visualizations on a whim. To
provide the required responsiveness, Hillview introduces visualization
sketches, or vizketches, as a simple idea to produce compact data
visualizations. Vizketches combine algorithmic techniques for data
summarization with computer graphics principles for efficient rendering. While
simple, vizketches are effective at scaling the spreadsheet by parallelizing
computation, reducing communication, providing progressive visualizations, and
offering precise accuracy guarantees. Using Hillview running on eight servers,
we can navigate and visualize datasets of tens of billions of rows and
trillions of cells, much beyond the published capabilities of competing
systems.

</details>


### [190] [Streaming 1.9 Billion Hypersparse Network Updates per Second with D4M](https://arxiv.org/abs/1907.04217)
*Jeremy Kepner,Vijay Gadepally,Lauren Milechin,Siddharth Samsi,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Michael Houle,Michael Jones,Anne Klein,Peter Michaleas,Julie Mullen,Andrew Prout,Antonio Rosa,Charles Yee,Albert Reuther*

Main category: cs.DC

TL;DR: D4M 库通过引入分层关联数组，优化了内存使用和更新速度，使其能够高效处理大规模网络数据。


<details>
  <summary>Details</summary>
Motivation: D4M 库在处理网络数据时，其流式更新对内存层次结构造成巨大压力，需要一种能降低内存压力并大幅提高更新速率的解决方案。

Method: 通过实现分层关联数组来优化 D4M，该数组通过控制级联更新前每个层中的条目数来管理内存压力，并可针对不同应用进行调整。

Result: 优化的分层关联数组在单实例中实现了超过 40,000 次/秒的更新速率。在 MIT SuperCloud 的 1,100 个服务器节点上扩展到 34,000 个分层 D4M 关联数组实例，实现了 1,900,000,000 次/秒的持续更新速率，能够分析超大规模流式网络数据。

Conclusion: D4M 依赖于结合了电子表格、数据库、矩阵、图形和网络特性的关联数组，并提供严格的数学保证。通过优化分层关联数组的实现，显著减少了内存压力并提高了更新速率。

Abstract: The Dynamic Distributed Dimensional Data Model (D4M) library implements
associative arrays in a variety of languages (Python, Julia, and Matlab/Octave)
and provides a lightweight in-memory database implementation of hypersparse
arrays that are ideal for analyzing many types of network data. D4M relies on
associative arrays which combine properties of spreadsheets, databases,
matrices, graphs, and networks, while providing rigorous mathematical
guarantees, such as linearity. Streaming updates of D4M associative arrays put
enormous pressure on the memory hierarchy. This work describes the design and
performance optimization of an implementation of hierarchical associative
arrays that reduces memory pressure and dramatically increases the update rate
into an associative array. The parameters of hierarchical associative arrays
rely on controlling the number of entries in each level in the hierarchy before
an update is cascaded. The parameters are easily tunable to achieve optimal
performance for a variety of applications. Hierarchical arrays achieve over
40,000 updates per second in a single instance. Scaling to 34,000 instances of
hierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud
achieved a sustained update rate of 1,900,000,000 updates per second. This
capability allows the MIT SuperCloud to analyze extremely large streaming
network data sets.

</details>


<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [191] [Real-time stock analysis for blending recipes in industrial plants](https://arxiv.org/abs/1909.02960)
*Florin Zamfir,Nicolae Paraschiv,Emil Pricop*

Main category: cs.OH

TL;DR: 该论文开发了一个实时的库存分析应用程序，以解决 Excel 电子表格在库存管理和流程数据计算方面的缺点。该应用程序使用 C# 编写，并集成了规划算法，可以帮助操作员做出决策，确定所需成分、成品数量和最佳成品数量。


<details>
  <summary>Details</summary>
Motivation: 许多公司使用 Excel 电子表格来维护库存记录和计算特定于流程的数据，但这些电子表格难以理解和跟踪，而且容易被意外更改或删除公式。因此，有必要开发一个能够以集中的形式提供这些数据并协助操作员进行决策的应用程序。

Method: 开发了一个实时的库存分析应用程序，并集成了规划算法，用于确定实现所需数量的混合过程所需的成分、使用现有成分可以生产的成品数量以及最佳成品数量。

Result: 该应用程序可以帮助操作员做出决策，因为它集成了规划算法，可以确定实现混合过程所需的成分、使用现有成分可以生产的成品数量以及最佳成品数量。

Conclusion: 该应用使用 C# 编写，并集成了规划算法，可以帮助操作员做出决策，该应用可以分步构建结果。

Abstract: Many companies use Excel spreadsheets to keep stock records and to calculate
process-specific data. These spreadsheets are often hard to understand and
track. And if the user does not protect them, there is a risk that the user
randomly changes or erase formulas. The paper focuses on the stocks of products
used in a blending process with a known recipe. Developing an application that
can bring this data in a centralized form and that can assist the operator in
decide is a necessity. When a programmer implements an application that uses
data from plants he needs to consider one fundamental aspect as reading
real-time data from the process. The real-time stock analysis application takes
into account all the above elements. The application is easy to use by an
operator in the command room of installation because of the planning algorithms
integrated into it. The algorithms proposed and implemented in this paper have
well-defined goals: identifying the ingredients needed to achieve the blending
process for required quantities, determine the quantities of the finished
product that can be made with the existing ingredients and determine the
optimum quantities of the finished product. The application implemented in C#
intensively uses these algorithms and gives the user the ability to build the
result step by step.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [192] [Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!](https://arxiv.org/abs/2309.02110)
*James P. Dilger*

Main category: math.HO

TL;DR: 通过分析 Wordle 玩家的起始词数据，研究发现存在玩家作弊、玩家对起始词的偏好以及玩家受外部因素影响的情况，并提供了量化证据。


<details>
  <summary>Details</summary>
Motivation: Wordle 游戏在全球范围内拥有大量玩家，对其玩家行为进行量化分析具有重要意义，特别是关于作弊、玩家偏好和外部影响的研究。

Method: 通过收集和分析 2023 年 5 月至 8 月的 Wordle 玩家起始猜测词数据，并运用信息论评估玩家的运气和技巧。

Result: A) 约 0.2-0.5% 的玩家能在一次性猜中每日目标词，远高于随机猜测的概率 (0.043%)，表明存在大量玩家通过非正常途径获取答案。B) 至少有 1/3 的玩家有固定的起始词，并且即使在目标词出现后，玩家似乎仍倾向于使用其偏爱的起始词。C) 在 2023 年 8 月 15 日，约有 30,000 名玩家突然更换了起始词，这可能与填字游戏的线索有关，表明 Wordle 玩家易受影响。

Conclusion: 该研究通过分析 Wordle 玩家的起始猜测词数据，发现了玩家行为的几个有趣模式，包括作弊行为、对起始词的偏好以及受外部因素影响的可能性，并提供了量化证据。

Abstract: Wordle is a popular, online word game offered by the New York Times
(nytimes.com). Currently there are some 2 million players of the English
version worldwide. Players have 6 attempts to guess the daily word (target
word) and after each attempt, the player receives color-coded information about
the correctness and position of each letter in the guess. After either a
successful completion of the puzzle or the final unsuccessful attempt, software
can assess the player's luck and skill using Information Theory and can display
data for the first, second, ..., sixth guesses of a random sample of all
players. Recently, I discovered that the latter data is presented in a format
that can easily be copied and pasted into a spreadsheet. I compiled data on
Wordle players' first guesses from May 2023 - August 2023 and inferred some
interesting information about Wordle players. A) Every day, about 0.2-0.5% of
players solve the puzzle in one attempt. Because the odds of guessing the one
of 2,315 possible target words at random is 0.043%, this implies that 4,000 -
10,000 players cheat by obtaining the target word outside of playing the game!
B) At least 1/3 of the players have a favorite starting word, or cycle through
several. And even though players should be aware that target words are never
repeated, most players appear to remain loyal to their starting word even after
its appearance as a target word. C) On August 15, 2023, about 30,000 players
abruptly changed their starting word, presumably based on a crossword puzzle
clue! Wordle players can be influenced! This study goes beyond social media
postings, surveys, and Google Trends to provide solid, quantitative evidence
about cheating in Wordle.

</details>


### [193] [GeoGebra e situações que envolvem modelação numa abordagem STEAM](https://arxiv.org/abs/1907.02099)
*J. M. D. S. Dos Santos,A. P. Silveira,A. E. S. Trocado*

Main category: math.HO

TL;DR: 本研究为lusophone地区数学教学引入GeoGebra软件，支持STEAM教育。通过易于上手的任务，教师和学生可以学习几何建模与分析，促进跨学科联系，并计划材料将翻译成西班牙语和英语。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于推动lusophone地区数学教育的STEAM化，特别是通过引入互动数学软件GeoGebra来增强教学效果。研究者希望为教师提供易于使用的工具和方法，以应对二维和三维几何问题的教学挑战，并促进数学与科学、艺术的跨学科联系。通过GeoGebra的应用，旨在提升学生对数学概念的理解和应用能力，并培养其解决实际问题的能力。

Method: 论文提出了一系列使用GeoGebra软件的任务，用于在lusophone地区的数学教学中实施STEAM教育方法。这些任务首先面向教师进行培训，随后将进行调整后应用于学生。任务内容涵盖二维和三维几何问题的建模与分析，利用GeoGebra的2D、3D窗口、CAS窗口、电子表格以及额外的二维窗口来研究截面和平面的性质。任务设计考虑了不同用户的GeoGebra熟练程度，并提供脚本指导用户使用相关工具和命令。

Result: 该论文介绍了为lusophone地区数学教学设计的GeoGebra应用任务。这些任务能够支持STEAM教育方法，促进跨学科学习，并帮助用户（教师和学生）掌握GeoGebra软件在几何建模与分析中的应用。该项目已通过对葡萄牙语国家GeoGebra培训师的培训课程产生影响，并计划将材料扩展到西班牙语和英语，以期在全球范围内推广。

Conclusion: 该论文旨在为lusophone地区数学课程中的STEAM教育方法提供技术支持，具体是利用GeoGebra软件。论文提出的任务旨在帮助教师掌握GeoGebra的使用，并将在后续阶段应用于学生。这些任务涵盖了二维和三维几何问题的建模与分析，并利用GeoGebra的多种窗口功能（包括2D、3D、CAS、电子表格和附加二维窗口）来研究截面和平面的性质。论文强调，这些任务设计易于上手，即使是GeoGebra新手也能轻松 অনুসরণ。此外，这些任务还促进了数学与其他科学及艺术领域的联系，并支持学生通过项目巩固数学知识。该项目源于2019年开始的葡萄牙语国家GeoGebra培训师培训课程，得到了伊比利亚美洲教育、科学和文化组织（OEI）的大力支持，并计划将课程材料改编成西班牙语和英语，以满足更广泛用户的需求。

Abstract: In order to implement a STEAM approach including the use of technology,
namely the use of interactive mathematics software GeoGebra, in mathematics
classes, in the lusophone space, the materials presented here were conceived,
to be implemented in a first phase among teachers. Later, with the necessary
adaptations, these tasks will be applied to the students. The tasks deal with
modeling situations, in two- and three-dimensional geometric problems, in order
to apply GeoGebra software in its analysis to illustrate its capabilities. The
different windows of this software are used, namely the 2D and 3D windows, CAS
window, spreadsheet and extra two dimensional windows in order to study cutting
planes in solids and some surfaces. The tasks are presented so that any user,
regardless of the degree of knowledge they have of the software, can follow
them, being supported in scripts with some indications of the tools and
commands to use. Designed for the teaching and learning of Mathematics, from a
STEAM approach, these tasks allow connections with other Sciences and the Arts,
and allow the development of projects using and consolidating relevant
mathematical contents. These tasks are part of the proposals of activities of
the participants of the Training Courses for Trainers in GeoGebra for
Portuguese Speaking Countries, which from 2019 have an impact on the STEAM
approach. These courses are carried out with the high sponsorship of the
Organization of Ibero-American States for Education, Science and Culture (OEI).
Given the interest that the tasks have for the users of the Iberian space, as
well as their dissemination at a global level, the materials initially
developed in Portuguese language will be adapted for Spanish and English
speakers.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [194] [LEI2JSON: Schema-based Validation and Conversion of Livestock Event Information](https://arxiv.org/abs/2310.17414)
*Mahir Habib,Muhammad Ashad Kabir,Lihong Zheng*

Main category: eess.SY

TL;DR: LEI2JSON是一个Google表格插件，可将家畜事件数据标准化为JSON格式。


<details>
  <summary>Details</summary>
Motivation: 为家畜生产者提供一种有效的机制来标准化他们的事件数据，从而节省时间和资源。

Method: LEI2JSON通过创建包含适当列标题、注释和验证规则的电子表格模板，将电子表格数据转换为JSON格式，并根据LEI模式验证输出来实现其目标。

Result: LEI2JSON可以促进将家畜事件信息无缝地以JSON格式存储在本地或Google云端硬盘中。该工具的有效性已通过广泛的实验评估得到证实。

Conclusion: LEI2JSON是一个用于Google表格的插件，可以帮助家畜生产者标准化他们的事件数据，并将其转换为JSON格式。

Abstract: Livestock producers often need help in standardising (i.e., converting and
validating) their livestock event data. This article introduces a novel
solution, LEI2JSON (Livestock Event Information To JSON). The tool is an add-on
for Google Sheets, adhering to the livestock event information (LEI) schema.
The core objective of LEI2JSON is to provide livestock producers with an
efficient mechanism to standardise their data, leading to substantial savings
in time and resources. This is achieved by building the spreadsheet template
with the appropriate column headers, notes, and validation rules, converting
the spreadsheet data into JSON format, and validating the output against the
schema. LEI2JSON facilitates the seamless storage of livestock event
information locally or on Google Drive in JSON. Additionally, we have conducted
an extensive experimental evaluation to assess the effectiveness of the tool.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [195] [A CNN toolbox for skin cancer classification](https://arxiv.org/abs/1908.08187)
*Fabrizio Nunnari,Daniel Sonntag*

Main category: eess.IV

TL;DR: 一个用于皮肤癌分类的深度学习软件工具箱，可让技术和非技术用户轻松配置CNN模型，并已初步评估了图像增强、分辨率和重缩放对黑色素瘤检测性能的影响。


<details>
  <summary>Details</summary>
Motivation: 该研究的主要动机是简化皮肤癌分类领域中深度神经网络的配置过程。通过提供一个灵活且用户友好的软件工具箱，旨在让开发者能够快速构建和调整CNN模型，同时使非技术用户也能参与到模型配置的探索中。

Method: 该研究描述了一个用于配置深度神经网络（特别是在皮肤癌分类领域）的软件工具箱。该工具箱的特点是其软件架构，支持快速设置新的卷积神经网络（CNN）架构和超参数配置。此外，它还包括一个用户界面，该界面可以像电子表格一样进行管理，使得非技术用户也能轻松地探索和更改配置设置，以适应不同的数据集。

Result: 研究初步评估了该工具箱在黑色素瘤检测方面的性能。通过在皮肤镜图像上使用两个CNN模型，研究量化了图像增强、图像分辨率和重缩放滤波器等因素对整体检测性能和训练时间的影响。

Conclusion: 该软件工具箱通过允许开发者快速设置新的卷积神经网络（CNN）架构和超参数配置，并提供简单的电子表格用户界面，使非技术用户能够探索不同的配置设置，从而简化了皮肤癌分类领域深度神经网络的配置。

Abstract: We describe a software toolbox for the configuration of deep neural networks
in the domain of skin cancer classification. The implemented software
architecture allows developers to quickly set up new convolutional neural
network (CNN) architectures and hyper-parameter configurations. At the same
time, the user interface, manageable as a simple spreadsheet, allows
non-technical users to explore different configuration settings that need to be
explored when switching to different data sets. In future versions, meta
leaning frameworks can be added, or AutoML systems that continuously improve
over time. Preliminary results, conducted with two CNNs in the context melanoma
detection on dermoscopic images, quantify the impact of image augmentation,
image resolution, and rescaling filter on the overall detection performance and
training time.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [196] [Data Encryption Battlefield: A Deep Dive into the Dynamic Confrontations in Ransomware Attacks](https://arxiv.org/abs/2504.20681)
*Arash Mahboubi,Hamed Aboutorab,Seyit Camtepe,Hang Thanh Bui,Khanh Luong,Keyvan Ansari,Shenlu Wang,Bazara Barry*

Main category: cs.CR

TL;DR: 本研究评估了机器学习算法在检测复杂勒索软件加密技术方面的能力，发现Hoeffding Tree和Random Forest在不同类型的加密检测中各有优势。


<details>
  <summary>Details</summary>
Motivation: 为了应对日益复杂的勒索软件加密技术（如Base64编码和部分加密），本研究旨在探索机器学习算法在检测这些不断变化的威胁方面的有效性。

Method: 本研究利用在线增量机器学习算法来预测文件加密活动，并针对75种不同的勒索软件家族进行测试。分析的数据集包含32.6 GB，涵盖多种文件格式。

Result: Hoeffding Tree算法在处理传统和AES-Base64加密方面表现出优越的增量学习能力，而Random Forest分类器在检测间歇性加密方面效果显著。

Conclusion: Hoeffding Tree算法在检测传统和AES-Base64加密方面表现出色，而具有暖启动功能的Random Forest分类器则在识别间歇性加密方面表现突出。这表明需要量身定制的机器学习解决方案来应对复杂的勒索软件策略。

Abstract: In the rapidly evolving landscape of cybersecurity threats, ransomware
represents a significant challenge. Attackers increasingly employ sophisticated
encryption methods, such as entropy reduction through Base64 encoding, and
partial or intermittent encryption to evade traditional detection methods. This
study explores the dynamic battle between adversaries who continuously refine
encryption strategies and defenders developing advanced countermeasures to
protect vulnerable data. We investigate the application of online incremental
machine learning algorithms designed to predict file encryption activities
despite adversaries evolving obfuscation techniques. Our analysis utilizes an
extensive dataset of 32.6 GB, comprising 11,928 files across multiple formats,
including Microsoft Word documents (doc), PowerPoint presentations (ppt), Excel
spreadsheets (xlsx), image formats (jpg, jpeg, png, tif, gif), PDFs (pdf),
audio (mp3), and video (mp4) files. These files were encrypted by 75 distinct
ransomware families, facilitating a robust empirical evaluation of machine
learning classifiers effectiveness against diverse encryption tactics. Results
highlight the Hoeffding Tree algorithms superior incremental learning
capability, particularly effective in detecting traditional and AES-Base64
encryption methods employed to lower entropy. Conversely, the Random Forest
classifier with warm-start functionality excels at identifying intermittent
encryption methods, demonstrating the necessity of tailored machine learning
solutions to counter sophisticated ransomware strategies.

</details>


### [197] [JUBILEE: Secure Debt Relief and Forgiveness](https://arxiv.org/abs/2109.07267)
*David Cerezo Sánchez*

Main category: cs.CR

TL;DR: JUBILEE是一种创新的安全计算机制，用于无摩擦地进行债务减免和豁免，无需第三方，激励信息披露，提高债务结算效率和成功率。


<details>
  <summary>Details</summary>
Motivation: JUBILEE旨在实现一个安全、无摩擦的债务减免和豁免机制，激励各方如实披露私人信息，以达成更和谐的债务结算。

Method: JUBILEE通过引入安全计算技术到债务减免中，首次实现了“债务人恩惠”，与之前所有方法相比，在以下方面有所改进：个体理性、激励相容、真实/策略证明、事后有效、私人信息债务减免和豁免的最优机制。

Result: JUBILEE实现了“债务人恩惠”，即债务结算具有比未使用安全计算更高期望利润和更高成功率。

Conclusion: JUBILEE是一个安全计算的机制，用于以无摩擦的方式进行债务减免和豁免，无需可信第三方，通过激励各方如实披露其私人信息，从而实现更和谐的债务结算。

Abstract: JUBILEE is a securely computed mechanism for debt relief and forgiveness in a
frictionless manner without involving trusted third parties, leading to more
harmonious debt settlements by incentivising the parties to truthfully reveal
their private information. JUBILEE improves over all previous methods:
  - individually rational, incentive-compatible, truthful/strategy-proof,
ex-post efficient, optimal mechanism for debt relief and forgiveness with
private information
  - by the novel introduction of secure computation techniques to debt relief,
the "blessing of the debtor" is hereby granted for the first time: debt
settlements with higher expected profits and a higher probability of success
than without using secure computation
  A simple and practical implementation is included for "The Secure
Spreadsheet". Another implementation is realised using Raziel smart contracts
on a blockchain with Pravuil consensus.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [198] [Using the Abstract Computer Architecture Description Language to Model AI Hardware Accelerators](https://arxiv.org/abs/2402.00069)
*Mika Markus Müller,Alexander Richard Manfred Borst,Konstantin Lübeck,Alexander Louis-Ferdinand Jung,Oliver Bringmann*

Main category: cs.AR

TL;DR: AI hardware accelerator selection is complex. This paper shows how to use ACADL to model accelerators, map DNNs, and simulate performance for better decision-making.


<details>
  <summary>Details</summary>
Motivation: The proliferation of Deep Neural Networks (DNNs) has created a need for specialized hardware accelerators. However, selecting and configuring these accelerators is challenging for manufacturers due to the complexity of comparing different design alternatives using traditional methods like datasheets or slow simulators.

Method: The paper utilizes the Abstract Computer Architecture Description Language (ACADL) to model AI hardware accelerators and map Deep Neural Networks (DNNs) onto them. Timing simulation semantics are explained to collect performance data.

Result: The paper explains how ACADL can be used to model AI hardware accelerators, map DNNs onto them, and gather performance results through timing simulations, offering a more efficient way to understand performance characteristics.

Conclusion: This paper demonstrates how to use ACADL to model AI hardware accelerators, map DNNs onto them, and gather performance results through timing simulations.

Abstract: Artificial Intelligence (AI) has witnessed remarkable growth, particularly
through the proliferation of Deep Neural Networks (DNNs). These powerful models
drive technological advancements across various domains. However, to harness
their potential in real-world applications, specialized hardware accelerators
are essential. This demand has sparked a market for parameterizable AI hardware
accelerators offered by different vendors.
  Manufacturers of AI-integrated products face a critical challenge: selecting
an accelerator that aligns with their product's performance requirements. The
decision involves choosing the right hardware and configuring a suitable set of
parameters. However, comparing different accelerator design alternatives
remains a complex task. Often, engineers rely on data sheets, spreadsheet
calculations, or slow black-box simulators, which only offer a coarse
understanding of the performance characteristics.
  The Abstract Computer Architecture Description Language (ACADL) is a concise
formalization of computer architecture block diagrams, which helps to
communicate computer architecture on different abstraction levels and allows
for inferring performance characteristics. In this paper, we demonstrate how to
use the ACADL to model AI hardware accelerators, use their ACADL description to
map DNNs onto them, and explain the timing simulation semantics to gather
performance results.

</details>


### [199] [Online Model Swapping in Architectural Simulation](https://arxiv.org/abs/2012.01571)
*Patrick Lavin,Jeffrey Young,Rich Vuduc,Jonathan Beard*

Main category: cs.AR

TL;DR: 提出了一种通过用简单统计模型替换详细模型来加速仿真的方法，在SVE-Cachesim中实现，将L1D缓存的仿真误差控制在8%以内，同时显著减少了计算量。


<details>
  <summary>Details</summary>
Motivation: 随着系统和应用的复杂性增加，详细仿真耗时过长，迫使架构师在需要快速迭代设计时使用电子表格等简单模型。然而，从简单仿真迁移到详细仿真通常需要多次执行来确定简单模型的有效性，这可能比直接运行详细模型更昂贵。此外，架构师常常需要依赖直觉来选择简单模型，这进一步增加了问题的复杂性。

Method: 通过在线监控仿真行为，自动用更简单的统计模型替换详细模型。

Result: 在SVE-Cachesim开源模拟器中实现了该方法，用于替换内存层次结构中的一级数据缓存（L1D）。实验证明，该技术能够处理非平凡的用例，不仅可以近似本地时不变统计量，还可以近似随时间变化的统计量（如L1D作为一种时间序列函数），以及下游的副作用（如L1D过滤L2缓存的访问）。在近似缓存模型占模拟90%以上的情况下，仿真周期数误差仅为8%，且计算量减少了2到8倍。

Conclusion: 该研究提出了一种在线监控仿真行为并自动用更简单的统计模型替换详细模型的方法，以缩小简单和详细仿真之间的差距。

Abstract: As systems and applications grow more complex, detailed simulation takes an
ever increasing amount of time. The prospect of increased simulation time
resulting in slower design iteration forces architects to use simpler models,
such as spreadsheets, when they want to iterate quickly on a design. However,
the task of migrating from a simple simulation to one with more detail often
requires multiple executions to find where simple models could be effective,
which could be more expensive than running the detailed model in the first
place. Also, architects must often rely on intuition to choose these simpler
models, further complicating the problem.
  In this work, we present a method of bridging the gap between simple and
detailed simulation by monitoring simulation behavior online and automatically
swapping out detailed models with simpler statistical approximations. We
demonstrate the potential of our methodology by implementing it in the
open-source simulator SVE-Cachesim to swap out the level one data cache (L1D)
within a memory hierarchy. This proof of concept demonstrates that our
technique can handle a non-trivial use-case in not just approximation of local
time-invariant statistics, but also those that vary with time (e.g., the L1D is
a form of a time-series function), and downstream side-effects (e.g., the L1D
filters accesses for the level two cache). Our simulation swaps out the
built-in cache model with only an 8% error in the simulated cycle count while
using the approximated cache models for over 90% of the simulation, and our
simpler models require two to eight times less computation per "execution" of
the model

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [200] [Mathematics of Digital Hyperspace](https://arxiv.org/abs/2103.15203)
*Jeremy Kepner,Timothy Davis,Vijay Gadepally,Hayden Jananthan,Lauren Milechin*

Main category: cs.MS

TL;DR: 本文提出了一种名为“半链”的数学概念，并论证了GraphBLAS通过支持该概念以及其他相关数学工具，可以成为处理非结构化数据的更强大工具，并有望取代电子表格和数据库等传统系统。


<details>
  <summary>Details</summary>
Motivation: 数字高维空间的数据通常是非结构化的，并且以连续流的形式存在，这给传统的数据处理方式带来了挑战。需要一种新的数学方法来有效地表示、遍历和转换这些数据。

Method: 本文探索了一个新的数学概念——半链（semilink），它结合了半环（semirings）的对，为图分析、数据库操作和机器学习提供基本操作。

Result: 通过添加基于键的索引（如指向字符串的指针）和半链，GraphBLAS可以增强其作为关联数组代数的功能，并作为现有数据处理工具的替代品。

Conclusion: GraphBLAS通过支持超图、超稀疏矩阵以及半链所需的数学，并且无缝执行图、网络和矩阵操作，可以成为一个更丰富的关联数组代数，并能作为电子表格、数据库表和数据中心操作系统的即插即用替代品，从而增强对数字高维空间中非结构化数据的导航。

Abstract: Social media, e-commerce, streaming video, e-mail, cloud documents, web
pages, traffic flows, and network packets fill vast digital lakes, rivers, and
oceans that we each navigate daily. This digital hyperspace is an amorphous
flow of data supported by continuous streams that stretch standard concepts of
type and dimension. The unstructured data of digital hyperspace can be
elegantly represented, traversed, and transformed via the mathematics of
hypergraphs, hypersparse matrices, and associative array algebra. This paper
explores a novel mathematical concept, the semilink, that combines pairs of
semirings to provide the essential operations for graph analytics, database
operations, and machine learning. The GraphBLAS standard currently supports
hypergraphs, hypersparse matrices, the mathematics required for semilinks, and
seamlessly performs graph, network, and matrix operations. With the addition of
key based indices (such as pointers to strings) and semilinks, GraphBLAS can
become a richer associative array algebra and be a plug-in replacement for
spreadsheets, database tables, and data centric operating systems, enhancing
the navigation of unstructured data found in digital hyperspace.

</details>
