<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.HC](#cs.HC) [Total: 5]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.SE](#cs.SE) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Vision Language Models for Spreadsheet Understanding: Challenges and Opportunities](https://arxiv.org/abs/2405.16234)
*Shiyu Xia,Junyu Xiong,Haoyu Dong,Jianbo Zhao,Yuzhang Tian,Mengyu Zhou,Yeye He,Shi Han,Dongmei Zhang*

Main category: cs.CV

TL;DR: 该论文探讨了视觉语言模型在电子表格理解方面的能力，并提出了三个自监督挑战来评估VLMs在OCR、空间感知和视觉格式识别方面的能力。


<details>
  <summary>Details</summary>
Motivation: 评估视觉语言模型在电子表格理解方面的能力，特别是在光学字符识别（OCR）、空间感知和视觉格式识别方面的能力。

Method: 提出了三个自监督挑战，并设计了相应的评估指标。此外，还利用电子表格表格检测任务来评估VLMs的整体性能，并提出了三种电子表格到图像的设置：列宽调整、样式更改和地址增强。提出了不同的提示来解决上述任务。

Result: VLMs在OCR方面表现出良好的能力，但由于单元格遗漏和未对准，产生的结果并不令人满意，并且在空间和格式识别技能方面明显不足。

Conclusion: 未来的工作应该集中在利用论文中提出的方法来增强VLMs的电子表格数据理解能力，生成各种设置中的大量电子表格-图像对。

Abstract: This paper explores capabilities of Vision Language Models on spreadsheet comprehension. We propose three self-supervised challenges with corresponding evaluation metrics to comprehensively evaluate VLMs on Optical Character Recognition (OCR), spatial perception, and visual format recognition. Additionally, we utilize the spreadsheet table detection task to assess the overall performance of VLMs by integrating these challenges. To probe VLMs more finely, we propose three spreadsheet-to-image settings: column width adjustment, style change, and address augmentation. We propose variants of prompts to address the above tasks in different settings. Notably, to leverage the strengths of VLMs in understanding text rather than two-dimensional positioning, we propose to decode cell values on the four boundaries of the table in spreadsheet boundary detection. Our findings reveal that VLMs demonstrate promising OCR capabilities but produce unsatisfactory results due to cell omission and misalignment, and they notably exhibit insufficient spatial and format recognition skills, motivating future work to enhance VLMs' spreadsheet data comprehension capabilities using our methods to generate extensive spreadsheet-image pairs in various settings.

</details>


### [2] [High-Throughput Phenotyping using Computer Vision and Machine Learning](https://arxiv.org/abs/2407.06354)
*Vivaan Singhvi,Langalibalele Lunga,Pragya Nidhi,Chris Keum,Varrun Prakash*

Main category: cs.CV

TL;DR: This study uses machine learning and image processing techniques to analyze images of Populus Trichocarpa, achieving 94.31% accuracy in OCR for label extraction and around 60% accuracy in classifying leaf characteristics and treatment. The study also identifies limitations in EXIF data for assessing leaf size and phenotype correlations.


<details>
  <summary>Details</summary>
Motivation: To improve plant phenotyping efficiency by integrating machine learning with high-throughput image analysis, addressing the challenge of analyzing large datasets and extracting specific traits.

Method: Utilizes OCR to read labels, image segmentation and machine learning for morphological classifications, and EXIF tag analysis to extract plant features from 1,672 images of Populus Trichocarpa.

Result: Achieved 94.31% accuracy in OCR label extraction, approximately 62.82% accuracy in classifying leaf shape, color, and brown splotches, and 60.08% accuracy in predicting plant treatment. Identified missing information in EXIF tags that prevented leaf size assessment and phenotype correlation analysis.

Conclusion: The study demonstrates the potential of machine learning and image processing for plant phenotyping but also highlights limitations in current datasets, suggesting future studies should focus on improving data completeness for better feature assessment and correlation analysis.

Abstract: High-throughput phenotyping refers to the non-destructive and efficient evaluation of plant phenotypes. In recent years, it has been coupled with machine learning in order to improve the process of phenotyping plants by increasing efficiency in handling large datasets and developing methods for the extraction of specific traits. Previous studies have developed methods to advance these challenges through the application of deep neural networks in tandem with automated cameras; however, the datasets being studied often excluded physical labels. In this study, we used a dataset provided by Oak Ridge National Laboratory with 1,672 images of Populus Trichocarpa with white labels displaying treatment (control or drought), block, row, position, and genotype. Optical character recognition (OCR) was used to read these labels on the plants, image segmentation techniques in conjunction with machine learning algorithms were used for morphological classifications, machine learning models were used to predict treatment based on those classifications, and analyzed encoded EXIF tags were used for the purpose of finding leaf size and correlations between phenotypes. We found that our OCR model had an accuracy of 94.31% for non-null text extractions, allowing for the information to be accurately placed in a spreadsheet. Our classification models identified leaf shape, color, and level of brown splotches with an average accuracy of 62.82%, and plant treatment with an accuracy of 60.08%. Finally, we identified a few crucial pieces of information absent from the EXIF tags that prevented the assessment of the leaf size. There was also missing information that prevented the assessment of correlations between phenotypes and conditions. However, future studies could improve upon this to allow for the assessment of these features.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [3] [General Table Question Answering via Answer-Formula Joint Generation](https://arxiv.org/abs/2503.12345)
*Zhongyuan Wang,Richong Zhang,Zhijie Nie,Hangyu Mao*

Main category: cs.CL

TL;DR: 该论文提出了一种新的表格问答框架TabAF，它使用电子表格公式作为可执行的表示，以解决不同结构表格上的复杂推理问题。该框架在多个数据集上取得了新的state-of-the-art性能。


<details>
  <summary>Details</summary>
Motivation: 现有表格问答方法缺乏处理特定问题类型或表格结构的多功能性。电子表格公式作为一种广泛使用且定义良好的表格数据操作语言，尚未被充分探索以解决表格问答问题。

Method: 该论文构建了一个大型的Formula标注的TableQA数据集FormulaQA，并提出了一个通用表格问答框架TabAF，该框架使用单个LLM主干解码答案和公式。

Result: TabAF在WikiTableQuestion、HiTab和TabFact上取得了新的state-of-the-art性能。

Conclusion: 该论文证明了TabAF的多功能性和泛化性。

Abstract: Advanced table question answering (TableQA) methods prompt large language models (LLMs) to generate answer text, SQL query, Python code, or custom operation, which impressively improve the complex reasoning problems in the TableQA task. However, these methods lack the versatility to cope with specific question types or table structures. In contrast, the Spreadsheet Formula, the widely used and well-defined operation language for tabular data, has not been thoroughly explored to solve TableQA. In this paper, we first attempt to use the Formula as the executable representation for solving complex reasoning on tables with different structures. Specifically, we construct \texttt{FromulaQA}, a large Formula-annotated TableQA dataset from existing datasets. In addition, we propose \texttt{TabAF}, a general table answering framework to solve multiple types of tasks over multiple types of tables simultaneously, which decodes answers and Formulas with a single LLM backbone. Extensive experiments demonstrate the versatility and generalization of \texttt{TabAF}. Under the same model size, \texttt{TabAF} achieves new state-of-the-art performance on the WikiTableQuestion, HiTab, and TabFact.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [4] [The Paradox of Spreadsheet Self-Efficacy: Social Incentives for Informal Knowledge Sharing in End-User Programming](https://arxiv.org/abs/2408.08068)
*Qing,Xia,Advait Sarkar,Duncan P. Brumby,Anna Cox*

Main category: cs.HC

TL;DR: This study investigates factors influencing knowledge sharing (KS) intention among spreadsheet users.


<details>
  <summary>Details</summary>
Motivation: Understanding how personal, social, and software-related variables impact spreadsheet KS intention.

Method: Multiple regressions analysis based on survey data from 100 spreadsheet users.

Result: Spreadsheet self-efficacy and reputational gains positively predict KS intention, while knowledge codification effort negatively predicts it. Users reported lower general spreadsheet self-efficacy compared to job-related self-efficacy.

Conclusion: Acknowledging social and personal variables can encourage knowledge sharing and has implications for spreadsheet design.

Abstract: Informal Knowledge Sharing (KS) is vital for end-user programmers to gain expertise. To better understand how personal (self-efficacy), social (reputational gains, trust between colleagues), and software-related (codification effort) variables influence spreadsheet KS intention, we conducted a multiple regressions analysis based on survey data from spreadsheet users (n=100) in administrative and finance roles. We found that high levels of spreadsheet self-efficacy and a perception that sharing would result in reputational gains predicted higher KS intention, but individuals who found knowledge codification effortful showed lower KS intention. We also observed that regardless of occupation, users tended to report a lower sense of self-efficacy in their general spreadsheet proficiency, despite also reporting high self-efficacy in spreadsheet use for job-related contexts. Our findings suggest that acknowledging and designing for these social and personal variables can help avoid situations where experienced individuals refrain unnecessarily from sharing, with implications for spreadsheet design.

</details>


### [5] [Subject integration with spreadsheets -- Ignoring education is the greatest risk ever](https://arxiv.org/abs/2409.12975)
*Mária Csernoch,Ádám Gulácsi,Júlia Csernoch*

Main category: cs.HC

TL;DR: Subject integration using spreadsheets in Grade 3 can bridge the gap between informatics and digital literacy.


<details>
  <summary>Details</summary>
Motivation: Introducing meaningful digitalization and digitization in schools through subject integration.

Method: Solving three traditional Grade 3 tasks in spreadsheets.

Result: Identification of skills, competencies, and computer science knowledge developed in teachers and students. Importance of task analysis, understanding, planning, and discussion.

Conclusion: Spreadsheet activities are crucial in preparing students for future jobs.

Abstract: Within the framework of Technological Pedagogical and Content Knowledge, subject integration is one possible solution for the introduction of meaningful digitalization and digitization in schools. This process incorporates that any school subject can be taught with digital support, informatics (computer) classes can be contextualized, and the gap between 'serious informatics' and 'digital literacy' can be minimized. The present paper details how three traditional Grade 3 tasks can be solved in spreadsheets, what skills, competencies, and computer science knowledge of both teachers and students can be developed. The solutions also reveal that analysing, understanding, planning, and discussing tasks is as important as the activity in the spreadsheets, which process plays a crucial role in the preparation of students for their future jobs.

</details>


### [6] [Data Guards: Challenges and Solutions for Fostering Trust in Data](https://arxiv.org/abs/2407.14042)
*Nicole Sultanum,Dennis Bromley,Michael Correll*

Main category: cs.HC

TL;DR: This paper explores the challenges of building trust in data artifacts and proposes data guards to address the need for data validation and verification.


<details>
  <summary>Details</summary>
Motivation: The motivation is the presence of threats to data validity, ranging from dirty data to intentional deception, which necessitates trust-building mechanisms for data-driven decisions.

Method: The method involves conducting interviews with both producers and consumers of data artifacts to understand strategies and obstacles related to building trust in data.

Result: The result highlights a recurring need for data validation and verification, particularly among data consumers, but a lack of existing standards to fulfill this need.

Conclusion: The conclusion proposes a set of data guards: methods and tools designed to foster trust in data artifacts.

Abstract: From dirty data to intentional deception, there are many threats to the validity of data-driven decisions. Making use of data, especially new or unfamiliar data, therefore requires a degree of trust or verification. How is this trust established? In this paper, we present the results of a series of interviews with both producers and consumers of data artifacts (outputs of data ecosystems like spreadsheets, charts, and dashboards) aimed at understanding strategies and obstacles to building trust in data. We find a recurring need, but lack of existing standards, for data validation and verification, especially among data consumers. We therefore propose a set of data guards: methods and tools for fostering trust in data artifacts.

</details>


### [7] [Smart Portable Computer](https://arxiv.org/abs/2405.05292)
*Niladri Das*

Main category: cs.HC

TL;DR: The "Portable Smart Computer" is introduced as a compact, energy-efficient, and cost-effective alternative to traditional PCs, especially beneficial during the COVID-19 pandemic for students and professionals.


<details>
  <summary>Details</summary>
Motivation: The difficulty students and professionals faced in acquiring affordable and adequately specified PCs during the COVID-19 pandemic, as well as the cumbersome nature of traditional laptops.

Method: Introducing a new device called the "Portable Smart Computer".

Result: The device offers comparable speed and performance to traditional desktops in a compact, energy-efficient, and cost-effective package. It supports various applications including document editing, browsing, spreadsheet management, presentations, and programming in languages like Python, C, and C++.

Conclusion: The "Portable Smart Computer" provides a seamless desktop experience and caters to the needs of programmers, offering a viable alternative to traditional PCs and laptops.

Abstract: Amidst the COVID-19 pandemic, with many organizations, schools, colleges, and universities transitioning to virtual platforms, students encountered difficulties in acquiring PCs such as desktops or laptops. The starting prices, around 15,000 INR, often failed to offer adequate system specifications, posing a challenge for consumers. Additionally, those reliant on laptops for work found the conventional approach cumbersome. Enter the "Portable Smart Computer," a leap into the future of computing. This innovative device boasts speed and performance comparable to traditional desktops but in a compact, energy-efficient, and cost-effective package. It delivers a seamless desktop experience, whether one is editing documents, browsing multiple tabs, managing spreadsheets, or creating presentations. Moreover, it supports programming languages like Python, C, C++, as well as compilers such as Keil and Xilinx, catering to the needs of programmers.

</details>


### [8] ["My toxic trait is thinking I'll remember this": gaps in the learner experience of video tutorials for feature-rich software](https://arxiv.org/abs/2404.07114)
*Ian Drosos,Advait Sarkar,Andrew D. Gordon*

Main category: cs.HC

TL;DR: This paper analyzes gaps encountered by learners using video tutorials for feature-rich software like Excel, proposes a taxonomy of these gaps, and suggests design improvements.


<details>
  <summary>Details</summary>
Motivation: Learners face issues (gaps) when using video tutorials for software learning.

Method: Collected and analyzed 360 viewer comments from 90 Excel tutorials across YouTube, TikTok, and Instagram. Conducted interviews with 8 tutorial creators. Presented creators with two designs for feedback.

Result: Developed a theory and taxonomy of gaps that act as barriers to learning.

Conclusion: The study identifies gaps in video tutorials and proposes design solutions to improve the learning experience.

Abstract: Video tutorials are a popular medium for informal and formal learning. However, when learners attempt to view and follow along with these tutorials, they encounter what we call gaps, that is, issues that can prevent learning. We examine the gaps encountered by users of video tutorials for feature-rich software, such as spreadsheets. We develop a theory and taxonomy of such gaps, identifying how they act as barriers to learning, by collecting and analyzing 360 viewer comments from 90 Microsoft Excel video tutorials published by 43 creators across YouTube, TikTok, and Instagram. We conducted contextual interviews with 8 highly influential tutorial creators to investigate the gaps their viewers experience and how they address them. Further, we obtain insights into their creative process and frustrations when creating video tutorials. Finally, we present creators with two designs that aim to address gaps identified in the comment analysis for feedback and alternative design ideas.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [9] [Consensus-Free Spreadsheet Integration](https://arxiv.org/abs/2209.14457)
*Brandon Baylor,Eric Daimler,James Hansen,Esteban Montero,Ryan Wisnesky*

Main category: cs.DB

TL;DR: This paper introduces a novel method for merging spreadsheets by representing them as algebraic theories and models, using category theory to integrate them.


<details>
  <summary>Details</summary>
Motivation: The motivation is to merge engineering models without requiring consensus among authors.

Method: The method involves expressing spreadsheets as algebraic theories and models, using theory and model morphisms to represent overlap, and applying category theory (colimit, lifting, Kan-extension) to compute an integrated theory and model.

Result: A case study on oil and gas calculation shows the application of the method. The paper also describes the automated theorem proving burden.

Conclusion: The paper concludes with thoughts on scaling engineering efforts across the enterprise using this methodology.

Abstract: We describe a method for merging multiple spreadsheets into one sheet, and/or exchanging data among the sheets, by expressing each sheet's formulae as an algebraic (equational) theory and each sheet's values as a model of its theory, expressing the overlap between the sheets as theory and model morphisms, and then performing colimit, lifting, and Kan-extension constructions from category theory to compute a canonically universal integrated theory and model, which can then be expressed as a spreadsheet. Our motivation is to find methods of merging engineering models that do not require consensus (agreement) among the authors of the models being merged, a condition fulfilled by our method because theory and model morphisms are semantics-preserving. We describe a case study of this methodology on a real-world oil and gas calculation at a major energy company, describing the theories and models that arise when integrating two different casing pressure test (MASP) calculation spreadsheets constructed by two non-interacting engineers. We also describe the automated theorem proving burden associated with both verifying the semantics preservation of the overlap mappings as well as verifying the conservativity/consistency of the resulting integrated sheet. We conclude with thoughts on how to apply the methodology to scale engineering efforts across the enterprise.

</details>


### [10] [A System and Benchmark for LLM-based Q&A on Heterogeneous Data](https://arxiv.org/abs/2409.05735)
*Achille Fokoue,Srideepika Jayaraman,Elham Khabiri,Jeffrey O. Kephart,Yingjie Li,Dhruv Shah,Youssef Drissi,Fenno F. Heath,Anu Bhamidipaty,Fateh A. Tipu,Robert J. Baseman*

Main category: cs.DB

TL;DR: This paper introduces siwarex, a platform that enables natural language access to both databases and APIs, addressing data source heterogeneity in industrial settings.


<details>
  <summary>Details</summary>
Motivation: Existing Text-to-SQL applications struggle with data source heterogeneity in real-world industrial settings where data is spread across databases and APIs.

Method: The authors introduce the siwarex platform and extend the Spider dataset by replacing some tables with data retrieval APIs to create a benchmark.

Result: Siwarex effectively handles data source heterogeneity.

Conclusion: The siwarex platform demonstrates good performance in coping with data source heterogeneity, as shown by the modified Spider benchmark.

Abstract: In many industrial settings, users wish to ask questions whose answers may be found in structured data sources such as a spreadsheets, databases, APIs, or combinations thereof. Often, the user doesn't know how to identify or access the right data source. This problem is compounded even further if multiple (and potentially siloed) data sources must be assembled to derive the answer. Recently, various Text-to-SQL applications that leverage Large Language Models (LLMs) have addressed some of these problems by enabling users to ask questions in natural language. However, these applications remain impractical in realistic industrial settings because they fail to cope with the data source heterogeneity that typifies such environments. In this paper, we address heterogeneity by introducing the siwarex platform, which enables seamless natural language access to both databases and APIs. To demonstrate the effectiveness of siwarex, we extend the popular Spider dataset and benchmark by replacing some of its tables by data retrieval APIs. We find that siwarex does a good job of coping with data source heterogeneity. Our modified Spider benchmark will soon be available to the research community

</details>


### [11] [Auditable and reusable crosswalks for fast, scaled integration of scattered tabular data](https://arxiv.org/abs/2409.01517)
*Gavin Chait*

Main category: cs.DB

TL;DR: The paper introduces an open-source toolkit for curating and restructuring complex tabular data into interoperable formats using schema-centric transformations.


<details>
  <summary>Details</summary>
Motivation: To provide a tool for transforming complex and scattered tabular data into well-structured and interoperable data, addressing the challenges of data integration from diverse sources.

Method: The toolkit divides curation into discrete, schema-centric components and uses high-level sequential scripts to describe schema-to-schema mappings. It is available as a Python package and a visual web application.

Result: The toolkit enables the transformation of data meeting a schema definition using a crosswalk, as demonstrated by integrating data from hundreds of local councils into a single database.

Conclusion: The open-source curatorial toolkit facilitates the production of well-structured and interoperable data through schema-centric transformations and task separation, offering both a Python package and a 'no-code' visual web application.

Abstract: This paper presents an open-source curatorial toolkit intended to produce well-structured and interoperable data. Curation is divided into discrete components, with a schema-centric focus for auditable restructuring of complex and scattered tabular data to conform to a destination schema. Task separation allows development of software and analysis without source data being present. Transformations are captured as high-level sequential scripts describing schema-to-schema mappings, reducing complexity and resource requirements. Ultimately, data are transformed, but the objective is that any data meeting a schema definition can be restructured using a crosswalk. The toolkit is available both as a Python package, and as a 'no-code' visual web application. A visual example is presented, derived from a longitudinal study where scattered source data from hundreds of local councils are integrated into a single database.

</details>


### [12] [Auto-Formula: Recommend Formulas in Spreadsheets using Contrastive Learning for Table Representations](https://arxiv.org/abs/2404.12608)
*Sibei Chen,Yeye He,Weiwei Cui,Ju Fan,Song Ge,Haidong Zhang,Dongmei Zhang,Surajit Chaudhuri*

Main category: cs.DB

TL;DR: This paper introduces Auto-Formula, a system that predicts spreadsheet formulas by learning from similar spreadsheets using contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Authoring complex spreadsheet formulas is challenging for non-technical users.

Method: The paper uses contrastive-learning techniques inspired by similar-face recognition to learn and adapt formulas from similar spreadsheets.

Result: Evaluations on 2K formulas show Auto-Formula's effectiveness.

Conclusion: Auto-Formula effectively predicts formulas by learning from similar spreadsheets.

Abstract: Spreadsheets are widely recognized as the most popular end-user programming tools, which blend the power of formula-based computation, with an intuitive table-based interface. Today, spreadsheets are used by billions of users to manipulate tables, most of whom are neither database experts nor professional programmers.
  Despite the success of spreadsheets, authoring complex formulas remains challenging, as non-technical users need to look up and understand non-trivial formula syntax. To address this pain point, we leverage the observation that there is often an abundance of similar-looking spreadsheets in the same organization, which not only have similar data, but also share similar computation logic encoded as formulas. We develop an Auto-Formula system that can accurately predict formulas that users want to author in a target spreadsheet cell, by learning and adapting formulas that already exist in similar spreadsheets, using contrastive-learning techniques inspired by "similar-face recognition" from compute vision.
  Extensive evaluations on over 2K test formulas extracted from real enterprise spreadsheets show the effectiveness of Auto-Formula over alternatives. Our benchmark data is available at https://github.com/microsoft/Auto-Formula to facilitate future research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [13] [Oh That Looks Familiar: A Novel Similarity Measure for Spreadsheet Template Discovery](https://arxiv.org/abs/2511.06973)
*Anand Krishnakumar,Vengadesh Ravikumaran*

Main category: cs.LG

TL;DR: This paper introduces a hybrid distance metric that combines semantic embeddings, data type information, and spatial positioning to quantify spreadsheet similarity.


<details>
  <summary>Details</summary>
Motivation: Traditional methods fail to capture the spatial layouts and type patterns defining templates.

Method: The method converts spreadsheets into cell-level embeddings and then uses aggregation techniques like Chamfer and Hausdorff distances.

Result: The method achieves superior unsupervised clustering performance compared to the graph-based Mondrian baseline, achieving perfect template reconstruction on the FUSTE dataset.

Conclusion: This approach facilitates large-scale automated template discovery, which in turn enables downstream applications such as retrieval-augmented generation over tabular collections, model training, and bulk data cleaning.

Abstract: Traditional methods for identifying structurally similar spreadsheets fail to capture the spatial layouts and type patterns defining templates. To quantify spreadsheet similarity, we introduce a hybrid distance metric that combines semantic embeddings, data type information, and spatial positioning. In order to calculate spreadsheet similarity, our method converts spreadsheets into cell-level embeddings and then uses aggregation techniques like Chamfer and Hausdorff distances. Experiments across template families demonstrate superior unsupervised clustering performance compared to the graph-based Mondrian baseline, achieving perfect template reconstruction (Adjusted Rand Index of 1.00 versus 0.90) on the FUSTE dataset. Our approach facilitates large-scale automated template discovery, which in turn enables downstream applications such as retrieval-augmented generation over tabular collections, model training, and bulk data cleaning.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [14] [Excel: Automated Ledger or Analytics IDE?](https://arxiv.org/abs/2409.12976)
*Andrew Kumiega*

Main category: cs.CY

TL;DR: The paper discusses the evolution of Excel into an analytics IDE and the need for a comprehensive risk framework.


<details>
  <summary>Details</summary>
Motivation: The motivation is that Excel has evolved into an analytics IDE, but its risk management framework hasn't kept pace.

Method: The method involves explaining how the current risk framework for spreadsheets needs to be expanded.

Result: The result will likely be a proposed expansion of the current risk framework.

Conclusion: The conclusion is that the current risk framework for spreadsheets needs to be expanded to manage the growing risks of using Excel as an IDE for analytics.

Abstract: Since the inception of VisiCalc over four decades ago, spreadsheets have undergone a gradual transformation, evolving from simple ledger automation tools to the current state of Excel, which can be described as an Integrated Development Environment (IDE) for analytics. The slow evolution of Excel from an automation tool for ledgers to an IDE for analytics explains why many people have not noticed that Excel includes a fully functional database, an OLAP Engine, multiple statistical programming languages, multiple third-party software libraries, dynamic charts, and real time data connectors. The simplicity of accessing these multiple tools is a low-code framework controlled from the Excel tool that is effectively an IDE. Once we acknowledge Excel's shift from a desk top application to an IDE for analytics, the importance of establishing a comprehensive risk framework for managing this distinctive development environment becomes clear. In this paper we will explain how the current risk framework for spreadsheets needs to be expanded to manage the growing risks of using Excel as an IDE for analytics.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [15] [Billion-files File Systems (BfFS): A Comparison](https://arxiv.org/abs/2408.01805)
*Sohail Shaikh*

Main category: cs.PF

TL;DR: 研究分析了多种 Linux 文件系统（EXT4、XFS、BtrFS、ZFS 和 F2FS）在创建、存储和读取大量文件时的性能表现。


<details>
  <summary>Details</summary>
Motivation: 为了应对数据量爆炸式增长带来的快速处理需求，需要减少数据传输延迟，因此本地文件系统受到关注。

Method: 通过在本地文件系统中创建、存储和读取十亿个文件来分析文件系统。

Result: 研究捕捉并分析了读/写吞吐量、存储块使用情况、磁盘空间利用率和开销以及其他指标。

Conclusion: 研究还探讨了在创建大量文件和文件夹期间及之后文件系统性能下降等其他副作用。

Abstract: As the volume of data being produced is increasing at an exponential rate that needs to be processed quickly, it is reasonable that the data needs to be available very close to the compute devices to reduce transfer latency. Due to this need, local filesystems are getting close attention to understand their inner workings, performance, and more importantly their limitations. This study analyzes few popular Linux filesystems: EXT4, XFS, BtrFS, ZFS, and F2FS by creating, storing, and then reading back one billion files from the local filesystem. The study also captured and analyzed read/write throughput, storage blocks usage, disk space utilization and overheads, and other metrics useful for system designers and integrators. Furthermore, the study explored other side effects such as filesystem performance degradation during and after these large numbers of files and folders are created.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [16] [Solving Data-centric Tasks using Large Language Models](https://arxiv.org/abs/2402.11734)
*Shraddha Barke,Christian Poelitz,Carina Suzana Negreanu,Benjamin Zorn,José Cambronero,Andrew D. Gordon,Vu Le,Elnaz Nouri,Nadia Polikarpova,Advait Sarkar,Brian Slininger,Neil Toronto,Jack Williams*

Main category: cs.PL

TL;DR: 该论文研究了大型语言模型（LLM）在处理数据中心任务时的提示问题，特别是如何选择包含在提示中的数据量和数据内容。


<details>
  <summary>Details</summary>
Motivation: 解决非专业程序员和最终用户在处理表格数据时，仅通过自然语言描述难以表达意图的问题。

Method: 1. 创建了一个从StackOverflow帖子中挖掘的真实NL-to-code任务数据集，用于处理表格数据。2. 提出了一种聚类-选择提示技术，该技术将输入数据中最具代表性的行添加到LLM提示中。

Result: 实验表明，LLM的性能确实对提示中传递的数据量敏感，并且对于输入表具有大量句法变异的任务，该聚类-选择技术优于随机选择基线。

Conclusion: 该研究表明，在LLM处理表格数据时，选择合适的数据量和数据内容可以显著提高性能，并且聚类-选择提示技术是一种有效的方法。

Abstract: Large language models (LLMs) are rapidly replacing help forums like StackOverflow, and are especially helpful for non-professional programmers and end users. These users are often interested in data-centric tasks, such as spreadsheet manipulation and data wrangling, which are hard to solve if the intent is only communicated using a natural-language description, without including the data. But how do we decide how much data and which data to include in the prompt? This paper makes two contributions towards answering this question. First, we create a dataset of real-world NL-to-code tasks manipulating tabular data, mined from StackOverflow posts. Second, we introduce a cluster-then-select prompting technique, which adds the most representative rows from the input data to the LLM prompt. Our experiments show that LLM performance is indeed sensitive to the amount of data passed in the prompt, and that for tasks with a lot of syntactic variation in the input table, our cluster-then-select technique outperforms a random selection baseline.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [17] [Knowledge engineering for open science: Building and deploying knowledge bases for metadata standards](https://arxiv.org/abs/2507.22391)
*Mark A. Musen,Martin J. O'Connor,Josef Hardi,Marcos Martinez-Romero*

Main category: cs.DL

TL;DR: CEDAR builds technology that enables scientists to encode metadata standards as templates.


<details>
  <summary>Details</summary>
Motivation: Scientists strive to make their datasets available in open repositories, with the goal that they be findable, accessible, interoperable, and reusable (FAIR).

Method: CEDAR templates describing community metadata preferences have been used to standardize metadata for a variety of scientific consortia. They have been used as the basis for data-annotation systems that acquire metadata through Web forms or through spreadsheets, and they can help correct metadata to ensure adherence to standards.

Result: They provide a mechanism for scientific communities to create shared metadata standards and to encode their preferences for the application of those standards.

Conclusion: CEDAR templates capture the knowledge in symbolic form, and they allow that knowledge to be applied in a variety of settings to promote open science.

Abstract: Scientists strive to make their datasets available in open repositories, with the goal that they be findable, accessible, interoperable, and reusable (FAIR). Although it is hard for most investigators to remember all the guiding principles associated with FAIR data, there is one overarching requirement: The data need to be annotated with rich, discipline-specific, standardized metadata. The Center for Expanded Data Annotation and Retrieval (CEDAR) builds technology that enables scientists to encode metadata standards as templates that enumerate the attributes of different kinds of experiments. These metadata templates capture preferences regarding how data should be described and what a third party needs to know to make sense of the datasets. CEDAR templates describing community metadata preferences have been used to standardize metadata for a variety of scientific consortia. They have been used as the basis for data-annotation systems that acquire metadata through Web forms or through spreadsheets, and they can help correct metadata to ensure adherence to standards. Like the declarative knowledge bases that underpinned intelligent systems decades ago, CEDAR templates capture the knowledge in symbolic form, and they allow that knowledge to be applied in a variety of settings. They provide a mechanism for scientific communities to create shared metadata standards and to encode their preferences for the application of those standards, and for deploying those standards in a range of intelligent systems to promote open science.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [18] [MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.05587)
*Junjie Xing,Yeye He,Mengyu Zhou,Haoyu Dong,Shi Han,Lingjiao Chen,Dongmei Zhang,Surajit Chaudhuri,H. V. Jagadish*

Main category: cs.AI

TL;DR: MMTU is a new benchmark for evaluating LLMs on real-world table tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for table-related tasks are limited and do not cover the full range of tasks faced by professional users.

Method: Introduce MMTU, a large-scale benchmark with over 28K questions across 25 real-world table tasks.

Result: Current frontier models struggle on MMTU, suggesting significant room for improvement.

Conclusion: MMTU can drive further advances in understanding and developing foundation models for structured data processing and analysis.

Abstract: Tables and table-based use cases play a crucial role in many important real-world applications, such as spreadsheets, databases, and computational notebooks, which traditionally require expert-level users like data engineers, data analysts, and database administrators to operate. Although LLMs have shown remarkable progress in working with tables (e.g., in spreadsheet and database copilot scenarios), comprehensive benchmarking of such capabilities remains limited. In contrast to an extensive and growing list of NLP benchmarks, evaluations of table-related tasks are scarce, and narrowly focus on tasks like NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks that professional users face. This gap limits our understanding and model progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 28K questions across 25 real-world table tasks, designed to comprehensively evaluate models ability to understand, reason, and manipulate real tables at the expert-level. These tasks are drawn from decades' worth of computer science research on tabular data, with a focus on complex table tasks faced by professional users. We show that MMTU require a combination of skills -- including table understanding, reasoning, and coding -- that remain challenging for today's frontier models, where even frontier reasoning models like OpenAI GPT-5 and DeepSeek R1 score only around 69\% and 57\% respectively, suggesting significant room for improvement. We highlight key findings in our evaluation using MMTU and hope that this benchmark drives further advances in understanding and developing foundation models for structured data processing and analysis.
  Our code and data are available at https://github.com/MMTU-Benchmark/MMTU and https://huggingface.co/datasets/MMTU-benchmark/MMTU.

</details>


### [19] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: This paper discusses why relational learning has not become more prominent in the field of AI, despite the world being composed of entities with properties and relations.


<details>
  <summary>Details</summary>
Motivation: The motivation is that AI is focusing on modeling pixels, words, and phonemes, while the world is made up of entities with properties and relations. Valuable data is in relational formats, but relational learning is not taking over the world.

Method: The paper explains why relational learning is not taking over the world and what needs to be done to bring it to its rightful prominence.

Result: The result is not explicitly stated in the abstract.

Conclusion: The conclusion is what needs to be done to bring relational learning to its rightful prominence.

Abstract: Artificial intelligence seems to be taking over the world with systems that model pixels, words, and phonemes. The world is arguably made up, not of pixels, words, and phonemes but of entities (objects, things, including events) with properties and relations among them. Surely we should model these, not the perception or description of them. You might suspect that concentrating on modeling words and pixels is because all of the (valuable) data in the world is in terms of text and images. If you look into almost any company you will find their most valuable data is in spreadsheets, databases and other relational formats. These are not the form that are studied in introductory machine learning, but are full of product numbers, student numbers, transaction numbers and other identifiers that can't be interpreted naively as numbers. The field that studies this sort of data has various names including relational learning, statistical relational AI, and many others. This paper explains why relational learning is not taking over the world -- except in a few cases with restricted relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [20] [TableTalk: Scaffolding Spreadsheet Development with a Language Agent](https://arxiv.org/abs/2502.09787)
*Jenny T. Liang,Aayush Kumar,Yasharth Bajpai,Sumit Gulwani,Vu Le,Chris Parnin,Arjun Radhakrishna,Ashish Tiwari,Emerson Murphy-Hill,Guastavo Soares*

Main category: cs.SE

TL;DR: TableTalk is a spreadsheet programming agent that helps programmers create higher-quality spreadsheets with reduced cognitive load.


<details>
  <summary>Details</summary>
Motivation: Spreadsheet programming is challenging, and large language models offer promise for automating spreadsheet creation.

Method: TableTalk uses scaffolding, flexibility, and incrementality to guide programmers through structured plans and generate spreadsheet components.

Result: TableTalk produced higher-quality spreadsheets 2.3 times more likely to be preferred and reduced cognitive load and thinking time by 12.6%.

Conclusion: The study derives design guidelines for agentic spreadsheet programming tools and discusses implications on spreadsheet programming, end-user programming, AI-assisted programming, and human-agent collaboration.

Abstract: Spreadsheet programming is challenging. Programmers use spreadsheet programming knowledge (e.g., formulas) and problem-solving skills to combine actions into complex tasks. Advancements in large language models have introduced language agents that observe, plan, and perform tasks, showing promise for spreadsheet creation. We present TableTalk, a spreadsheet programming agent embodying three design principles -- scaffolding, flexibility, and incrementality -- derived from studies with seven spreadsheet programmers and 85 Excel templates. TableTalk guides programmers through structured plans based on professional workflows, generating three potential next steps to adapt plans to programmer needs. It uses pre-defined tools to generate spreadsheet components and incrementally build spreadsheets. In a study with 20 programmers, TableTalk produced higher-quality spreadsheets 2.3 times more likely to be preferred than the baseline. It reduced cognitive load and thinking time by 12.6%. From this, we derive design guidelines for agentic spreadsheet programming tools and discuss implications on spreadsheet programming, end-user programming, AI-assisted programming, and human-agent collaboration.

</details>


### [21] [MaxMind: A Memory Loop Network to Enhance Software Productivity based on Large Language Models](https://arxiv.org/abs/2408.03841)
*Yuchen Dong,XiaoXiang Fang,Yuchen Hu,Renshuang Jiang,Zhe Jiang*

Main category: cs.SE

TL;DR: This paper introduces Memory-Loop Networks and a knowledge precision segmentation-enhanced RAG mechanism to improve the performance of large language models in automated software operations and tool generation (SOTG).


<details>
  <summary>Details</summary>
Motivation: Current research often overlooks converting real-time task experiences into system memory and differentiating the value of existing knowledge for future reference in SOTG tasks.

Method: The paper evolves external memory models into Memory-Loop Networks and enhances a RAG mechanism with knowledge precision segmentation to utilize memory based on value differentiation, designing the MaxMind model for SOTG.

Result: Experiments with MaxMind4Sheet demonstrate a 3%-6% improvement in task success rate per round and a 25% boost in task execution efficiency due to memory recycling.

Conclusion: MaxMind has significant potential to enhance the capabilities and productivity of LLM systems in SOTG by accumulating and recycling task memories and addressing the retraining issue faced by LLMs when handling specialized tasks through memories transfer.

Abstract: The application of large language models to facilitate automated software operations and tool generation (SOTG), thus augmenting software productivity, mirrors the early stages of human evolution when the ability to create and use tools accelerated the progress of civilization. These complex tasks require AI to continuously summarize and improve. Current research often overlooks the importance of converting real-time task experiences into system memory and differentiating the value of existing knowledge for future reference. This paper addresses these issues by evolving external memory models into Memory-Loop Networks for timely memorization and experience referencing. We also enhance a RAG mechanism with knowledge precision segmentation to utilize memory based on value differentiation, and design the MaxMind model for SOTG accordingly.To demonstrate our approach, we developed MaxMind4Sheet, an electronic spreadsheet processing system aligned with the MaxMind philosophy. Comparative experiments with SheetCopilot have demonstrated that the accumulation and recycling of task memories lead to a steady enhancement in task success rate, with an improvement rate of approximately 3%-6% per round in this implementation example. Note that as the memories continue to grow, this cumulative improvement may be substantial. The inclusion of memory recycling can also boost the system's task execution efficiency by up to 25%, and it can address the retraining issue faced by LLMs when handling specialized tasks through memories transfer.These suggest that MaxMind has significant potential to enhance the capabilities and productivity of LLM systems in SOTG.

</details>
