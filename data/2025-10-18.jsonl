{"id": "2209.14457", "title": "Consensus-Free Spreadsheet Integration", "url": "https://arxiv.org/abs/2209.14457", "pdf": "https://arxiv.org/pdf/2209.14457", "abs": "https://arxiv.org/abs/2209.14457", "authors": ["Brandon Baylor", "Eric Daimler", "James Hansen", "Esteban Montero", "Ryan Wisnesky"], "categories": ["cs.DB", "cs.SE"], "comment": null, "summary": "We describe a method for merging multiple spreadsheets into one sheet, and/or\nexchanging data among the sheets, by expressing each sheet's formulae as an\nalgebraic (equational) theory and each sheet's values as a model of its theory,\nexpressing the overlap between the sheets as theory and model morphisms, and\nthen performing colimit, lifting, and Kan-extension constructions from category\ntheory to compute a canonically universal integrated theory and model, which\ncan then be expressed as a spreadsheet. Our motivation is to find methods of\nmerging engineering models that do not require consensus (agreement) among the\nauthors of the models being merged, a condition fulfilled by our method because\ntheory and model morphisms are semantics-preserving. We describe a case study\nof this methodology on a real-world oil and gas calculation at a major energy\ncompany, describing the theories and models that arise when integrating two\ndifferent casing pressure test (MASP) calculation spreadsheets constructed by\ntwo non-interacting engineers. We also describe the automated theorem proving\nburden associated with both verifying the semantics preservation of the overlap\nmappings as well as verifying the conservativity/consistency of the resulting\nintegrated sheet. We conclude with thoughts on how to apply the methodology to\nscale engineering efforts across the enterprise."}
{"id": "2503.12345", "title": "General Table Question Answering via Answer-Formula Joint Generation", "url": "https://arxiv.org/abs/2503.12345", "pdf": "https://arxiv.org/pdf/2503.12345", "abs": "https://arxiv.org/abs/2503.12345", "authors": ["Zhongyuan Wang", "Richong Zhang", "Zhijie Nie", "Hangyu Mao"], "categories": ["cs.CL", "cs.AI"], "comment": "work in progress", "summary": "Advanced table question answering (TableQA) methods prompt large language\nmodels (LLMs) to generate answer text, SQL query, Python code, or custom\noperation, which impressively improve the complex reasoning problems in the\nTableQA task. However, these methods lack the versatility to cope with specific\nquestion types or table structures. In contrast, the Spreadsheet Formula, the\nwidely used and well-defined operation language for tabular data, has not been\nthoroughly explored to solve TableQA. In this paper, we first attempt to use\nthe Formula as the executable representation for solving complex reasoning on\ntables with different structures. Specifically, we construct\n\\texttt{FromulaQA}, a large Formula-annotated TableQA dataset from existing\ndatasets. In addition, we propose \\texttt{TabAF}, a general table answering\nframework to solve multiple types of tasks over multiple types of tables\nsimultaneously, which decodes answers and Formulas with a single LLM backbone.\nExtensive experiments demonstrate the versatility and generalization of\n\\texttt{TabAF}. Under the same model size, \\texttt{TabAF} achieves new\nstate-of-the-art performance on the WikiTableQuestion, HiTab, and TabFact."}
{"id": "2502.09787", "title": "TableTalk: Scaffolding Spreadsheet Development with a Language Agent", "url": "https://arxiv.org/abs/2502.09787", "pdf": "https://arxiv.org/pdf/2502.09787", "abs": "https://arxiv.org/abs/2502.09787", "authors": ["Jenny T. Liang", "Aayush Kumar", "Yasharth Bajpai", "Sumit Gulwani", "Vu Le", "Chris Parnin", "Arjun Radhakrishna", "Ashish Tiwari", "Emerson Murphy-Hill", "Guastavo Soares"], "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": null, "summary": "Spreadsheet programming is challenging. Programmers use spreadsheet\nprogramming knowledge (e.g., formulas) and problem-solving skills to combine\nactions into complex tasks. Advancements in large language models have\nintroduced language agents that observe, plan, and perform tasks, showing\npromise for spreadsheet creation. We present TableTalk, a spreadsheet\nprogramming agent embodying three design principles -- scaffolding,\nflexibility, and incrementality -- derived from studies with seven spreadsheet\nprogrammers and 85 Excel templates. TableTalk guides programmers through\nstructured plans based on professional workflows, generating three potential\nnext steps to adapt plans to programmer needs. It uses pre-defined tools to\ngenerate spreadsheet components and incrementally build spreadsheets. In a\nstudy with 20 programmers, TableTalk produced higher-quality spreadsheets 2.3\ntimes more likely to be preferred than the baseline. It reduced cognitive load\nand thinking time by 12.6%. From this, we derive design guidelines for agentic\nspreadsheet programming tools and discuss implications on spreadsheet\nprogramming, end-user programming, AI-assisted programming, and human-agent\ncollaboration."}
{"id": "2507.13558", "title": "Why Isn't Relational Learning Taking Over the World?", "url": "https://arxiv.org/abs/2507.13558", "pdf": "https://arxiv.org/pdf/2507.13558", "abs": "https://arxiv.org/abs/2507.13558", "authors": ["David Poole"], "categories": ["cs.AI", "cs.DB", "cs.LG"], "comment": "10 pages (6 pages + references + appendices)", "summary": "Artificial intelligence seems to be taking over the world with systems that\nmodel pixels, words, and phonemes. The world is arguably made up, not of\npixels, words, and phonemes but of entities (objects, things, including events)\nwith properties and relations among them. Surely we should model these, not the\nperception or description of them. You might suspect that concentrating on\nmodeling words and pixels is because all of the (valuable) data in the world is\nin terms of text and images. If you look into almost any company you will find\ntheir most valuable data is in spreadsheets, databases and other relational\nformats. These are not the form that are studied in introductory machine\nlearning, but are full of product numbers, student numbers, transaction numbers\nand other identifiers that can't be interpreted naively as numbers. The field\nthat studies this sort of data has various names including relational learning,\nstatistical relational AI, and many others. This paper explains why relational\nlearning is not taking over the world -- except in a few cases with restricted\nrelations -- and what needs to be done to bring it to it's rightful prominence."}
{"id": "2506.05587", "title": "MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark", "url": "https://arxiv.org/abs/2506.05587", "pdf": "https://arxiv.org/pdf/2506.05587", "abs": "https://arxiv.org/abs/2506.05587", "authors": ["Junjie Xing", "Yeye He", "Mengyu Zhou", "Haoyu Dong", "Shi Han", "Lingjiao Chen", "Dongmei Zhang", "Surajit Chaudhuri", "H. V. Jagadish"], "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.LG"], "comment": "Included additional benchmark results covering 24 LLMs", "summary": "Tables and table-based use cases play a crucial role in many important\nreal-world applications, such as spreadsheets, databases, and computational\nnotebooks, which traditionally require expert-level users like data engineers,\ndata analysts, and database administrators to operate. Although LLMs have shown\nremarkable progress in working with tables (e.g., in spreadsheet and database\ncopilot scenarios), comprehensive benchmarking of such capabilities remains\nlimited. In contrast to an extensive and growing list of NLP benchmarks,\nevaluations of table-related tasks are scarce, and narrowly focus on tasks like\nNL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks\nthat professional users face. This gap limits our understanding and model\nprogress in this important area.\n  In this work, we introduce MMTU, a large-scale benchmark with over 30K\nquestions across 25 real-world table tasks, designed to comprehensively\nevaluate models ability to understand, reason, and manipulate real tables at\nthe expert-level. These tasks are drawn from decades' worth of computer science\nresearch on tabular data, with a focus on complex table tasks faced by\nprofessional users. We show that MMTU require a combination of skills --\nincluding table understanding, reasoning, and coding -- that remain challenging\nfor today's frontier models, where even frontier reasoning models like OpenAI\no4-mini and DeepSeek R1 score only around 60%, suggesting significant room for\nimprovement. We highlight key findings in our evaluation using MMTU and hope\nthat this benchmark drives further advances in understanding and developing\nfoundation models for structured data processing and analysis. Our code and\ndata are available at https://github.com/MMTU-Benchmark/MMTU and\nhttps://huggingface.co/datasets/MMTU-benchmark/MMTU."}
{"id": "2507.10456", "title": "Radif Corpus: A Symbolic Dataset for Non-Metric Iranian Classical Music", "url": "https://arxiv.org/abs/2507.10456", "pdf": "https://arxiv.org/pdf/2507.10456", "abs": "https://arxiv.org/abs/2507.10456", "authors": ["Maziar Kanani", "Sean O Leary", "James McDermott"], "categories": ["cs.SD", "eess.AS"], "comment": null, "summary": "Non-metric music forms the core of the repertoire in Iranian classical music.\nDastgahi music serves as the underlying theoretical system for both Iranian art\nmusic and certain folk traditions. At the heart of Iranian classical music lies\nthe radif, a foundational repertoire that organizes melodic material central to\nperformance and pedagogy.\n  In this study, we introduce a digital corpus representing the complete\nnon-metrical radif repertoire, covering all 13 existing components of this\nrepertoire. We provide MIDI files (about 281 minutes in total) and data\nspreadsheets describing notes, note durations, intervals, and hierarchical\nstructures for 228 pieces of music. We faithfully represent the tonality\nincluding quarter-tones, and the non-metric aspect. Furthermore, we provide\nsupporting basic statistics, and measures of complexity and similarity over the\ncorpus.\n  Our corpus provides a platform for computational studies of Iranian classical\nmusic. Researchers might employ it in studying melodic patterns, investigating\nimprovisational styles, or for other tasks in music information retrieval,\nmusic theory, and computational (ethno)musicology."}
{"id": "2408.08068", "title": "The Paradox of Spreadsheet Self-Efficacy: Social Incentives for Informal Knowledge Sharing in End-User Programming", "url": "https://arxiv.org/abs/2408.08068", "pdf": "https://arxiv.org/pdf/2408.08068", "abs": "https://arxiv.org/abs/2408.08068", "authors": ["Qing", "Xia", "Advait Sarkar", "Duncan P. Brumby", "Anna Cox"], "categories": ["cs.HC"], "comment": "8 pages", "summary": "Informal Knowledge Sharing (KS) is vital for end-user programmers to gain\nexpertise. To better understand how personal (self-efficacy), social\n(reputational gains, trust between colleagues), and software-related\n(codification effort) variables influence spreadsheet KS intention, we\nconducted a multiple regressions analysis based on survey data from spreadsheet\nusers (n=100) in administrative and finance roles. We found that high levels of\nspreadsheet self-efficacy and a perception that sharing would result in\nreputational gains predicted higher KS intention, but individuals who found\nknowledge codification effortful showed lower KS intention. We also observed\nthat regardless of occupation, users tended to report a lower sense of\nself-efficacy in their general spreadsheet proficiency, despite also reporting\nhigh self-efficacy in spreadsheet use for job-related contexts. Our findings\nsuggest that acknowledging and designing for these social and personal\nvariables can help avoid situations where experienced individuals refrain\nunnecessarily from sharing, with implications for spreadsheet design."}
{"id": "2507.16073", "title": "Buckaroo: A Direct Manipulation Visual Data Wrangler", "url": "https://arxiv.org/abs/2507.16073", "pdf": "https://arxiv.org/pdf/2507.16073", "abs": "https://arxiv.org/abs/2507.16073", "authors": ["Annabelle Warner", "Andrew McNutt", "Paul Rosen", "El Kindi Rezig"], "categories": ["cs.HC", "cs.DB"], "comment": "Accepted to VLDB25 Demo track", "summary": "Preparing datasets -- a critical phase known as data wrangling -- constitutes\nthe dominant phase of data science development, consuming upwards of 80% of the\ntotal project time. This phase encompasses a myriad of tasks: parsing data,\nrestructuring it for analysis, repairing inaccuracies, merging sources,\neliminating duplicates, and ensuring overall data integrity. Traditional\napproaches, typically through manual coding in languages such as Python or\nusing spreadsheets, are not only laborious but also error-prone. These issues\nrange from missing entries and formatting inconsistencies to data type\ninaccuracies, all of which can affect the quality of downstream tasks if not\nproperly corrected. To address these challenges, we present Buckaroo, a\nvisualization system to highlight discrepancies in data and enable on-the-spot\ncorrections through direct manipulations of visual objects. Buckaroo (1)\nautomatically finds \"interesting\" data groups that exhibit anomalies compared\nto the rest of the groups and recommends them for inspection; (2) suggests\nwrangling actions that the user can choose to repair the anomalies; and (3)\nallows users to visually manipulate their data by displaying the effects of\ntheir wrangling actions and offering the ability to undo or redo these actions,\nwhich supports the iterative nature of data wrangling. A video companion is\navailable at https://youtu.be/iXdCYbvpQVE"}
{"id": "2407.10657", "title": "An Empirical Study of Validating Synthetic Data for Formula Generation", "url": "https://arxiv.org/abs/2407.10657", "pdf": "https://arxiv.org/pdf/2407.10657", "abs": "https://arxiv.org/abs/2407.10657", "authors": ["Usneek Singh", "José Cambronero", "Sumit Gulwani", "Aditya Kanade", "Anirudh Khatry", "Vu Le", "Mukul Singh", "Gust Verbruggen"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Findings of NAACL", "summary": "Large language models (LLMs) can be leveraged to help with writing formulas\nin spreadsheets, but resources on these formulas are scarce, impacting both the\nbase performance of pre-trained models and limiting the ability to fine-tune\nthem. Given a corpus of formulas, we can use a(nother) model to generate\nsynthetic natural language utterances for fine-tuning. However, it is important\nto validate whether the NL generated by the LLM is indeed accurate to be\nbeneficial for fine-tuning. In this paper, we provide empirical results on the\nimpact of validating these synthetic training examples with surrogate\nobjectives that evaluate the accuracy of the synthetic annotations. We\ndemonstrate that validation improves performance over raw data across four\nmodels (2 open and 2 closed weight). Interestingly, we show that although\nvalidation tends to prune more challenging examples, it increases the\ncomplexity of problems that models can solve after being fine-tuned on\nvalidated data."}
{"id": "2507.06171", "title": "Data-Semantics-Aware Recommendation of Diverse Pivot Tables", "url": "https://arxiv.org/abs/2507.06171", "pdf": "https://arxiv.org/pdf/2507.06171", "abs": "https://arxiv.org/abs/2507.06171", "authors": ["Whanhee Cho", "Anna Fariha"], "categories": ["cs.DB"], "comment": null, "summary": "Data summarization is essential to discover insights from large datasets. In\na spreadsheets, pivot tables offer a convenient way to summarize tabular data\nby computing aggregates over some attributes, grouped by others. However,\nidentifying attribute combinations that will result in useful pivot tables\nremains a challenge, especially for high-dimensional datasets. We formalize the\nproblem of automatically recommending insightful and interpretable pivot\ntables, eliminating the tedious manual process. A crucial aspect of\nrecommending a set of pivot tables is to diversify them. Traditional works\ninadequately address the table-diversification problem, which leads us to\nconsider the problem of pivot table diversification.\n  We present SAGE, a data-semantics-aware system for recommending k-budgeted\ndiverse pivot tables, overcoming the shortcomings of prior work for top-k\nrecommendations that cause redundancy. SAGE ensures that each pivot table is\ninsightful, interpretable, and adaptive to the user's actions and preferences,\nwhile also guaranteeing that the set of pivot tables are different from each\nother, offering a diverse recommendation. We make two key technical\ncontributions: (1) a data-semantics-aware model to measure the utility of a\nsingle pivot table and the diversity of a set of pivot tables, and (2) a\nscalable greedy algorithm that can efficiently select a set of diverse pivot\ntables of high utility, by leveraging data semantics to significantly reduce\nthe combinatorial search space. Our extensive experiments on three real-world\ndatasets show that SAGE outperforms alternative approaches, and efficiently\nscales to accommodate high-dimensional datasets. Additionally, we present\nseveral case studies to highlight SAGE's qualitative effectiveness over\ncommercial software and Large Language Models (LLMs)."}
{"id": "2506.17330", "title": "Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE", "url": "https://arxiv.org/abs/2506.17330", "pdf": "https://arxiv.org/pdf/2506.17330", "abs": "https://arxiv.org/abs/2506.17330", "authors": ["Simon Thorne"], "categories": ["cs.SE"], "comment": "18 Pages, 10 Tables, 1 Colour Figure", "summary": "Large Language Models (LLMs) have demonstrated some significant capabilities\nacross various domains; however, their effectiveness in spreadsheet related\ntasks remains underexplored. This study introduces a foundation for a\ncomprehensive benchmark framework to evaluate the performance of leading LLMs\nin executing spreadsheet functions, formula generation and data manipulation\ntasks. The benchmark encompasses tasks ranging from basic formula creation to\ncomplex, real world spreadsheet scenarios. Our findings reveal that while LLMs\nexhibit proficiency in straightforward tasks, they often falter in complex,\nmulti step operations, frequently producing plausible yet incorrect outputs.\nThese results underscore the limitations of current LLMs in handling\nspreadsheet tasks that require precise logical reasoning and highlight the need\nfor integrating symbolic reasoning capabilities into LLM architectures. To\nsupport this, we introduce FLARE (Formula Logic, Auditing, Reasoning and\nEvaluation) a new benchmark for evaluating LLM performance on real-world\nspreadsheet logic, auditing, and reasoning tasks."}
{"id": "2506.12339", "title": "SheetMind: An End-to-End LLM-Powered Multi-Agent Framework for Spreadsheet Automation", "url": "https://arxiv.org/abs/2506.12339", "pdf": "https://arxiv.org/pdf/2506.12339", "abs": "https://arxiv.org/abs/2506.12339", "authors": ["Ruiyan Zhu", "Xi Cheng", "Ke Liu", "Brian Zhu", "Daniel Jin", "Neeraj Parihar", "Zhoutian Xu", "Oliver Gao"], "categories": ["cs.HC", "cs.AI"], "comment": "Ruiyan Zhu and Xi Cheng contributed equally to this work", "summary": "We present SheetMind, a modular multi-agent framework powered by large\nlanguage models (LLMs) for spreadsheet automation via natural language\ninstructions. The system comprises three specialized agents: a Manager Agent\nthat decomposes complex user instructions into subtasks; an Action Agent that\ntranslates these into structured commands using a Backus Naur Form (BNF)\ngrammar; and a Reflection Agent that validates alignment between generated\nactions and the user's original intent. Integrated into Google Sheets via a\nWorkspace extension, SheetMind supports real-time interaction without requiring\nscripting or formula knowledge. Experiments on benchmark datasets demonstrate\nan 80 percent success rate on single step tasks and approximately 70 percent on\nmulti step instructions, outperforming ablated and baseline variants. Our\nresults highlight the effectiveness of multi agent decomposition and grammar\nbased execution for bridging natural language and spreadsheet functionalities."}
{"id": "2506.09216", "title": "\"How do you even know that stuff?\": Barriers to expertise sharing among spreadsheet users", "url": "https://arxiv.org/abs/2506.09216", "pdf": "https://arxiv.org/pdf/2506.09216", "abs": "https://arxiv.org/abs/2506.09216", "authors": ["Qing", "Xia", "Advait Sarkar", "Duncan Brumby", "Anna Cox"], "categories": ["cs.HC", "cs.CY", "H.5"], "comment": "Accepted at CSCW 2025", "summary": "Spreadsheet collaboration provides valuable opportunities for learning and\nexpertise sharing between colleagues. Sharing expertise is essential for the\nretention of important technical skillsets within organisations, but previous\nstudies suggest that spreadsheet experts often fail to disseminate their\nknowledge to others. We suggest that social norms and beliefs surrounding the\nvalue of spreadsheet use significantly influence user engagement in sharing\nbehaviours. To explore this, we conducted 31 semi-structured interviews with\nprofessional spreadsheet users from two separate samples. We found that\nspreadsheet providers face challenges in adapting highly personalised\nstrategies to often subjective standards and evaluating the appropriate social\ntiming of sharing. In addition, conflicted self-evaluations of one's\nspreadsheet expertise, dismissive normative beliefs about the value of this\nknowledge, and concerns about the potential disruptions associated with\ncollaboration can further deter sharing. We suggest these observations reflect\nthe challenges of long-term learning in feature-rich software designed\nprimarily with initial learnability in mind. We therefore provide implications\nfor design to navigate this tension. Overall, our findings demonstrate how the\ncomplex interaction between technology design and social dynamics can shape\ncollaborative learning behaviours in the context of feature-rich software."}
{"id": "2506.03232", "title": "Pivoting the paradigm: the role of spreadsheets in K-12 data science", "url": "https://arxiv.org/abs/2506.03232", "pdf": "https://arxiv.org/pdf/2506.03232", "abs": "https://arxiv.org/abs/2506.03232", "authors": ["Oren Tirschwell", "Nicholas Jon Horton"], "categories": ["stat.OT", "cs.CY"], "comment": null, "summary": "Spreadsheet tools are widely accessible to and commonly used by K-12 students\nand teachers. They have an important role in data collection and organization.\nBeyond data organization, spreadsheets also make data visible and easy to\ninteract with, facilitating student engagement in data exploration and\nanalysis. Though not suitable for all circumstances, spreadsheets can and do\nhelp foster data and computing skills for K-12 students. This paper 1) reviews\nprior frameworks on K-12 data tools; 2) proposes data-driven learning outcomes\nthat can be accomplished by incorporating spreadsheets into the curriculum; and\n3) discusses how spreadsheets can help develop data acumen and computational\nfluency. We provide example class activities, identify challenges and barriers\nto adoption, suggest pedagogical approaches to ease the learning curve for\ninstructors and students, and discuss the need for professional development to\nfacilitate deeper use of spreadsheets for data science and STEM disciplines."}
{"id": "2505.23667", "title": "Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models", "url": "https://arxiv.org/abs/2505.23667", "pdf": "https://arxiv.org/pdf/2505.23667", "abs": "https://arxiv.org/abs/2505.23667", "authors": ["Lang Cao", "Jingxian Xu", "Hanbing Liu", "Jinyu Wang", "Mengyu Zhou", "Haoyu Dong", "Shi Han", "Dongmei Zhang"], "categories": ["cs.AI"], "comment": null, "summary": "Tables are a fundamental structure for organizing and analyzing data, making\neffective table understanding a critical capability for intelligent systems.\nWhile large language models (LMs) demonstrate strong general reasoning\nabilities, they continue to struggle with accurate numerical or symbolic\nreasoning over tabular data, especially in complex scenarios. Spreadsheet\nformulas provide a powerful and expressive medium for representing executable\nsymbolic operations, encoding rich reasoning patterns that remain largely\nunderutilized. In this paper, we propose Formula Tuning (Fortune), a\nreinforcement learning (RL) framework that trains LMs to generate executable\nspreadsheet formulas for question answering over general tabular data. Formula\nTuning reduces the reliance on supervised formula annotations by using binary\nanswer correctness as a reward signal, guiding the model to learn formula\nderivation through reasoning. We provide a theoretical analysis of its\nadvantages and demonstrate its effectiveness through extensive experiments on\nseven table reasoning benchmarks. Formula Tuning substantially enhances LM\nperformance, particularly on multi-step numerical and symbolic reasoning tasks,\nenabling a 7B model to outperform OpenAI o1 on table understanding. This\nhighlights the potential of formula-driven RL to advance symbolic table\nreasoning in LMs."}
{"id": "2505.23296", "title": "Is spreadsheet syntax better than numeric indexing for cell selection?", "url": "https://arxiv.org/abs/2505.23296", "pdf": "https://arxiv.org/pdf/2505.23296", "abs": "https://arxiv.org/abs/2505.23296", "authors": ["Philip Heltweg", "Dirk Riehle", "Georg-Daniel Schwarz"], "categories": ["cs.PL"], "comment": null, "summary": "Selecting a subset of cells is a common task in data engineering, for\nexample, to remove errors or select only specific parts of a table. Multiple\napproaches to express this selection exist. One option is numeric indexing,\ncommonly found in general programming languages, where a tuple of numbers\nidentifies the cell. Alternatively, the separate dimensions can be referred to\nusing different enumeration schemes like \"A1\" for the first cell, commonly\nfound in software such as spreadsheet systems.\n  In a large-scale controlled experiment with student participants as proxy for\ndata practitioners, we compare the two options with respect to speed and\ncorrectness of reading and writing code.\n  The results show that, when reading code, participants make less mistakes\nusing spreadsheet-style syntax. Additionally, when writing code, they make\nfewer mistakes and are faster when using spreadsheet syntax compared to numeric\nsyntax.\n  From this, a domain-specific syntax, such as spreadsheet syntax for data\nengineering, appears to be a promising alternative to explore in future tools\nto support practitioners without a software engineering background."}
{"id": "2504.20681", "title": "Data Encryption Battlefield: A Deep Dive into the Dynamic Confrontations in Ransomware Attacks", "url": "https://arxiv.org/abs/2504.20681", "pdf": "https://arxiv.org/pdf/2504.20681", "abs": "https://arxiv.org/abs/2504.20681", "authors": ["Arash Mahboubi", "Hamed Aboutorab", "Seyit Camtepe", "Hang Thanh Bui", "Khanh Luong", "Keyvan Ansari", "Shenlu Wang", "Bazara Barry"], "categories": ["cs.CR", "68M25"], "comment": null, "summary": "In the rapidly evolving landscape of cybersecurity threats, ransomware\nrepresents a significant challenge. Attackers increasingly employ sophisticated\nencryption methods, such as entropy reduction through Base64 encoding, and\npartial or intermittent encryption to evade traditional detection methods. This\nstudy explores the dynamic battle between adversaries who continuously refine\nencryption strategies and defenders developing advanced countermeasures to\nprotect vulnerable data. We investigate the application of online incremental\nmachine learning algorithms designed to predict file encryption activities\ndespite adversaries evolving obfuscation techniques. Our analysis utilizes an\nextensive dataset of 32.6 GB, comprising 11,928 files across multiple formats,\nincluding Microsoft Word documents (doc), PowerPoint presentations (ppt), Excel\nspreadsheets (xlsx), image formats (jpg, jpeg, png, tif, gif), PDFs (pdf),\naudio (mp3), and video (mp4) files. These files were encrypted by 75 distinct\nransomware families, facilitating a robust empirical evaluation of machine\nlearning classifiers effectiveness against diverse encryption tactics. Results\nhighlight the Hoeffding Tree algorithms superior incremental learning\ncapability, particularly effective in detecting traditional and AES-Base64\nencryption methods employed to lower entropy. Conversely, the Random Forest\nclassifier with warm-start functionality excels at identifying intermittent\nencryption methods, demonstrating the necessity of tailored machine learning\nsolutions to counter sophisticated ransomware strategies."}
{"id": "2504.20657", "title": "Image deidentification in the XNAT ecosystem: use cases and solutions", "url": "https://arxiv.org/abs/2504.20657", "pdf": "https://arxiv.org/pdf/2504.20657", "abs": "https://arxiv.org/abs/2504.20657", "authors": ["Alex Michie", "Simon J Doran"], "categories": ["cs.CV", "J.3"], "comment": "For submission to MELBA (Machine Learning for Biomedical Imaging)\n  special issue on the MIDI-B deidentification challenge\n  (https://www.synapse.org/Synapse:syn53065760). 11 pages, 1 fig, 2 tables; 1\n  supplementary data file (supplementary_tables_S1_S2_S3.xlsx) containing three\n  spreadsheet tabs", "summary": "XNAT is a server-based data management platform widely used in academia for\ncurating large databases of DICOM images for research projects. We describe in\ndetail a deidentification workflow for DICOM data using facilities in XNAT,\ntogether with independent tools in the XNAT \"ecosystem\". We list different\ncontexts in which deidentification might be needed, based on our prior\nexperience. The starting point for participation in the Medical Image\nDe-Identification Benchmark (MIDI-B) challenge was a set of pre-existing local\nmethodologies, which were adapted during the validation phase of the challenge.\nOur result in the test phase was 97.91\\%, considerably lower than our peers,\ndue largely to an arcane technical incompatibility of our methodology with the\nchallenge's Synapse platform, which prevented us receiving feedback during the\nvalidation phase. Post-submission, additional discrepancy reports from the\norganisers and via the MIDI-B Continuous Benchmarking facility, enabled us to\nimprove this score significantly to 99.61\\%. An entirely rule-based approach\nwas shown to be capable of removing all name-related information in the test\ncorpus, but exhibited failures in dealing fully with address data. Initial\nexperiments using published machine-learning models to remove addresses were\npartially successful but showed the models to be \"over-aggressive\" on other\ntypes of free-text data, leading to a slight overall degradation in performance\nto 99.54\\%. Future development will therefore focus on improving\naddress-recognition capabilities, but also on better removal of identifiable\ndata burned into the image pixels. Several technical aspects relating to the\n\"answer key\" are still under discussion with the challenge organisers, but we\nestimate that our percentage of genuine deidentification failures on the MIDI-B\ntest corpus currently stands at 0.19\\%. (Abridged from original for arXiv\nsubmission)"}
{"id": "2408.12622", "title": "The AI Risk Repository: A Comprehensive Meta-Review, Database, and Taxonomy of Risks From Artificial Intelligence", "url": "https://arxiv.org/abs/2408.12622", "pdf": "https://arxiv.org/pdf/2408.12622", "abs": "https://arxiv.org/abs/2408.12622", "authors": ["Peter Slattery", "Alexander K. Saeri", "Emily A. C. Grundy", "Jess Graham", "Michael Noetel", "Risto Uuk", "James Dao", "Soroush Pour", "Stephen Casper", "Neil Thompson"], "categories": ["cs.AI", "cs.CR", "cs.ET", "cs.LG", "cs.SY", "eess.SY", "I.2.0; K.4.1; K.4.1; K.4.2; K.4.3; K.6.0"], "comment": null, "summary": "The risks posed by Artificial Intelligence (AI) are of considerable concern\nto academics, auditors, policymakers, AI companies, and the public. However, a\nlack of shared understanding of AI risks can impede our ability to\ncomprehensively discuss, research, and react to them. This paper addresses this\ngap by creating an AI Risk Repository to serve as a common frame of reference.\nThis comprises a living database of 777 risks extracted from 43 taxonomies,\nwhich can be filtered based on two overarching taxonomies and easily accessed,\nmodified, and updated via our website and online spreadsheets. We construct our\nRepository with a systematic review of taxonomies and other structured\nclassifications of AI risk followed by an expert consultation. We develop our\ntaxonomies of AI risk using a best-fit framework synthesis. Our high-level\nCausal Taxonomy of AI Risks classifies each risk by its causal factors (1)\nEntity: Human, AI; (2) Intentionality: Intentional, Unintentional; and (3)\nTiming: Pre-deployment; Post-deployment. Our mid-level Domain Taxonomy of AI\nRisks classifies risks into seven AI risk domains: (1) Discrimination &\ntoxicity, (2) Privacy & security, (3) Misinformation, (4) Malicious actors &\nmisuse, (5) Human-computer interaction, (6) Socioeconomic & environmental, and\n(7) AI system safety, failures, & limitations. These are further divided into\n23 subdomains. The AI Risk Repository is, to our knowledge, the first attempt\nto rigorously curate, analyze, and extract AI risk frameworks into a publicly\naccessible, comprehensive, extensible, and categorized risk database. This\ncreates a foundation for a more coordinated, coherent, and complete approach to\ndefining, auditing, and managing the risks posed by AI systems."}
{"id": "2407.09025", "title": "SpreadsheetLLM: Encoding Spreadsheets for Large Language Models", "url": "https://arxiv.org/abs/2407.09025", "pdf": "https://arxiv.org/pdf/2407.09025", "abs": "https://arxiv.org/abs/2407.09025", "authors": ["Haoyu Dong", "Jianbo Zhao", "Yuzhang Tian", "Junyu Xiong", "Shiyu Xia", "Mengyu Zhou", "Yun Lin", "José Cambronero", "Yeye He", "Shi Han", "Dongmei Zhang"], "categories": ["cs.AI"], "comment": null, "summary": "Spreadsheets are characterized by their extensive two-dimensional grids,\nflexible layouts, and varied formatting options, which pose significant\nchallenges for large language models (LLMs). In response, we introduce\nSpreadsheetLLM, pioneering an efficient encoding method designed to unleash and\noptimize LLMs' powerful understanding and reasoning capability on spreadsheets.\nInitially, we propose a vanilla serialization approach that incorporates cell\naddresses, values, and formats. However, this approach was limited by LLMs'\ntoken constraints, making it impractical for most applications. To tackle this\nchallenge, we develop SheetCompressor, an innovative encoding framework that\ncompresses spreadsheets effectively for LLMs. It comprises three modules:\nstructural-anchor-based compression, inverse index translation, and\ndata-format-aware aggregation. It significantly improves performance in the\nspreadsheet table detection task, outperforming the vanilla approach by 25.6%\nin GPT4's in-context learning setting. Moreover, fine-tuned LLM with\nSheetCompressor has an average compression ratio of 25 times, and achieves a\nstate-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%.\nFinally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet\nunderstanding and validate it in a new and demanding spreadsheet QA task. We\nmethodically leverage the inherent layout and structure of spreadsheets,\ndemonstrating that SpreadsheetLLM is highly effective across a variety of\nspreadsheet tasks."}
{"id": "2403.03636", "title": "SheetAgent: Towards A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models", "url": "https://arxiv.org/abs/2403.03636", "pdf": "https://arxiv.org/pdf/2403.03636", "abs": "https://arxiv.org/abs/2403.03636", "authors": ["Yibin Chen", "Yifu Yuan", "Zeyu Zhang", "Yan Zheng", "Jinyi Liu", "Fei Ni", "Jianye Hao", "Hangyu Mao", "Fuzheng Zhang"], "categories": ["cs.AI", "cs.LG"], "comment": "Accepted by International World Wide Web Conference (WWW) 2025 (oral)", "summary": "Spreadsheets are ubiquitous across the World Wide Web, playing a critical\nrole in enhancing work efficiency across various domains. Large language model\n(LLM) has been recently attempted for automatic spreadsheet manipulation but\nhas not yet been investigated in complicated and realistic tasks where\nreasoning challenges exist (e.g., long horizon manipulation with multi-step\nreasoning and ambiguous requirements). To bridge the gap with the real-world\nrequirements, we introduce SheetRM, a benchmark featuring long-horizon and\nmulti-category tasks with reasoning-dependent manipulation caused by real-life\nchallenges. To mitigate the above challenges, we further propose SheetAgent, a\nnovel autonomous agent that utilizes the power of LLMs. SheetAgent consists of\nthree collaborative modules: Planner, Informer, and Retriever, achieving both\nadvanced reasoning and accurate manipulation over spreadsheets without human\ninteraction through iterative task reasoning and reflection. Extensive\nexperiments demonstrate that SheetAgent delivers 20--40\\% pass rate\nimprovements on multiple benchmarks over baselines, achieving enhanced\nprecision in spreadsheet manipulation and demonstrating superior table\nreasoning abilities. More details and visualizations are available at the\nproject website: https://sheetagent.github.io/. The datasets and source code\nare available at https://anonymous.4open.science/r/SheetAgent."}
{"id": "2403.19318", "title": "TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios", "url": "https://arxiv.org/abs/2403.19318", "pdf": "https://arxiv.org/pdf/2403.19318", "abs": "https://arxiv.org/abs/2403.19318", "authors": ["Xiaokang Zhang", "Sijia Luo", "Bohan Zhang", "Zeyao Ma", "Jing Zhang", "Yang Li", "Guanlin Li", "Zijun Yao", "Kangli Xu", "Jinchang Zhou", "Daniel Zhang-Li", "Jifan Yu", "Shu Zhao", "Juanzi Li", "Jie Tang"], "categories": ["cs.CL"], "comment": "https://tablellm.github.io/", "summary": "We introduce TableLLM, a robust large language model (LLM) with 8 billion\nparameters, purpose-built for proficiently handling tabular data manipulation\ntasks, whether they are embedded within documents or spreadsheets, catering to\nreal-world office scenarios. We propose a distant supervision method for\ntraining, which comprises a reasoning process extension strategy, aiding in\ntraining LLMs to understand reasoning patterns more effectively as well as a\ncross-way validation strategy, ensuring the quality of the automatically\ngenerated data. To evaluate the performance of TableLLM, we have crafted\nbenchmarks tailored to address both document and spreadsheet formats as well as\nconstructed a well-organized evaluation pipeline capable of handling both\nscenarios. Thorough evaluations underscore the advantages of TableLLM when\ncompared to various existing general-purpose and tabular data-focused LLMs. We\nhave publicly released the model checkpoint, source code, benchmarks, and a web\napplication for user interaction. Our codes and data are publicly available at\nhttps://github.com/TableLLM/TableLLM."}
{"id": "2502.11267", "title": "Prompting in the Dark: Assessing Human Performance in Prompt Engineering for Data Labeling When Gold Labels Are Absent", "url": "https://arxiv.org/abs/2502.11267", "pdf": "https://arxiv.org/pdf/2502.11267", "abs": "https://arxiv.org/abs/2502.11267", "authors": ["Zeyu He", "Saniya Naphade", "Ting-Hao 'Kenneth' Huang"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted By CHI 2025", "summary": "Millions of users prompt large language models (LLMs) for various tasks, but\nhow good are people at prompt engineering? Do users actually get closer to\ntheir desired outcome over multiple iterations of their prompts? These\nquestions are crucial when no gold-standard labels are available to measure\nprogress. This paper investigates a scenario in LLM-powered data labeling,\n\"prompting in the dark,\" where users iteratively prompt LLMs to label data\nwithout using manually-labeled benchmarks. We developed PromptingSheet, a\nGoogle Sheets add-on that enables users to compose, revise, and iteratively\nlabel data through spreadsheets. Through a study with 20 participants, we found\nthat prompting in the dark was highly unreliable -- only 9 participants\nimproved labeling accuracy after four or more iterations. Automated prompt\noptimization tools like DSPy also struggled when few gold labels were\navailable. Our findings highlight the importance of gold labels and the needs,\nas well as the risks, of automated support in human prompt engineering,\nproviding insights for future tool design."}
{"id": "2502.05113", "title": "GiesKaNe: Bridging Past and Present in Grammatical Theory and Practical Application", "url": "https://arxiv.org/abs/2502.05113", "pdf": "https://arxiv.org/pdf/2502.05113", "abs": "https://arxiv.org/abs/2502.05113", "authors": ["Volker Emmrich"], "categories": ["cs.CL"], "comment": null, "summary": "This article explores the requirements for corpus compilation within the\nGiesKaNe project (University of Giessen and Kassel, Syntactic Basic Structures\nof New High German). The project is defined by three central characteristics:\nit is a reference corpus, a historical corpus, and a syntactically deeply\nannotated treebank. As a historical corpus, GiesKaNe aims to establish\nconnections with both historical and contemporary corpora, ensuring its\nrelevance across temporal and linguistic contexts. The compilation process\nstrikes the balance between innovation and adherence to standards, addressing\nboth internal project goals and the broader interests of the research\ncommunity. The methodological complexity of such a project is managed through a\ncomplementary interplay of human expertise and machine-assisted processes. The\narticle discusses foundational topics such as tokenization, normalization,\nsentence definition, tagging, parsing, and inter-annotator agreement, alongside\nadvanced considerations. These include comparisons between grammatical models,\nannotation schemas, and established de facto annotation standards as well as\nthe integration of human and machine collaboration. Notably, a novel method for\nmachine-assisted classification of texts along the continuum of conceptual\norality and literacy is proposed, offering new perspectives on text selection.\nFurthermore, the article introduces an approach to deriving de facto standard\nannotations from existing ones, mediating between standardization and\ninnovation. In the course of describing the workflow the article demonstrates\nthat even ambitious projects like GiesKaNe can be effectively implemented using\nexisting research infrastructure, requiring no specialized annotation tools.\nInstead, it is shown that the workflow can be based on the strategic use of a\nsimple spreadsheet and integrates the capabilities of the existing\ninfrastructure."}
{"id": "2502.04389", "title": "Overcoming Vision Language Model Challenges in Diagram Understanding: A Proof-of-Concept with XML-Driven Large Language Models Solutions", "url": "https://arxiv.org/abs/2502.04389", "pdf": "https://arxiv.org/pdf/2502.04389", "abs": "https://arxiv.org/abs/2502.04389", "authors": ["Shue Shiinoki", "Ryo Koshihara", "Hayato Motegi", "Masumi Morishige"], "categories": ["cs.SE", "cs.AI"], "comment": "The related code is available at\n  \\url{https://github.com/galirage/spreadsheet-intelligence}, which provides\n  the core library developed for this research. The experimental code using\n  this library can be found at\n  \\url{https://github.com/galirage/XMLDriven-Diagram-Understanding}", "summary": "Diagrams play a crucial role in visually conveying complex relationships and\nprocesses within business documentation. Despite recent advances in\nVision-Language Models (VLMs) for various image understanding tasks, accurately\nidentifying and extracting the structures and relationships depicted in\ndiagrams continues to pose significant challenges. This study addresses these\nchallenges by proposing a text-driven approach that bypasses reliance on VLMs'\nvisual recognition capabilities. Instead, it utilizes the editable source\nfiles--such as xlsx, pptx or docx--where diagram elements (e.g., shapes, lines,\nannotations) are preserved as textual metadata. In our proof-of-concept, we\nextracted diagram information from xlsx-based system design documents and\ntransformed the extracted shape data into textual input for Large Language\nModels (LLMs). This approach allowed the LLM to analyze relationships and\ngenerate responses to business-oriented questions without the bottleneck of\nimage-based processing. Experimental comparisons with a VLM-based method\ndemonstrated that the proposed text-driven framework yielded more accurate\nanswers for questions requiring detailed comprehension of diagram\nstructures.The results obtained in this study are not limited to the tested\n.xlsx files but can also be extended to diagrams in other documents with source\nfiles, such as Office pptx and docx formats. These findings highlight the\nfeasibility of circumventing VLM constraints through direct textual extraction\nfrom original source files. By enabling robust diagram understanding through\nLLMs, our method offers a promising path toward enhanced workflow efficiency\nand information analysis in real-world business scenarios."}
{"id": "2501.18268", "title": "Reducing Aleatoric and Epistemic Uncertainty through Multi-modal Data Acquisition", "url": "https://arxiv.org/abs/2501.18268", "pdf": "https://arxiv.org/pdf/2501.18268", "abs": "https://arxiv.org/abs/2501.18268", "authors": ["Arthur Hoarau", "Benjamin Quost", "Sébastien Destercke", "Willem Waegeman"], "categories": ["cs.LG"], "comment": null, "summary": "To generate accurate and reliable predictions, modern AI systems need to\ncombine data from multiple modalities, such as text, images, audio,\nspreadsheets, and time series. Multi-modal data introduces new opportunities\nand challenges for disentangling uncertainty: it is commonly assumed in the\nmachine learning community that epistemic uncertainty can be reduced by\ncollecting more data, while aleatoric uncertainty is irreducible. However, this\nassumption is challenged in modern AI systems when information is obtained from\ndifferent modalities. This paper introduces an innovative data acquisition\nframework where uncertainty disentanglement leads to actionable decisions,\nallowing sampling in two directions: sample size and data modality. The main\nhypothesis is that aleatoric uncertainty decreases as the number of modalities\nincreases, while epistemic uncertainty decreases by collecting more\nobservations. We provide proof-of-concept implementations on two multi-modal\ndatasets to showcase our data acquisition framework, which combines ideas from\nactive learning, active feature acquisition and uncertainty quantification."}
{"id": "2407.04065", "title": "On the Workflows and Smells of Leaderboard Operations (LBOps): An Exploratory Study of Foundation Model Leaderboards", "url": "https://arxiv.org/abs/2407.04065", "pdf": "https://arxiv.org/pdf/2407.04065", "abs": "https://arxiv.org/abs/2407.04065", "authors": ["Zhimin Zhao", "Abdul Ali Bangash", "Filipe Roseiro Côgo", "Bram Adams", "Ahmed E. Hassan"], "categories": ["cs.SE", "cs.LG"], "comment": "Awesome Foundation Model Leaderboard List:\n  https://github.com/SAILResearch/awesome-foundation-model-leaderboards;\n  Foundation Model Leaderboard Search Toolkit:\n  https://huggingface.co/spaces/zhiminy/awesome-foundation-model-leaderboard-search", "summary": "Foundation models (FM), such as large language models (LLMs), which are\nlarge-scale machine learning (ML) models, have demonstrated remarkable\nadaptability in various downstream software engineering (SE) tasks, such as\ncode completion, code understanding, and software development. As a result, FM\nleaderboards have become essential tools for SE teams to compare and select the\nbest third-party FMs for their specific products and purposes. However, the\nlack of standardized guidelines for FM evaluation and comparison threatens the\ntransparency of FM leaderboards and limits stakeholders' ability to perform\neffective FM selection. As a first step towards addressing this challenge, our\nresearch focuses on understanding how these FM leaderboards operate in\nreal-world scenarios (\"leaderboard operations\") and identifying potential\npitfalls and areas for improvement (\"leaderboard smells\"). In this regard, we\ncollect up to 1,045 FM leaderboards from five different sources: GitHub,\nHugging Face Spaces, Papers With Code, spreadsheet and independent platform, to\nexamine their documentation and engage in direct communication with leaderboard\noperators to understand their workflows. Through card sorting and negotiated\nagreement, we identify five distinct workflow patterns and develop a domain\nmodel that captures the key components and their interactions within these\nworkflows. We then identify eight unique types of leaderboard smells in LBOps.\nBy mitigating these smells, SE teams can improve transparency, accountability,\nand collaboration in current LBOps practices, fostering a more robust and\nresponsible ecosystem for FM comparison and selection."}
{"id": "2412.11711", "title": "MiMoTable: A Multi-scale Spreadsheet Benchmark with Meta Operations for Table Reasoning", "url": "https://arxiv.org/abs/2412.11711", "pdf": "https://arxiv.org/pdf/2412.11711", "abs": "https://arxiv.org/abs/2412.11711", "authors": ["Zheng Li", "Yang Du", "Mao Zheng", "Mingyang Song"], "categories": ["cs.CL"], "comment": "Accepted by COLING 2025", "summary": "Extensive research has been conducted to explore the capability of Large\nLanguage Models (LLMs) for table reasoning and has significantly improved the\nperformance on existing benchmarks. However, tables and user questions in\nreal-world applications are more complex and diverse, presenting an unignorable\ngap compared to the existing benchmarks. To fill the gap, we propose a\n\\textbf{M}ult\\textbf{i}-scale spreadsheet benchmark with \\textbf{M}eta\n\\textbf{o}perations for \\textbf{Table} reasoning, named as MiMoTable.\nSpecifically, MiMoTable incorporates two key features. First, the tables in\nMiMoTable are all spreadsheets used in real-world scenarios, which cover seven\ndomains and contain different types. Second, we define a new criterion with six\ncategories of meta operations for measuring the difficulty of each question in\nMiMoTable, simultaneously as a new perspective for measuring the difficulty of\nthe existing benchmarks. Experimental results show that Claude-3.5-Sonnet\nachieves the best performance with 77.4\\% accuracy, indicating that there is\nstill significant room to improve for LLMs on MiMoTable. Furthermore, we grade\nthe difficulty of existing benchmarks according to our new criteria.\nExperiments have shown that the performance of LLMs decreases as the difficulty\nof benchmarks increases, thereby proving the effectiveness of our proposed new\ncriterion."}
{"id": "2412.15030", "title": "When Copilot Becomes Autopilot: Generative AI's Critical Risk to Knowledge Work and a Critical Solution", "url": "https://arxiv.org/abs/2412.15030", "pdf": "https://arxiv.org/pdf/2412.15030", "abs": "https://arxiv.org/abs/2412.15030", "authors": ["Advait Sarkar", "Xiaotong", "Xu", "Neil Toronto", "Ian Drosos", "Christian Poelitz"], "categories": ["cs.HC"], "comment": null, "summary": "Generative AI, with its tendency to \"hallucinate\" incorrect results, may pose\na risk to knowledge work by introducing errors. On the other hand, it may also\nprovide unprecedented opportunities for users, particularly non-experts, to\nlearn and apply advanced software features and greatly increase the scope and\ncomplexity of tasks they can successfully achieve.\n  As an example of a complex knowledge workflow that is subject to risks and\nopportunities from generative AI, we consider the spreadsheet. AI\nhallucinations are an important challenge, but they are not the greatest risk\nposed by generative AI to spreadsheet workflows. Rather, as more work can be\nsafely delegated to AI, the risk is that human critical thinking -- the ability\nto holistically and rigorously evaluate a problem and its solutions -- is\ndegraded in the process. The solution is to design the interfaces of generative\nAI systems deliberately to foster and encourage critical thinking in knowledge\nwork, building primarily on a long history of research on critical thinking\ntools for education.\n  We discuss a prototype system for the activity of critical shortlisting in\nspreadsheets. The system uses generative AI to suggest shortlisting criteria\nand applies these criteria to sort rows in a spreadsheet. It also generates\n\"provocations\": short text snippets that critique the AI-generated criteria,\nhighlighting risks, shortcomings, and alternatives. Our prototype opens up a\nrich and completely unexplored design space of critical thinking tools for\nmodern AI-assisted knowledge work. We outline a research agenda for AI as a\ncritic or provocateur, including questions about where and when provocations\nshould appear, their form and content, and potential design trade-offs."}
{"id": "2412.14062", "title": "Understanding and Evaluating Trust in Generative AI and Large Language Models for Spreadsheets", "url": "https://arxiv.org/abs/2412.14062", "pdf": "https://arxiv.org/pdf/2412.14062", "abs": "https://arxiv.org/abs/2412.14062", "authors": ["Simon Thorne"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Generative AI and Large Language Models (LLMs) hold promise for automating\nspreadsheet formula creation. However, due to hallucinations, bias and variable\nuser skill, outputs obtained from generative AI cannot be assumed to be\naccurate or trustworthy. To address these challenges, a trustworthiness\nframework is proposed based on evaluating the transparency and dependability of\nthe formula. The transparency of the formula is explored through explainability\n(understanding the formula's reasoning) and visibility (inspecting the\nunderlying algorithms). The dependability of the generated formula is evaluated\nin terms of reliability (consistency and accuracy) and ethical considerations\n(bias and fairness). The paper also examines the drivers to these metrics in\nthe form of hallucinations, training data bias and poorly constructed prompts.\nFinally, examples of mistrust in technology are considered and the consequences\nexplored."}
{"id": "2412.02357", "title": "Dynamic Prompt Middleware: Contextual Prompt Refinement Controls for Comprehension Tasks", "url": "https://arxiv.org/abs/2412.02357", "pdf": "https://arxiv.org/pdf/2412.02357", "abs": "https://arxiv.org/abs/2412.02357", "authors": ["Ian Drosos", "Jack Williams", "Advait Sarkar", "Nicholas Wilson"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Effective prompting of generative AI is challenging for many users,\nparticularly in expressing context for comprehension tasks such as explaining\nspreadsheet formulas, Python code, and text passages. Prompt middleware aims to\naddress this barrier by assisting in prompt construction, but barriers remain\nfor users in expressing adequate control so that they can receive AI-responses\nthat match their preferences.\n  We conduct a formative survey (n=38) investigating user needs for control\nover AI-generated explanations in comprehension tasks, which uncovers a\ntrade-off between standardized but predictable support for prompting, and\nadaptive but unpredictable support tailored to the user and task. To explore\nthis trade-off, we implement two prompt middleware approaches: Dynamic Prompt\nRefinement Control (Dynamic PRC) and Static Prompt Refinement Control (Static\nPRC). The Dynamic PRC approach generates context-specific UI elements that\nprovide prompt refinements based on the user's prompt and user needs from the\nAI, while the Static PRC approach offers a preset list of generally applicable\nrefinements.\n  We evaluate these two approaches with a controlled user study (n=16) to\nassess the impact of these approaches on user control of AI responses for\ncrafting better explanations. Results show a preference for the Dynamic PRC\napproach as it afforded more control, lowered barriers to providing context,\nand encouraged exploration and reflection of the tasks, but that reasoning\nabout the effects of different generated controls on the final output remains\nchallenging. Drawing on participant feedback, we discuss design implications\nfor future Dynamic PRC systems that enhance user control of AI responses. Our\nfindings suggest that dynamic prompt middleware can improve the user experience\nof generative AI workflows by affording greater control and guide users to a\nbetter AI response."}
{"id": "2406.12031", "title": "Large Scale Transfer Learning for Tabular Data via Language Modeling", "url": "https://arxiv.org/abs/2406.12031", "pdf": "https://arxiv.org/pdf/2406.12031", "abs": "https://arxiv.org/abs/2406.12031", "authors": ["Josh Gardner", "Juan C. Perdomo", "Ludwig Schmidt"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "NeurIPS 2024 camera-ready updates", "summary": "Tabular data -- structured, heterogeneous, spreadsheet-style data with rows\nand columns -- is widely used in practice across many domains. However, while\nrecent foundation models have reduced the need for developing task-specific\ndatasets and predictors in domains such as language modeling and computer\nvision, this transfer learning paradigm has not had similar impact in the\ntabular domain. In this work, we seek to narrow this gap and present TabuLa-8B,\na language model for tabular prediction. We define a process for extracting a\nlarge, high-quality training dataset from the TabLib corpus, proposing methods\nfor tabular data filtering and quality control. Using the resulting dataset,\nwhich comprises over 2.1B rows from over 4M unique tables, we fine-tune a Llama\n3-8B large language model (LLM) for tabular data prediction (classification and\nbinned regression) using a novel packing and attention scheme for tabular\nprediction. Through evaluation across a test suite of 329 datasets, we find\nthat TabuLa-8B has zero-shot accuracy on unseen tables that is over 15\npercentage points (pp) higher than random guessing, a feat that is not possible\nwith existing state-of-the-art tabular prediction models (e.g. XGBoost,\nTabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the\ntarget datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN\nmodels that are explicitly trained on equal, or even up to 16x more data. We\nrelease our model, code, and data along with the publication of this paper."}
{"id": "2402.05121", "title": "Large Language Model for Table Processing: A Survey", "url": "https://arxiv.org/abs/2402.05121", "pdf": "https://arxiv.org/pdf/2402.05121", "abs": "https://arxiv.org/abs/2402.05121", "authors": ["Weizheng Lu", "Jing Zhang", "Ju Fan", "Zihao Fu", "Yueguo Chen", "Xiaoyong Du"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Tables, typically two-dimensional and structured to store large amounts of\ndata, are essential in daily activities like database queries, spreadsheet\nmanipulations, web table question answering, and image table information\nextraction. Automating these table-centric tasks with Large Language Models\n(LLMs) or Visual Language Models (VLMs) offers significant public benefits,\ngarnering interest from academia and industry. This survey provides a\ncomprehensive overview of table-related tasks, examining both user scenarios\nand technical aspects. It covers traditional tasks like table question\nanswering as well as emerging fields such as spreadsheet manipulation and table\ndata analysis. We summarize the training techniques for LLMs and VLMs tailored\nfor table processing. Additionally, we discuss prompt engineering, particularly\nthe use of LLM-powered agents, for various table-related tasks. Finally, we\nhighlight several challenges, including diverse user input when serving and\nslow thinking using chain-of-thought."}
{"id": "2406.14991", "title": "SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation", "url": "https://arxiv.org/abs/2406.14991", "pdf": "https://arxiv.org/pdf/2406.14991", "abs": "https://arxiv.org/abs/2406.14991", "authors": ["Zeyao Ma", "Bohan Zhang", "Jing Zhang", "Jifan Yu", "Xiaokang Zhang", "Xiaohan Zhang", "Sijia Luo", "Xi Wang", "Jie Tang"], "categories": ["cs.CL", "cs.SE"], "comment": "Neurips 2024 (Spotlight); Homepage:\n  https://spreadsheetbench.github.io/", "summary": "We introduce SpreadsheetBench, a challenging spreadsheet manipulation\nbenchmark exclusively derived from real-world scenarios, designed to immerse\ncurrent large language models (LLMs) in the actual workflow of spreadsheet\nusers. Unlike existing benchmarks that rely on synthesized queries and\nsimplified spreadsheet files, SpreadsheetBench is built from 912 real questions\ngathered from online Excel forums, which reflect the intricate needs of users.\nThe associated spreadsheets from the forums contain a variety of tabular data\nsuch as multiple tables, non-standard relational tables, and abundant\nnon-textual elements. Furthermore, we propose a more reliable evaluation metric\nakin to online judge platforms, where multiple spreadsheet files are created as\ntest cases for each instruction, ensuring the evaluation of robust solutions\ncapable of handling spreadsheets with varying values. Our comprehensive\nevaluation of various LLMs under both single-round and multi-round inference\nsettings reveals a substantial gap between the state-of-the-art (SOTA) models\nand human performance, highlighting the benchmark's difficulty."}
{"id": "2409.20224", "title": "Trapped in Transformative Agreements? A Multifaceted Analysis of >1,000 Contracts", "url": "https://arxiv.org/abs/2409.20224", "pdf": "https://arxiv.org/pdf/2409.20224", "abs": "https://arxiv.org/abs/2409.20224", "authors": ["Laura Rothfritz", "W. Benedikt Schmal", "Ulrich Herb"], "categories": ["cs.DL"], "comment": "37 pages, appendix", "summary": "Transformative agreements between academic publishers and research\ninstitutions are ubiquitous. The 'Efficiency and Standards for Article Charges'\n(ESAC) Initiative lists more than 1,000 contracts in its database. We make use\nof this unique dataset by web-scraping the details of every contract to\nsubstantially expand the overview spreadsheet provided by the ESAC Initiative.\nBased on that hitherto unused data source, we combine qualitative and\nquantitative methods to conduct an in-depth analysis of the contract\ncharacteristics and the TA landscape. Our analysis demonstrates that research\ninstitutions seem to be 'trapped' in transformative agreements. Instead of\nbeing a bridge towards a fully Open Access world, academia is stuck in the\nhybrid system. This endows the legacy (non-Open Access) publishing houses with\nsubstantial market power. It raises entry barriers, lowers competition, and\nincreases costs for libraries and universities."}
{"id": "2409.08897", "title": "Ensuring Adherence to Standards in Experiment-Related Metadata Entered Via Spreadsheets", "url": "https://arxiv.org/abs/2409.08897", "pdf": "https://arxiv.org/pdf/2409.08897", "abs": "https://arxiv.org/abs/2409.08897", "authors": ["Martin J. O'Connor", "Josef Hardi", "Marcos Martínez-Romero", "Sowmya Somasundaram", "Brendan Honick", "Stephen A. Fisher", "Ajay Pillai", "Mark A. Musen"], "categories": ["cs.DL"], "comment": null, "summary": "Scientists increasingly recognize the importance of providing rich,\nstandards-adherent metadata to describe their experimental results. Despite the\navailability of sophisticated tools to assist in the process of data\nannotation, investigators generally seem to prefer to use spreadsheets when\nsupplying metadata, despite the limitations of spreadsheets in ensuring\nmetadata consistency and compliance with formal specifications. In this paper,\nwe describe an end-to-end approach that supports spreadsheet-based entry of\nmetadata, while ensuring rigorous adherence to community-based metadata\nstandards and providing quality control. Our methods employ several key\ncomponents, including customizable templates that capture metadata standards\nand that can inform the spreadsheets that investigators use to author metadata,\ncontrolled terminologies and ontologies for defining metadata values that can\nbe accessed directly from a spreadsheet, and an interactive Web-based tool that\nallows users to rapidly identify and fix errors in their spreadsheet-based\nmetadata. We demonstrate how this approach is being deployed in a biomedical\nconsortium known as HuBMAP to define and collect metadata about a wide range of\nbiological assays."}
{"id": "2409.05735", "title": "A System and Benchmark for LLM-based Q&A on Heterogeneous Data", "url": "https://arxiv.org/abs/2409.05735", "pdf": "https://arxiv.org/pdf/2409.05735", "abs": "https://arxiv.org/abs/2409.05735", "authors": ["Achille Fokoue", "Srideepika Jayaraman", "Elham Khabiri", "Jeffrey O. Kephart", "Yingjie Li", "Dhruv Shah", "Youssef Drissi", "Fenno F. Heath III", "Anu Bhamidipaty", "Fateh A. Tipu", "Robert J. Baseman"], "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "In many industrial settings, users wish to ask questions whose answers may be\nfound in structured data sources such as a spreadsheets, databases, APIs, or\ncombinations thereof. Often, the user doesn't know how to identify or access\nthe right data source. This problem is compounded even further if multiple (and\npotentially siloed) data sources must be assembled to derive the answer.\nRecently, various Text-to-SQL applications that leverage Large Language Models\n(LLMs) have addressed some of these problems by enabling users to ask questions\nin natural language. However, these applications remain impractical in\nrealistic industrial settings because they fail to cope with the data source\nheterogeneity that typifies such environments. In this paper, we address\nheterogeneity by introducing the siwarex platform, which enables seamless\nnatural language access to both databases and APIs. To demonstrate the\neffectiveness of siwarex, we extend the popular Spider dataset and benchmark by\nreplacing some of its tables by data retrieval APIs. We find that siwarex does\na good job of coping with data source heterogeneity. Our modified Spider\nbenchmark will soon be available to the research community"}
{"id": "2409.01517", "title": "Auditable and reusable crosswalks for fast, scaled integration of scattered tabular data", "url": "https://arxiv.org/abs/2409.01517", "pdf": "https://arxiv.org/pdf/2409.01517", "abs": "https://arxiv.org/abs/2409.01517", "authors": ["Gavin Chait"], "categories": ["cs.DB"], "comment": "14 pages, 12 colour figures", "summary": "This paper presents an open-source curatorial toolkit intended to produce\nwell-structured and interoperable data. Curation is divided into discrete\ncomponents, with a schema-centric focus for auditable restructuring of complex\nand scattered tabular data to conform to a destination schema. Task separation\nallows development of software and analysis without source data being present.\nTransformations are captured as high-level sequential scripts describing\nschema-to-schema mappings, reducing complexity and resource requirements.\nUltimately, data are transformed, but the objective is that any data meeting a\nschema definition can be restructured using a crosswalk. The toolkit is\navailable both as a Python package, and as a 'no-code' visual web application.\nA visual example is presented, derived from a longitudinal study where\nscattered source data from hundreds of local councils are integrated into a\nsingle database."}
{"id": "2409.12976", "title": "Excel: Automated Ledger or Analytics IDE?", "url": "https://arxiv.org/abs/2409.12976", "pdf": "https://arxiv.org/pdf/2409.12976", "abs": "https://arxiv.org/abs/2409.12976", "authors": ["Andrew Kumiega"], "categories": ["cs.CY"], "comment": "9 pages, one table", "summary": "Since the inception of VisiCalc over four decades ago, spreadsheets have\nundergone a gradual transformation, evolving from simple ledger automation\ntools to the current state of Excel, which can be described as an Integrated\nDevelopment Environment (IDE) for analytics. The slow evolution of Excel from\nan automation tool for ledgers to an IDE for analytics explains why many people\nhave not noticed that Excel includes a fully functional database, an OLAP\nEngine, multiple statistical programming languages, multiple third-party\nsoftware libraries, dynamic charts, and real time data connectors. The\nsimplicity of accessing these multiple tools is a low-code framework controlled\nfrom the Excel tool that is effectively an IDE. Once we acknowledge Excel's\nshift from a desk top application to an IDE for analytics, the importance of\nestablishing a comprehensive risk framework for managing this distinctive\ndevelopment environment becomes clear. In this paper we will explain how the\ncurrent risk framework for spreadsheets needs to be expanded to manage the\ngrowing risks of using Excel as an IDE for analytics."}
{"id": "2409.12975", "title": "Subject integration with spreadsheets -- Ignoring education is the greatest risk ever", "url": "https://arxiv.org/abs/2409.12975", "pdf": "https://arxiv.org/pdf/2409.12975", "abs": "https://arxiv.org/abs/2409.12975", "authors": ["Mária Csernoch", "Ádám Gulácsi", "Júlia Csernoch"], "categories": ["cs.HC"], "comment": "16 pages, 17 colour figures. Proceedings of the EuSpRIG 2024\n  Conference 'Spreadsheet Productivity & Risks' ISBN : 978-1-905404-59-9", "summary": "Within the framework of Technological Pedagogical and Content Knowledge,\nsubject integration is one possible solution for the introduction of meaningful\ndigitalization and digitization in schools. This process incorporates that any\nschool subject can be taught with digital support, informatics (computer)\nclasses can be contextualized, and the gap between 'serious informatics' and\n'digital literacy' can be minimized. The present paper details how three\ntraditional Grade 3 tasks can be solved in spreadsheets, what skills,\ncompetencies, and computer science knowledge of both teachers and students can\nbe developed. The solutions also reveal that analysing, understanding,\nplanning, and discussing tasks is as important as the activity in the\nspreadsheets, which process plays a crucial role in the preparation of students\nfor their future jobs."}
{"id": "2409.12974", "title": "Exploring Higher Education Competencies through Spreadsheet Self-Assessment and Time", "url": "https://arxiv.org/abs/2409.12974", "pdf": "https://arxiv.org/pdf/2409.12974", "abs": "https://arxiv.org/abs/2409.12974", "authors": ["Maria Csernoch", "Judit T. Kiss", "Viktor Takács", "Domicián Máté"], "categories": ["cs.HC"], "comment": "16 pages, 10 colour figures, 9 tables", "summary": "The present paper aims to explore higher education students' spreadsheet\ncompetencies and reliability through self-assessment and real-world\nproblem-solving practices. Digital natives alleged skills and competences\nallowed us to hypothesize that students perform better in Excel than on paper,\nbut the findings cannot confirm this hypothesis. However, our results indicate\nthat students tend to inaccurately assess their spreadsheet competencies\ncompared to their actual performance in both paper-based and Excel tasks. It\nhas also be found that students need at least twice as much time to achieve the\nsame high scores in the digital environment as they do on paper. The results\nviolated the widely accepted assumption that digital native students do not\nneed computer science education, since they are born with it. This study\nhighlights the importance of accurate self-assessment in digital skill\ndevelopment and time management within higher education contexts, particularly\nin technology-driven disciplines."}
{"id": "2405.16234", "title": "Vision Language Models for Spreadsheet Understanding: Challenges and Opportunities", "url": "https://arxiv.org/abs/2405.16234", "pdf": "https://arxiv.org/pdf/2405.16234", "abs": "https://arxiv.org/abs/2405.16234", "authors": ["Shiyu Xia", "Junyu Xiong", "Haoyu Dong", "Jianbo Zhao", "Yuzhang Tian", "Mengyu Zhou", "Yeye He", "Shi Han", "Dongmei Zhang"], "categories": ["cs.CV"], "comment": null, "summary": "This paper explores capabilities of Vision Language Models on spreadsheet\ncomprehension. We propose three self-supervised challenges with corresponding\nevaluation metrics to comprehensively evaluate VLMs on Optical Character\nRecognition (OCR), spatial perception, and visual format recognition.\nAdditionally, we utilize the spreadsheet table detection task to assess the\noverall performance of VLMs by integrating these challenges. To probe VLMs more\nfinely, we propose three spreadsheet-to-image settings: column width\nadjustment, style change, and address augmentation. We propose variants of\nprompts to address the above tasks in different settings. Notably, to leverage\nthe strengths of VLMs in understanding text rather than two-dimensional\npositioning, we propose to decode cell values on the four boundaries of the\ntable in spreadsheet boundary detection. Our findings reveal that VLMs\ndemonstrate promising OCR capabilities but produce unsatisfactory results due\nto cell omission and misalignment, and they notably exhibit insufficient\nspatial and format recognition skills, motivating future work to enhance VLMs'\nspreadsheet data comprehension capabilities using our methods to generate\nextensive spreadsheet-image pairs in various settings."}
{"id": "2408.03841", "title": "MaxMind: A Memory Loop Network to Enhance Software Productivity based on Large Language Models", "url": "https://arxiv.org/abs/2408.03841", "pdf": "https://arxiv.org/pdf/2408.03841", "abs": "https://arxiv.org/abs/2408.03841", "authors": ["Yuchen Dong", "XiaoXiang Fang", "Yuchen Hu", "Renshuang Jiang", "Zhe Jiang"], "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "The application of large language models to facilitate automated software\noperations and tool generation (SOTG), thus augmenting software productivity,\nmirrors the early stages of human evolution when the ability to create and use\ntools accelerated the progress of civilization. These complex tasks require AI\nto continuously summarize and improve. Current research often overlooks the\nimportance of converting real-time task experiences into system memory and\ndifferentiating the value of existing knowledge for future reference. This\npaper addresses these issues by evolving external memory models into\nMemory-Loop Networks for timely memorization and experience referencing. We\nalso enhance a RAG mechanism with knowledge precision segmentation to utilize\nmemory based on value differentiation, and design the MaxMind model for SOTG\naccordingly.To demonstrate our approach, we developed MaxMind4Sheet, an\nelectronic spreadsheet processing system aligned with the MaxMind philosophy.\nComparative experiments with SheetCopilot have demonstrated that the\naccumulation and recycling of task memories lead to a steady enhancement in\ntask success rate, with an improvement rate of approximately 3%-6% per round in\nthis implementation example. Note that as the memories continue to grow, this\ncumulative improvement may be substantial. The inclusion of memory recycling\ncan also boost the system's task execution efficiency by up to 25%, and it can\naddress the retraining issue faced by LLMs when handling specialized tasks\nthrough memories transfer.These suggest that MaxMind has significant potential\nto enhance the capabilities and productivity of LLM systems in SOTG."}
{"id": "2408.01805", "title": "Billion-files File Systems (BfFS): A Comparison", "url": "https://arxiv.org/abs/2408.01805", "pdf": "https://arxiv.org/pdf/2408.01805", "abs": "https://arxiv.org/abs/2408.01805", "authors": ["Sohail Shaikh"], "categories": ["cs.PF"], "comment": "Paper is 9 pages. Source code is open and uploaded to Git, along with\n  an Excel spreadsheet used for analysis", "summary": "As the volume of data being produced is increasing at an exponential rate\nthat needs to be processed quickly, it is reasonable that the data needs to be\navailable very close to the compute devices to reduce transfer latency. Due to\nthis need, local filesystems are getting close attention to understand their\ninner workings, performance, and more importantly their limitations. This study\nanalyzes few popular Linux filesystems: EXT4, XFS, BtrFS, ZFS, and F2FS by\ncreating, storing, and then reading back one billion files from the local\nfilesystem. The study also captured and analyzed read/write throughput, storage\nblocks usage, disk space utilization and overheads, and other metrics useful\nfor system designers and integrators. Furthermore, the study explored other\nside effects such as filesystem performance degradation during and after these\nlarge numbers of files and folders are created."}
{"id": "2407.14042", "title": "Data Guards: Challenges and Solutions for Fostering Trust in Data", "url": "https://arxiv.org/abs/2407.14042", "pdf": "https://arxiv.org/pdf/2407.14042", "abs": "https://arxiv.org/abs/2407.14042", "authors": ["Nicole Sultanum", "Dennis Bromley", "Michael Correll"], "categories": ["cs.HC"], "comment": "VIS 2024 Short paper - 5 pages", "summary": "From dirty data to intentional deception, there are many threats to the\nvalidity of data-driven decisions. Making use of data, especially new or\nunfamiliar data, therefore requires a degree of trust or verification. How is\nthis trust established? In this paper, we present the results of a series of\ninterviews with both producers and consumers of data artifacts (outputs of data\necosystems like spreadsheets, charts, and dashboards) aimed at understanding\nstrategies and obstacles to building trust in data. We find a recurring need,\nbut lack of existing standards, for data validation and verification,\nespecially among data consumers. We therefore propose a set of data guards:\nmethods and tools for fostering trust in data artifacts."}
{"id": "2407.06354", "title": "High-Throughput Phenotyping using Computer Vision and Machine Learning", "url": "https://arxiv.org/abs/2407.06354", "pdf": "https://arxiv.org/pdf/2407.06354", "abs": "https://arxiv.org/abs/2407.06354", "authors": ["Vivaan Singhvi", "Langalibalele Lunga", "Pragya Nidhi", "Chris Keum", "Varrun Prakash"], "categories": ["cs.CV", "I.4.6; I.4.7"], "comment": "Presented for the Smoky Mountains Computational Sciences and\n  Engineering Conference: Best Paper Award", "summary": "High-throughput phenotyping refers to the non-destructive and efficient\nevaluation of plant phenotypes. In recent years, it has been coupled with\nmachine learning in order to improve the process of phenotyping plants by\nincreasing efficiency in handling large datasets and developing methods for the\nextraction of specific traits. Previous studies have developed methods to\nadvance these challenges through the application of deep neural networks in\ntandem with automated cameras; however, the datasets being studied often\nexcluded physical labels. In this study, we used a dataset provided by Oak\nRidge National Laboratory with 1,672 images of Populus Trichocarpa with white\nlabels displaying treatment (control or drought), block, row, position, and\ngenotype. Optical character recognition (OCR) was used to read these labels on\nthe plants, image segmentation techniques in conjunction with machine learning\nalgorithms were used for morphological classifications, machine learning models\nwere used to predict treatment based on those classifications, and analyzed\nencoded EXIF tags were used for the purpose of finding leaf size and\ncorrelations between phenotypes. We found that our OCR model had an accuracy of\n94.31% for non-null text extractions, allowing for the information to be\naccurately placed in a spreadsheet. Our classification models identified leaf\nshape, color, and level of brown splotches with an average accuracy of 62.82%,\nand plant treatment with an accuracy of 60.08%. Finally, we identified a few\ncrucial pieces of information absent from the EXIF tags that prevented the\nassessment of the leaf size. There was also missing information that prevented\nthe assessment of correlations between phenotypes and conditions. However,\nfuture studies could improve upon this to allow for the assessment of these\nfeatures."}
{"id": "2405.05292", "title": "Smart Portable Computer", "url": "https://arxiv.org/abs/2405.05292", "pdf": "https://arxiv.org/pdf/2405.05292", "abs": "https://arxiv.org/abs/2405.05292", "authors": ["Niladri Das"], "categories": ["cs.HC", "cs.AI", "cs.RO"], "comment": "34 pages", "summary": "Amidst the COVID-19 pandemic, with many organizations, schools, colleges, and\nuniversities transitioning to virtual platforms, students encountered\ndifficulties in acquiring PCs such as desktops or laptops. The starting prices,\naround 15,000 INR, often failed to offer adequate system specifications, posing\na challenge for consumers. Additionally, those reliant on laptops for work\nfound the conventional approach cumbersome. Enter the \"Portable Smart\nComputer,\" a leap into the future of computing. This innovative device boasts\nspeed and performance comparable to traditional desktops but in a compact,\nenergy-efficient, and cost-effective package. It delivers a seamless desktop\nexperience, whether one is editing documents, browsing multiple tabs, managing\nspreadsheets, or creating presentations. Moreover, it supports programming\nlanguages like Python, C, C++, as well as compilers such as Keil and Xilinx,\ncatering to the needs of programmers."}
{"id": "2404.12608", "title": "Auto-Formula: Recommend Formulas in Spreadsheets using Contrastive Learning for Table Representations", "url": "https://arxiv.org/abs/2404.12608", "pdf": "https://arxiv.org/pdf/2404.12608", "abs": "https://arxiv.org/abs/2404.12608", "authors": ["Sibei Chen", "Yeye He", "Weiwei Cui", "Ju Fan", "Song Ge", "Haidong Zhang", "Dongmei Zhang", "Surajit Chaudhuri"], "categories": ["cs.DB", "cs.CL", "cs.PL"], "comment": "full version of a paper to appear in SIGMOD 2024", "summary": "Spreadsheets are widely recognized as the most popular end-user programming\ntools, which blend the power of formula-based computation, with an intuitive\ntable-based interface. Today, spreadsheets are used by billions of users to\nmanipulate tables, most of whom are neither database experts nor professional\nprogrammers.\n  Despite the success of spreadsheets, authoring complex formulas remains\nchallenging, as non-technical users need to look up and understand non-trivial\nformula syntax. To address this pain point, we leverage the observation that\nthere is often an abundance of similar-looking spreadsheets in the same\norganization, which not only have similar data, but also share similar\ncomputation logic encoded as formulas. We develop an Auto-Formula system that\ncan accurately predict formulas that users want to author in a target\nspreadsheet cell, by learning and adapting formulas that already exist in\nsimilar spreadsheets, using contrastive-learning techniques inspired by\n\"similar-face recognition\" from compute vision.\n  Extensive evaluations on over 2K test formulas extracted from real enterprise\nspreadsheets show the effectiveness of Auto-Formula over alternatives. Our\nbenchmark data is available at https://github.com/microsoft/Auto-Formula to\nfacilitate future research."}
{"id": "2404.07114", "title": "\"My toxic trait is thinking I'll remember this\": gaps in the learner experience of video tutorials for feature-rich software", "url": "https://arxiv.org/abs/2404.07114", "pdf": "https://arxiv.org/pdf/2404.07114", "abs": "https://arxiv.org/abs/2404.07114", "authors": ["Ian Drosos", "Advait Sarkar", "Andrew D. Gordon"], "categories": ["cs.HC"], "comment": null, "summary": "Video tutorials are a popular medium for informal and formal learning.\nHowever, when learners attempt to view and follow along with these tutorials,\nthey encounter what we call gaps, that is, issues that can prevent learning. We\nexamine the gaps encountered by users of video tutorials for feature-rich\nsoftware, such as spreadsheets. We develop a theory and taxonomy of such gaps,\nidentifying how they act as barriers to learning, by collecting and analyzing\n360 viewer comments from 90 Microsoft Excel video tutorials published by 43\ncreators across YouTube, TikTok, and Instagram. We conducted contextual\ninterviews with 8 highly influential tutorial creators to investigate the gaps\ntheir viewers experience and how they address them. Further, we obtain insights\ninto their creative process and frustrations when creating video tutorials.\nFinally, we present creators with two designs that aim to address gaps\nidentified in the comment analysis for feedback and alternative design ideas."}
{"id": "2402.11734", "title": "Solving Data-centric Tasks using Large Language Models", "url": "https://arxiv.org/abs/2402.11734", "pdf": "https://arxiv.org/pdf/2402.11734", "abs": "https://arxiv.org/abs/2402.11734", "authors": ["Shraddha Barke", "Christian Poelitz", "Carina Suzana Negreanu", "Benjamin Zorn", "José Cambronero", "Andrew D. Gordon", "Vu Le", "Elnaz Nouri", "Nadia Polikarpova", "Advait Sarkar", "Brian Slininger", "Neil Toronto", "Jack Williams"], "categories": ["cs.PL", "cs.AI", "cs.SE"], "comment": "Paper accepted to NAACL 2024 (Findings)", "summary": "Large language models (LLMs) are rapidly replacing help forums like\nStackOverflow, and are especially helpful for non-professional programmers and\nend users. These users are often interested in data-centric tasks, such as\nspreadsheet manipulation and data wrangling, which are hard to solve if the\nintent is only communicated using a natural-language description, without\nincluding the data. But how do we decide how much data and which data to\ninclude in the prompt? This paper makes two contributions towards answering\nthis question. First, we create a dataset of real-world NL-to-code tasks\nmanipulating tabular data, mined from StackOverflow posts. Second, we introduce\na cluster-then-select prompting technique, which adds the most representative\nrows from the input data to the LLM prompt. Our experiments show that LLM\nperformance is indeed sensitive to the amount of data passed in the prompt, and\nthat for tasks with a lot of syntactic variation in the input table, our\ncluster-then-select technique outperforms a random selection baseline."}
{"id": "2403.07762", "title": "Supporting Annotators with Affordances for Efficiently Labeling Conversational Data", "url": "https://arxiv.org/abs/2403.07762", "pdf": "https://arxiv.org/pdf/2403.07762", "abs": "https://arxiv.org/abs/2403.07762", "authors": ["Austin Z. Henley", "David Piorkowski"], "categories": ["cs.HC"], "comment": null, "summary": "Without well-labeled ground truth data, machine learning-based systems would\nnot be as ubiquitous as they are today, but these systems rely on substantial\namounts of correctly labeled data. Unfortunately, crowdsourced labeling is time\nconsuming and expensive. To address the concerns of effort and tedium, we\ndesigned CAL, a novel interface to aid in data labeling. We made several key\ndesign decisions for CAL, which include preventing inapt labels from being\nselected, guiding users in selecting an appropriate label when they need\nassistance, incorporating labeling documentation into the interface, and\nproviding an efficient means to view previous labels. We implemented a\nproduction-quality implementation of CAL and report a user-study evaluation\nthat compares CAL to a standard spreadsheet. Key findings of our study include\nusers using CAL reported lower cognitive load, did not increase task time,\nusers rated CAL to be easier to use, and users preferred CAL over the\nspreadsheet."}
{"id": "2310.09985", "title": "Prompting for Discovery: Flexible Sense-Making for AI Art-Making with Dreamsheets", "url": "https://arxiv.org/abs/2310.09985", "pdf": "https://arxiv.org/pdf/2310.09985", "abs": "https://arxiv.org/abs/2310.09985", "authors": ["Shm Garanganao Almeda", "J. D. Zamfirescu-Pereira", "Kyu Won Kim", "Pradeep Mani Rathnam", "Bjoern Hartmann"], "categories": ["cs.HC"], "comment": "13 pages, 14 figures, currently under review", "summary": "Design space exploration (DSE) for Text-to-Image (TTI) models entails\nnavigating a vast, opaque space of possible image outputs, through a\ncommensurately vast input space of hyperparameters and prompt text. Minor\nadjustments to prompt input can surface unexpectedly disparate images. How can\ninterfaces support end-users in reliably steering prompt-space explorations\ntowards interesting results? Our design probe, DreamSheets, supports\nexploration strategies with LLM-based functions for assisted prompt\nconstruction and simultaneous display of generated results, hosted in a\nspreadsheet interface. The flexible layout and novel generative functions\nenable experimentation with user-defined workflows. Two studies, a preliminary\nlab study and a longitudinal study with five expert artists, revealed a set of\nstrategies participants use to tackle the challenges of TTI design space\nexploration, and the interface features required to support them - like using\ntext-generation to define local \"axes\" of exploration. We distill these\ninsights into a UI mockup to guide future interfaces."}
{"id": "2402.14853", "title": "NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries", "url": "https://arxiv.org/abs/2402.14853", "pdf": "https://arxiv.org/pdf/2402.14853", "abs": "https://arxiv.org/abs/2402.14853", "authors": ["Wei Zhao", "Zhitao Hou", "Siyuan Wu", "Yan Gao", "Haoyu Dong", "Yao Wan", "Hongyu Zhang", "Yulei Sui", "Haidong Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear at EACL 2024", "summary": "Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets,\nis a widespread practice among users performing data analysis. However,\ncrafting formulas on spreadsheets remains a tedious and error-prone task for\nmany end-users, particularly when dealing with complex operations. To alleviate\nthe burden associated with writing spreadsheet formulas, this paper introduces\na novel benchmark task called NL2Formula, with the aim to generate executable\nformulas that are grounded on a spreadsheet table, given a Natural Language\n(NL) query as input. To accomplish this, we construct a comprehensive dataset\nconsisting of 70,799 paired NL queries and corresponding spreadsheet formulas,\ncovering 21,670 tables and 37 types of formula functions. We realize the\nNL2Formula task by providing a sequence-to-sequence baseline implementation\ncalled fCoder. Experimental results validate the effectiveness of fCoder,\ndemonstrating its superior performance compared to the baseline models.\nFurthermore, we also compare fCoder with an initial GPT-3.5 model (i.e.,\ntext-davinci-003). Lastly, through in-depth error analysis, we identify\npotential challenges in the NL2Formula task and advocate for further\ninvestigation."}
{"id": "2006.14706", "title": "Will Dynamic Arrays finally change the way Models are built?", "url": "https://arxiv.org/abs/2006.14706", "pdf": "https://arxiv.org/pdf/2006.14706", "abs": "https://arxiv.org/abs/2006.14706", "authors": ["Peter Bartholomew"], "categories": ["cs.SE"], "comment": "11 Pages, 5 Figures, Numerous Spreadsheet Formulae. This version\n  email address update", "summary": "Spreadsheets offer a supremely successful and intuitive means of processing\nand exchanging numerical content. Its intuitive ad-hoc nature makes it hugely\npopular for use in diverse areas including business and engineering, yet these\nvery same characteristics make it extraordinarily error-prone; many would\nquestion whether it is suitable for serious analysis or modelling tasks. A\nprevious EuSpRIG paper examined the role of Names in increasing solution\ntransparency and providing a readable notation to forge links with the problem\ndomain. Extensive use was made of CSE array formulas, but it is acknowledged\nthat their use makes spreadsheet development a distinctly cumbersome task.\nSince that time, the new dynamic arrays have been introduced and array\ncalculation is now the default mode of operation for Excel. This paper examines\nthe thesis that their adoption within a more professional development\nenvironment could replace traditional techniques where solution integrity is\nimportant. A major advantage of fully dynamic models is that they require less\nmanual intervention to keep them updated and so have the potential to reduce\nthe attendant errors and risk."}
{"id": "1704.01142", "title": "A Structured Approach to the development of Solutions in Excel", "url": "https://arxiv.org/abs/1704.01142", "pdf": "https://arxiv.org/pdf/1704.01142", "abs": "https://arxiv.org/abs/1704.01142", "authors": ["Peter Bartholomew"], "categories": ["cs.SE"], "comment": "12 pages, 6 figures. This version updated email address", "summary": "Spreadsheets offer a supremely successful democratisation platform, placing\nthe manipulation and presentation of numbers within the grasp of users that\nhave little or no mathematical expertise or IT experience. What appears to be\nalmost completely lacking within a \"normal\" solution built using Excel default\nsettings is the deployment of any structure that extends beyond a single-cell\nformula. The structural elements that allow conventional code to scale without\nescalating errors appear to be absent. This paper considers the use of\ncontroversial or lesser-used techniques to create a coherent solution strategy\nin which the problem is solved by a sequence of formulas resembling the steps\nof a programmed language."}
{"id": "2402.00069", "title": "Using the Abstract Computer Architecture Description Language to Model AI Hardware Accelerators", "url": "https://arxiv.org/abs/2402.00069", "pdf": "https://arxiv.org/pdf/2402.00069", "abs": "https://arxiv.org/abs/2402.00069", "authors": ["Mika Markus Müller", "Alexander Richard Manfred Borst", "Konstantin Lübeck", "Alexander Louis-Ferdinand Jung", "Oliver Bringmann"], "categories": ["cs.AR", "cs.AI"], "comment": "Accepted Version for: MBMV'24", "summary": "Artificial Intelligence (AI) has witnessed remarkable growth, particularly\nthrough the proliferation of Deep Neural Networks (DNNs). These powerful models\ndrive technological advancements across various domains. However, to harness\ntheir potential in real-world applications, specialized hardware accelerators\nare essential. This demand has sparked a market for parameterizable AI hardware\naccelerators offered by different vendors.\n  Manufacturers of AI-integrated products face a critical challenge: selecting\nan accelerator that aligns with their product's performance requirements. The\ndecision involves choosing the right hardware and configuring a suitable set of\nparameters. However, comparing different accelerator design alternatives\nremains a complex task. Often, engineers rely on data sheets, spreadsheet\ncalculations, or slow black-box simulators, which only offer a coarse\nunderstanding of the performance characteristics.\n  The Abstract Computer Architecture Description Language (ACADL) is a concise\nformalization of computer architecture block diagrams, which helps to\ncommunicate computer architecture on different abstraction levels and allows\nfor inferring performance characteristics. In this paper, we demonstrate how to\nuse the ACADL to model AI hardware accelerators, use their ACADL description to\nmap DNNs onto them, and explain the timing simulation semantics to gather\nperformance results."}
{"id": "2401.11042", "title": "Does Using ChatGPT Result in Human Cognitive Augmentation?", "url": "https://arxiv.org/abs/2401.11042", "pdf": "https://arxiv.org/pdf/2401.11042", "abs": "https://arxiv.org/abs/2401.11042", "authors": ["Ron Fulbright", "Miranda Morrison"], "categories": ["cs.HC"], "comment": "12 pages, 5 figures", "summary": "Human cognitive performance is enhanced by the use of tools. For example, a\nhuman can produce a much greater, and more accurate, volume of mathematical\ncalculation in a unit of time using a calculator or a spreadsheet application\non a computer. Such tools have taken over the burden of lower level cognitive\ngrunt work but the human still serves the role of the expert performing higher\nlevel thinking and reasoning. Recently, however, unsupervised, deep, machine\nlearning has produced cognitive systems able to outperform humans in several\ndomains. When humans use these tools in a human cog ensemble, the cognitive\nability of the human is augmented. In some cases, even non experts can achieve,\nand even exceed, the performance of experts in a particular domain, synthetic\nexpertise. A new cognitive system, ChatGPT, has burst onto the scene during the\npast year. This paper investigates human cognitive augmentation due to using\nChatGPT by presenting the results of two experiments comparing responses\ncreated using ChatGPT with results created not using ChatGPT. We find using\nChatGPT does not always result in cognitive augmentation and does not yet\nreplace human judgement, discernment, and evaluation in certain types of tasks.\nIn fact, ChatGPT was observed to result in misleading users resulting in\nnegative cognitive augmentation."}
{"id": "2301.13779", "title": "FLAME: A small language model for spreadsheet formulas", "url": "https://arxiv.org/abs/2301.13779", "pdf": "https://arxiv.org/pdf/2301.13779", "abs": "https://arxiv.org/abs/2301.13779", "authors": ["Harshit Joshi", "Abishai Ebenezer", "José Cambronero", "Sumit Gulwani", "Aditya Kanade", "Vu Le", "Ivan Radiček", "Gust Verbruggen"], "categories": ["cs.PL", "cs.AI", "cs.SE"], "comment": "Accepted to AAAI 2024", "summary": "Spreadsheets are a vital tool for end-user data management. Using large\nlanguage models for formula authoring assistance in these environments can be\ndifficult, as these models are expensive to train and challenging to deploy due\nto their size (up to billions of parameters). We present FLAME, a\ntransformer-based model trained exclusively on Excel formulas that leverages\ndomain insights to achieve competitive performance while being substantially\nsmaller (60M parameters) and training on two orders of magnitude less data. We\ncurate a training dataset using sketch deduplication, introduce an\nExcel-specific formula tokenizer, and use domain-specific versions of masked\nspan prediction and noisy auto-encoding as pre-training objectives. We evaluate\nFLAME on formula repair, formula completion, and similarity-based formula\nretrieval. FLAME can outperform much larger models, such as the Davinci (175B)\nand Cushman (12B) variants of Codex and CodeT5 (220M), in 10 of 14 evaluation\nsettings for the repair and completion tasks. For formula retrieval, FLAME\noutperforms CodeT5, CodeBERT, and GraphCodeBERT."}
{"id": "2312.09107", "title": "A Comprehensive Approach to Ensuring Quality in Spreadsheet-Based Metadata", "url": "https://arxiv.org/abs/2312.09107", "pdf": "https://arxiv.org/pdf/2312.09107", "abs": "https://arxiv.org/abs/2312.09107", "authors": ["Martin J. O'Connor", "Marcos Martínez-Romero", "Mete Ugur Akdogan", "Josef Hardi", "Mark A. Musen"], "categories": ["cs.DL"], "comment": null, "summary": "While scientists increasingly recognize the importance of metadata in\ndescribing their data, spreadsheets remain the preferred tool for supplying\nthis information despite their limitations in ensuring compliance and quality.\nVarious tools have been developed to address these limitations, but they suffer\nfrom their own shortcomings, such as steep learning curves and limited\ncustomization. In this paper, we describe an end-to-end approach that supports\nspreadsheet-based entry of metadata while providing rigorous compliance and\nquality control. Our approach employs several key strategies, including\ncustomizable templates for defining metadata, integral support for the use of\ncontrolled terminologies when defining these templates, and an interactive\nWeb-based tool that allows users to rapidly identify and fix errors in the\nspreadsheet-based metadata they supply. We demonstrate how this approach is\nbeing deployed in a biomedical consortium to define and collect metadata about\nscientific experiments."}
{"id": "2312.06517", "title": "Facilitating Digital Agriculture with Simple Databases", "url": "https://arxiv.org/abs/2312.06517", "pdf": "https://arxiv.org/pdf/2312.06517", "abs": "https://arxiv.org/abs/2312.06517", "authors": ["Dennis Buckmaster", "Sami Basir", "Hanae Sakata"], "categories": ["cs.DB", "H.4; E.0; K.8"], "comment": "6 pages, 1 table, 1 figure. Journal of Extension Tools of the Trade,\n  in press", "summary": "As an on-ramp to databases, we offer several well-structured private database\ntemplates as open source resources for agriculturalists, particularly those\nwith modest spreadsheet skills. These farmer-oriented Air table databases use\nsimple data-validated forms, with the look and feel of a customized app, to\nyield operational data that is tidy, machine- and human-readable, editable, and\nexportable for analysis in other software. Such data can facilitate logistics,\nprovide contextual metadata, and improve enterprise analysis. A recorded\nworkshop explaining how to build a database for activity records is presented.\nThese resources may facilitate infusion of digital agriculture principles\nthrough Extension and structured educational programming."}
{"id": "2310.17306", "title": "FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language", "url": "https://arxiv.org/abs/2310.17306", "pdf": "https://arxiv.org/pdf/2310.17306", "abs": "https://arxiv.org/abs/2310.17306", "authors": ["Mukul Singh", "José Cambronero", "Sumit Gulwani", "Vu Le", "Carina Negreanu", "Elnaz Nouri", "Mohammad Raza", "Gust Verbruggen"], "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.PL"], "comment": "Contains inappropriately sourced conjecture of OpenAI's ChatGPT\n  parameter count from\n  www.forbes.com/sites/forbestechcouncil/2023/02/17/is-bigger-better-why-the-chatgpt-vs-gpt-3-vs-gpt-4-battle-is-just-a-family-chat,\n  a citation which was omitted. The authors do not have direct knowledge or\n  verification of this information, and relied solely on this article, which\n  may lead to public confusion", "summary": "Formatting is an important property in tables for visualization,\npresentation, and analysis. Spreadsheet software allows users to automatically\nformat their tables by writing data-dependent conditional formatting (CF)\nrules. Writing such rules is often challenging for users as it requires them to\nunderstand and implement the underlying logic. We present FormaT5, a\ntransformer-based model that can generate a CF rule given the target table and\na natural language description of the desired formatting logic. We find that\nuser descriptions for these tasks are often under-specified or ambiguous,\nmaking it harder for code generation systems to accurately learn the desired\nrule in a single step. To tackle this problem of under-specification and\nminimise argument errors, FormaT5 learns to predict placeholders though an\nabstention objective. These placeholders can then be filled by a second model\nor, when examples of rows that should be formatted are available, by a\nprogramming-by-example system. To evaluate FormaT5 on diverse and real\nscenarios, we create an extensive benchmark of 1053 CF tasks, containing\nreal-world descriptions collected from four different sources. We release our\nbenchmarks to encourage research in this area. Abstention and filling allow\nFormaT5 to outperform 8 different neural approaches on our benchmarks, both\nwith and without examples. Our results illustrate the value of building\ndomain-specific learning systems."}
{"id": "2310.20395", "title": "Spreadsheet-based Configuration of Families of Real-Time Specifications", "url": "https://arxiv.org/abs/2310.20395", "pdf": "https://arxiv.org/pdf/2310.20395", "abs": "https://arxiv.org/abs/2310.20395", "authors": ["José Proença", "David Pereira", "Giann Spilere Nandi", "Sina Borrami", "Jonas Melchert"], "categories": ["cs.SE"], "comment": "In Proceedings TiCSA 2023, arXiv:2310.18720", "summary": "Model checking real-time systems is complex, and requires a careful trade-off\nbetween including enough detail to be useful and not too much detail to avoid\nstate explosion. This work exploits variability of the formal model being\nanalysed and the requirements being checked, to facilitate the model-checking\nof variations of real-time specifications. This work results from the\ncollaboration between academics and Alstom, a railway company with a concrete\nuse-case, in the context of the VALU3S European project. The configuration of\nthe variability of the formal specifications is described in MS Excel\nspreadsheets with a particular structure, making it easy to use also by\ndevelopers. These spreadsheets are processed automatically by our prototype\ntool that generates instances and runs the model checker. We propose the\nextension of our previous work by exploiting analysis over valid combination of\nfeatures, while preserving the simplicity of a spreadsheet-based interface with\nthe model checker."}
{"id": "2305.19308", "title": "SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models", "url": "https://arxiv.org/abs/2305.19308", "pdf": "https://arxiv.org/pdf/2305.19308", "abs": "https://arxiv.org/abs/2305.19308", "authors": ["Hongxin Li", "Jingran Su", "Yuntao Chen", "Qing Li", "Zhaoxiang Zhang"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "Accepted to NeurIPS 2023", "summary": "Computer end users have spent billions of hours completing daily tasks like\ntabular data processing and project timeline scheduling. Most of these tasks\nare repetitive and error-prone, yet most end users lack the skill to automate\nthese burdensome works. With the advent of large language models (LLMs),\ndirecting software with natural language user requests become a reachable goal.\nIn this work, we propose a SheetCopilot agent that takes natural language task\nand control spreadsheet to fulfill the requirements. We propose a set of atomic\nactions as an abstraction of spreadsheet software functionalities. We further\ndesign a state machine-based task planning framework for LLMs to robustly\ninteract with spreadsheets. We curate a representative dataset containing 221\nspreadsheet control tasks and establish a fully automated evaluation pipeline\nfor rigorously benchmarking the ability of LLMs in software control tasks. Our\nSheetCopilot correctly completes 44.3\\% of tasks for a single generation,\noutperforming the strong code generation baseline by a wide margin. Our project\npage:https://sheetcopilot.github.io/."}
{"id": "2310.17414", "title": "LEI2JSON: Schema-based Validation and Conversion of Livestock Event Information", "url": "https://arxiv.org/abs/2310.17414", "pdf": "https://arxiv.org/pdf/2310.17414", "abs": "https://arxiv.org/abs/2310.17414", "authors": ["Mahir Habib", "Muhammad Ashad Kabir", "Lihong Zheng"], "categories": ["eess.SY", "cs.SE", "cs.SY"], "comment": "20 pages, 6 figures", "summary": "Livestock producers often need help in standardising (i.e., converting and\nvalidating) their livestock event data. This article introduces a novel\nsolution, LEI2JSON (Livestock Event Information To JSON). The tool is an add-on\nfor Google Sheets, adhering to the livestock event information (LEI) schema.\nThe core objective of LEI2JSON is to provide livestock producers with an\nefficient mechanism to standardise their data, leading to substantial savings\nin time and resources. This is achieved by building the spreadsheet template\nwith the appropriate column headers, notes, and validation rules, converting\nthe spreadsheet data into JSON format, and validating the output against the\nschema. LEI2JSON facilitates the seamless storage of livestock event\ninformation locally or on Google Drive in JSON. Additionally, we have conducted\nan extensive experimental evaluation to assess the effectiveness of the tool."}
{"id": "2310.16700", "title": "Streamlining Knowledge Graph Construction with a façade: The SPARQL Anything project", "url": "https://arxiv.org/abs/2310.16700", "pdf": "https://arxiv.org/pdf/2310.16700", "abs": "https://arxiv.org/abs/2310.16700", "authors": ["Luigi Asprino", "Enrico Daga", "Justin Dowdy", "Paul Mulholland", "Aldo Gangemi", "Marco Ratta"], "categories": ["cs.DB", "cs.DS"], "comment": "15 pages", "summary": "What should a data integration framework for knowledge engineers look like?\nRecent research on Knowledge Graph construction proposes the design of a\nfa\\c{c}ade, a notion borrowed from object-oriented software engineering. This\nidea is applied to SPARQL Anything, a system that allows querying heterogeneous\nresources as-if they were in RDF, in plain SPARQL 1.1, by overloading the\nSERVICE clause. SPARQL Anything supports a wide variety of file formats, from\npopular ones (CSV, JSON, XML, Spreadsheets) to others that are not supported by\nalternative solutions (Markdown, YAML, DOCx, Bibtex). Features include querying\nWeb APIs with high flexibility, parametrised queries, and chaining multiple\ntransformations into complex pipelines. In this paper, we describe the design\nrationale and software architecture of the SPARQL Anything system. We provide\nreferences to an extensive set of reusable, real-world scenarios from various\napplication domains. We report on the value-to-users of the founding\nassumptions of its design, compared to alternative solutions through a\ncommunity survey and a field report from the industry."}
{"id": "2310.14495", "title": "InstructExcel: A Benchmark for Natural Language Instruction in Excel", "url": "https://arxiv.org/abs/2310.14495", "pdf": "https://arxiv.org/pdf/2310.14495", "abs": "https://arxiv.org/abs/2310.14495", "authors": ["Justin Payan", "Swaroop Mishra", "Mukul Singh", "Carina Negreanu", "Christian Poelitz", "Chitta Baral", "Subhro Roy", "Rasika Chakravarthy", "Benjamin Van Durme", "Elnaz Nouri"], "categories": ["cs.CL", "cs.AI"], "comment": "Findings of EMNLP 2023, 18 pages", "summary": "With the evolution of Large Language Models (LLMs) we can solve increasingly\nmore complex NLP tasks across various domains, including spreadsheets. This\nwork investigates whether LLMs can generate code (Excel OfficeScripts, a\nTypeScript API for executing many tasks in Excel) that solves Excel specific\ntasks provided via natural language user instructions. To do so we introduce a\nnew large-scale benchmark, InstructExcel, created by leveraging the 'Automate'\nfeature in Excel to automatically generate OfficeScripts from users' actions.\nOur benchmark includes over 10k samples covering 170+ Excel operations across\n2,000 publicly available Excel spreadsheets. Experiments across various\nzero-shot and few-shot settings show that InstructExcel is a hard benchmark for\nstate of the art models like GPT-4. We observe that (1) using GPT-4 over\nGPT-3.5, (2) providing more in-context examples, and (3) dynamic prompting can\nhelp improve performance on this benchmark."}
{"id": "2311.10728", "title": "Improving Feedback from Automated Reviews of Student Spreadsheets", "url": "https://arxiv.org/abs/2311.10728", "pdf": "https://arxiv.org/pdf/2311.10728", "abs": "https://arxiv.org/abs/2311.10728", "authors": ["Sören Aguirre Reid", "Frank Kammer", "Jonas-Ian Kuche", "Pia-Doreen Ritzke", "Markus Siepermann", "Max Stephan", "Armin Wagenknecht"], "categories": ["cs.CY", "D.2.0"], "comment": null, "summary": "Spreadsheets are one of the most widely used tools for end users. As a\nresult, spreadsheets such as Excel are now included in many curricula. However,\ndigital solutions for assessing spreadsheet assignments are still scarce in the\nteaching context. Therefore, we have developed an Intelligent Tutoring System\n(ITS) to review students' Excel submissions and provide individualized feedback\nautomatically. Although the lecturer only needs to provide one reference\nsolution, the students' submissions are analyzed automatically in several ways:\nvalue matching, detailed analysis of the formulas, and quality assessment of\nthe solution. To take the students' learning level into account, we have\ndeveloped feedback levels for an ITS that provide gradually more information\nabout the error by using one of the different analyses. Feedback at a higher\nlevel has been shown to lead to a higher percentage of correct submissions and\nwas also perceived as well understandable and helpful by the students."}
{"id": "2309.02110", "title": "Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!", "url": "https://arxiv.org/abs/2309.02110", "pdf": "https://arxiv.org/pdf/2309.02110", "abs": "https://arxiv.org/abs/2309.02110", "authors": ["James P. Dilger"], "categories": ["math.HO", "cs.CL"], "comment": null, "summary": "Wordle is a popular, online word game offered by the New York Times\n(nytimes.com). Currently there are some 2 million players of the English\nversion worldwide. Players have 6 attempts to guess the daily word (target\nword) and after each attempt, the player receives color-coded information about\nthe correctness and position of each letter in the guess. After either a\nsuccessful completion of the puzzle or the final unsuccessful attempt, software\ncan assess the player's luck and skill using Information Theory and can display\ndata for the first, second, ..., sixth guesses of a random sample of all\nplayers. Recently, I discovered that the latter data is presented in a format\nthat can easily be copied and pasted into a spreadsheet. I compiled data on\nWordle players' first guesses from May 2023 - August 2023 and inferred some\ninteresting information about Wordle players. A) Every day, about 0.2-0.5% of\nplayers solve the puzzle in one attempt. Because the odds of guessing the one\nof 2,315 possible target words at random is 0.043%, this implies that 4,000 -\n10,000 players cheat by obtaining the target word outside of playing the game!\nB) At least 1/3 of the players have a favorite starting word, or cycle through\nseveral. And even though players should be aware that target words are never\nrepeated, most players appear to remain loyal to their starting word even after\nits appearance as a target word. C) On August 15, 2023, about 30,000 players\nabruptly changed their starting word, presumably based on a crossword puzzle\nclue! Wordle players can be influenced! This study goes beyond social media\npostings, surveys, and Google Trends to provide solid, quantitative evidence\nabout cheating in Wordle."}
{"id": "2310.01297", "title": "Co-audit: tools to help humans double-check AI-generated content", "url": "https://arxiv.org/abs/2310.01297", "pdf": "https://arxiv.org/pdf/2310.01297", "abs": "https://arxiv.org/abs/2310.01297", "authors": ["Andrew D. Gordon", "Carina Negreanu", "José Cambronero", "Rasika Chakravarthy", "Ian Drosos", "Hao Fang", "Bhaskar Mitra", "Hannah Richardson", "Advait Sarkar", "Stephanie Simmons", "Jack Williams", "Ben Zorn"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.PL"], "comment": null, "summary": "Users are increasingly being warned to check AI-generated content for\ncorrectness. Still, as LLMs (and other generative models) generate more complex\noutput, such as summaries, tables, or code, it becomes harder for the user to\naudit or evaluate the output for quality or correctness. Hence, we are seeing\nthe emergence of tool-assisted experiences to help the user double-check a\npiece of AI-generated content. We refer to these as co-audit tools. Co-audit\ntools complement prompt engineering techniques: one helps the user construct\nthe input prompt, while the other helps them check the output response. As a\nspecific example, this paper describes recent research on co-audit tools for\nspreadsheet computations powered by generative models. We explain why co-audit\nexperiences are essential for any application of generative AI where quality is\nimportant and errors are consequential (as is common in spreadsheet\ncomputations). We propose a preliminary list of principles for co-audit, and\noutline research challenges."}
{"id": "2309.00115", "title": "Excel as a Turing-complete Functional Programming Environment", "url": "https://arxiv.org/abs/2309.00115", "pdf": "https://arxiv.org/pdf/2309.00115", "abs": "https://arxiv.org/abs/2309.00115", "authors": ["Peter Bartholomew"], "categories": ["cs.SE"], "comment": "14 page, 6 figures", "summary": "Since the calculation engine of Excel was the subject of a major upgrade to\naccommodate Dynamic Arrays in 2018 there has been a series of seismic changes\nto the art of building spreadsheet solutions. This paper will show the ad-hoc\nend user practices of traditional spreadsheets can be replaced by radically\ndifferent approaches that have far more in common with formal programming. It\nis too early to guess the extent to which the new functionality will be adopted\nby the business and engineering communities and the impact that may have upon\nrisk. Nevertheless, some trends are emerging from pioneering work within the\nExcel community which we will discuss here."}
{"id": "2309.12353", "title": "How Beaufort, Neumann and Gates met? Subject integration with spreadsheeting", "url": "https://arxiv.org/abs/2309.12353", "pdf": "https://arxiv.org/pdf/2309.12353", "abs": "https://arxiv.org/abs/2309.12353", "authors": ["Maria Csernoch", "Julia Csernoch"], "categories": ["cs.CY"], "comment": "17 pages, 14 figures", "summary": "Computational thinking should be the fourth fundamental skill, along with\nreading, writing, and arithmetic (3R). To reach the level where computational\nthinking skills, especially digital problem solving have their own schemata,\nthere is a long way to go. In the present paper, a novel approach is detailed\nto support subject integration and building digital schemata, on the well-known\nBeaufort scale. The conversion of a traditional, paper-based problem and a data\nretrieval process are presented within the frame of a Grade 8 action research\nstudy. It is found that both students content knowledge and their digital\nskills developed more efficiently than in traditional course book and\ndecontextualized digital environments. Furthermore, the method presented here\ncan be adapted to any paper-based problems whose solutions would be more\neffective in a digital environment and which offer various forms for building\nschemata both in the subject matter and informatics."}
{"id": "2309.00104", "title": "A Use Case-Engineering Resources Taxonomy for Analytical Spreadsheet Models", "url": "https://arxiv.org/abs/2309.00104", "pdf": "https://arxiv.org/pdf/2309.00104", "abs": "https://arxiv.org/abs/2309.00104", "authors": ["Thomas A. Grossman", "Vijay Mehrotra"], "categories": ["cs.SE"], "comment": "13 Pages, 7 Figures, 2 Tables", "summary": "This paper presents a taxonomy for analytical spreadsheet models. It\nconsiders both the use case that a spreadsheet is meant to serve, and the\nengineering resources devoted to its development. We extend a previous\nthree-type taxonomy, to identify nine types of spreadsheet models, that\nencompass the many analytical spreadsheet models seen in the literature. We\nconnect disparate research literature to distinguish between an \"analytical\nsolution\" and an \"industrial-quality analytical spreadsheet model\". We explore\nthe nature of each of the nine types, propose definitions for some, relate them\nto the literature, and hypothesize on how they might arise. The taxonomy aids\nin identifying where various spreadsheet development guidelines are most\nuseful, provides a lens for viewing spreadsheet errors and risk, and offers a\nstructure for understanding how spreadsheets change over time. This taxonomy\nopens the door to many interesting research questions, including refinements to\nitself."}
{"id": "2309.00095", "title": "Experimenting with ChatGPT for Spreadsheet Formula Generation: Evidence of Risk in AI Generated Spreadsheets", "url": "https://arxiv.org/abs/2309.00095", "pdf": "https://arxiv.org/pdf/2309.00095", "abs": "https://arxiv.org/abs/2309.00095", "authors": ["Simon Thorne"], "categories": ["cs.SE"], "comment": "15 Pages", "summary": "Large Language Models (LLM) have become sophisticated enough that complex\ncomputer programs can be created through interpretation of plain English\nsentences and implemented in a variety of modern languages such as Python, Java\nScript, C++ and Spreadsheets. These tools are powerful and relatively accurate\nand therefore provide broad access to computer programming regardless of the\nbackground or knowledge of the individual using them. This paper presents a\nseries of experiments with ChatGPT to explore the tool's ability to produce\nvalid spreadsheet formulae and related computational outputs in situations\nwhere ChatGPT has to deduce, infer and problem solve the answer. The results\nshow that in certain circumstances, ChatGPT can produce correct spreadsheet\nformulae with correct reasoning, deduction and inference. However, when\ninformation is limited, uncertain or the problem is too complex, the accuracy\nof ChatGPT breaks down as does its ability to reason, infer and deduce. This\ncan also result in false statements and \"hallucinations\" that all subvert the\nprocess of creating spreadsheet formulae."}
{"id": "2308.14784", "title": "Generating tabular datasets under differential privacy", "url": "https://arxiv.org/abs/2308.14784", "pdf": "https://arxiv.org/pdf/2308.14784", "abs": "https://arxiv.org/abs/2308.14784", "authors": ["Gianluca Truda"], "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DB", "I.2.6"], "comment": null, "summary": "Machine Learning (ML) is accelerating progress across fields and industries,\nbut relies on accessible and high-quality training data. Some of the most\nimportant datasets are found in biomedical and financial domains in the form of\nspreadsheets and relational databases. But this tabular data is often sensitive\nin nature. Synthetic data generation offers the potential to unlock sensitive\ndata, but generative models tend to memorise and regurgitate training data,\nwhich undermines the privacy goal. To remedy this, researchers have\nincorporated the mathematical framework of Differential Privacy (DP) into the\ntraining process of deep neural networks. But this creates a trade-off between\nthe quality and privacy of the resulting data. Generative Adversarial Networks\n(GANs) are the dominant paradigm for synthesising tabular data under DP, but\nsuffer from unstable adversarial training and mode collapse, which are\nexacerbated by the privacy constraints and challenging tabular data modality.\nThis work optimises the quality-privacy trade-off of generative models,\nproducing higher quality tabular datasets with the same privacy guarantees. We\nimplement novel end-to-end models that leverage attention mechanisms to learn\nreversible tabular representations. We also introduce TableDiffusion, the first\ndifferentially-private diffusion model for tabular data synthesis. Our\nexperiments show that TableDiffusion produces higher-fidelity synthetic\ndatasets, avoids the mode collapse problem, and achieves state-of-the-art\nperformance on privatised tabular data synthesis. By implementing\nTableDiffusion to predict the added noise, we enabled it to bypass the\nchallenges of reconstructing mixed-type tabular data. Overall, the diffusion\nparadigm proves vastly more data and privacy efficient than the adversarial\nparadigm, due to augmented re-use of each data batch and a smoother iterative\ntraining process."}
{"id": "2308.10922", "title": "DataVinci: Learning Syntactic and Semantic String Repairs", "url": "https://arxiv.org/abs/2308.10922", "pdf": "https://arxiv.org/pdf/2308.10922", "abs": "https://arxiv.org/abs/2308.10922", "authors": ["Mukul Singh", "José Cambronero", "Sumit Gulwani", "Vu Le", "Carina Negreanu", "Gust Verbruggen"], "categories": ["cs.DB", "cs.AI"], "comment": "13 pages", "summary": "String data is common in real-world datasets: 67.6% of values in a sample of\n1.8 million real Excel spreadsheets from the web were represented as text.\nSystems that successfully clean such string data can have a significant impact\non real users. While prior work has explored errors in string data, proposed\napproaches have often been limited to error detection or require that the user\nprovide annotations, examples, or constraints to fix the errors. Furthermore,\nthese systems have focused independently on syntactic errors or semantic errors\nin strings, but ignore that strings often contain both syntactic and semantic\nsubstrings. We introduce DataVinci, a fully unsupervised string data error\ndetection and repair system. DataVinci learns regular-expression-based patterns\nthat cover a majority of values in a column and reports values that do not\nsatisfy such patterns as data errors. DataVinci can automatically derive edits\nto the data error based on the majority patterns and constraints learned over\nother columns without the need for further user interaction. To handle strings\nwith both syntactic and semantic substrings, DataVinci uses an LLM to abstract\n(and re-concretize) portions of strings that are semantic prior to learning\nmajority patterns and deriving edits. Because not all data can result in\nmajority patterns, DataVinci leverages execution information from an existing\nprogram (which reads the target data) to identify and correct data repairs that\nwould not otherwise be identified. DataVinci outperforms 7 baselines on both\nerror detection and repair when evaluated on 4 existing and new benchmarks."}
{"id": "2308.07357", "title": "Demonstration of CORNET: A System For Learning Spreadsheet Formatting Rules By Example", "url": "https://arxiv.org/abs/2308.07357", "pdf": "https://arxiv.org/pdf/2308.07357", "abs": "https://arxiv.org/abs/2308.07357", "authors": ["Mukul Singh", "Jose Cambronero", "Sumit Gulwani", "Vu Le", "Carina Negreanu", "Gust Verbruggen"], "categories": ["cs.SE", "cs.AI", "cs.DB"], "comment": "4 Pages, VLDB 2023 Demonstration Track", "summary": "Data management and analysis tasks are often carried out using spreadsheet\nsoftware. A popular feature in most spreadsheet platforms is the ability to\ndefine data-dependent formatting rules. These rules can express actions such as\n\"color red all entries in a column that are negative\" or \"bold all rows not\ncontaining error or failure.\" Unfortunately, users who want to exercise this\nfunctionality need to manually write these conditional formatting (CF) rules.\nWe introduce CORNET, a system that automatically learns such conditional\nformatting rules from user examples. CORNET takes inspiration from inductive\nprogram synthesis and combines symbolic rule enumeration, based on\nsemi-supervised clustering and iterative decision tree learning, with a neural\nranker to produce accurate conditional formatting rules. In this demonstration,\nwe show CORNET in action as a simple add-in to Microsoft Excel. After the user\nprovides one or two formatted cells as examples, CORNET generates formatting\nrule suggestions for the user to apply to the spreadsheet."}
{"id": "2307.14565", "title": "Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples", "url": "https://arxiv.org/abs/2307.14565", "pdf": "https://arxiv.org/pdf/2307.14565", "abs": "https://arxiv.org/abs/2307.14565", "authors": ["Peng Li", "Yeye He", "Cong Yan", "Yue Wang", "Surajit Chaudhuri"], "categories": ["cs.DB", "cs.LG"], "comment": "full version of a paper accepted to VLDB 2023", "summary": "Relational tables, where each row corresponds to an entity and each column\ncorresponds to an attribute, have been the standard for tables in relational\ndatabases. However, such a standard cannot be taken for granted when dealing\nwith tables \"in the wild\". Our survey of real spreadsheet-tables and web-tables\nshows that over 30% of such tables do not conform to the relational standard,\nfor which complex table-restructuring transformations are needed before these\ntables can be queried easily using SQL-based analytics tools. Unfortunately,\nthe required transformations are non-trivial to program, which has become a\nsubstantial pain point for technical and non-technical users alike, as\nevidenced by large numbers of forum questions in places like StackOverflow and\nExcel/Power-BI/Tableau forums.\n  We develop an Auto-Tables system that can automatically synthesize pipelines\nwith multi-step transformations (in Python or other languages), to transform\nnon-relational tables into standard relational forms for downstream analytics,\nobviating the need for users to manually program transformations. We compile an\nextensive benchmark for this new task, by collecting 244 real test cases from\nuser spreadsheets and online forums. Our evaluation suggests that Auto-Tables\ncan successfully synthesize transformations for over 70% of test cases at\ninteractive speeds, without requiring any input from users, making this an\neffective tool for both technical and non-technical users to prepare data for\nanalytics."}
{"id": "2309.12317", "title": "Using a Catenary Trajectory to Reduce Wellbore Friction in Horizontal Extended Reach Drilling", "url": "https://arxiv.org/abs/2309.12317", "pdf": "https://arxiv.org/pdf/2309.12317", "abs": "https://arxiv.org/abs/2309.12317", "authors": ["Vu Nguyen"], "categories": ["cs.RO"], "comment": null, "summary": "Wellbore friction is one of the biggest concerns when drilling due to its\nrelation to the total cost. The catenary concept was introduced to reduce\nwellbore friction, but it requires detailed analyses. This project would fill\nthis gap. A catenary shape is simply the natural shape of a rope, chain, or\ndrill string. The drill string will then hang freely inside the wellbore.\nPerfectly, there should be no contact between the hole and the string, and thus\nno friction. Torque and drag should be minimized this way. A case study is\nintroduced to examine the outcome between Catenary Trajectory Design and\ntraditional 2D Arc design. The calculation procedure of Catenary Trajectory and\n2D Arc Design can be found in an MS Excel spreadsheet which is easy to use and\nreliable for designing catenary well trajectories for extended-reach wells."}
{"id": "2306.12850", "title": "Don't Treat the Symptom, Find the Cause! Efficient Artificial-Intelligence Methods for (Interactive) Debugging", "url": "https://arxiv.org/abs/2306.12850", "pdf": "https://arxiv.org/pdf/2306.12850", "abs": "https://arxiv.org/abs/2306.12850", "authors": ["Patrick Rodler"], "categories": ["cs.AI", "cs.DM", "cs.LO"], "comment": "Habilitation Thesis", "summary": "In the modern world, we are permanently using, leveraging, interacting with,\nand relying upon systems of ever higher sophistication, ranging from our cars,\nrecommender systems in e-commerce, and networks when we go online, to\nintegrated circuits when using our PCs and smartphones, the power grid to\nensure our energy supply, security-critical software when accessing our bank\naccounts, and spreadsheets for financial planning and decision making. The\ncomplexity of these systems coupled with our high dependency on them implies\nboth a non-negligible likelihood of system failures, and a high potential that\nsuch failures have significant negative effects on our everyday life. For that\nreason, it is a vital requirement to keep the harm of emerging failures to a\nminimum, which means minimizing the system downtime as well as the cost of\nsystem repair. This is where model-based diagnosis comes into play.\n  Model-based diagnosis is a principled, domain-independent approach that can\nbe generally applied to troubleshoot systems of a wide variety of types,\nincluding all the ones mentioned above, and many more. It exploits and\norchestrates i.a. techniques for knowledge representation, automated reasoning,\nheuristic problem solving, intelligent search, optimization, stochastics,\nstatistics, decision making under uncertainty, machine learning, as well as\ncalculus, combinatorics and set theory to detect, localize, and fix faults in\nabnormally behaving systems.\n  In this thesis, we will give an introduction to the topic of model-based\ndiagnosis, point out the major challenges in the field, and discuss a selection\nof approaches from our research addressing these issues."}
{"id": "2304.07303", "title": "Smart Metro: Deep Learning Approaches to Forecasting the MRT Line 3 Ridership", "url": "https://arxiv.org/abs/2304.07303", "pdf": "https://arxiv.org/pdf/2304.07303", "abs": "https://arxiv.org/abs/2304.07303", "authors": ["Jayrald Empino", "Jean Allyson Junsay", "Mary Grace Verzon", "Mideth Abisado", "Shekinah Lor Huyo-a", "Gabriel Avelino Sampedro"], "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": null, "summary": "Since its establishment in 1999, the Metro Rail Transit Line 3 (MRT3) has\nserved as a transportation option for numerous passengers in Metro Manila,\nPhilippines. The Philippine government's transportation department records more\nthan a thousand people using the MRT3 daily and forecasting the daily passenger\ncount may be rather challenging. The MRT3's daily ridership fluctuates owing to\nvariables such as holidays, working days, and other unexpected issues.\nCommuters do not know how many other commuters are on their route on a given\nday, which may hinder their ability to plan an efficient itinerary. Currently,\nthe DOTr depends on spreadsheets containing historical data, which might be\nchallenging to examine. This study presents a time series prediction of daily\ntraffic to anticipate future attendance at a particular station on specific\ndays."}
{"id": "2304.06597", "title": "\"What It Wants Me To Say\": Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models", "url": "https://arxiv.org/abs/2304.06597", "pdf": "https://arxiv.org/pdf/2304.06597", "abs": "https://arxiv.org/abs/2304.06597", "authors": ["Michael Xieyang Liu", "Advait Sarkar", "Carina Negreanu", "Ben Zorn", "Jack Williams", "Neil Toronto", "Andrew D. Gordon"], "categories": ["cs.HC"], "comment": null, "summary": "Code-generating large language models translate natural language into code.\nHowever, only a small portion of the infinite space of naturalistic utterances\nis effective at guiding code generation. For non-expert end-user programmers,\nlearning this is the challenge of abstraction matching. We examine this\nchallenge in the specific context of data analysis in spreadsheets, in a system\nthat maps the users natural language query to Python code using the Codex\ngenerator, executes the code, and shows the result. We propose grounded\nabstraction matching, which bridges the abstraction gap by translating the code\nback into a systematic and predictable naturalistic utterance. In a\nbetween-subjects, think-aloud study (n=24), we compare grounded abstraction\nmatching to an ungrounded alternative based on previously established query\nframing principles. We find that the grounded approach improves end-users'\nunderstanding of the scope and capabilities of the code-generating model, and\nthe kind of language needed to use it effectively."}
{"id": "2210.13619", "title": "A Simpler Method for Understanding Emergency Shelter Access Patterns", "url": "https://arxiv.org/abs/2210.13619", "pdf": "https://arxiv.org/pdf/2210.13619", "abs": "https://arxiv.org/abs/2210.13619", "authors": ["Geoffrey G. Messier"], "categories": ["cs.CY"], "comment": null, "summary": "The Simplified Access Metric (SAM) is a new approach for characterizing\nemergency shelter access patterns as a measure of shelter client vulnerability.\nThe goal of SAM is to provide shelter operators with an intuitive way to\nunderstand access patterns that can be implemented by non-technical staff using\nspreadsheet operations. Client data from a large North American shelter will be\nused to demonstrate that SAM produces similar results to traditional\ntransitional, episodic and chronic client cluster analysis. Since SAM requires\nless data than cluster analysis, it is also able to generate a real time\npicture of how shelter access patterns are affected by external factors.\nTimelines generated from nine years of shelter client data using SAM\ndemonstrate the impact of Housing First programming and the COVID-19 lockdown\non how people access shelter. Finally, SAM allows shelter staff to move beyond\nassigning transitional, episodic and chronic labels and instead use the \"soft\"\noutput of SAM directly as a measure of vulnerability."}
{"id": "2302.05482", "title": "Efficient and Compact Spreadsheet Formula Graphs", "url": "https://arxiv.org/abs/2302.05482", "pdf": "https://arxiv.org/pdf/2302.05482", "abs": "https://arxiv.org/abs/2302.05482", "authors": ["Dixin Tang", "Fanchao Chen", "Christopher De Leon", "Tana Wattanawaroon", "Jeaseok Yun", "Srinivasan Seshadri", "Aditya G. Parameswaran"], "categories": ["cs.DB"], "comment": null, "summary": "Spreadsheets are one of the most popular data analysis tools, wherein users\ncan express computation as formulae alongside data. The ensuing dependencies\nare tracked as formula graphs. Efficiently querying and maintaining these\nformula graphs is critical for interactivity across multiple settings.\nUnfortunately, formula graphs are often large and complex such that querying\nand maintaining them is time-consuming, reducing interactivity. We propose\nTACO, a framework for efficiently compressing formula graphs, thereby reducing\nthe time for querying and maintenance. The efficiency of TACO stems from a key\nspreadsheet property: tabular locality, which means that cells close to each\nother are likely to have similar formula structures. We leverage four such\ntabular locality-based patterns and develop algorithms for compressing formula\ngraphs using these patterns, directly querying the compressed graph without\ndecompression, and incrementally maintaining the graph during updates. We\nintegrate TACO into an open-source spreadsheet system and show that TACO can\nsignificantly reduce formula graph sizes. For querying formula graphs, the\nspeedups of TACO over a baseline implemented in our framework and a commercial\nspreadsheet system are up to 34,972x and 632x, respectively."}
{"id": "2301.11964", "title": "Adversarial Networks and Machine Learning for File Classification", "url": "https://arxiv.org/abs/2301.11964", "pdf": "https://arxiv.org/pdf/2301.11964", "abs": "https://arxiv.org/abs/2301.11964", "authors": ["Ken St. Germain", "Josh Angichiodo"], "categories": ["cs.LG"], "comment": null, "summary": "Correctly identifying the type of file under examination is a critical part\nof a forensic investigation. The file type alone suggests the embedded content,\nsuch as a picture, video, manuscript, spreadsheet, etc. In cases where a system\nowner might desire to keep their files inaccessible or file type concealed, we\npropose using an adversarially-trained machine learning neural network to\ndetermine a file's true type even if the extension or file header is obfuscated\nto complicate its discovery. Our semi-supervised generative adversarial network\n(SGAN) achieved 97.6% accuracy in classifying files across 11 different types.\nWe also compared our network against a traditional standalone neural network\nand three other machine learning algorithms. The adversarially-trained network\nproved to be the most precise file classifier especially in scenarios with few\nsupervised samples available. Our implementation of a file classifier using an\nSGAN is implemented on GitHub (https://ksaintg.github.io/SGAN-File-Classier)."}
{"id": "2208.06032", "title": "CORNET: Learning Table Formatting Rules By Example", "url": "https://arxiv.org/abs/2208.06032", "pdf": "https://arxiv.org/pdf/2208.06032", "abs": "https://arxiv.org/abs/2208.06032", "authors": ["Mukul Singh", "José Cambronero", "Sumit Gulwani", "Vu Le", "Carina Negreanu", "Mohammad Raza", "Gust Verbruggen"], "categories": ["cs.AI", "cs.DB", "cs.SE"], "comment": "12 pages content, 2 pages references", "summary": "Spreadsheets are widely used for table manipulation and presentation.\nStylistic formatting of these tables is an important property for both\npresentation and analysis. As a result, popular spreadsheet software, such as\nExcel, supports automatically formatting tables based on rules. Unfortunately,\nwriting such formatting rules can be challenging for users as it requires\nknowledge of the underlying rule language and data logic. We present CORNET, a\nsystem that tackles the novel problem of automatically learning such formatting\nrules from user examples in the form of formatted cells. CORNET takes\ninspiration from advances in inductive programming and combines symbolic rule\nenumeration with a neural ranker to learn conditional formatting rules. To\nmotivate and evaluate our approach, we extracted tables with over 450K unique\nformatting rules from a corpus of over 1.8M real worksheets. Since we are the\nfirst to introduce conditional formatting, we compare CORNET to a wide range of\nsymbolic and neural baselines adapted from related domains. Our results show\nthat CORNET accurately learns rules across varying evaluation setups.\nAdditionally, we show that CORNET finds shorter rules than those that a user\nhas written and discovers rules in spreadsheets that users have manually\nformatted."}
{"id": "2211.04128", "title": "Active Learning with Tabular Language Models", "url": "https://arxiv.org/abs/2211.04128", "pdf": "https://arxiv.org/pdf/2211.04128", "abs": "https://arxiv.org/abs/2211.04128", "authors": ["Martin Ringsquandl", "Aneta Koleva"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "Despite recent advancements in tabular language model research, real-world\napplications are still challenging. In industry, there is an abundance of\ntables found in spreadsheets, but acquisition of substantial amounts of labels\nis expensive, since only experts can annotate the often highly technical and\ndomain-specific tables. Active learning could potentially reduce labeling\ncosts, however, so far there are no works related to active learning in\nconjunction with tabular language models. In this paper we investigate\ndifferent acquisition functions in a real-world industrial tabular language\nmodel use case for sub-cell named entity recognition. Our results show that\ncell-level acquisition functions with built-in diversity can significantly\nreduce the labeling effort, while enforced table diversity is detrimental. We\nfurther see open fundamental questions concerning computational efficiency and\nthe perspective of human annotators."}
{"id": "2211.06333", "title": "Excel Spreadsheet Analyzer", "url": "https://arxiv.org/abs/2211.06333", "pdf": "https://arxiv.org/pdf/2211.06333", "abs": "https://arxiv.org/abs/2211.06333", "authors": ["Amir Nassereldine", "Patrick Chen", "Jinjun Xiong"], "categories": ["cs.SE", "cs.PL"], "comment": "10 pages, 9 figures", "summary": "Spreadsheets are widely used in various fields to do large numerical\nanalysis. While several companies have relied on spreadsheets for decades, data\nscientists are going in the direction of using scientific programming languages\nsuch as python to do their data analysis due to the support, community, and\nvast amount of libraries. While using python to analyze a company's\nspreadsheets, some information such as the formulas and dependencies of a cell\nare lost. We propose a tool that creates an abstract intermediate\nrepresentation (AIR) of a spreadsheet. This representation facilitates the\ntransfer from spreadsheets into scientific programming languages while\npreserving inter-dependency information about data. In addition to that, we\nbuild a python library on top of our tool to perform some data analysis in\npython."}
{"id": "2210.09928", "title": "Team OS's System for Dialogue Robot Competition 2022", "url": "https://arxiv.org/abs/2210.09928", "pdf": "https://arxiv.org/pdf/2210.09928", "abs": "https://arxiv.org/abs/2210.09928", "authors": ["Yuki Kubo", "Ryo Yanagimoto", "Hayato Futase", "Mikio Nakano", "Zhaojie Luo", "Kazunori Komatani"], "categories": ["cs.HC"], "comment": "This paper is part of the proceedings of the Dialogue Robot\n  Competition 2022", "summary": "This paper describes our dialogue robot system, OSbot, developed for Dialogue\nRobot Competition 2022. The dialogue flow is based on state transitions\ndescribed manually and the transition conditions use the results of keyword\nextraction and sentiment analysis. The transitions can be easily viewed and\nedited by managing them on a spreadsheet. The keyword extraction is based on\nnamed entity extraction and our predefined keyword set. The sentiment analysis\nis text-based and uses SVM, which was trained with the multimodal dialogue\ncorpus Hazumi. We quickly checked and edited a dialogue flow by using a logging\nfunction. In the competition's preliminary round, our system ended up in third\nplace."}
{"id": "2210.09162", "title": "Table-To-Text generation and pre-training with TabT5", "url": "https://arxiv.org/abs/2210.09162", "pdf": "https://arxiv.org/pdf/2210.09162", "abs": "https://arxiv.org/abs/2210.09162", "authors": ["Ewa Andrejczuk", "Julian Martin Eisenschlos", "Francesco Piccinno", "Syrine Krichene", "Yasemin Altun"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to Findings of EMNLP 2022", "summary": "Encoder-only transformer models have been successfully applied to different\ntable understanding tasks, as in TAPAS (Herzig et al., 2020). A major\nlimitation of these architectures is that they are constrained to\nclassification-like tasks such as cell selection or entailment detection. We\npresent TABT5, an encoder-decoder model that generates natural language text\nbased on tables and textual inputs. TABT5 overcomes the encoder-only limitation\nby incorporating a decoder component and leverages the input structure with\ntable specific embeddings and pre-training. TABT5 achieves new state-of-the-art\nresults on several domains, including spreadsheet formula prediction with a 15%\nincrease in sequence accuracy, QA with a 2.5% increase in sequence accuracy and\ndata-to-text generation with a 2.5% increase in BLEU."}
{"id": "2208.06213", "title": "What is it like to program with artificial intelligence?", "url": "https://arxiv.org/abs/2208.06213", "pdf": "https://arxiv.org/pdf/2208.06213", "abs": "https://arxiv.org/abs/2208.06213", "authors": ["Advait Sarkar", "Andrew D. Gordon", "Carina Negreanu", "Christian Poelitz", "Sruti Srinivasa Ragavan", "Ben Zorn"], "categories": ["cs.HC", "cs.AI", "cs.PL", "D.2.3; D.2.6; I.2.5; I.2.7; H.5.2"], "comment": "Proceedings of the 33rd Annual Conference of the Psychology of\n  Programming Interest Group (PPIG 2022)", "summary": "Large language models, such as OpenAI's codex and Deepmind's AlphaCode, can\ngenerate code to solve a variety of problems expressed in natural language.\nThis technology has already been commercialised in at least one widely-used\nprogramming editor extension: GitHub Copilot.\n  In this paper, we explore how programming with large language models\n(LLM-assisted programming) is similar to, and differs from, prior\nconceptualisations of programmer assistance. We draw upon publicly available\nexperience reports of LLM-assisted programming, as well as prior usability and\ndesign studies. We find that while LLM-assisted programming shares some\nproperties of compilation, pair programming, and programming via search and\nreuse, there are fundamental differences both in the technical possibilities as\nwell as the practical experience. Thus, LLM-assisted programming ought to be\nviewed as a new way of programming with its own distinct properties and\nchallenges.\n  Finally, we draw upon observations from a user study in which non-expert end\nuser programmers use LLM-assisted tools for solving data tasks in spreadsheets.\nWe discuss the issues that might arise, and open research challenges, in\napplying large language models to end-user programming, particularly with users\nwho have little or no programming expertise."}
{"id": "2209.14812", "title": "Named Entity Recognition in Industrial Tables using Tabular Language Models", "url": "https://arxiv.org/abs/2209.14812", "pdf": "https://arxiv.org/pdf/2209.14812", "abs": "https://arxiv.org/abs/2209.14812", "authors": ["Aneta Koleva", "Martin Ringsquandl", "Mark Buckley", "Rakebul Hasan", "Volker Tresp"], "categories": ["cs.AI", "cs.CL"], "comment": "EMNLP 2022 Industry Track", "summary": "Specialized transformer-based models for encoding tabular data have gained\ninterest in academia. Although tabular data is omnipresent in industry,\napplications of table transformers are still missing. In this paper, we study\nhow these models can be applied to an industrial Named Entity Recognition (NER)\nproblem where the entities are mentioned in tabular-structured spreadsheets.\nThe highly technical nature of spreadsheets as well as the lack of labeled data\npresent major challenges for fine-tuning transformer-based models. Therefore,\nwe develop a dedicated table data augmentation strategy based on available\ndomain-specific knowledge graphs. We show that this boosts performance in our\nlow-resource scenario considerably. Further, we investigate the benefits of\ntabular structure as inductive bias compared to tables as linearized sequences.\nOur experiments confirm that a table transformer outperforms other baselines\nand that its tabular inductive bias is vital for convergence of\ntransformer-based models."}
{"id": "2209.12560", "title": "Hazard Analysis of Collaborative Automation Systems: A Two-layer Approach based on Supervisory Control and Simulation", "url": "https://arxiv.org/abs/2209.12560", "pdf": "https://arxiv.org/pdf/2209.12560", "abs": "https://arxiv.org/abs/2209.12560", "authors": ["Tom P. Huck", "Yuvaraj Selvaraj", "Constantin Cronrath", "Christoph Ledermann", "Martin Fabian", "Bengt Lennartson", "Torsten Kröger"], "categories": ["cs.RO"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Safety critical systems are typically subjected to hazard analysis before\ncommissioning to identify and analyse potentially hazardous system states that\nmay arise during operation. Currently, hazard analysis is mainly based on human\nreasoning, past experiences, and simple tools such as checklists and\nspreadsheets. Increasing system complexity makes such approaches decreasingly\nsuitable. Furthermore, testing-based hazard analysis is often not suitable due\nto high costs or dangers of physical faults. A remedy for this are model-based\nhazard analysis methods, which either rely on formal models or on simulation\nmodels, each with their own benefits and drawbacks. This paper proposes a\ntwo-layer approach that combines the benefits of exhaustive analysis using\nformal methods with detailed analysis using simulation. Unsafe behaviours that\nlead to unsafe states are first synthesised from a formal model of the system\nusing Supervisory Control Theory. The result is then input to the simulation\nwhere detailed analyses using domain-specific risk metrics are performed.\nThough the presented approach is generally applicable, this paper demonstrates\nthe benefits of the approach on an industrial human-robot collaboration system."}
{"id": "2208.04738", "title": "Long-Term Mentoring for Computer Science Researchers", "url": "https://arxiv.org/abs/2208.04738", "pdf": "https://arxiv.org/pdf/2208.04738", "abs": "https://arxiv.org/abs/2208.04738", "authors": ["Emily Ruppel", "Sihang Liu", "Elba Garza", "Sukyoung Ryu", "Alexandra Silva", "Talia Ringer"], "categories": ["cs.CY", "cs.GL", "cs.PL"], "comment": null, "summary": "Early in the pandemic, we -- leaders in the research areas of programming\nlanguages (PL) and computer architecture (CA) -- realized that we had a\nproblem: the only way to form new lasting connections in the community was to\nalready have lasting connections in the community. Both of our academic\ncommunities had wonderful short-term mentoring programs to address this\nproblem, but it was clear that we needed long-term mentoring programs.\n  Those of us in CA approached this scientifically, making an evidence-backed\ncase for community-wide long-term mentoring. In the meantime, one of us in PL\nhad impulsively launched an unofficial long-term mentoring program, founded on\nchaos and spreadsheets. In January 2021, the latter grew to an official\ncross-institutional long-term mentoring program called SIGPLAN-M; in January\n2022, the former grew to Computer Architecture Long-term Mentoring (CALM).\n  The impacts have been strong: SIGPLAN-M reaches 328 mentees and 234 mentors\nacross 41 countries, and mentees have described it as \"life changing\" and \"a\ncareer saver.\" And while CALM is in its pilot phase -- with 13 mentors and 21\nmentees across 7 countries -- it has received very positive feedback. The\nleaders of SIGPLAN-M and CALM shared our designs, impacts, and challenges along\nthe way. Now, we wish to share those with you. We hope this will kick-start a\nlarger long-term mentoring effort across all of computer science."}
{"id": "2209.05739", "title": "MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization", "url": "https://arxiv.org/abs/2209.05739", "pdf": "https://arxiv.org/pdf/2209.05739", "abs": "https://arxiv.org/abs/2209.05739", "authors": ["Lu Ying", "Xinhuan Shu", "Dazhen Deng", "Yuchen Yang", "Tan Tang", "Lingyun Yu", "Yingcai Wu"], "categories": ["cs.HC"], "comment": null, "summary": "Glyph-based visualization achieves an impressive graphic design when\nassociated with comprehensive visual metaphors, which help audiences\neffectively grasp the conveyed information through revealing data semantics.\nHowever, creating such metaphoric glyph-based visualization (MGV) is not an\neasy task, as it requires not only a deep understanding of data but also\nprofessional design skills. This paper proposes MetaGlyph, an automatic system\nfor generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct\na qualitative analysis to understand the design of current MGVs from the\nperspectives of metaphor embodiment and glyph design. Based on the results, we\nintroduce a novel framework for generating MGVs by metaphoric image selection\nand an MGV construction. Specifically, MetaGlyph automatically selects\nmetaphors with corresponding images from online resources based on the input\ndata semantics. We then integrate a Monte Carlo tree search algorithm that\nexplores the design of an MGV by associating visual elements with data\ndimensions given the data importance, semantic relevance, and glyph\nnon-overlap. The system also provides editing feedback that allows users to\ncustomize the MGVs according to their design preferences. We demonstrate the\nuse of MetaGlyph through a set of examples, one usage scenario, and validate\nits effectiveness through a series of expert interviews."}
{"id": "2204.03128", "title": "Sigma Workbook: A Spreadsheet for Cloud Data Warehouses", "url": "https://arxiv.org/abs/2204.03128", "pdf": "https://arxiv.org/pdf/2204.03128", "abs": "https://arxiv.org/abs/2204.03128", "authors": ["James Gale", "Max Seiden", "Deepanshu Utkarsh", "Jason Frantz", "Rob Woollen", "Çağatay Demiralp"], "categories": ["cs.DB", "cs.HC"], "comment": "VLDB'22 Demonstrations", "summary": "Cloud data warehouses (CDWs) bring large-scale data and compute power closer\nto users in enterprises. However, existing tools for analyzing data in CDWs are\neither limited in ad-hoc transformations or difficult to use for business\nusers. Here we introduce Sigma Workbook, a new interactive system that enables\nbusiness users to easily perform a visual analysis of data in CDWs at scale.\nFor this, Sigma Workbook provides an accessible spreadsheet-like interface for\nanalysis through direct manipulation. Sigma Workbook dynamically constructs\nmatching SQL queries from user interactions, building on the versatility and\nexpressivity of SQL. Constructed queries are directly executed on CDWs,\nleveraging the superior characteristics of the new generation CDWs, including\nscalability. We demonstrate Sigma Workbook through 3 real-life use cases --\ncohort analysis, sessionization, and data augmentation -- and underline\nWorkbook's ease of use, scalability, and expressivity."}
{"id": "2204.00598", "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language", "url": "https://arxiv.org/abs/2204.00598", "pdf": "https://arxiv.org/pdf/2204.00598", "abs": "https://arxiv.org/abs/2204.00598", "authors": ["Andy Zeng", "Maria Attarian", "Brian Ichter", "Krzysztof Choromanski", "Adrian Wong", "Stefan Welker", "Federico Tombari", "Aveek Purohit", "Michael Ryoo", "Vikas Sindhwani", "Johnny Lee", "Vincent Vanhoucke", "Pete Florence"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "https://socraticmodels.github.io/", "summary": "Large pretrained (e.g., \"foundation\") models exhibit distinct capabilities\ndepending on the domain of data they are trained on. While these domains are\ngeneric, they may only barely overlap. For example, visual-language models\n(VLMs) are trained on Internet-scale image captions, but large language models\n(LMs) are further trained on Internet-scale text with no images (e.g.,\nspreadsheets, SAT questions, code). As a result, these models store different\nforms of commonsense knowledge across different domains. In this work, we show\nthat this diversity is symbiotic, and can be leveraged through Socratic Models\n(SMs): a modular framework in which multiple pretrained models may be composed\nzero-shot i.e., via multimodal-informed prompting, to exchange information with\neach other and capture new multimodal capabilities, without requiring\nfinetuning. With minimal engineering, SMs are not only competitive with\nstate-of-the-art zero-shot image captioning and video-to-text retrieval, but\nalso enable new applications such as (i) answering free-form questions about\negocentric video, (ii) engaging in multimodal assistive dialogue with people\n(e.g., for cooking recipes) by interfacing with external APIs and databases\n(e.g., web search), and (iii) robot perception and planning."}
{"id": "2201.09745", "title": "Table Pre-training: A Survey on Model Architectures, Pre-training Objectives, and Downstream Tasks", "url": "https://arxiv.org/abs/2201.09745", "pdf": "https://arxiv.org/pdf/2201.09745", "abs": "https://arxiv.org/abs/2201.09745", "authors": ["Haoyu Dong", "Zhoujun Cheng", "Xinyi He", "Mengyu Zhou", "Anda Zhou", "Fan Zhou", "Ao Liu", "Shi Han", "Dongmei Zhang"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted by IJCAI'2022 survey track", "summary": "Since a vast number of tables can be easily collected from web pages,\nspreadsheets, PDFs, and various other document types, a flurry of table\npre-training frameworks have been proposed following the success of text and\nimages, and they have achieved new state-of-the-arts on various tasks such as\ntable question answering, table type recognition, column relation\nclassification, table search, formula prediction, etc. To fully use the\nsupervision signals in unlabeled tables, a variety of pre-training objectives\nhave been designed and evaluated, for example, denoising cell values,\npredicting numerical relationships, and implicitly executing SQLs. And to best\nleverage the characteristics of (semi-)structured tables, various tabular\nlanguage models, particularly with specially-designed attention mechanisms,\nhave been explored. Since tables usually appear and interact with free-form\ntext, table pre-training usually takes the form of table-text joint\npre-training, which attracts significant research interests from multiple\ndomains. This survey aims to provide a comprehensive review of different model\ndesigns, pre-training objectives, and downstream tasks for table pre-training,\nand we further share our thoughts and vision on existing challenges and future\nopportunities."}
{"id": "2109.07323", "title": "FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining", "url": "https://arxiv.org/abs/2109.07323", "pdf": "https://arxiv.org/pdf/2109.07323", "abs": "https://arxiv.org/abs/2109.07323", "authors": ["Zhoujun Cheng", "Haoyu Dong", "Ran Jia", "Pengfei Wu", "Shi Han", "Fan Cheng", "Dongmei Zhang"], "categories": ["cs.IR", "cs.LG"], "comment": "Accepted by ACL'22 main track", "summary": "Tables store rich numerical data, but numerical reasoning over tables is\nstill a challenge. In this paper, we find that the spreadsheet formula, which\nperforms calculations on numerical values in tables, is naturally a strong\nsupervision of numerical reasoning. More importantly, large amounts of\nspreadsheets with expert-made formulae are available on the web and can be\nobtained easily. FORTAP is the first method for numerical-reasoning-aware table\npretraining by leveraging large corpus of spreadsheet formulae. We design two\nformula pretraining tasks to explicitly guide FORTAP to learn numerical\nreference and calculation in semi-structured tables. FORTAP achieves\nstate-of-the-art results on two representative downstream tasks, cell type\nclassification and formula prediction, showing great potential of\nnumerical-reasoning-aware pretraining."}
{"id": "2202.13189", "title": "Efficient Specialized Spreadsheet Parsing for Data Science", "url": "https://arxiv.org/abs/2202.13189", "pdf": "https://arxiv.org/pdf/2202.13189", "abs": "https://arxiv.org/abs/2202.13189", "authors": ["Felix Henze", "Haralampos Gavriilidis", "Eleni Tzirita Zacharatou", "Volker Markl"], "categories": ["cs.DB"], "comment": "Accepted at the 24th International Workshop on Design, Optimization,\n  Languages and Analytical Processing of Big Data (DOLAP 2022), March 29, 2022,\n  Edinburgh, UK", "summary": "Spreadsheets are widely used for data exploration. Since spreadsheet systems\nhave limited capabilities, users often need to load spreadsheets to other data\nscience environments to perform advanced analytics. However, current approaches\nfor spreadsheet loading suffer from either high runtime or memory usage, which\nhinders data exploration on commodity systems. To make spreasheet loading\npractical on commodity systems, we introduce a novel parser that minimizes\nmemory usage by tightly coupling decompression and parsing. Furthermore, to\nreduce the runtime, we introduce optimized spreadsheet-specific parsing\nroutines and employ parallelism. To evaluate our approach, we implement a\nprototype for loading Excel spreadsheets into R environments. Our evaluation\nshows that our novel approach is up to 3x faster while consuming up to 40x less\nmemory than state-of-the-art approaches."}
{"id": "2203.16346", "title": "Enhanced Spreadsheet Computing with Finite-Domain Constraint Satisfaction", "url": "https://arxiv.org/abs/2203.16346", "pdf": "https://arxiv.org/pdf/2203.16346", "abs": "https://arxiv.org/abs/2203.16346", "authors": ["Ezana N. Beyenne", "Hai-Feng Guo"], "categories": ["cs.PL", "cs.AI", "I.2"], "comment": null, "summary": "The spreadsheet application is among the most widely used computing tools in\nmodern society. It provides excellent usability and usefulness, and it easily\nenables a non-programmer to perform programming-like tasks in a visual tabular\n\"pen and paper\" approach. However, spreadsheets are mostly limited to\nbookkeeping-like applications due to their mono-directional data flow. This\npaper shows how the spreadsheet computing paradigm is extended to break this\nlimitation for solving constraint satisfaction problems. We present an enhanced\nspreadsheet system where finite-domain constraint solving is well supported in\na visual environment. Furthermore, a spreadsheet-specific constraint language\nis constructed for general users to specify constraints among data cells in a\ndeclarative and scalable way. The new spreadsheet system significantly\nsimplifies the development of many constraint-based applications using a visual\ntabular interface. Examples are given to illustrate the usability and\nusefulness of the extended spreadsheet paradigm.\n  KEYWORDS: Spreadsheet computing, Finite-domain constraint satisfaction,\nConstraint logic programming"}
{"id": "2203.10944", "title": "Spreadsheet computing with Finite Domain Constraint Enhancements", "url": "https://arxiv.org/abs/2203.10944", "pdf": "https://arxiv.org/pdf/2203.10944", "abs": "https://arxiv.org/abs/2203.10944", "authors": ["Ezana N. Beyenne"], "categories": ["cs.AI", "I.2"], "comment": "2008 Master's thesis", "summary": "Spreadsheet computing is one of the more popular computing methodologies in\ntoday's modern society. The spreadsheet application's ease of use and\nusefulness has enabled non-programmers to perform programming-like tasks in a\nfamiliar setting modeled after the tabular \"pen and paper\" approach. However,\nspreadsheet applications are limited to bookkeeping-like tasks due to their\nsingle-direction data flow. This thesis demonstrates an extension of the\nspreadsheet computing paradigm in overcoming this limitation to solve\nconstraint satisfaction problems. We present a framework seamlessly\nincorporating a finite constraint solver with the spreadsheet computing\nparadigm. This framework allows the individual cells in the spreadsheet to be\nattached to either a finite domain or a constraint specifying the relationship\namong the cells. The framework provides an interface for constraint solving and\nfurther enhances the spreadsheet computing paradigm by providing a set of\nspreadsheet-specific constraints that will aid in controlling the scalability\nof large spreadsheet applications implementations. Finally, we provide examples\nto demonstrate the usability and usefulness of the extended spreadsheet\nparadigm.\n  Keywords: Spreadsheet computing, Constraint Logic Programming, Constraint\nsatisfaction, Domain-Specific language, Excel, SWI Prolog, C#"}
{"id": "2202.00454", "title": "TableQuery: Querying tabular data with natural language", "url": "https://arxiv.org/abs/2202.00454", "pdf": "https://arxiv.org/pdf/2202.00454", "abs": "https://arxiv.org/abs/2202.00454", "authors": ["Abhijith Neil Abraham", "Fariz Rahman", "Damanpreet Kaur"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "11 pages, 1 figures", "summary": "This paper presents TableQuery, a novel tool for querying tabular data using\ndeep learning models pre-trained to answer questions on free text. Existing\ndeep learning methods for question answering on tabular data have various\nlimitations, such as having to feed the entire table as input into a neural\nnetwork model, making them unsuitable for most real-world applications. Since\nreal-world data might contain millions of rows, it may not entirely fit into\nthe memory. Moreover, data could be stored in live databases, which are updated\nin real-time, and it is impractical to serialize an entire database to a neural\nnetwork-friendly format each time it is updated. In TableQuery, we use deep\nlearning models pre-trained for question answering on free text to convert\nnatural language queries to structured queries, which can be run against a\ndatabase or a spreadsheet. This method eliminates the need for fitting the\nentire data into memory as well as serializing databases. Furthermore, deep\nlearning models pre-trained for question answering on free text are readily\navailable on platforms such as HuggingFace Model Hub (7). TableQuery does not\nrequire re-training; when a newly trained model for question answering with\nbetter performance is available, it can replace the existing model in\nTableQuery."}
{"id": "2201.06337", "title": "PoVRPoint: Authoring Presentations in Mobile Virtual Reality", "url": "https://arxiv.org/abs/2201.06337", "pdf": "https://arxiv.org/pdf/2201.06337", "abs": "https://arxiv.org/abs/2201.06337", "authors": ["Verena Biener", "Travis Gesslein", "Daniel Schneider", "Felix Kawala", "Alexander Otte", "Per Ola Kristensson", "Michel Pahud", "Eyal Ofek", "Cuauhtli Campos", "Matjaž Kljun", "Klen Čopič Pucihar", "Jens Grubert"], "categories": ["cs.HC", "I.3.7"], "comment": "IEEE VR 2022; to appear in IEEE transactions on visualization and\n  computer graphics, 2022", "summary": "Virtual Reality (VR) has the potential to support mobile knowledge workers by\ncomplementing traditional input devices with a large three-dimensional output\nspace and spatial input. Previous research on supporting VR knowledge work\nexplored domains such as text entry using physical keyboards and spreadsheet\ninteraction using combined pen and touch input. Inspired by such work, this\npaper probes the VR design space for authoring presentations in mobile\nsettings. We propose PoVRPoint -- a set of tools coupling pen- and touch-based\nediting of presentations on mobile devices, such as tablets, with the\ninteraction capabilities afforded by VR. We study the utility of extended\ndisplay space to, for example, assist users in identifying target slides,\nsupporting spatial manipulation of objects on a slide, creating animations, and\nfacilitating arrangements of multiple, possibly occluded, shapes. Among other\nthings, our results indicate that 1) the wide field of view afforded by VR\nresults in significantly faster target slide identification times compared to a\ntablet-only interface for visually salient targets; and 2) the\nthree-dimensional view in VR enables significantly faster object reordering in\nthe presence of occlusion compared to two baseline interfaces. A user study\nfurther confirmed that the interaction techniques were found to be usable and\nenjoyable."}
{"id": "2201.01654", "title": "TableParser: Automatic Table Parsing with Weak Supervision from Spreadsheets", "url": "https://arxiv.org/abs/2201.01654", "pdf": "https://arxiv.org/pdf/2201.01654", "abs": "https://arxiv.org/abs/2201.01654", "authors": ["Susie Xi Rao", "Johannes Rausch", "Peter Egger", "Ce Zhang"], "categories": ["cs.CV"], "comment": "accepted in the AAAI-22 Workshop on Scientific Document Understanding\n  at the Thirty-Sixth AAAI Conference on Artificial Intelligence (SDU@AAAI-22)", "summary": "Tables have been an ever-existing structure to store data. There exist now\ndifferent approaches to store tabular data physically. PDFs, images,\nspreadsheets, and CSVs are leading examples. Being able to parse table\nstructures and extract content bounded by these structures is of high\nimportance in many applications. In this paper, we devise TableParser, a system\ncapable of parsing tables in both native PDFs and scanned images with high\nprecision. We have conducted extensive experiments to show the efficacy of\ndomain adaptation in developing such a tool. Moreover, we create TableAnnotator\nand ExcelAnnotator, which constitute a spreadsheet-based weak supervision\nmechanism and a pipeline to enable table parsing. We share these resources with\nthe research community to facilitate further research in this interesting\ndirection."}
{"id": "2201.07696", "title": "Exploring Spreadsheet Use and Practices in a Technologically Constrained Setting", "url": "https://arxiv.org/abs/2201.07696", "pdf": "https://arxiv.org/pdf/2201.07696", "abs": "https://arxiv.org/abs/2201.07696", "authors": ["Khwima Mckinley Mkamanga", "Simon Thorne"], "categories": ["cs.SE"], "comment": "23 pages, 7 colour figures, 17 Tables and a Sample Questionnaire", "summary": "This paper explores the impacts of spreadsheets on business operations in a\nwater utility parastatal in Malawi, Sub-Saharan Africa. The organisation is a\ntypical example of a semi-government body operating in a technologically\nunderdeveloped country. The study focused on spreadsheet scope of use and life\ncycle as well as organisational policy and governance. The results will help\ndefine future spreadsheet usage by influencing new approaches for managing\npotential risks associated with spreadsheets in the organization. Generally,\nfindings indicate that the proliferation of spreadsheets in the organization\nhas provided an enabling environment for business automation. The paper also\nhighlights management, technological and human factor issues contributing to\nhigh risks associated with the pervasive spreadsheet use. The conclusions drawn\nfrom the research confirms that there is ample room for improvement in many\nareas such as implementation of comprehensive policies and regulations\ngoverning spreadsheet development processes and adoption."}
{"id": "2110.12829", "title": "Spread2RML: Constructing Knowledge Graphs by Predicting RML Mappings on Messy Spreadsheets", "url": "https://arxiv.org/abs/2110.12829", "pdf": "https://arxiv.org/pdf/2110.12829", "abs": "https://arxiv.org/abs/2110.12829", "authors": ["Markus Schröder", "Christian Jilek", "Andreas Dengel"], "categories": ["cs.DB"], "comment": "17 pages, 1 figure, 2 tables, accepted at K-CAP 2021", "summary": "The RDF Mapping Language (RML) allows to map semi-structured data to RDF\nknowledge graphs. Besides CSV, JSON and XML, this also includes the mapping of\nspreadsheet tables. Since spreadsheets have a complex data model and can become\nrather messy, their mapping creation tends to be very time consuming. In order\nto reduce such efforts, this paper presents Spread2RML which predicts RML\nmappings on messy spreadsheets. This is done with an extensible set of RML\nobject map templates which are applied for each column based on heuristics. In\nour evaluation, three datasets are used ranging from very messy synthetic data\nto spreadsheets from data.gov which are less messy. We obtained first promising\nresults especially with regard to our approach being fully automatic and\ndealing with rather messy data."}
{"id": "2110.11575", "title": "Methodology for Assessing the State of the Practice for Domain X", "url": "https://arxiv.org/abs/2110.11575", "pdf": "https://arxiv.org/pdf/2110.11575", "abs": "https://arxiv.org/abs/2110.11575", "authors": ["Spencer Smith", "Jacques Carette", "Peter Michalski", "Ao Dong", "Olu Owojaiye"], "categories": ["cs.SE", "D.2.0"], "comment": "35 pages, 3 figures", "summary": "To improve software development methods and tools for research software, we\nfirst need to understand the current state of the practice. Therefore, we have\ndeveloped a methodology for assessing the state of the software development\npractices for a given research software domain. For each domain we wish to\nanswer questions such as: i) What artifacts (documents, code, test cases, etc.)\nare present? ii) What tools are used? iii) What principles, process and\nmethodologies are used? iv) What are the pain points for developers? v) What\nactions are used to improve qualities like maintainability and reproducibility?\nTo answer these questions, our methodology prescribes the following steps: i)\nIdentify the domain; ii) Identify a list of candidate software packages; iii)\nFilter the list to a length of about 30 packages; iv) Gather source code and\ndocumentation for each package; v) Collect repository related data on each\nsoftware package, like number of stars, number of open issues, number of lines\nof code; vi) Fill in the measurement template (the template consists of 108\nquestions to assess 9 qualities (including the qualities of installability,\nusability and visibility)); vii) Interview developers (the interview consists\nof 20 questions and takes about an hour); viii) Rank the software using the\nAnalytic Hierarchy Process (AHP); and, ix) Analyze the data to answer the\nquestions posed above. A domain expert should be engaged throughout the\nprocess, to ensure that implicit information about the domain is properly\nrepresented and to assist with conducting an analysis of the commonalities and\nvariabilities between the 30 selected packages. Using our methodology,\nspreadsheet templates and AHP tool, we estimate (based on our experience with\nusing the process) the time to complete an assessment for a given domain at 173\nperson hours."}
{"id": "2110.08993", "title": "Typed Image-based Programming with Structure Editing", "url": "https://arxiv.org/abs/2110.08993", "pdf": "https://arxiv.org/pdf/2110.08993", "abs": "https://arxiv.org/abs/2110.08993", "authors": ["Jonathan Edwards", "Tomas Petricek"], "categories": ["cs.PL", "cs.SE"], "comment": "Accepted to: Human Aspects of Types and Reasoning Assistants\n  (HATRA'21), Oct 19, 2021, Chicago, US", "summary": "Many beloved programming systems are image-based: self-contained worlds that\npersist both code and data in a single file. Examples include Smalltalk, LISP,\nHyperCard, Flash, and spreadsheets. Image-based programming avoids much of the\ncomplexity of modern programming technology stacks and encourages more casual\nand exploratory programming. However conventional file-based programming has\nbetter support for collaboration and deployment. These problems have been\nblamed for the limited commercial success of Smalltalk. We propose to enable\ncollaboration in image-based programming via types and structure editing.\n  We focus on the problem of schema change on persistent data. We turn to\nstatic types, which paradoxically require more schema change but also provide a\nmechanism to express and execute those changes. To determine those changes we\nturn to structure editing, so that we can capture changes in type definitions\nwith sufficient fidelity to automatically adapt the data to suit. We conjecture\nthat typical schema changes can be handled through structure editing of static\ntypes.\n  That positions us to tackle collaboration with what could be called version\ncontrol for structure editing. We present a theory realizing this idea, which\nis our main technical contribution. While we focus here on editing types, if we\ncan extend the approach to cover the entire programming experience then it\nwould offer a new way to collaborate in image-based programming."}
{"id": "2109.06630", "title": "Detecting Layout Templates in Complex Multiregion Files", "url": "https://arxiv.org/abs/2109.06630", "pdf": "https://arxiv.org/pdf/2109.06630", "abs": "https://arxiv.org/abs/2109.06630", "authors": ["Gerardo Vitagliano", "Lan Jiang", "Felix Naumann"], "categories": ["cs.IR"], "comment": null, "summary": "Spreadsheets are among the most commonly used file formats for data\nmanagement, distribution, and analysis. Their widespread employment makes it\neasy to gather large collections of data, but their flexible canvas-based\nstructure makes automated analysis difficult without heavy preparation. One of\nthe common problems that practitioners face is the presence of multiple,\nindependent regions in a single spreadsheet, possibly separated by repeated\nempty cells. We define such files as \"multiregion\" files. In collections of\nvarious spreadsheets, we can observe that some share the same layout. We\npresent the Mondrian approach to automatically identify layout templates across\nmultiple files and systematically extract the corresponding regions. Our\napproach is composed of three phases: first, each file is rendered as an image\nand inspected for elements that could form regions; then, using a clustering\nalgorithm, the identified elements are grouped to form regions; finally, every\nfile layout is represented as a graph and compared with others to find layout\ntemplates. We compare our method to state-of-the-art table recognition\nalgorithms on two corpora of real-world enterprise spreadsheets. Our approach\nshows the best performances in detecting reliable region boundaries within each\nfile and can correctly identify recurring layouts across files."}
{"id": "2109.07267", "title": "JUBILEE: Secure Debt Relief and Forgiveness", "url": "https://arxiv.org/abs/2109.07267", "pdf": "https://arxiv.org/pdf/2109.07267", "abs": "https://arxiv.org/abs/2109.07267", "authors": ["David Cerezo Sánchez"], "categories": ["cs.CR", "cs.GT", "econ.GN", "q-fin.EC"], "comment": null, "summary": "JUBILEE is a securely computed mechanism for debt relief and forgiveness in a\nfrictionless manner without involving trusted third parties, leading to more\nharmonious debt settlements by incentivising the parties to truthfully reveal\ntheir private information. JUBILEE improves over all previous methods:\n  - individually rational, incentive-compatible, truthful/strategy-proof,\nex-post efficient, optimal mechanism for debt relief and forgiveness with\nprivate information\n  - by the novel introduction of secure computation techniques to debt relief,\nthe \"blessing of the debtor\" is hereby granted for the first time: debt\nsettlements with higher expected profits and a higher probability of success\nthan without using secure computation\n  A simple and practical implementation is included for \"The Secure\nSpreadsheet\". Another implementation is realised using Raziel smart contracts\non a blockchain with Pravuil consensus."}
{"id": "2108.11525", "title": "Supercomputing Enabled Deployable Analytics for Disaster Response", "url": "https://arxiv.org/abs/2108.11525", "pdf": "https://arxiv.org/pdf/2108.11525", "abs": "https://arxiv.org/abs/2108.11525", "authors": ["Kaira Samuel", "Jeremy Kepner", "Michael Jones", "Lauren Milechin", "Vijay Gadepally", "William Arcand", "David Bestor", "William Bergeron", "Chansup Byun", "Matthew Hubbell", "Michael Houle", "Anna Klein", "Victor Lopez", "Julie Mullen", "Andrew Prout", "Albert Reuther", "Antonio Rosa", "Sid Samsi", "Charles Yee", "Peter Michaleas"], "categories": ["cs.DB", "cs.DC", "cs.GR", "cs.HC", "cs.MM"], "comment": "5 pages, 11 figures, 17 references, accepted to IEEE HPEC 2021", "summary": "First responders and other forward deployed essential workers can benefit\nfrom advanced analytics. Limited network access and software security\nrequirements prevent the usage of standard cloud based microservice analytic\nplatforms that are typically used in industry. One solution is to precompute a\nwide range of analytics as files that can be used with standard preinstalled\nsoftware that does not require network access or additional software and can\nrun on a wide range of legacy hardware. In response to the COVID-19 pandemic,\nthis approach was tested for providing geo-spatial census data to allow quick\nanalysis of demographic data for better responding to emergencies. These data\nwere processed using the MIT SuperCloud to create several thousand Google Earth\nand Microsoft Excel files representative of many advanced analytics. The fast\nmapping of census data using Google Earth and Microsoft Excel has the potential\nto give emergency responders a powerful tool to improve emergency preparedness.\nOur approach displays relevant census data (total population, population under\n15, population over 65, median age) per census block, sorted by county, through\na Microsoft Excel spreadsheet (xlsx file) and Google Earth map (kml file). The\nspreadsheet interface includes features that allow users to convert between\ndifferent longitude and latitude coordinate units. For the Google Earth files,\na variety of absolute and relative colors maps of population density have been\nexplored to provide an intuitive and meaningful interface. Using several\nhundred cores on the MIT SuperCloud, new analytics can be generated in a few\nminutes."}
{"id": "2108.00567", "title": "Agile Elicitation of Scalability Requirements for Open Systems: A Case Study", "url": "https://arxiv.org/abs/2108.00567", "pdf": "https://arxiv.org/pdf/2108.00567", "abs": "https://arxiv.org/abs/2108.00567", "authors": ["Gunnar Brataas", "Antonio Martini", "Geir Kjetil Hanssen", "Georg Ræder"], "categories": ["cs.SE", "cs.PF", "C.4; D.2.1"], "comment": "36 pages, 7 figures, 6 tables, accepted for publication in Journal of\n  Systems and Software", "summary": "Eliciting scalability requirements during agile software development is\ncomplicated and poorly described in previous research. This article presents a\nlightweight artifact for eliciting scalability requirements during agile\nsoftware development: the ScrumScale model. The ScrumScale model is a simple\nspreadsheet. The scalability concepts underlying the ScrumScale model are\nclarified in this design science research, which also utilizes coordination\ntheory. This paper describes the open banking case study, where a legacy\nbanking system becomes open. This challenges the scalability of this legacy\nsystem. The first step in understanding this challenge is to elicit the new\nscalability requirements. In the open banking case study, key stakeholders from\nTietoEVRY spent 55 hours eliciting TietoEVRY's open banking project's\nscalability requirements. According to TietoEVRY, the ScrumScale model provided\na systematic way of producing scalability requirements. For TietoEVRY, the\nscalability concepts behind the ScrumScale model also offered significant\nadvantages in dialogues with other stakeholders."}
{"id": "2107.13957", "title": "Towards Semantic Interoperability in Historical Research: Documenting Research Data and Knowledge with Synthesis", "url": "https://arxiv.org/abs/2107.13957", "pdf": "https://arxiv.org/pdf/2107.13957", "abs": "https://arxiv.org/abs/2107.13957", "authors": ["Pavlos Fafalios", "Konstantina Konsolaki", "Lida Charami", "Kostas Petrakis", "Manos Paterakis", "Dimitris Angelakis", "Yannis Tzitzikas", "Chrysoula Bekiari", "Martin Doerr"], "categories": ["cs.DL", "cs.DB"], "comment": "This is a preprint of an article accepted for publication at the 20th\n  International Semantic Web Conference (ISWC 2021)", "summary": "A vast area of research in historical science concerns the documentation and\nstudy of artefacts and related evidence. Current practice mostly uses\nspreadsheets or simple relational databases to organise the information as rows\nwith multiple columns of related attributes. This form offers itself for data\nanalysis and scholarly interpretation, however it also poses problems including\ni) the difficulty for collaborative but controlled documentation by a large\nnumber of users, ii) the lack of representation of the details from which the\ndocumented relations are inferred, iii) the difficulty to extend the underlying\ndata structures as well as to combine and integrate data from multiple and\ndiverse information sources, and iv) the limitation to reuse the data beyond\nthe context of a particular research activity. To support historians to cope\nwith these problems, in this paper we describe the Synthesis documentation\nsystem and its use by a large number of historians in the context of an ongoing\nresearch project in the field of History of Art. The system is Web-based and\ncollaborative, and makes use of existing standards for information\ndocumentation and publication (CIDOC-CRM, RDF), focusing on semantic\ninteroperability and the production of data of high value and long-term\nvalidity."}
{"id": "2010.12537", "title": "TUTA: Tree-based Transformers for Generally Structured Table Pre-training", "url": "https://arxiv.org/abs/2010.12537", "pdf": "https://arxiv.org/pdf/2010.12537", "abs": "https://arxiv.org/abs/2010.12537", "authors": ["Zhiruo Wang", "Haoyu Dong", "Ran Jia", "Jia Li", "Zhiyi Fu", "Shi Han", "Dongmei Zhang"], "categories": ["cs.IR", "cs.AI", "cs.DB"], "comment": "KDD'21", "summary": "Tables are widely used with various structures to organize and present data.\nRecent attempts on table understanding mainly focus on relational tables, yet\noverlook to other common table structures. In this paper, we propose TUTA, a\nunified pre-training architecture for understanding generally structured\ntables. Noticing that understanding a table requires spatial, hierarchical, and\nsemantic information, we enhance transformers with three novel structure-aware\nmechanisms. First, we devise a unified tree-based structure, called a\nbi-dimensional coordinate tree, to describe both the spatial and hierarchical\ninformation of generally structured tables. Upon this, we propose tree-based\nattention and position embedding to better capture the spatial and hierarchical\ninformation. Moreover, we devise three progressive pre-training objectives to\nenable representations at the token, cell, and table levels. We pre-train TUTA\non a wide range of unlabeled web and spreadsheet tables and fine-tune it on two\ncritical tasks in the field of table structure understanding: cell type\nclassification and table type classification. Experiments show that TUTA is\nhighly effective, achieving state-of-the-art on five widely-studied datasets."}
{"id": "2106.15005", "title": "Untidy Data: The Unreasonable Effectiveness of Tables", "url": "https://arxiv.org/abs/2106.15005", "pdf": "https://arxiv.org/pdf/2106.15005", "abs": "https://arxiv.org/abs/2106.15005", "authors": ["Lyn Bartram", "Michael Correll", "Melanie Tory"], "categories": ["cs.HC"], "comment": null, "summary": "Working with data in table form is usually considered a preparatory and\ntedious step in the sensemaking pipeline; a way of getting the data ready for\nmore sophisticated visualization and analytical tools. But for many people,\nspreadsheets -- the quintessential table tool -- remain a critical part of\ntheir information ecosystem, allowing them to interact with their data in ways\nthat are hidden or abstracted in more complex tools. This is particularly true\nfor data workers: people who work with data as part of their job but do not\nidentify as professional analysts or data scientists. We report on a\nqualitative study of how these workers interact with and reason about their\ndata. Our findings show that data tables serve a broader purpose beyond data\ncleanup at the initial stage of a linear analytic flow: users want to see and\n\"get their hands on\" the underlying data throughout the analytics process,\nreshaping and augmenting it to support sensemaking. They reorganize, mark up,\nlayer on levels of detail, and spawn alternatives within the context of the\nbase data. These direct interactions and human-readable table representations\nform a rich and cognitively important part of building understanding of what\nthe data mean and what they can do with it. We argue that interactive tables\nare an important visualization idiom in their own right; that the direct data\ninteraction they afford offers a fertile design space for visual analytics; and\nthat sense making can be enriched by more flexible human-data interaction than\nis currently supported in visual analytics tools."}
{"id": "2008.11015", "title": "Table2Charts: Recommending Charts by Learning Shared Table Representations", "url": "https://arxiv.org/abs/2008.11015", "pdf": "https://arxiv.org/pdf/2008.11015", "abs": "https://arxiv.org/abs/2008.11015", "authors": ["Mengyu Zhou", "Qingtao Li", "Xinyi He", "Yuejiang Li", "Yibo Liu", "Wei Ji", "Shi Han", "Yining Chen", "Daxin Jiang", "Dongmei Zhang"], "categories": ["cs.DB", "cs.CL", "cs.HC"], "comment": "9 + 2(appendix) pages, accepted by KDD'21 conference", "summary": "It is common for people to create different types of charts to explore a\nmulti-dimensional dataset (table). However, to recommend commonly composed\ncharts in real world, one should take the challenges of efficiency, imbalanced\ndata and table context into consideration. In this paper, we propose\nTable2Charts framework which learns common patterns from a large corpus of\n(table, charts) pairs. Based on deep Q-learning with copying mechanism and\nheuristic searching, Table2Charts does table-to-sequence generation, where each\nsequence follows a chart template. On a large spreadsheet corpus with 165k\ntables and 266k charts, we show that Table2Charts could learn a shared\nrepresentation of table fields so that recommendation tasks on different chart\ntypes could mutually enhance each other. Table2Charts outperforms other chart\nrecommendation systems in both multi-type task (with doubled recall numbers\nR@3=0.61 and R@1=0.43) and human evaluations."}
{"id": "2106.15339", "title": "SpreadsheetCoder: Formula Prediction from Semi-structured Context", "url": "https://arxiv.org/abs/2106.15339", "pdf": "https://arxiv.org/pdf/2106.15339", "abs": "https://arxiv.org/abs/2106.15339", "authors": ["Xinyun Chen", "Petros Maniatis", "Rishabh Singh", "Charles Sutton", "Hanjun Dai", "Max Lin", "Denny Zhou"], "categories": ["cs.SE", "cs.LG", "cs.PL"], "comment": "Published in ICML 2021", "summary": "Spreadsheet formula prediction has been an important program synthesis\nproblem with many real-world applications. Previous works typically utilize\ninput-output examples as the specification for spreadsheet formula synthesis,\nwhere each input-output pair simulates a separate row in the spreadsheet.\nHowever, this formulation does not fully capture the rich context in real-world\nspreadsheets. First, spreadsheet data entries are organized as tables, thus\nrows and columns are not necessarily independent from each other. In addition,\nmany spreadsheet tables include headers, which provide high-level descriptions\nof the cell data. However, previous synthesis approaches do not consider\nheaders as part of the specification. In this work, we present the first\napproach for synthesizing spreadsheet formulas from tabular context, which\nincludes both headers and semi-structured tabular data. In particular, we\npropose SpreadsheetCoder, a BERT-based model architecture to represent the\ntabular context in both row-based and column-based formats. We train our model\non a large dataset of spreadsheets, and demonstrate that SpreadsheetCoder\nachieves top-1 prediction accuracy of 42.51%, which is a considerable\nimprovement over baselines that do not employ rich tabular context. Compared to\nthe rule-based system, SpreadsheetCoder assists 82% more users in composing\nformulas on Google Sheets."}
{"id": "2106.13500", "title": "TableSense: Spreadsheet Table Detection with Convolutional Neural Networks", "url": "https://arxiv.org/abs/2106.13500", "pdf": "https://arxiv.org/pdf/2106.13500", "abs": "https://arxiv.org/abs/2106.13500", "authors": ["Haoyu Dong", "Shijie Liu", "Shi Han", "Zhouyu Fu", "Dongmei Zhang"], "categories": ["cs.IR"], "comment": null, "summary": "Spreadsheet table detection is the task of detecting all tables on a given\nsheet and locating their respective ranges. Automatic table detection is a key\nenabling technique and an initial step in spreadsheet data intelligence.\nHowever, the detection task is challenged by the diversity of table structures\nand table layouts on the spreadsheet. Considering the analogy between a cell\nmatrix as spreadsheet and a pixel matrix as image, and encouraged by the\nsuccessful application of Convolutional Neural Networks (CNN) in computer\nvision, we have developed TableSense, a novel end-to-end framework for\nspreadsheet table detection. First, we devise an effective cell featurization\nscheme to better leverage the rich information in each cell; second, we develop\nan enhanced convolutional neural network model for table detection to meet the\ndomain-specific requirement on precise table boundary detection; third, we\npropose an effective uncertainty metric to guide an active learning based smart\nsampling algorithm, which enables the efficient build-up of a training dataset\nwith 22,176 tables on 10,220 sheets with broad coverage of diverse table\nstructures and layouts. Our evaluation shows that TableSense is highly\neffective with 91.3\\% recall and 86.5\\% precision in EoB-2 metric, a\nsignificant improvement over both the current detection algorithm that are used\nin commodity spreadsheet tools and state-of-the-art convolutional neural\nnetworks in computer vision."}
{"id": "2106.03096", "title": "TabularNet: A Neural Network Architecture for Understanding Semantic Structures of Tabular Data", "url": "https://arxiv.org/abs/2106.03096", "pdf": "https://arxiv.org/pdf/2106.03096", "abs": "https://arxiv.org/abs/2106.03096", "authors": ["Lun Du", "Fei Gao", "Xu Chen", "Ran Jia", "Junshan Wang", "Jiang Zhang", "Shi Han", "Dongmei Zhang"], "categories": ["cs.LG"], "comment": "10 pages, 7 figures, to be published in the proceedings of KDD 2021", "summary": "Tabular data are ubiquitous for the widespread applications of tables and\nhence have attracted the attention of researchers to extract underlying\ninformation. One of the critical problems in mining tabular data is how to\nunderstand their inherent semantic structures automatically. Existing studies\ntypically adopt Convolutional Neural Network (CNN) to model the spatial\ninformation of tabular structures yet ignore more diverse relational\ninformation between cells, such as the hierarchical and paratactic\nrelationships. To simultaneously extract spatial and relational information\nfrom tables, we propose a novel neural network architecture, TabularNet. The\nspatial encoder of TabularNet utilizes the row/column-level Pooling and the\nBidirectional Gated Recurrent Unit (Bi-GRU) to capture statistical information\nand local positional correlation, respectively. For relational information, we\ndesign a new graph construction method based on the WordNet tree and adopt a\nGraph Convolutional Network (GCN) based encoder that focuses on the\nhierarchical and paratactic relationships between cells. Our neural network\narchitecture can be a unified neural backbone for different understanding tasks\nand utilized in a multitask scenario. We conduct extensive experiments on three\nclassification tasks with two real-world spreadsheet data sets, and the results\ndemonstrate the effectiveness of our proposed TabularNet over state-of-the-art\nbaselines."}
{"id": "2105.13733", "title": "FAST CAT: Collaborative Data Entry and Curation for Semantic Interoperability in Digital Humanities", "url": "https://arxiv.org/abs/2105.13733", "pdf": "https://arxiv.org/pdf/2105.13733", "abs": "https://arxiv.org/abs/2105.13733", "authors": ["Pavlos Fafalios", "Kostas Petrakis", "Georgios Samaritakis", "Korina Doerr", "Athina Kritsotaki", "Yannis Tzitzikas", "Martin Doerr"], "categories": ["cs.DL", "cs.DB"], "comment": "This is a preprint of an article accepted for publication at the ACM\n  Journal on Computing and Cultural Heritage (JOCCH)", "summary": "Descriptive and empirical sciences, such as History, are the sciences that\ncollect, observe and describe phenomena in order to explain them and draw\ninterpretative conclusions about influences, driving forces and impacts under\ngiven circumstances. Spreadsheet software and relational database management\nsystems are still the dominant tools for quantitative analysis and overall data\nmanagement in these these sciences, allowing researchers to directly analyse\nthe gathered data and perform scholarly interpretation. However, this current\npractice has a set of limitations, including the high dependency of the\ncollected data on the initial research hypothesis, usually useless for other\nresearch, the lack of representation of the details from which the registered\nrelations are inferred, and the difficulty to revisit the original data sources\nfor verification, corrections or improvements. To cope with these problems, in\nthis paper we present FAST CAT, a collaborative system for assistive data entry\nand curation in Digital Humanities and similar forms of empirical research. We\ndescribe the related challenges, the overall methodology we follow for\nsupporting semantic interoperability, and discuss the use of FAST CAT in the\ncontext of a European (ERC) project of Maritime History, called SeaLiT, which\nexamines economic, social and demographic impacts of the introduction of\nsteamboats in the Mediterranean area between the 1850s and the 1920s."}
{"id": "2012.00697", "title": "Sigma Worksheet: Interactive Construction of OLAP Queries", "url": "https://arxiv.org/abs/2012.00697", "pdf": "https://arxiv.org/pdf/2012.00697", "abs": "https://arxiv.org/abs/2012.00697", "authors": ["James Gale", "Max Seiden", "Gretchen Atwood", "Jason Frantz", "Rob Woollen", "Çağatay Demiralp"], "categories": ["cs.DB", "cs.HC"], "comment": null, "summary": "The new generation of cloud data warehouses (CDWs) brings large amounts of\ndata and compute power closer to users in enterprises. The ability to directly\naccess the warehouse data, interactively analyze and explore it at scale can\nempower users to improve their decision making cycles. However, existing tools\nfor analyzing data in CDWs are either limited in ad-hoc transformations or\ndifficult to use for business users, the largest user segment in enterprises.\nHere we introduce Sigma Worksheet, a new interactive system that enables users\nto easily perform ad-hoc visual analysis of data in CDWs at scale. For this,\nSigma Worksheet provides an accessible spreadsheet-like interface for data\nanalysis through direct manipulation. Sigma Worksheet dynamically constructs\nmatching SQL queries from user interactions on this familiar interface,\nbuilding on the versatility and expressivity of SQL. Sigma Worksheet executes\nconstructed queries directly on CDWs, leveraging the superior characteristics\nof the new generation CDWs, including scalability. To evaluate Sigma Worksheet,\nwe first demonstrate its expressivity through two real life use cases, cohort\nanalysis and sessionization. We then measure the performance of the Worksheet\ngenerated queries with a set of experiments using the TPC-H benchmark. Results\nshow the performance of our compiled SQL queries is comparable to that of the\nreference queries of the benchmark. Finally, to assess the usefulness of Sigma\nWorksheet in deployment, we elicit feedback through a 100-person survey\nfollowed by a semi-structured interview study with 70 participants. We find\nthat Sigma Worksheet is easier to use and learn, improving the productivity of\nusers. Our findings also suggest Sigma Worksheet can further improve user\nexperience by providing guidance to users at various steps of data analysis."}
{"id": "2104.13600", "title": "Mapping Spreadsheets to RDF: Supporting Excel in RML", "url": "https://arxiv.org/abs/2104.13600", "pdf": "https://arxiv.org/pdf/2104.13600", "abs": "https://arxiv.org/abs/2104.13600", "authors": ["Markus Schröder", "Christian Jilek", "Andreas Dengel"], "categories": ["cs.DB"], "comment": "6 pages, submitted to Second International Workshop on Knowledge\n  Graph Construction", "summary": "The RDF Mapping Language (RML) enables, among other formats, the mapping of\ntabular data as Comma-Separated Values (CSV) files to RDF graphs.\nUnfortunately, the widely used spreadsheet format is currently neglected by its\nspecification and well-known implementations. Therefore, we extended one of the\ntools which is RML Mapper to support Microsoft Excel spreadsheet files and\ndemonstrate its capabilities in an interactive online demo. Our approach allows\nto access various meta data of spreadsheet cells in typical RML maps. Some\nexperimental features for more specific use cases are also provided. The\nimplementation code is publicly available in a GitHub fork."}
{"id": "2104.13576", "title": "Dataset Generation Patterns for Evaluating Knowledge Graph Construction", "url": "https://arxiv.org/abs/2104.13576", "pdf": "https://arxiv.org/pdf/2104.13576", "abs": "https://arxiv.org/abs/2104.13576", "authors": ["Markus Schröder", "Christian Jilek", "Andreas Dengel"], "categories": ["cs.DB"], "comment": "5 pages, submitted to ESWC demo track", "summary": "Confidentiality hinders the publication of authentic, labeled datasets of\npersonal and enterprise data, although they could be useful for evaluating\nknowledge graph construction approaches in industrial scenarios. Therefore, our\nplan is to synthetically generate such data in a way that it appears as\nauthentic as possible. Based on our assumption that knowledge workers have\ncertain habits when they produce or manage data, generation patterns could be\ndiscovered which can be utilized by data generators to imitate real datasets.\nIn this paper, we initially derived 11 distinct patterns found in real\nspreadsheets from industry and demonstrate a suitable generator called Data\nSprout that is able to reproduce them. We describe how the generator produces\nspreadsheets in general and what altering effects the implemented patterns\nhave."}
{"id": "2103.15203", "title": "Mathematics of Digital Hyperspace", "url": "https://arxiv.org/abs/2103.15203", "pdf": "https://arxiv.org/pdf/2103.15203", "abs": "https://arxiv.org/abs/2103.15203", "authors": ["Jeremy Kepner", "Timothy Davis", "Vijay Gadepally", "Hayden Jananthan", "Lauren Milechin"], "categories": ["cs.MS", "cs.DB", "cs.DM", "cs.NE", "math.RA"], "comment": "9 pages, 8 figures, 2 tables, accepted to GrAPL 2021. arXiv admin\n  note: text overlap with arXiv:1807.03165, arXiv:2004.01181, arXiv:1909.05631,\n  arXiv:1708.02937", "summary": "Social media, e-commerce, streaming video, e-mail, cloud documents, web\npages, traffic flows, and network packets fill vast digital lakes, rivers, and\noceans that we each navigate daily. This digital hyperspace is an amorphous\nflow of data supported by continuous streams that stretch standard concepts of\ntype and dimension. The unstructured data of digital hyperspace can be\nelegantly represented, traversed, and transformed via the mathematics of\nhypergraphs, hypersparse matrices, and associative array algebra. This paper\nexplores a novel mathematical concept, the semilink, that combines pairs of\nsemirings to provide the essential operations for graph analytics, database\noperations, and machine learning. The GraphBLAS standard currently supports\nhypergraphs, hypersparse matrices, the mathematics required for semilinks, and\nseamlessly performs graph, network, and matrix operations. With the addition of\nkey based indices (such as pointers to strings) and semilinks, GraphBLAS can\nbecome a richer associative array algebra and be a plug-in replacement for\nspreadsheets, database tables, and data centric operating systems, enhancing\nthe navigation of unstructured data found in digital hyperspace."}
{"id": "2103.10472", "title": "Dynamic Kernel Matching for Non-conforming Data: A Case Study of T-cell Receptor Datasets", "url": "https://arxiv.org/abs/2103.10472", "pdf": "https://arxiv.org/pdf/2103.10472", "abs": "https://arxiv.org/abs/2103.10472", "authors": ["Jared Ostmeyer", "Scott Christley", "Lindsay Cowell"], "categories": ["q-bio.QM", "cs.LG"], "comment": null, "summary": "Most statistical classifiers are designed to find patterns in data where\nnumbers fit into rows and columns, like in a spreadsheet, but many kinds of\ndata do not conform to this structure. To uncover patterns in non-conforming\ndata, we describe an approach for modifying established statistical classifiers\nto handle non-conforming data, which we call dynamic kernel matching (DKM). As\nexamples of non-conforming data, we consider (i) a dataset of T-cell receptor\n(TCR) sequences labelled by disease antigen and (ii) a dataset of sequenced TCR\nrepertoires labelled by patient cytomegalovirus (CMV) serostatus, anticipating\nthat both datasets contain signatures for diagnosing disease. We successfully\nfit statistical classifiers augmented with DKM to both datasets and report the\nperformance on holdout data using standard metrics and metrics allowing for\nindeterminant diagnoses. Finally, we identify the patterns used by our\nstatistical classifiers to generate predictions and show that these patterns\nagree with observations from experimental studies."}
{"id": "2103.03537", "title": "Interactively Constructing Knowledge Graphs from Messy User-Generated Spreadsheets", "url": "https://arxiv.org/abs/2103.03537", "pdf": "https://arxiv.org/pdf/2103.03537", "abs": "https://arxiv.org/abs/2103.03537", "authors": ["Markus Schröder", "Christian Jilek", "Michael Schulze", "Andreas Dengel"], "categories": ["cs.DB"], "comment": "15 pages", "summary": "When spreadsheets are filled freely by knowledge workers, they can contain\nrather unstructured content. For humans and especially machines it becomes\ndifficult to interpret such data properly. Therefore, spreadsheets are often\nconverted to a more explicit, formal and structured form, for example, to a\nknowledge graph. However, if a data maintenance strategy has been missing and\nuser-generated data becomes \"messy\", the construction of knowledge graphs will\nbe a challenging task. In this paper, we catalog several of those challenges\nand propose an interactive approach to solve them. Our approach includes a\ngraphical user interface which enables knowledge engineers to bulk-annotate\nspreadsheet cells with extracted information. Based on the cells' annotations a\nknowledge graph is ultimately formed. Using five spreadsheets from an\nindustrial scenario, we built a 25k-triple graph during our evaluation. We\ncompared our method with the state-of-the-art RDF Mapping Language (RML)\nattempt. The comparison highlights contributions of our approach."}
{"id": "2102.09461", "title": "Optimization Helps Scheduling Nursing Staff at the Long-Term Care Homes of the City of Toronto", "url": "https://arxiv.org/abs/2102.09461", "pdf": "https://arxiv.org/pdf/2102.09461", "abs": "https://arxiv.org/abs/2102.09461", "authors": ["Manion Anderson", "Merve Bodur", "Scott Rathwell", "Vahid Sarhangian"], "categories": ["cs.CY", "math.OC"], "comment": null, "summary": "The City of Toronto Long Term Care Homes & Services (LTCH&S) division is one\nof the largest providers of long-term care in the Canadian province of Ontario,\nproviding care to 2,640 residents at 10 homes across Toronto. Our collaboration\nwith LTCH&S was initiated to facilitate the increasingly challenging task of\nscheduling nursing staff and reduce high absenteeism rate observed among the\npart-time nurses. We developed a spreadsheet-based scheduling tool to automate\nthe generation of schedules and incorporate nurses' preferences for different\nshifts into the schedules. At the core of the scheduling tool is a hierarchical\noptimization model that generates a feasible schedule with the highest total\npreference score while satisfying the maximum possible demand. Feasible\nschedules had to abide by a set of complex seniority requirements which\nprioritized more senior nurses when allocating the available shifts. Our\nscheduling tool was implemented in a 391-bed home in Toronto. The tool allowed\nnursing managers to generate feasible schedules within a fraction of an hour,\nin contrast to the status-quo manual approach which could took up to tens of\nhours. In addition, the schedules successfully accounted for preferences with\non average above 94% of the allocated shifts ranked as most preferred."}
{"id": "2012.01571", "title": "Online Model Swapping in Architectural Simulation", "url": "https://arxiv.org/abs/2012.01571", "pdf": "https://arxiv.org/pdf/2012.01571", "abs": "https://arxiv.org/abs/2012.01571", "authors": ["Patrick Lavin", "Jeffrey Young", "Rich Vuduc", "Jonathan Beard"], "categories": ["cs.AR"], "comment": null, "summary": "As systems and applications grow more complex, detailed simulation takes an\never increasing amount of time. The prospect of increased simulation time\nresulting in slower design iteration forces architects to use simpler models,\nsuch as spreadsheets, when they want to iterate quickly on a design. However,\nthe task of migrating from a simple simulation to one with more detail often\nrequires multiple executions to find where simple models could be effective,\nwhich could be more expensive than running the detailed model in the first\nplace. Also, architects must often rely on intuition to choose these simpler\nmodels, further complicating the problem.\n  In this work, we present a method of bridging the gap between simple and\ndetailed simulation by monitoring simulation behavior online and automatically\nswapping out detailed models with simpler statistical approximations. We\ndemonstrate the potential of our methodology by implementing it in the\nopen-source simulator SVE-Cachesim to swap out the level one data cache (L1D)\nwithin a memory hierarchy. This proof of concept demonstrates that our\ntechnique can handle a non-trivial use-case in not just approximation of local\ntime-invariant statistics, but also those that vary with time (e.g., the L1D is\na form of a time-series function), and downstream side-effects (e.g., the L1D\nfilters accesses for the level two cache). Our simulation swaps out the\nbuilt-in cache model with only an 8% error in the simulated cycle count while\nusing the approximated cache models for over 90% of the simulation, and our\nsimpler models require two to eight times less computation per \"execution\" of\nthe model"}
{"id": "1909.00855", "title": "Defining and Adopting an End User Computing Policy: A Case Study", "url": "https://arxiv.org/abs/1909.00855", "pdf": "https://arxiv.org/pdf/1909.00855", "abs": "https://arxiv.org/abs/1909.00855", "authors": ["Roger Turner"], "categories": ["cs.HC", "cs.CY"], "comment": "25 Pages, 12 Colour Figures. 1 Table. First presented at the EuSpRIG\n  2018 Conference at Imperial College, London. Revised and updated following a\n  further presentation at the EuSpRIG 2019 Conference also at Imperial College,\n  London", "summary": "End User Computing carries significant risks if not well controlled. This\npaper is a case study of the introduction of an updated End User Computing\npolicy at the Wesleyan Assurance Society. The paper outlines the plan and\nidentifies various challenges. The paper explains how these challenges were\novercome. We wrote an End User Computing Risk Assessment Application which\ncalculates a risk rating band based on the Complexity, Materiality and Control\n(or lack of it) pertaining to any given application and the basis of assessment\nis given in this paper. The policy uses a risk based approach for assessing and\nmitigating against the highest risks first and obtaining the quickest benefit."}
{"id": "2011.05978", "title": "The Impact of Text Presentation on Translator Performance", "url": "https://arxiv.org/abs/2011.05978", "pdf": "https://arxiv.org/pdf/2011.05978", "abs": "https://arxiv.org/abs/2011.05978", "authors": ["Samuel Läubli", "Patrick Simianer", "Joern Wuebker", "Geza Kovacs", "Rico Sennrich", "Spence Green"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted for publication in Target", "summary": "Widely used computer-aided translation (CAT) tools divide documents into\nsegments such as sentences and arrange them in a side-by-side, spreadsheet-like\nview. We present the first controlled evaluation of these design choices on\ntranslator performance, measuring speed and accuracy in three experimental text\nprocessing tasks. We find significant evidence that sentence-by-sentence\npresentation enables faster text reproduction and within-sentence error\nidentification compared to unsegmented text, and that a top-and-bottom\narrangement of source and target sentences enables faster text reproduction\ncompared to a side-by-side arrangement. For revision, on the other hand, our\nresults suggest that presenting unsegmented text results in the highest\naccuracy and time efficiency. Our findings have direct implications for best\npractices in designing CAT tools."}
{"id": "2010.09975", "title": "Calliope: Automatic Visual Data Story Generation from a Spreadsheet", "url": "https://arxiv.org/abs/2010.09975", "pdf": "https://arxiv.org/pdf/2010.09975", "abs": "https://arxiv.org/abs/2010.09975", "authors": ["Danqing Shi", "Xinyue Xu", "Fuling Sun", "Yang Shi", "Nan Cao"], "categories": ["cs.HC"], "comment": null, "summary": "Visual data stories shown in the form of narrative visualizations such as a\nposter or a data video, are frequently used in data-oriented storytelling to\nfacilitate the understanding and memorization of the story content. Although\nuseful, technique barriers, such as data analysis, visualization, and\nscripting, make the generation of a visual data story difficult. Existing\nauthoring tools rely on users' skills and experiences, which are usually\ninefficient and still difficult. In this paper, we introduce a novel visual\ndata story generating system, Calliope, which creates visual data stories from\nan input spreadsheet through an automatic process and facilities the easy\nrevision of the generated story based on an online story editor. Particularly,\nCalliope incorporates a new logic-oriented Monte Carlo tree search algorithm\nthat explores the data space given by the input spreadsheet to progressively\ngenerate story pieces (i.e., data facts) and organize them in a logical order.\nThe importance of data facts is measured based on information theory, and each\ndata fact is visualized in a chart and captioned by an automatically generated\ndescription. We evaluate the proposed technique through three example stories,\ntwo controlled experiments, and a series of interviews with 10 domain experts.\nOur evaluation shows that Calliope is beneficial to efficient visual data story\ngeneration."}
{"id": "2009.03520", "title": "Leam: An Interactive System for In-situ Visual Text Analysis", "url": "https://arxiv.org/abs/2009.03520", "pdf": "https://arxiv.org/pdf/2009.03520", "abs": "https://arxiv.org/abs/2009.03520", "authors": ["Sajjadur Rahman", "Peter Griggs", "Çağatay Demiralp"], "categories": ["cs.DB", "cs.CL", "cs.HC"], "comment": null, "summary": "With the increase in scale and availability of digital text generated on the\nweb, enterprises such as online retailers and aggregators often use text\nanalytics to mine and analyze the data to improve their services and products\nalike. Text data analysis is an iterative, non-linear process with diverse\nworkflows spanning multiple stages, from data cleaning to visualization.\nExisting text analytics systems usually accommodate a subset of these stages\nand often fail to address challenges related to data heterogeneity, provenance,\nworkflow reusability and reproducibility, and compatibility with established\npractices. Based on a set of design considerations we derive from these\nchallenges, we propose Leam, a system that treats the text analysis process as\na single continuum by combining advantages of computational notebooks,\nspreadsheets, and visualization tools. Leam features an interactive user\ninterface for running text analysis workflows, a new data model for managing\nmultiple atomic and composite data types, and an expressive algebra that\ncaptures diverse sets of operations representing various stages of text\nanalysis and enables coordination among different components of the system,\nincluding data, code, and visualizations. We report our current progress in\nLeam development while demonstrating its usefulness with usage examples.\nFinally, we outline a number of enhancements to Leam and identify several\nresearch directions for developing an interactive visual text analysis system."}
{"id": "2008.04543", "title": "Pen-based Interaction with Spreadsheets in Mobile Virtual Reality", "url": "https://arxiv.org/abs/2008.04543", "pdf": "https://arxiv.org/pdf/2008.04543", "abs": "https://arxiv.org/abs/2008.04543", "authors": ["Travis Gesslein", "Verena Biener", "Philipp Gagel", "Daniel Schneider", "Per Ola Kristensson", "Eyal Ofek", "Michel Pahud", "Jens Grubert"], "categories": ["cs.HC", "I.3.7"], "comment": "10 pages, 11 figures, ISMAR 2020", "summary": "Virtual Reality (VR) can enhance the display and interaction of mobile\nknowledge work and in particular, spreadsheet applications. While spreadsheets\nare widely used yet are challenging to interact with, especially on mobile\ndevices, using them in VR has not been explored in depth. A special uniqueness\nof the domain is the contrast between the immersive and large display space\nafforded by VR, contrasted by the very limited interaction space that may be\nafforded for the information worker on the go, such as an airplane seat or a\nsmall work-space. To close this gap, we present a tool-set for enhancing\nspreadsheet interaction on tablets using immersive VR headsets and pen-based\ninput. This combination opens up many possibilities for enhancing the\nproductivity for spreadsheet interaction. We propose to use the space around\nand in front of the tablet for enhanced visualization of spreadsheet data and\nmeta-data. For example, extending sheet display beyond the bounds of the\nphysical screen, or easier debugging by uncovering hidden dependencies between\nsheet's cells. Combining the precise on-screen input of a pen with spatial\nsensing around the tablet, we propose tools for the efficient creation and\nediting of spreadsheets functions such as off-the-screen layered menus,\nvisualization of sheets dependencies, and gaze-and-touch-based switching\nbetween spreadsheet tabs. We study the feasibility of the proposed tool-set\nusing a video-based online survey and an expert-based assessment of indicative\nhuman performance potential."}
{"id": "2005.05227", "title": "ObjTables: structured spreadsheets that promote data quality, reuse, and integration", "url": "https://arxiv.org/abs/2005.05227", "pdf": "https://arxiv.org/pdf/2005.05227", "abs": "https://arxiv.org/abs/2005.05227", "authors": ["Jonathan R. Karr", "Wolfram Liebermeister", "Arthur P. Goldberg", "John A. P. Sekar", "Bilal Shaikh"], "categories": ["cs.DB", "q-bio.QM"], "comment": "5 pages, 1 figures, 18 pages of supplementary information, 3\n  supplementary datasets", "summary": "A central challenge in science is to understand how systems behaviors emerge\nfrom complex networks. This often requires aggregating, reusing, and\nintegrating heterogeneous information. Supplementary spreadsheets to articles\nare a key data source. Spreadsheets are popular because they are easy to read\nand write. However, spreadsheets are often difficult to reanalyze because they\ncapture data ad hoc without schemas that define the objects, relationships, and\nattributes that they represent. To help researchers reuse and compose\nspreadsheets, we developed ObjTables, a toolkit that makes spreadsheets human-\nand machine-readable by combining spreadsheets with schemas and an\nobject-relational mapping system. ObjTables includes a format for schemas;\nmarkup for indicating the class and attribute represented by each spreadsheet\nand column; numerous data types for scientific information; and high-level\nsoftware for using schemas to read, write, validate, compare, merge, revision,\nand analyze spreadsheets. By making spreadsheets easier to reuse, ObjTables\ncould enable unprecedented secondary meta-analyses. By making it easy to build\nnew formats and associated software for new types of data, ObjTables can also\naccelerate emerging scientific fields."}
{"id": "2007.00003", "title": "EQUS -- helping to see formulae", "url": "https://arxiv.org/abs/2007.00003", "pdf": "https://arxiv.org/pdf/2007.00003", "abs": "https://arxiv.org/abs/2007.00003", "authors": ["Chris Roast"], "categories": ["cs.HC", "cs.SE"], "comment": "12 Pages, 7 Colour Figures", "summary": "Visualisation is often presented as a means of simplifying information and\nhelping people understand complex data. In this paper we describe the design,\ndevelopment and evaluation of an interactive visualisation for spreadsheet\nformulae (EQUS). The work is justified on the grounds that these are widely\nused tools for significant numerical processing and modeling, yet the formula\ndeveloped can be easily misunderstood. The development process was one of\niterative refinement engaging an initial target audience of mid-teen learners,\ninvolving re-design and formative evaluation. The resulting visualisation\ntechniques have been found to be broadly relevant to spreadsheet users beyond\nthe initial target audience. EQUS has since been developed as fully integrated\nplug-in for MS Excel."}
{"id": "2006.14694", "title": "From webtables to datatables", "url": "https://arxiv.org/abs/2006.14694", "pdf": "https://arxiv.org/pdf/2006.14694", "abs": "https://arxiv.org/abs/2006.14694", "authors": ["Mária Csernoch"], "categories": ["cs.SE"], "comment": "22 pages, 34 Formulae & 21 Colour Figures", "summary": "Webtables -- tables and table-like structures on webpages -- are excellent\nsources for teaching spreadsheeting, in commercial and professional\norganisations by utilizing and developing knowledge-transfer items, presenting\nand handling various real-world problems and solutions, discussing and\ndebugging, and in general, developing and utilizing computational thinking\nskills. In the present paper the conversion process of one of the LOL Boards\n(League of Legends, Riot Games Inc. 2019) is detailed. After presenting the\nalgorithm of the conversion, two solutions are offered -- one in a word\nprocessor, the other purely in a spreadsheet application -- leaving space for\ndiscussions, inventing other solutions and combining them."}
{"id": "2006.08224", "title": "Needles in the 'Sheet'stack: Augmented Analytics to get Insights from Spreadsheets", "url": "https://arxiv.org/abs/2006.08224", "pdf": "https://arxiv.org/pdf/2006.08224", "abs": "https://arxiv.org/abs/2006.08224", "authors": ["Medha Atre", "Anand Deshpande", "Reshma Godse", "Pooja Deokar", "Sandip Moharir", "Dhruva Ray", "Akshay Chitlangia", "Trupti Phadnis", "Yugansh Goyal"], "categories": ["cs.DB", "cs.HC", "H.2.8"], "comment": null, "summary": "Business intelligence (BI) tools for database analytics have come a long way\nand nowadays also provide ready insights or visual query explorations, e.g.\nQuickInsights by Microsoft Power BI, SpotIQ by ThoughtSpot, Zenvisage, etc. In\nthis demo, we focus on providing insights by examining periodic spreadsheets of\ndifferent reports (aka views), without prior knowledge of the schema of the\ndatabase or reports, or data information. Such a solution is targeted at users\nwithout the familiarity with the database schema or resources to conduct\nanalytics in the contemporary way."}
{"id": "2006.05814", "title": "Implementation Strategies for Multidimensional Spreadsheets", "url": "https://arxiv.org/abs/2006.05814", "pdf": "https://arxiv.org/pdf/2006.05814", "abs": "https://arxiv.org/abs/2006.05814", "authors": ["Paul Mireault"], "categories": ["cs.SE"], "comment": "12 Pages, 18 Colour Figures. arXiv admin note: text overlap with\n  arXiv:1801.09777", "summary": "Seasoned Excel developers were invited to participate in a challenge to\nimplement a spreadsheet with multi-dimensional variables. We analyzed their\nspreadsheet to see the different implement strategies employed. We identified\ntwo strategies: most participants used a projection of three or\nfour-dimensional variables on the two-dimensional plane used by Excel. A few\nparticipants used a database approach where the multi-dimensional variables are\npresented in the form of a dataset table with the appropriate primary key. This\napproach leads to simpler formulas."}
{"id": "2006.04794", "title": "Abstracting spreadsheet data flow through hypergraph redrawing", "url": "https://arxiv.org/abs/2006.04794", "pdf": "https://arxiv.org/pdf/2006.04794", "abs": "https://arxiv.org/abs/2006.04794", "authors": ["David Birch", "Nicolai Stawinoga", "Jack Binks", "Bruno Nicoletti", "Paul Kelly"], "categories": ["cs.SE"], "comment": "23 Pages, 12 Colour Figures", "summary": "We believe the error prone nature of traditional spreadsheets is due to their\nlow level of abstraction. End user programmers are forced to construct their\ndata models from low level cells which we define as \"a data container or\nmanipulator linked by user-intent to model their world and positioned to\nreflect its structure\". Spreadsheet cells are limited in what they may contain\n(scalar values) and the links between them are inherently hidden. This paper\nproposes a method of raising the level of abstraction of spreadsheets by\n\"redrawing the boundary\" of the cell. To expose the hidden linkage structure we\ntransform spreadsheets into fine-grained graphs with operators and values as\nnodes. \"cells\" are then represented as hypergraph edges by drawing a boundary\n\"wall\" around a set of operator/data nodes. To extend what cells may contain\nand to create a higher level model of the spreadsheet we propose that\nresearchers should seek techniques to redraw these boundaries to create higher\nlevel \"cells\" which will more faithfully represent the end-user's real\nworld/mental model. We illustrate this approach via common sub-expression\nidentification and the application of sub-tree isomorphisms for the detection\nof vector (array) operations."}
{"id": "2006.04793", "title": "Developing Excel Thought Leadership", "url": "https://arxiv.org/abs/2006.04793", "pdf": "https://arxiv.org/pdf/2006.04793", "abs": "https://arxiv.org/abs/2006.04793", "authors": ["David Lyford-Smith"], "categories": ["cs.CY"], "comment": "8 Pages", "summary": "Over a period of five years, the Institute of Chartered Accountants in\nEngland and Wales (ICAEW) has developed a suite of three 'thought leadership'\npapers surrounding good practice in spreadsheet use and spreadsheet work\nenvironments. We will review the history of these three papers, the key lessons\nwhich each has to teach, and discuss how the process of making them has helped\nICAEW to develop its position in the field."}
{"id": "2004.11113", "title": "Human-Machine Collaboration for Democratizing Data Science", "url": "https://arxiv.org/abs/2004.11113", "pdf": "https://arxiv.org/pdf/2004.11113", "abs": "https://arxiv.org/abs/2004.11113", "authors": ["Clément Gautrais", "Yann Dauxais", "Stefano Teso", "Samuel Kolb", "Gust Verbruggen", "Luc De Raedt"], "categories": ["cs.AI", "cs.HC", "I.2.1; H.5.2"], "comment": "26 pages", "summary": "Everybody wants to analyse their data, but only few posses the data science\nexpertise to to this. Motivated by this observation we introduce a novel\nframework and system \\textsc{VisualSynth} for human-machine collaboration in\ndata science.\n  It wants to democratize data science by allowing users to interact with\nstandard spreadsheet software in order to perform and automate various data\nanalysis tasks ranging from data wrangling, data selection, clustering,\nconstraint learning, predictive modeling and auto-completion.\n\\textsc{VisualSynth} relies on the user providing colored sketches, i.e.,\ncoloring parts of the spreadsheet, to partially specify data science tasks,\nwhich are then determined and executed using artificial intelligence\ntechniques."}
{"id": "2001.01007", "title": "Automated Discovery of Data Transformations for Robotic Process Automation", "url": "https://arxiv.org/abs/2001.01007", "pdf": "https://arxiv.org/pdf/2001.01007", "abs": "https://arxiv.org/abs/2001.01007", "authors": ["Volodymyr Leno", "Marlon Dumas", "Marcello La Rosa", "Fabrizio Maria Maggi", "Artem Polyvyanyy"], "categories": ["cs.AI"], "comment": "8 pages, 5 figures. To be published in proceedings of AAAI-20\n  workshop on Intelligent Process Automation", "summary": "Robotic Process Automation (RPA) is a technology for automating repetitive\nroutines consisting of sequences of user interactions with one or more\napplications. In order to fully exploit the opportunities opened by RPA,\ncompanies need to discover which specific routines may be automated, and how.\nIn this setting, this paper addresses the problem of analyzing User Interaction\n(UI) logs in order to discover routines where a user transfers data from one\nspreadsheet or (Web) form to another. The paper maps this problem to that of\ndiscovering data transformations by example - a problem for which several\ntechniques are available. The paper shows that a naive application of a\nstate-of-the-art technique for data transformation discovery is computationally\ninefficient. Accordingly, the paper proposes two optimizations that take\nadvantage of the information in the UI log and the fact that data transfers\nacross applications typically involve copying alphabetic and numeric tokens\nseparately. The proposed approach and its optimizations are evaluated using UI\nlogs that replicate a real-life repetitive data transfer routine."}
{"id": "1912.09209", "title": "Comprehensive review for common types of errors using spreadsheets", "url": "https://arxiv.org/abs/1912.09209", "pdf": "https://arxiv.org/pdf/1912.09209", "abs": "https://arxiv.org/abs/1912.09209", "authors": ["Ali Aburas"], "categories": ["cs.SE"], "comment": null, "summary": "Thanks to their flexibility and capability to perform different tasks and\norganize data in the best form and format, spreadsheets are widely used in\ndifferent organizations and by different end users. Many business organizations\nrely on spreadsheets to fulfill their various tasks. On the other hand, the\nnumber of spreadsheets that contain errors are very high, thus researchers have\ndeveloped different tools aimed at the prevention, detection, and correction of\nerrors in spreadsheets. This research work is a comprehensive review that\ndescribes and classifies approaches on finding and fixing errors in\nspreadsheets. The paper discusses up-to-date research work approaches in terms\nof definition, how they work, and kinds of errors they can find in\nspreadsheets. The paper looks also for the kinds of errors that end users\ncommonly make in spreadsheets."}
{"id": "1910.05685", "title": "A Coding-free Software Framework of Developing Web Data Management Systems", "url": "https://arxiv.org/abs/1910.05685", "pdf": "https://arxiv.org/pdf/1910.05685", "abs": "https://arxiv.org/abs/1910.05685", "authors": ["Can Yang", "Shiying Pan", "Runmin Li", "Yu Liu", "Lizhang Peng"], "categories": ["cs.SE"], "comment": "16pages, 11 figures, 2 tables", "summary": "More and more enterprises recently intend to deploy data management systems\nin the cloud. Due to the professionalism of software development, it has still\nbeen difficult for non-programmers to develop this kind of systems, even a\nsmall one. However, the development of SaaS brings forth the more feasibility\nof coding-free software development than before. Based on the SaaS\narchitecture, this paper presents a set of theory and method for coding-free\nconstruction of a data management system, on which our contributions involve in\na practical application platform, a set of construction method and a set of\ninterface on data exchange. By abstracting the common features of data\nmanagement systems, we design a universal web platform to quickly generate and\npublish customized system instances. Moreover, we propose a kind of method to\ndevelop a data management system using a specific requirements table in\nspreadsheet. The corresponding platform maps the requirements table into a\nsystem instance through parsing the table model and implementing the objective\nsystem in the running stage. Finally, we implement the proposed framework and\ndeploy it on web. The empirical result demonstrates the feasibility and\navailability of the coding-free method in developing web data management\nsystems."}
{"id": "1712.05944", "title": "Taggle: Combining Overview and Details in Tabular Data Visualizations", "url": "https://arxiv.org/abs/1712.05944", "pdf": "https://arxiv.org/pdf/1712.05944", "abs": "https://arxiv.org/abs/1712.05944", "authors": ["Katarina Furmanova", "Samuel Gratzl", "Holger Stitz", "Thomas Zichner", "Miroslava Jaresova", "Alexander Lex", "Marc Streit"], "categories": ["cs.HC"], "comment": null, "summary": "Most tabular data visualization techniques focus on overviews, yet many\npractical analysis tasks are concerned with investigating individual items of\ninterest. At the same time, relating an item to the rest of a potentially large\ntable is important. In this work we present Taggle, a tabular visualization\ntechnique for exploring and presenting large and complex tables. Taggle takes\nan item-centric, spreadsheet-like approach, visualizing each row in the source\ndata individually using visual encodings for the cells. At the same time,\nTaggle introduces data-driven aggregation of data subsets. The aggregation\nstrategy is complemented by interaction methods tailored to answer specific\nanalysis questions, such as sorting based on multiple columns and rich data\nselection and filtering capabilities. We demonstrate Taggle using a case study\nconducted by a domain expert on complex genomics data analysis for the purpose\nof drug discovery."}
{"id": "1909.07462", "title": "A Case Study of Spreadsheet Use within the Finance and Academic Registry units within a Higher Education Institution", "url": "https://arxiv.org/abs/1909.07462", "pdf": "https://arxiv.org/pdf/1909.07462", "abs": "https://arxiv.org/abs/1909.07462", "authors": ["Simon Thorne", "Jamie Hancock"], "categories": ["cs.CY"], "comment": "15 Pages, 20 Tables", "summary": "This paper presents the findings of a case study of spreadsheet use in a\nhigher education institution in the UK. The paper considers the use of\nspreadsheets in two units of the organisation, academic registry and finance.\nSpreadsheet use is explored in terms of importance, training, experience,\npurpose, techniques deployed, size of spreadsheets created and sharing of\nspreadsheets. The implications of the results are then considered in terms of\naccurate reporting to external funding bodies such the funding councils,\ninternal data integrity and internal data efficiencies. The results show a\nlarge volume of spreadsheets being created and used, that the profile of\nspreadsheet developers is typical of other studies of spreadsheet use and the\nneed for the organisation to have clear principles and guidelines for the\ndevelopment of spreadsheet models in the organisation to ensure data integrity,\nreduce duplication of effort and to optimise the use of spreadsheets to meet\nthe institutions goals."}
{"id": "1909.00891", "title": "Structured Spreadsheet Modelling and Implementation with Multiple Dimensions -- Part 2: Implementation", "url": "https://arxiv.org/abs/1909.00891", "pdf": "https://arxiv.org/pdf/1909.00891", "abs": "https://arxiv.org/abs/1909.00891", "authors": ["Paul Mireault"], "categories": ["cs.SE"], "comment": "14 Pages, 12 Colour Figures, 3 Tables. First presented at EuSpRIG\n  2018, Imperial College, London", "summary": "In Part 1, we showed how to develop a conceptual model of a problem involving\nvariables of multiple dimensions, like Products, Regions, Sectors and Months.\nThe conceptual model is presented as a Formula Diagram, giving a global view of\nthe interaction between all the variables, and a Formula List, giving a precise\nview of the interaction between the variables. In this paper, we present\nprecise steps to implement a multi-dimensional problem in a way that will\nproduce a spreadsheet that is easy to maintain"}
{"id": "1909.00865", "title": "Are digital natives spreadsheet natives?", "url": "https://arxiv.org/abs/1909.00865", "pdf": "https://arxiv.org/pdf/1909.00865", "abs": "https://arxiv.org/abs/1909.00865", "authors": ["Maria Csernoch", "Piroska Biró"], "categories": ["cs.HC"], "comment": "13 Pages, 6 Colour Figures, 9 Tables. First Presented at the EuSpRIG\n  2018 conference, Imperial College, London", "summary": "The present paper reports the results of testing first year students of\nInformatics on their algorithmic skills and knowledge transfer abilities in\nspreadsheet environments. The selection of students plays a crucial role in the\nproject. On the one hand, they have officially finished their spreadsheet\ntraining - they know everything - while on the other hand, they do not need any\ntraining, since they are digital natives, to whom digital skills are assigned\nby birth. However, we found that the students had serious difficulties in\nsolving the spreadsheet problems presented: so low were their results that it\nallowed us to form broad tendencies. Considering computational thinking,\nalgorithmic skills, and knowledge transfer abilities, it is clear that those\nstudents performed better who used algorithm-based, multilevel array formulas\ninstead of problem specific, unconnected built-in functions. Furthermore, we\ncan conclude that students, regardless of their birth date and digital\ngeneration assigned to them, are in great need of official, high-mathability,\nalgorithm-based training with expert teachers."}
{"id": "1909.02960", "title": "Real-time stock analysis for blending recipes in industrial plants", "url": "https://arxiv.org/abs/1909.02960", "pdf": "https://arxiv.org/pdf/1909.02960", "abs": "https://arxiv.org/abs/1909.02960", "authors": ["Florin Zamfir", "Nicolae Paraschiv", "Emil Pricop"], "categories": ["cs.OH"], "comment": "Accepted for presentation at 23rd International Conference on System\n  Theory, Control and Computing (ICSTCC 2019), October 9-11, 2019, Sinaia,\n  Romania", "summary": "Many companies use Excel spreadsheets to keep stock records and to calculate\nprocess-specific data. These spreadsheets are often hard to understand and\ntrack. And if the user does not protect them, there is a risk that the user\nrandomly changes or erase formulas. The paper focuses on the stocks of products\nused in a blending process with a known recipe. Developing an application that\ncan bring this data in a centralized form and that can assist the operator in\ndecide is a necessity. When a programmer implements an application that uses\ndata from plants he needs to consider one fundamental aspect as reading\nreal-time data from the process. The real-time stock analysis application takes\ninto account all the above elements. The application is easy to use by an\noperator in the command room of installation because of the planning algorithms\nintegrated into it. The algorithms proposed and implemented in this paper have\nwell-defined goals: identifying the ingredients needed to achieve the blending\nprocess for required quantities, determine the quantities of the finished\nproduct that can be made with the existing ingredients and determine the\noptimum quantities of the finished product. The application implemented in C#\nintensively uses these algorithms and gives the user the ability to build the\nresult step by step."}
{"id": "1908.08187", "title": "A CNN toolbox for skin cancer classification", "url": "https://arxiv.org/abs/1908.08187", "pdf": "https://arxiv.org/pdf/1908.08187", "abs": "https://arxiv.org/abs/1908.08187", "authors": ["Fabrizio Nunnari", "Daniel Sonntag"], "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "DFKI Technical Report", "summary": "We describe a software toolbox for the configuration of deep neural networks\nin the domain of skin cancer classification. The implemented software\narchitecture allows developers to quickly set up new convolutional neural\nnetwork (CNN) architectures and hyper-parameter configurations. At the same\ntime, the user interface, manageable as a simple spreadsheet, allows\nnon-technical users to explore different configuration settings that need to be\nexplored when switching to different data sets. In future versions, meta\nleaning frameworks can be added, or AutoML systems that continuously improve\nover time. Preliminary results, conducted with two CNNs in the context melanoma\ndetection on dermoscopic images, quantify the impact of image augmentation,\nimage resolution, and rescaling filter on the overall detection performance and\ntraining time."}
{"id": "1907.03595", "title": "Recommending Related Tables", "url": "https://arxiv.org/abs/1907.03595", "pdf": "https://arxiv.org/pdf/1907.03595", "abs": "https://arxiv.org/abs/1907.03595", "authors": ["Shuo Zhang", "Krisztian Balog"], "categories": ["cs.IR"], "comment": null, "summary": "Tables are an extremely powerful visual and interactive tool for structuring\nand manipulating data, making spreadsheet programs one of the most popular\ncomputer applications. In this paper we introduce and address the task of\nrecommending related tables: given an input table, identifying and returning a\nranked list of relevant tables. One of the many possible application scenarios\nfor this task is to provide users of a spreadsheet program proactively with\nrecommendations for related structured content on the Web. At its core, the\nrelated table recommendation task boils down to computing the similarity\nbetween a pair of tables. We develop a theoretically sound framework for\nperforming table matching. Our approach hinges on the idea of representing\ntable elements in multiple semantic spaces, and then combining element-level\nsimilarities using a discriminative learning model. Using a purpose-built test\ncollection from Wikipedia tables, we demonstrate that the proposed approach\ndelivers state-of-the-art performance."}
{"id": "1907.04827", "title": "Hillview: A trillion-cell spreadsheet for big data", "url": "https://arxiv.org/abs/1907.04827", "pdf": "https://arxiv.org/pdf/1907.04827", "abs": "https://arxiv.org/abs/1907.04827", "authors": ["Mihai Budiu", "Parikshit Gopalan", "Lalith Suresh", "Udi Wieder", "Han Kruiger", "Marcos K. Aguilera"], "categories": ["cs.DC"], "comment": null, "summary": "Hillview is a distributed spreadsheet for browsing very large datasets that\ncannot be handled by a single machine. As a spreadsheet, Hillview provides a\nhigh degree of interactivity that permits data analysts to explore information\nquickly along many dimensions while switching visualizations on a whim. To\nprovide the required responsiveness, Hillview introduces visualization\nsketches, or vizketches, as a simple idea to produce compact data\nvisualizations. Vizketches combine algorithmic techniques for data\nsummarization with computer graphics principles for efficient rendering. While\nsimple, vizketches are effective at scaling the spreadsheet by parallelizing\ncomputation, reducing communication, providing progressive visualizations, and\noffering precise accuracy guarantees. Using Hillview running on eight servers,\nwe can navigate and visualize datasets of tens of billions of rows and\ntrillions of cells, much beyond the published capabilities of competing\nsystems."}
{"id": "1907.04217", "title": "Streaming 1.9 Billion Hypersparse Network Updates per Second with D4M", "url": "https://arxiv.org/abs/1907.04217", "pdf": "https://arxiv.org/pdf/1907.04217", "abs": "https://arxiv.org/abs/1907.04217", "authors": ["Jeremy Kepner", "Vijay Gadepally", "Lauren Milechin", "Siddharth Samsi", "William Arcand", "David Bestor", "William Bergeron", "Chansup Byun", "Matthew Hubbell", "Michael Houle", "Michael Jones", "Anne Klein", "Peter Michaleas", "Julie Mullen", "Andrew Prout", "Antonio Rosa", "Charles Yee", "Albert Reuther"], "categories": ["cs.DC", "cs.DB", "cs.DS", "cs.IR", "cs.PF"], "comment": "6 pages; 6 figures; accepted to IEEE High Performance Extreme\n  Computing (HPEC) Conference 2019. arXiv admin note: text overlap with\n  arXiv:1807.05308, arXiv:1902.00846", "summary": "The Dynamic Distributed Dimensional Data Model (D4M) library implements\nassociative arrays in a variety of languages (Python, Julia, and Matlab/Octave)\nand provides a lightweight in-memory database implementation of hypersparse\narrays that are ideal for analyzing many types of network data. D4M relies on\nassociative arrays which combine properties of spreadsheets, databases,\nmatrices, graphs, and networks, while providing rigorous mathematical\nguarantees, such as linearity. Streaming updates of D4M associative arrays put\nenormous pressure on the memory hierarchy. This work describes the design and\nperformance optimization of an implementation of hierarchical associative\narrays that reduces memory pressure and dramatically increases the update rate\ninto an associative array. The parameters of hierarchical associative arrays\nrely on controlling the number of entries in each level in the hierarchy before\nan update is cascaded. The parameters are easily tunable to achieve optimal\nperformance for a variety of applications. Hierarchical arrays achieve over\n40,000 updates per second in a single instance. Scaling to 34,000 instances of\nhierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud\nachieved a sustained update rate of 1,900,000,000 updates per second. This\ncapability allows the MIT SuperCloud to analyze extremely large streaming\nnetwork data sets."}
{"id": "1907.02099", "title": "GeoGebra e situações que envolvem modelação numa abordagem STEAM", "url": "https://arxiv.org/abs/1907.02099", "pdf": "https://arxiv.org/pdf/1907.02099", "abs": "https://arxiv.org/abs/1907.02099", "authors": ["J. M. D. S. Dos Santos", "A. P. Silveira", "A. E. S. Trocado"], "categories": ["math.HO", "cs.CY", "00A35, 97C70", "G.4; K.3.1"], "comment": "in Portuguese", "summary": "In order to implement a STEAM approach including the use of technology,\nnamely the use of interactive mathematics software GeoGebra, in mathematics\nclasses, in the lusophone space, the materials presented here were conceived,\nto be implemented in a first phase among teachers. Later, with the necessary\nadaptations, these tasks will be applied to the students. The tasks deal with\nmodeling situations, in two- and three-dimensional geometric problems, in order\nto apply GeoGebra software in its analysis to illustrate its capabilities. The\ndifferent windows of this software are used, namely the 2D and 3D windows, CAS\nwindow, spreadsheet and extra two dimensional windows in order to study cutting\nplanes in solids and some surfaces. The tasks are presented so that any user,\nregardless of the degree of knowledge they have of the software, can follow\nthem, being supported in scripts with some indications of the tools and\ncommands to use. Designed for the teaching and learning of Mathematics, from a\nSTEAM approach, these tasks allow connections with other Sciences and the Arts,\nand allow the development of projects using and consolidating relevant\nmathematical contents. These tasks are part of the proposals of activities of\nthe participants of the Training Courses for Trainers in GeoGebra for\nPortuguese Speaking Countries, which from 2019 have an impact on the STEAM\napproach. These courses are carried out with the high sponsorship of the\nOrganization of Ibero-American States for Education, Science and Culture (OEI).\nGiven the interest that the tasks have for the users of the Iberian space, as\nwell as their dissemination at a global level, the materials initially\ndeveloped in Portuguese language will be adapted for Spanish and English\nspeakers."}
{"id": "1906.04011", "title": "Visual Backpropagation", "url": "https://arxiv.org/abs/1906.04011", "pdf": "https://arxiv.org/pdf/1906.04011", "abs": "https://arxiv.org/abs/1906.04011", "authors": ["Roy S. Freedman"], "categories": ["cs.LG", "cs.PL"], "comment": null, "summary": "We show how a declarative functional programming specification of\nbackpropagation yields a visual and transparent implementation within\nspreadsheets. We call our method Visual Backpropagation. This backpropagation\nimplementation exploits array worksheet formulas, manual calculation, and has a\nsequential order of computation similar to the processing of a systolic array.\nThe implementation uses no hidden macros nor user-defined functions; there are\nno loops, assignment statements, or links to any procedural programs written in\nconventional languages. As an illustration, we compare a Visual Backpropagation\nsolution to a Tensorflow (Python) solution on a standard regression problem."}
{"id": "1905.13072", "title": "Somewhere Around That Number: An Interview Study of How Spreadsheet Users Manage Uncertainty", "url": "https://arxiv.org/abs/1905.13072", "pdf": "https://arxiv.org/pdf/1905.13072", "abs": "https://arxiv.org/abs/1905.13072", "authors": ["Judith Borghouts", "Andrew D. Gordon", "Advait Sarkar", "Kenton P. O'Hara", "Neil Toronto"], "categories": ["cs.HC"], "comment": null, "summary": "Spreadsheet users regularly deal with uncertainty in their data, for example\ndue to errors and estimates. While an insight into data uncertainty can help in\nmaking better informed decisions, prior research suggests that people often use\ninformal heuristics to reason with probabilities, which leads to incorrect\nconclusions. Moreover, people often ignore or simplify uncertainty. To\nunderstand how people currently encounter and deal with uncertainty in\nspreadsheets, we conducted an interview study with 11 spreadsheet users from a\nrange of domains. We found that how people deal with uncertainty is influenced\nby the role the spreadsheet plays in people's work and the user's aims.\nSpreadsheets are used as a database, template, calculation tool, notepad and\nexploration tool. In doing so, participants' aims were to compute and compare\ndifferent scenarios, understand something about the nature of the uncertainty\nin their situation, and translate the complexity of data uncertainty into\nsimplified presentations to other people, usually decision-makers. Spreadsheets\ncurrently provide limited tools to support these aims, and participants had\nvarious workarounds."}
{"id": "1902.00846", "title": "A Billion Updates per Second Using 30,000 Hierarchical In-Memory D4M Databases", "url": "https://arxiv.org/abs/1902.00846", "pdf": "https://arxiv.org/pdf/1902.00846", "abs": "https://arxiv.org/abs/1902.00846", "authors": ["Jeremy Kepner", "Vijay Gadepally", "Lauren Milechin", "Siddharth Samsi", "William Arcand", "David Bestor", "William Bergeron", "Chansup Byun", "Matthew Hubbell", "Micheal Houle", "Micheal Jones", "Anne Klein", "Peter Michaleas", "Julie Mullen", "Andrew Prout", "Antonio Rosa", "Charles Yee", "Albert Reuther"], "categories": ["cs.DB", "cs.DC", "cs.DS", "cs.NI"], "comment": "Northeast Database Data 2019 (MIT)", "summary": "Analyzing large scale networks requires high performance streaming updates of\ngraph representations of these data. Associative arrays are mathematical\nobjects combining properties of spreadsheets, databases, matrices, and graphs,\nand are well-suited for representing and analyzing streaming network data. The\nDynamic Distributed Dimensional Data Model (D4M) library implements associative\narrays in a variety of languages (Python, Julia, and Matlab/Octave) and\nprovides a lightweight in-memory database. Associative arrays are designed for\nblock updates. Streaming updates to a large associative array requires a\nhierarchical implementation to optimize the performance of the memory\nhierarchy. Running 34,000 instances of a hierarchical D4M associative arrays on\n1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of\n1,900,000,000 updates per second. This capability allows the MIT SuperCloud to\nanalyze extremely large streaming network data sets."}
{"id": "1901.11100", "title": "ExceLint: Automatically Finding Spreadsheet Formula Errors", "url": "https://arxiv.org/abs/1901.11100", "pdf": "https://arxiv.org/pdf/1901.11100", "abs": "https://arxiv.org/abs/1901.11100", "authors": ["Daniel W. Barowy", "Emery D. Berger", "Benjamin Zorn"], "categories": ["cs.PL", "cs.SE"], "comment": "Appeared at OOPSLA 2018", "summary": "Spreadsheets are one of the most widely used programming environments, and\nare widely deployed in domains like finance where errors can have catastrophic\nconsequences. We present a static analysis specifically designed to find\nspreadsheet formula errors. Our analysis directly leverages the rectangular\ncharacter of spreadsheets. It uses an information-theoretic approach to\nidentify formulas that are especially surprising disruptions to nearby\nrectangular regions. We present ExceLint, an implementation of our static\nanalysis for Microsoft Excel. We demonstrate that ExceLint is fast and\neffective: across a corpus of 70 spreadsheets, ExceLint takes a median of 5\nseconds per spreadsheet, and it significantly outperforms the state of the art\nanalysis."}
{"id": "1807.00018", "title": "Computer Simulation of Neural Networks Using Spreadsheets: The Dawn of the Age of Camelot", "url": "https://arxiv.org/abs/1807.00018", "pdf": "https://arxiv.org/pdf/1807.00018", "abs": "https://arxiv.org/abs/1807.00018", "authors": ["Serhiy O. Semerikov", "Illia O. Teplytskyi", "Yuliia V. Yechkalo", "Arnold E. Kiv"], "categories": ["cs.CY", "68T99", "K.3.1; I.2.6; K.2"], "comment": "26 pages, 8 figures; submitted to the 1st International Workshop on\n  Augmented Reality in Education (AREdu 2018)", "summary": "The article substantiates the necessity to develop training methods of\ncomputer simulation of neural networks in the spreadsheet environment. The\nsystematic review of their application to simulating artificial neural networks\nis performed. The authors distinguish basic approaches to solving the problem\nof network computer simulation training in the spreadsheet environment, joint\napplication of spreadsheets and tools of neural network simulation, application\nof third-party add-ins to spreadsheets, development of macros using the\nembedded languages of spreadsheets; use of standard spreadsheet add-ins for\nnon-linear optimization, creation of neural networks in the spreadsheet\nenvironment without add-ins and macros. After analyzing a collection of\nwritings of 1890-1950, the research determines the role of the scientific\njournal \"Bulletin of Mathematical Biophysics\", its founder Nicolas Rashevsky\nand the scientific community around the journal in creating and developing\nmodels and methods of computational neuroscience. There are identified\npsychophysical basics of creating neural networks, mathematical foundations of\nneural computing and methods of neuroengineering (image recognition, in\nparticular). The role of Walter Pitts in combining the descriptive and\nquantitative theories of training is discussed. It is shown that to acquire\nneural simulation competences in the spreadsheet environment, one should master\nthe models based on the historical and genetic approach. It is indicated that\nthere are three groups of models, which are promising in terms of developing\ncorresponding methods - the continuous two-factor model of Rashevsky, the\ndiscrete model of McCulloch and Pitts, and the discrete-continuous models of\nHouseholder and Landahl."}
{"id": "1810.04542", "title": "On the Refinement of Spreadsheet Smells by means of Structure Information", "url": "https://arxiv.org/abs/1810.04542", "pdf": "https://arxiv.org/pdf/1810.04542", "abs": "https://arxiv.org/abs/1810.04542", "authors": ["Patrick Koch", "Birgit Hofer", "Franz Wotawa"], "categories": ["cs.SE"], "comment": null, "summary": "Spreadsheet users are often unaware of the risks imposed by poorly designed\nspreadsheets. One way to assess spreadsheet quality is to detect smells which\nattempt to identify parts of spreadsheets that are hard to comprehend or\nmaintain and which are more likely to be the root source of bugs.\nUnfortunately, current spreadsheet smell detection techniques suffer from a\nnumber of drawbacks that lead to incorrect or redundant smell reports. For\nexample, the same quality issue is often reported for every copy of a cell,\nwhich may overwhelm users. To deal with these issues, we propose to refine\nspreadsheet smells by exploiting inferred structural information for smell\ndetection. We therefore first provide a detailed description of our static\nanalysis approach to infer clusters and blocks of related cells. We then\nelaborate on how to improve existing smells by providing three example\nrefinements of existing smells that incorporate information about cell groups\nand computation blocks. Furthermore, we propose three novel smell detection\ntechniques that make use of the inferred spreadsheet structures. Empirical\nevaluation of the proposed techniques suggests that the refinements\nsuccessfully reduce the number of incorrectly and redundantly reported smells,\nand novel deficits are revealed by the newly introduced smells."}
{"id": "1809.03435", "title": "Now You're Thinking With Structures: A Concept for Structure-based Interactions with Spreadsheets", "url": "https://arxiv.org/abs/1809.03435", "pdf": "https://arxiv.org/pdf/1809.03435", "abs": "https://arxiv.org/abs/1809.03435", "authors": ["Patrick Koch"], "categories": ["cs.SE"], "comment": "In Proceedings of the 5th International Workshop on Software\n  Engineering Methods in Spreadsheets (arXiv:1808.09174)", "summary": "Spreadsheets are the go-to tool for computerized calculation and modelling,\nbut are hard to comprehend and adapt after reaching a certain complexity. In\ngeneral, cognition of complex systems is facilitated by having a higher order\nmental model of the system in question to work with. We therefore present a\nconcept for structure-aware understanding of and interaction with spreadsheets\nthat extends previous work on structure inference in the domain. Following this\nconcept, structural information is used to enrich visualizations, reactively\nenhance traditional user actions, and provide tools to proactively alter the\noverall spreadsheet makeup instead of individual cells The intended systems\nshould, in first approximation, not replace common spreadsheet tools, but\nprovide an additional layer of functionality alongside the established\ninterface. In ongoing work, we therefore implemented a tool for structure\ninference and visualization along the common spreadsheet layout. Based on this\nframework, we plan to introduce the envisioned proactive and reactive\ninteraction mechanics, and finally provide structure-aware unctionality as an\nadd-in for common spreadsheet processors. We believe that providing the tools\nfor thinking about and interacting with spreadsheets in this manner will\nbenefit users both in terms of productivity and overall spreadsheet quality."}
{"id": "1804.01186", "title": "Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples", "url": "https://arxiv.org/abs/1804.01186", "pdf": "https://arxiv.org/pdf/1804.01186", "abs": "https://arxiv.org/abs/1804.01186", "authors": ["Ashwin Kalyan", "Abhishek Mohta", "Oleksandr Polozov", "Dhruv Batra", "Prateek Jain", "Sumit Gulwani"], "categories": ["cs.AI", "cs.LG", "cs.PL"], "comment": "Published in ICLR 2018, International Conference on Learning\n  Representations (2018)", "summary": "Synthesizing user-intended programs from a small number of input-output\nexamples is a challenging problem with several important applications like\nspreadsheet manipulation, data wrangling and code refactoring. Existing\nsynthesis systems either completely rely on deductive logic techniques that are\nextensively hand-engineered or on purely statistical models that need massive\namounts of data, and in general fail to provide real-time synthesis on\nchallenging benchmarks. In this work, we propose Neural Guided Deductive Search\n(NGDS), a hybrid synthesis technique that combines the best of both symbolic\nlogic techniques and statistical models. Thus, it produces programs that\nsatisfy the provided specifications by construction and generalize well on\nunseen examples, similar to data-driven systems. Our technique effectively\nutilizes the deductive search framework to reduce the learning problem of the\nneural component to a simple supervised learning setup. Further, this allows us\nto both train on sparingly available real-world data and still leverage\npowerful recurrent neural network encoders. We demonstrate the effectiveness of\nour method by evaluating on real-world customer scenarios by synthesizing\naccurate programs with up to 12x speed-up compared to state-of-the-art systems."}
{"id": "1809.02746", "title": "Typed Table Transformations", "url": "https://arxiv.org/abs/1809.02746", "pdf": "https://arxiv.org/pdf/1809.02746", "abs": "https://arxiv.org/abs/1809.02746", "authors": ["Martin Erwig"], "categories": ["cs.SE"], "comment": "In Proceedings of the 5th International Workshop on Software\n  Engineering Methods in Spreadsheets (arXiv:1808.09174)", "summary": "Spreadsheet tables are often labeled, and these labels effectively constitute\ntypes for the data in the table. In such cases tables can be considered to be\nbuilt from typed data where the placement of values within the table is\ncontrolled by the types used for rows and columns. We present a new approach to\nthe transformations of spreadsheet tables that is based on transformations of\nrow and column types. We illustrate the basic idea of type-based table\nconstruction and transformation and lay out a series of research questions that\nshould be addressed in future work."}
{"id": "1809.00025", "title": "Implementing WHERE and ORDER BY as spreadsheet formulas", "url": "https://arxiv.org/abs/1809.00025", "pdf": "https://arxiv.org/pdf/1809.00025", "abs": "https://arxiv.org/abs/1809.00025", "authors": ["Paul Mireault"], "categories": ["cs.SE"], "comment": "In Proceedings of the 5th International Workshop on Software\n  Engineering Methods in Spreadsheets (arXiv:1808.09174)", "summary": "The WHERE and ORDER BY clauses of the SQL SELECT statement select a subset of\nrows in the result of a database query and present the result in the specified\norder. In a spreadsheet program like Microsoft Excel, one could use the filter\nand sort buttons, or use its Query or its Pivot Table tools to achieve a\nsimilar effect. The disadvantage of using those tools is that they don't react\nautomatically to changes in the calculated values of the spreadsheet. In this\npaper, we develop spreadsheet formulas that implement SQL's WHERE and ORDER BY\nclauses."}
{"id": "1808.10642", "title": "The use of Charts, Pivot Tables, and Array Formulas in two Popular Spreadsheet Corpora", "url": "https://arxiv.org/abs/1808.10642", "pdf": "https://arxiv.org/pdf/1808.10642", "abs": "https://arxiv.org/abs/1808.10642", "authors": ["Bas Jansen", "Felienne Hermans"], "categories": ["cs.SE"], "comment": "In Proceedings of the 5th International Workshop on Software\n  Engineering Methods in Spreadsheets (arXiv:1808.09174)", "summary": "The use of spreadsheets in industry is widespread. Companies base decisions\non information coming from spreadsheets. Unfortunately, spreadsheets are\nerror-prone and this increases the risk that companies base their decisions on\ninaccurate information, which can lead to incorrect decisions and loss of\nmoney. In general, spreadsheet research is aimed to reduce the error-proneness\nof spreadsheets. Most research is concentrated on the use of formulas. However,\nthere are other constructions in spreadsheets, like charts, pivot tables, and\narray formulas, that are also used to present decision support information to\nthe user. There is almost no research about how these constructions are used.\nTo improve spreadsheet quality it is important to understand how spreadsheets\nare used and to obtain a complete understanding, the use of charts, pivot\ntables, and array formulas should be included in research. In this paper, we\nanalyze two popular spreadsheet corpora: Enron and EUSES on the use of the\naforementioned constructions."}
{"id": "1808.10231", "title": "Asheetoxy: A Taxonomy for Classifying Negative Spreadsheet-related Phenomena", "url": "https://arxiv.org/abs/1808.10231", "pdf": "https://arxiv.org/pdf/1808.10231", "abs": "https://arxiv.org/abs/1808.10231", "authors": ["Daniel Kulesz", "Stefan Wagner"], "categories": ["cs.SE"], "comment": "In Proceedings of the 5th International Workshop on Software\n  Engineering Methods in Spreadsheets (arXiv:1808.09174)", "summary": "Spreadsheets (sometimes also called Excel programs) are powerful tools which\nplay a business-critical role in many organizations. However, due to faulty\nspreadsheets many bad decisions have been taken in recent years. Since then, a\nnumber of researchers have been studying spreadsheet errors. However, one issue\nthat hinders discussion among researchers and professionals is the lack of a\ncommonly accepted taxonomy.\n  Albeit a number of taxonomies for spreadsheet errors have been proposed in\nprevious work, a major issue is that they use the term error that itself is\nalready ambiguous. Furthermore, to apply most existing taxonomies, detailed\nknowledge about the underlying process and knowledge about the \"brain state\" of\nthe acting spreadsheet users is required. Due to these limitations, known\nerror-like phenomena in freely available spreadsheet corpora cannot be\nclassified with these taxonomies.\n  We propose Asheetoxy, a simple and phenomenon-oriented taxonomy that avoids\nthe problematic term error altogether. An initial study with 7 participants\nindicates that even non-spreadsheet researchers similarly classify real-world\nspreadsheet phenomena using Asheetoxy."}
{"id": "1808.09174", "title": "Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (SEMS'18)", "url": "https://arxiv.org/abs/1808.09174", "pdf": "https://arxiv.org/pdf/1808.09174", "abs": "https://arxiv.org/abs/1808.09174", "authors": ["Birgit Hofer", "Jorge Mendes"], "categories": ["cs.SE"], "comment": null, "summary": "Proceedings of the 5th International Workshop on Software Engineering Methods\nin Spreadsheets (SEMS'18), held on October 1st, 2018, in Lisbon, Portugal, and\nco-located with the 2018 IEEE Symposium on Visual Languages and Human-Centric\nComputing (VL/HCC)."}
{"id": "1806.04952", "title": "Towards Semantically Enhanced Data Understanding", "url": "https://arxiv.org/abs/1806.04952", "pdf": "https://arxiv.org/pdf/1806.04952", "abs": "https://arxiv.org/abs/1806.04952", "authors": ["Markus Schröder", "Christian Jilek", "Jörn Hees", "Andreas Dengel"], "categories": ["cs.DB", "cs.AI", "cs.HC"], "comment": "4 pages, 3 figures", "summary": "In the field of machine learning, data understanding is the practice of\ngetting initial insights in unknown datasets. Such knowledge-intensive tasks\nrequire a lot of documentation, which is necessary for data scientists to grasp\nthe meaning of the data. Usually, documentation is separate from the data in\nvarious external documents, diagrams, spreadsheets and tools which causes\nconsiderable look up overhead. Moreover, other supporting applications are not\nable to consume and utilize such unstructured data. That is why we propose a\nmethodology that uses a single semantic model that interlinks data with its\ndocumentation. Hence, data scientists are able to directly look up the\nconnected information about the data by simply following links. Equally, they\ncan browse the documentation which always refers to the data. Furthermore, the\nmodel can be used by other approaches providing additional support, like\nsearching, comparing, integrating or visualizing data. To showcase our approach\nwe also demonstrate an early prototype."}
{"id": "1805.10493", "title": "Combining Spreadsheet Smells for Improved Fault Prediction", "url": "https://arxiv.org/abs/1805.10493", "pdf": "https://arxiv.org/pdf/1805.10493", "abs": "https://arxiv.org/abs/1805.10493", "authors": ["Patrick Koch", "Konstantin Schekotihin", "Dietmar Jannach", "Birgit Hofer", "Franz Wotawa"], "categories": ["cs.SE"], "comment": "4 pages, 1 figure, to be published in 40th International Conference\n  on Software Engineering: New Ideas and Emerging Results Track", "summary": "Spreadsheets are commonly used in organizations as a programming tool for\nbusiness-related calculations and decision making. Since faults in spreadsheets\ncan have severe business impacts, a number of approaches from general software\nengineering have been applied to spreadsheets in recent years, among them the\nconcept of code smells. Smells can in particular be used for the task of fault\nprediction. An analysis of existing spreadsheet smells, however, revealed that\nthe predictive power of individual smells can be limited. In this work we\ntherefore propose a machine learning based approach which combines the\npredictions of individual smells by using an AdaBoost ensemble classifier.\nExperiments on two public datasets containing real-world spreadsheet faults\nshow significant improvements in terms of fault prediction accuracy."}
{"id": "1805.06353", "title": "SmartTable: A Spreadsheet Program with Intelligent Assistance", "url": "https://arxiv.org/abs/1805.06353", "pdf": "https://arxiv.org/pdf/1805.06353", "abs": "https://arxiv.org/abs/1805.06353", "authors": ["Shuo Zhang", "Vugar Abdul Zada", "Krisztian Balog"], "categories": ["cs.IR"], "comment": "The 41st International ACM SIGIR Conference on Research and\n  Development in Information Retrieval (SIGIR '18)", "summary": "We introduce SmartTable, an online spreadsheet application that is equipped\nwith intelligent assistance capabilities. With a focus on relational tables,\ndescribing entities along with their attributes, we offer assistance in two\nflavors: (i) for populating the table with additional entities (rows) and (ii)\nfor extending it with additional entity attributes (columns). We provide\ndetails of our implementation, which is also released as open source. The\napplication is available at http://smarttable.cc."}
{"id": "1804.04175", "title": "An Easy & Collaborative RDF Data Entry Method using the Spreadsheet Metaphor", "url": "https://arxiv.org/abs/1804.04175", "pdf": "https://arxiv.org/pdf/1804.04175", "abs": "https://arxiv.org/abs/1804.04175", "authors": ["Markus Schröder", "Christian Jilek", "Jörn Hees", "Sven Hertling", "Andreas Dengel"], "categories": ["cs.SE"], "comment": "15 pages", "summary": "Spreadsheets are widely used by knowledge workers, especially in the\nindustrial sector. Their methodology enables a well understood, easy and fast\npossibility to enter data. As filling out a spreadsheet is more accessible to\ncommon knowledge workers than defining RDF statements, in this paper, we\npropose an easy-to-use, zero-configuration, web-based spreadsheet editor that\nsimultaneously transfers spreadsheet entries into RDF statements. It enables\nvarious kinds of users to easily create semantic data whether they are RDF\nexperts or novices. The typical scenario we address focuses on creating\ninstance data starting with an empty knowledge base that is filled\nincrementally. In a user study, participants were able to create more\nstatements in shorter time, having similar or even significantly outperforming\nquality, compared to other approaches."}
{"id": "1801.09777", "title": "Structured Spreadsheet Modelling and Implementation with Multiple Dimensions - Part 1: Modelling", "url": "https://arxiv.org/abs/1801.09777", "pdf": "https://arxiv.org/pdf/1801.09777", "abs": "https://arxiv.org/abs/1801.09777", "authors": ["Paul Mireault"], "categories": ["cs.SE"], "comment": "13 Pages, 17 Tables and Figures", "summary": "Dimensions are an integral part of many models we use every day. Without\nthinking about it, we frequently use the time dimension: many financial and\naccounting spreadsheets have columns representing months or years. Representing\na second dimension is often done by repeating blocs of formulas in a worksheet\nor creating multiple worksheets with the same structure."}
{"id": "1802.01640", "title": "Mitigating Spreadsheet Risk in Complex Multi-Dimensional Models in Excel", "url": "https://arxiv.org/abs/1802.01640", "pdf": "https://arxiv.org/pdf/1802.01640", "abs": "https://arxiv.org/abs/1802.01640", "authors": ["Steve Litt"], "categories": ["cs.SE"], "comment": "13 Pages, 11 Colour Figures", "summary": "Microsoft Excel is the most ubiquitous analytical tool ever built. Companies\naround the world leverage it for its power, flexibility and ease of use.\nHowever, spreadsheets are manually intensive and prone to error, making it\ndifficult for companies to control spreadsheet risk. The following solution is\ndesigned to mitigate spreadsheet risk for a set of problems commonly addressed\nin a spreadsheet defined as \"complex multi-dimensional models\". \"Complex\"\nreferring to certain types of applications that require functionality such as\nsophisticated algorithms, challenging hierarchies and database write-back (i.e.\nplanning, forecasting, etc.) and \"multi-dimensional\" referring to providing\ncapabilities such as reporting, data input forms and ad hoc analysis on the\ndifferent attributes associated with the resulting model. The solution is\ndefined as a \"PivotModel\" because it works similarly to a PivotTable but is\ndesigned to leverage the robust capabilities of the Microsoft Excel platform."}
{"id": "1802.01628", "title": "Proposed Spreadsheet Transparency Definition and Measures", "url": "https://arxiv.org/abs/1802.01628", "pdf": "https://arxiv.org/pdf/1802.01628", "abs": "https://arxiv.org/abs/1802.01628", "authors": ["Craig Hatmaker"], "categories": ["cs.SE"], "comment": "13 Pages, 12 Screenshots", "summary": "Auditors demand financial models be transparent yet no consensus exists on\nwhat that means precisely. Without a clear modeling transparency definition we\ncannot know when our models are \"transparent\". The financial modeling community\ndebates which methods are more or less transparent as though transparency is a\nquantifiable entity yet no measures exist. Without a transparency measure\nmodelers cannot objectively evaluate methods and know which improves model\ntransparency.\n  This paper proposes a definition for spreadsheet modeling transparency that\nis specific enough to create measures and automation tools for auditors to\ndetermine if a model meets transparency requirements. The definition also\nprovides modelers the ability to objectively compare spreadsheet modeling\nmethods to select which best meets their goals."}
{"id": "1802.00496", "title": "Edu-Edition Spreadsheet Competency Framework", "url": "https://arxiv.org/abs/1802.00496", "pdf": "https://arxiv.org/pdf/1802.00496", "abs": "https://arxiv.org/abs/1802.00496", "authors": ["Maria Csernoch", "Piroska Biró"], "categories": ["cs.CY"], "comment": null, "summary": "Based on the Spreadsheet Competency Framework for finance professionals, in\nthe present paper we introduce the Edu-Edition of the Spreadsheet Competency\nFramework (E2SCF). We claim that building spreadsheet competences should start\nin education, as early as possible, and this process is a lot more effective if\nsupport arrives from expert teachers. The main feature of E2SCF is high\nmathability computer-supported real world problem solving. This approach is\nbased on - from the very beginning of training - a two-directional knowledge\ntransfer, data and error analysis and handling, and the programming aspect of\nspreadsheets. Based on these features, E2SCF is set up for basic and general\nusers to build up firm spreadsheet knowledge and to develop transferable\nproblem solving skills and competences."}
{"id": "1802.00484", "title": "Alternative Spreadsheet Model Designs for an Operations Management Model Embedded in a Periodic Business Process", "url": "https://arxiv.org/abs/1802.00484", "pdf": "https://arxiv.org/pdf/1802.00484", "abs": "https://arxiv.org/abs/1802.00484", "authors": ["Thomas A. Grossman", "Vijay Mehrotra", "Mouwafac Sidaoui"], "categories": ["cs.SE"], "comment": "12 Pages, 10 Colour Figures", "summary": "We present a widely-used operations management model used in supply and\ndistribution planning, that is typically embedded in a periodic business\nprocess that necessitates model modification and reuse. We consider three\nalternative spreadsheet implementations, a data-driven design, a canonical\n(textbook) design, and a novel (table-driven) technical design. We evaluate\neach regarding suitability for accuracy, modification, analysis, and transfer.\nWe consider the degree of training and technical sophistication required to\nutilize each design. The data-driven design provides insight into poor\nspreadsheet practices by na\\\"ive modelers. The technical design can be modified\nfor new data and new structural elements without manual writing or editing of\ncell formulas, thus speeding modification and reducing risk of error. The\ntechnical design has potential for use with other classes of models. We\nidentify opportunities for future research."}
{"id": "1801.03829", "title": "Characterizing Scalability Issues in Spreadsheet Software using Online Forums", "url": "https://arxiv.org/abs/1801.03829", "pdf": "https://arxiv.org/pdf/1801.03829", "abs": "https://arxiv.org/abs/1801.03829", "authors": ["Kelly Mack", "John Lee", "Kevin Chang", "Karrie Karahalios", "Aditya Parameswaran"], "categories": ["cs.HC"], "comment": null, "summary": "In traditional usability studies, researchers talk to users of tools to\nunderstand their needs and challenges. Insights gained via such interviews\noffer context, detail, and background. Due to costs in time and money, we are\nbeginning to see a new form of tool interrogation that prioritizes scale, cost,\nand breadth by utilizing existing data from online forums. In this case study,\nwe set out to apply this method of using online forum data to a specific\nissue---challenges that users face with Excel spreadsheets. Spreadsheets are a\nversatile and powerful processing tool if used properly. However, with\nversatility and power come errors, from both users and the software, which make\nusing spreadsheets less effective. By scraping posts from the website Reddit,\nwe collected a dataset of questions and complaints about Excel. Specifically,\nwe explored and characterized the issues users were facing with spreadsheet\nsoftware in general, and in particular, as resulting from a large amount of\ndata in their spreadsheets. We discuss the implications of our findings on the\ndesign of next-generation spreadsheet software."}
{"id": "1801.10249", "title": "The Reification of an Incorrect and Inappropriate Spreadsheet Model", "url": "https://arxiv.org/abs/1801.10249", "pdf": "https://arxiv.org/pdf/1801.10249", "abs": "https://arxiv.org/abs/1801.10249", "authors": ["Grenville J. Croll"], "categories": ["cs.HC"], "comment": "14 Pages, 4 Colour Figures, 2 Tables", "summary": "Once information is loaded into a spreadsheet, it acquires properties that it\nmay not deserve. These properties include believability, correctness,\nappropriateness, concreteness, integrity, tangibility, objectivity and\nauthority. The information becomes reified. We describe a case study through\nwhich we were able to observe at close hand the reification of a demonstrably\nincorrect and inappropriate spreadsheet model within a small non profit\norganisation."}
{"id": "1801.10231", "title": "The Future of Spreadsheets in the Big Data Era", "url": "https://arxiv.org/abs/1801.10231", "pdf": "https://arxiv.org/pdf/1801.10231", "abs": "https://arxiv.org/abs/1801.10231", "authors": ["David Birch", "David Lyford-Smith", "Yike Guo"], "categories": ["cs.CY"], "comment": "13 Pages, 1 Table", "summary": "The humble spreadsheet is the most widely used data storage, manipulation and\nmodelling tool. Its ubiquity over the past 30 years has seen its successful\napplication in every area of life. Surprisingly the spreadsheet has remained\nfundamentally unchanged over the past three decades. As spreadsheet technology\nenters its 4th decade a number of drivers of change are beginning to impact\nupon the spreadsheet. The rise of Big Data, increased end-user computing and\nmobile computing will undoubtedly increasingly shape the evolution and use of\nspreadsheet technology.\n  To explore the future of spreadsheet technology a workshop was convened with\nthe aim of \"bringing together academia and industry to examine the future\ndirection of spreadsheet technology and the consequences for users\". This paper\nrecords the views of the participants on the reasons for the success of the\nspreadsheet, the trends driving change and the likely directions of change for\nthe spreadsheet. We then set out key directions for further research in the\nevolution and use of spreadsheets. Finally we look at the implications of these\ntrends for the end users who after all are the reason for the remarkable\nsuccess of the spreadsheet."}
{"id": "1801.09771", "title": "Mitigating Spreadsheet Model Risk with Python Open Source Infrastructure", "url": "https://arxiv.org/abs/1801.09771", "pdf": "https://arxiv.org/pdf/1801.09771", "abs": "https://arxiv.org/abs/1801.09771", "authors": ["Oliver Beavers"], "categories": ["cs.SE"], "comment": "14 Pages, 15 Colour Diagrams", "summary": "Across an aggregation of EuSpRIG presentation papers, two maxims hold true:\nspreadsheets models are akin to software, yet spreadsheet developers are not\nsoftware engineers. As such, the lack of traditional software engineering tools\nand protocols invites a higher rate of error in the end result. This paper lays\nground work for spreadsheet modelling professionals to develop reproducible\naudit tools using freely available, open source packages built with the Python\nprogramming language, enabling stakeholders to develop clearly defined model\n\"oracles\" with which to test and audit spreadsheet calculations against."}
{"id": "1801.08603", "title": "Structuring Spreadsheets with the \"Lish\" Data Model", "url": "https://arxiv.org/abs/1801.08603", "pdf": "https://arxiv.org/pdf/1801.08603", "abs": "https://arxiv.org/abs/1801.08603", "authors": ["Alan Hall", "Michel Wermelinger", "Tony Hirst", "Santi Phithakkitnukoon"], "categories": ["cs.SE"], "comment": "4 colour figures", "summary": "A spreadsheet is remarkably flexible in representing various forms of\nstructured data, but the individual cells have no knowledge of the larger\nstructures of which they may form a part. This can hamper comprehension and\nincrease formula replication, increasing the risk of error on both scores. We\nexplore a novel data model (called the \"lish\") that could form an alternative\nto the traditional grid in a spreadsheet-like environment. Its aim is to\ncapture some of these higher structures while preserving the simplicity that\nmakes a spreadsheet so attractive. It is based on cells organised into nested\nlists, in each of which the user may optionally employ a template to prototype\nrepeating structures. These template elements can be likened to the marginal\n\"cells\" in the borders of a traditional worksheet, but are proper members of\nthe sheet and may themselves contain internal structure. A small demonstration\napplication shows the \"lish\" in operation."}
{"id": "1801.07782", "title": "The Role of Spreadsheets in Clinical Decision Support: A Survey of the Medical Algorithms Company User Community", "url": "https://arxiv.org/abs/1801.07782", "pdf": "https://arxiv.org/pdf/1801.07782", "abs": "https://arxiv.org/abs/1801.07782", "authors": ["Simon Thorne"], "categories": ["cs.CY"], "comment": "13 pages, 6 Colour Figures", "summary": "This paper presents and discusses the results of a small scoping survey of\nClinical Decision Support System (CDSS) users from the Medical Algorithms\nCompany website which hosts 24,000 different CDSS. These results are analysed,\ndiscussed, and compared with other similar studies and contribute to the wider\nunderstanding of how CDSS impact on clinical practice. The results show that\nCDSS provided by Medal are being used by clinical professionals in a variety of\nsettings, both as an operational tool and as a research and reference tool.\nWhilst these tools are implemented and executed in a database, the initial\nlogic is worked out on a spreadsheet. The paper describes that process and\nexamines some of the results of the survey."}
{"id": "1712.09797", "title": "Automated Refactoring of Nested-IF Formulae in Spreadsheets", "url": "https://arxiv.org/abs/1712.09797", "pdf": "https://arxiv.org/pdf/1712.09797", "abs": "https://arxiv.org/abs/1712.09797", "authors": ["Jie Zhang", "Shi Han", "Dan Hao", "Lu Zhang", "Dongmei Zhang"], "categories": ["cs.SE"], "comment": null, "summary": "Spreadsheets are the most popular end-user programming software, where\nformulae act like programs and also have smells. One well recognized common\nsmell of spreadsheet formulae is nest-IF expressions, which have low\nreadability and high cognitive cost for users, and are error-prone during reuse\nor maintenance. However, end users usually lack essential programming language\nknowledge and skills to tackle or even realize the problem. The previous\nresearch work has made very initial attempts in this aspect, while no effective\nand automated approach is currently available.\n  This paper firstly proposes an AST-based automated approach to systematically\nrefactoring nest-IF formulae. The general idea is two-fold. First, we detect\nand remove logic redundancy on the AST. Second, we identify higher-level\nsemantics that have been fragmented and scattered, and reassemble the syntax\nusing concise built-in functions. A comprehensive evaluation has been conducted\nagainst a real-world spreadsheet corpus, which is collected in a leading IT\ncompany for research purpose. The results with over 68,000 spreadsheets with 27\nmillion nest-IF formulae reveal that our approach is able to relieve the smell\nof over 99\\% of nest-IF formulae. Over 50% of the refactorings have reduced\nnesting levels of the nest-IFs by more than a half. In addition, a survey\ninvolving 49 participants indicates that for most cases the participants prefer\nthe refactored formulae, and agree on that such automated refactoring approach\nis necessary and helpful."}
{"id": "1612.03813", "title": "Spreadsheet Guardian: An Approach to Protecting Semantic Correctness throughout the Evolution of Spreadsheets", "url": "https://arxiv.org/abs/1612.03813", "pdf": "https://arxiv.org/pdf/1612.03813", "abs": "https://arxiv.org/abs/1612.03813", "authors": ["Daniel Kulesz", "Verena Käfer", "Stefan Wagner"], "categories": ["cs.SE", "cs.PL"], "comment": "30 pages, 15 figures, 4 tables", "summary": "Spreadsheets are powerful tools which play a business-critical role in many\norganizations. However, many bad decisions taken due to faulty spreadsheets\nshow that these tools need serious quality assurance. Furthermore, while\ncollaboration on spreadsheets for maintenance tasks is common, there has been\nalmost no support for ensuring that the spreadsheets remain correct during this\nprocess.\n  We have developed an approach named Spreadsheet Guardian which separates the\nspecification of spreadsheet test rules from their execution. By automatically\nexecuting user-defined test rules, our approach is able to detect semantic\nfaults. It also protects all collaborating spreadsheet users from introducing\nfaults during maintenance, even if only few end-users specify test rules. To\nevaluate Spreadsheet Guardian, we implemented a representative testing\ntechnique as an add-in for Microsoft Excel.\n  We evaluated the testing technique in two empirical evaluations with 29\nend-users and 42 computer science students. The results indicate that the\ntechnique is easy to learn and to apply. Furthermore, after finishing\nmaintenance, participants with spreadsheets \"protected\" by the technique are\nmore realistic about the correctness of their spreadsheets than participants\nwho employ only \"classic\", non-interactive test rules based on static analysis\ntechniques. Hence, we believe Spreadsheet Guardian can be of use for\nbusiness-critical spreadsheets."}
{"id": "1711.05787", "title": "WebRelate: Integrating Web Data with Spreadsheets using Examples", "url": "https://arxiv.org/abs/1711.05787", "pdf": "https://arxiv.org/pdf/1711.05787", "abs": "https://arxiv.org/abs/1711.05787", "authors": ["Jeevana Priya Inala", "Rishabh Singh"], "categories": ["cs.DB", "cs.PL"], "comment": "To appear in POPL 2018", "summary": "Data integration between web sources and relational data is a key challenge\nfaced by data scientists and spreadsheet users. There are two main challenges\nin programmatically joining web data with relational data. First, most websites\ndo not expose a direct interface to obtain tabular data, so the user needs to\nformulate a logic to get to different webpages for each input row in the\nrelational table. Second, after reaching the desired webpage, the user needs to\nwrite complex scripts to extract the relevant data, which is often conditioned\non the input data. Since many data scientists and end-users come from diverse\nbackgrounds, writing such complex regular-expression based logical scripts to\nperform data integration tasks is unfortunately often beyond their programming\nexpertise.\n  We present WebRelate, a system that allows users to join semi-structured web\ndata with relational data in spreadsheets using input-output examples.\nWebRelate decomposes the web data integration task into two sub-tasks of i) URL\nlearning and ii) input-dependent web extraction. The first sub-task generates\nthe URLs for the webpages containing the desired data for all rows in the\nrelational table. WebRelate achieves this by learning a string transformation\nprogram using a few example URLs. The second sub-task uses examples of desired\ndata to be extracted from the corresponding webpages and learns a program to\nextract the data for the other rows. We design expressive domain-specific\nlanguages for URL generation and web data extraction, and present efficient\nsynthesis algorithms for learning programs in these DSLs from few input-output\nexamples. We evaluate WebRelate on 88 real-world web data integration tasks\ntaken from online help forums and Excel product team, and show that WebRelate\ncan learn the desired programs within few seconds using only 1 example for the\nmajority of the tasks."}
{"id": "1710.03248", "title": "Synthesizing Bijective Lenses", "url": "https://arxiv.org/abs/1710.03248", "pdf": "https://arxiv.org/pdf/1710.03248", "abs": "https://arxiv.org/abs/1710.03248", "authors": ["Anders Miltner", "Kathleen Fisher", "Benjamin C. Pierce", "David Walker", "Steve Zdancewic"], "categories": ["cs.PL"], "comment": "127 Pages, Extended Version with Appendix", "summary": "Bidirectional transformations between different data representations occur\nfrequently in modern software systems. They appear as serializers and\ndeserializers, as database views and view updaters, and more. Manually building\nbidirectional transformations---by writing two separate functions that are\nintended to be inverses---is tedious and error prone. A better approach is to\nuse a domain-specific language in which both directions can be written as a\nsingle expression. However, these domain-specific languages can be difficult to\nprogram in, requiring programmers to manage fiddly details while working in a\ncomplex type system.\n  To solve this, we present Optician, a tool for type-directed synthesis of\nbijective string transformers. The inputs to Optician are two ordinary regular\nexpressions representing two data formats and a few concrete examples for\ndisambiguation. The output is a well-typed program in Boomerang (a\nbidirectional language based on the theory of lenses). The main technical\nchallenge involves navigating the vast program search space efficiently enough.\nUnlike most prior work on type-directed synthesis, our system operates in the\ncontext of a language with a rich equivalence relation on types (the theory of\nregular expressions). We synthesize terms of a equivalent language and convert\nthose generated terms into our lens language. We prove the correctness of our\nsynthesis algorithm. We also demonstrate empirically that our new language\nchanges the synthesis problem from one that admits intractable solutions to one\nthat admits highly efficient solutions. We evaluate Optician on a benchmark\nsuite of 39 examples including both microbenchmarks and realistic examples\nderived from other data management systems including Flash Fill, a tool for\nsynthesizing string transformations in spreadsheets, and Augeas, a tool for\nbidirectional processing of Linux system configuration files."}
{"id": "1708.06712", "title": "Towards a Holistic Integration of Spreadsheets with Databases: A Scalable Storage Engine for Presentational Data Management", "url": "https://arxiv.org/abs/1708.06712", "pdf": "https://arxiv.org/pdf/1708.06712", "abs": "https://arxiv.org/abs/1708.06712", "authors": ["Mangesh Bendre", "Vipul Venkataraman", "Xinyan Zhou", "Kevin Chang", "Aditya Parameswaran"], "categories": ["cs.DB"], "comment": null, "summary": "Spreadsheet software is the tool of choice for interactive ad-hoc data\nmanagement, with adoption by billions of users. However, spreadsheets are not\nscalable, unlike database systems. On the other hand, database systems, while\nhighly scalable, do not support interactivity as a first-class primitive. We\nare developing DataSpread, to holistically integrate spreadsheets as a\nfront-end interface with databases as a back-end datastore, providing\nscalability to spreadsheets, and interactivity to databases, an integration we\nterm presentational data management (PDM). In this paper, we make a first step\ntowards this vision: developing a storage engine for PDM, studying how to\nflexibly represent spreadsheet data within a database and how to support and\nmaintain access by position. We first conduct an extensive survey of\nspreadsheet use to motivate our functional requirements for a storage engine\nfor PDM. We develop a natural set of mechanisms for flexibly representing\nspreadsheet data and demonstrate that identifying the optimal representation is\nNP-Hard; however, we develop an efficient approach to identify the optimal\nrepresentation from an important and intuitive subclass of representations. We\nextend our mechanisms with positional access mechanisms that don't suffer from\ncascading update issues, leading to constant time access and modification\nperformance. We evaluate these representations on a workload of typical\nspreadsheets and spreadsheet operations, providing up to 20% reduction in\nstorage, and up to 50% reduction in formula evaluation time."}
{"id": "1709.04553", "title": "MOLTE: a Modular Optimal Learning Testing Environment", "url": "https://arxiv.org/abs/1709.04553", "pdf": "https://arxiv.org/pdf/1709.04553", "abs": "https://arxiv.org/abs/1709.04553", "authors": ["Yingfei Wang", "Warren Powell"], "categories": ["cs.LG"], "comment": null, "summary": "We address the relative paucity of empirical testing of learning algorithms\n(of any type) by introducing a new public-domain, Modular, Optimal Learning\nTesting Environment (MOLTE) for Bayesian ranking and selection problem,\nstochastic bandits or sequential experimental design problems. The Matlab-based\nsimulator allows the comparison of a number of learning policies (represented\nas a series of .m modules) in the context of a wide range of problems (each\nrepresented in its own .m module) which makes it easy to add new algorithms and\nnew test problems. State-of-the-art policies and various problem classes are\nprovided in the package. The choice of problems and policies is guided through\na spreadsheet-based interface. Different graphical metrics are included. MOLTE\nis designed to be compatible with parallel computing to scale up from local\ndesktop to clusters and clouds. We offer MOLTE as an easy-to-use tool for the\nresearch community that will make it possible to perform much more\ncomprehensive testing, spanning a broader selection of algorithms and test\nproblems. We demonstrate the capabilities of MOLTE through a series of\ncomparisons of policies on a starter library of test problems. We also address\nthe problem of tuning and constructing priors that have been largely overlooked\nin optimal learning literature. We envision MOLTE as a modest spur to provide\nresearchers an easy environment to study interesting questions involved in\noptimal learning."}
{"id": "1709.00362", "title": "An Email Attachment is Worth a Thousand Words, or Is It?", "url": "https://arxiv.org/abs/1709.00362", "pdf": "https://arxiv.org/pdf/1709.00362", "abs": "https://arxiv.org/abs/1709.00362", "authors": ["Gregory Tsipenyuk", "Jon Crowcroft"], "categories": ["cs.SI"], "comment": "12 pages, 4 figures, 7 tables, IML'17, Liverpool, UK", "summary": "There is an extensive body of research on Social Network Analysis (SNA) based\non the email archive. The network used in the analysis is generally extracted\neither by capturing the email communication in From, To, Cc and Bcc email\nheader fields or by the entities contained in the email message. In the latter\ncase, the entities could be, for instance, the bag of words, url's, names,\nphones, etc. It could also include the textual content of attachments, for\ninstance Microsoft Word documents, excel spreadsheets, or Adobe pdfs. The nodes\nin this network represent users and entities. The edges represent communication\nbetween users and relations to the entities. We suggest taking a different\napproach to the network extraction and use attachments shared between users as\nthe edges. The motivation for this is two-fold. First, attachments represent\nthe \"intimacy\" manifestation of the relation's strength. Second, the\nstatistical analysis of private email archives that we collected and Enron\nemail corpus shows that the attachments contribute in average around 80-90% to\nthe archive's disk-space usage, which means that most of the data is presently\nignored in the SNA of email archives. Consequently, we hypothesize that this\napproach might provide more insight into the social structure of the email\narchive. We extract the communication and shared attachments networks from\nEnron email corpus. We further analyze degree, betweenness, closeness, and\neigenvector centrality measures in both networks and review the differences and\nwhat can be learned from them. We use nearest neighbor algorithm to generate\nsimilarity groups for five Enron employees. The groups are consistent with\nEnron's organizational chart, which validates our approach."}
{"id": "1708.08731", "title": "Active Learning of Input Grammars", "url": "https://arxiv.org/abs/1708.08731", "pdf": "https://arxiv.org/pdf/1708.08731", "abs": "https://arxiv.org/abs/1708.08731", "authors": ["Matthias Höschele", "Alexander Kampmann", "Andreas Zeller"], "categories": ["cs.PL", "cs.FL", "F.4.2; F.3.2; D.2.5"], "comment": "12 pages", "summary": "Knowing the precise format of a program's input is a necessary prerequisite\nfor systematic testing. Given a program and a small set of sample inputs, we\n(1) track the data flow of inputs to aggregate input fragments that share the\nsame data flow through program execution into lexical and syntactic entities;\n(2) assign these entities names that are based on the associated variable and\nfunction identifiers; and (3) systematically generalize production rules by\nmeans of membership queries. As a result, we need only a minimal set of sample\ninputs to obtain human-readable context-free grammars that reflect valid input\nstructure. In our evaluation on inputs like URLs, spreadsheets, or\nconfiguration files, our AUTOGRAM prototype obtains input grammars that are\nboth accurate and very readable - and that can be directly fed into test\ngenerators for comprehensive automated testing."}
{"id": "1708.08721", "title": "EntiTables: Smart Assistance for Entity-Focused Tables", "url": "https://arxiv.org/abs/1708.08721", "pdf": "https://arxiv.org/pdf/1708.08721", "abs": "https://arxiv.org/abs/1708.08721", "authors": ["Shuo Zhang", "Krisztian Balog"], "categories": ["cs.IR"], "comment": "Proceedings of the 40th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR '17), 2017", "summary": "Tables are among the most powerful and practical tools for organizing and\nworking with data. Our motivation is to equip spreadsheet programs with smart\nassistance capabilities. We concentrate on one particular family of tables,\nnamely, tables with an entity focus. We introduce and focus on two specific\ntasks: populating rows with additional instances (entities) and populating\ncolumns with new headings. We develop generative probabilistic models for both\ntasks. For estimating the components of these models, we consider a knowledge\nbase as well as a large table corpus. Our experimental evaluation simulates the\nvarious stages of the user entering content into an actual table. A detailed\nanalysis of the results shows that the models' components are complimentary and\nthat our methods outperform existing approaches from the literature."}
{"id": "1707.00144", "title": "On Evidence-based Risk Management in Requirements Engineering", "url": "https://arxiv.org/abs/1707.00144", "pdf": "https://arxiv.org/pdf/1707.00144", "abs": "https://arxiv.org/abs/1707.00144", "authors": ["Daniel Méndez Fernández", "Michaela Tießler", "Marcos Kalinowski", "Michael Felderer", "Marco Kuhrmann"], "categories": ["cs.SE"], "comment": "20 pages, submitted to 10th Software Quality Days conference, 2018", "summary": "Background: The sensitivity of Requirements Engineering (RE) to the context\nmakes it difficult to efficiently control problems therein, thus, hampering an\neffective risk management devoted to allow for early corrective or even\npreventive measures. Problem: There is still little empirical knowledge about\ncontext-specific RE phenomena which would be necessary for an effective\ncontext- sensitive risk management in RE. Goal: We propose and validate an\nevidence-based approach to assess risks in RE using cross-company data about\nproblems, causes and effects. Research Method: We use survey data from 228\ncompanies and build a probabilistic network that supports the forecast of\ncontext-specific RE phenomena. We implement this approach using spreadsheets to\nsupport a light-weight risk assessment. Results: Our results from an initial\nvalidation in 6 companies strengthen our confidence that the approach increases\nthe awareness for individual risk factors in RE, and the feedback further\nallows for disseminating our approach into practice."}
