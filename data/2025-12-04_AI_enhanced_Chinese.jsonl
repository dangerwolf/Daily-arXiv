{"id": "2509.26557", "title": "The Invisible Mentor: Inferring User Actions from Screen Recordings to Recommend Better Workflows", "url": "https://arxiv.org/abs/2509.26557", "pdf": "https://arxiv.org/pdf/2509.26557", "abs": "https://arxiv.org/abs/2509.26557", "authors": ["Litao Yan", "Andrew Head", "Ken Milne", "Vu Le", "Sumit Gulwani", "Chris Parnin", "Emerson Murphy-Hill"], "categories": ["cs.HC"], "comment": "15 pages, 6 figures", "summary": "Many users struggle to notice when a more efficient workflow exists in feature-rich tools like Excel. Existing AI assistants offer help only after users describe their goals or problems, which can be effortful and imprecise. We present InvisibleMentor, a system that turns screen recordings of task completion into vision-grounded reflections on tasks. It detects issues such as repetitive edits and recommends more efficient alternatives based on observed behavior. Unlike prior systems that rely on logs, APIs, or user prompts, InvisibleMentor operates directly on screen recordings. It uses a two-stage pipeline: a vision-language model reconstructs actions and context, and a language model generates structured, high-fidelity suggestions. In evaluation, InvisibleMentor accurately identified inefficient workflows, and participants found its suggestions more actionable, tailored, and more helpful for learning and improvement compared to a prompt-based spreadsheet assistant.", "AI": {"tldr": "InvisibleMentor analyzes screen recordings to identify inefficient workflows in software like Excel and suggests improvements.", "motivation": "Users often miss efficient workflows in complex software, and existing AI assistants require explicit problem descriptions.", "method": "A vision-language model reconstructs actions from screen recordings, and a language model generates structured suggestions.", "result": "InvisibleMentor accurately identifies inefficient workflows, providing more actionable and helpful suggestions than prompt-based assistants.", "conclusion": "InvisibleMentor offers a novel approach to workflow optimization by directly analyzing screen recordings."}}
{"id": "2509.26331", "title": "AI Playing Business Games: Benchmarking Large Language Models on Managerial Decision-Making in Dynamic Simulations", "url": "https://arxiv.org/abs/2509.26331", "pdf": "https://arxiv.org/pdf/2509.26331", "abs": "https://arxiv.org/abs/2509.26331", "authors": ["Berdymyrat Ovezmyradov"], "categories": ["cs.AI"], "comment": "34 pages, 7 figures, 3 tables", "summary": "The rapid advancement of LLMs sparked significant interest in their potential to augment or automate managerial functions. One of the most recent trends in AI benchmarking is performance of Large Language Models (LLMs) over longer time horizons. While LLMs excel at tasks involving natural language and pattern recognition, their capabilities in multi-step, strategic business decision-making remain largely unexplored. Few studies demonstrated how results can be different from benchmarks in short-term tasks, as Vending-Bench revealed. Meanwhile, there is a shortage of alternative benchmarks for long-term coherence. This research analyses a novel benchmark using a business game for the decision making in business. The research contributes to the recent literature on AI by proposing a reproducible, open-access management simulator to the research community for LLM benchmarking. This novel framework is used for evaluating the performance of five leading LLMs available in free online interface: Gemini, ChatGPT, Meta AI, Mistral AI, and Grok. LLM makes decisions for a simulated retail company. A dynamic, month-by-month management simulation provides transparently in spreadsheet model as experimental environment. In each of twelve months, the LLMs are provided with a structured prompt containing a full business report from the previous period and are tasked with making key strategic decisions: pricing, order size, marketing budget, hiring, dismissal, loans, training expense, R&D expense, sales forecast, income forecast The methodology is designed to compare the LLMs on quantitative metrics: profit, revenue, and market share, and other KPIs. LLM decisions are analyzed in their strategic coherence, adaptability to market changes, and the rationale provided for their decisions. This approach allows to move beyond simple performance metrics for assessment of the long-term decision-making.", "AI": {"tldr": "This paper introduces a novel benchmark using a business game to evaluate the long-term strategic decision-making capabilities of LLMs, finding limitations in their performance.", "motivation": "The motivation is to address the gap in evaluating LLMs' multi-step strategic business decision-making capabilities over longer time horizons, as existing benchmarks primarily focus on short-term tasks.", "method": "The method involves using a dynamic, month-by-month management simulation where LLMs make strategic decisions for a simulated retail company, with performance evaluated on quantitative metrics and qualitative analysis of decision rationale.", "result": "The research evaluates the performance of five leading LLMs (Gemini, ChatGPT, Meta AI, Mistral AI, and Grok) using the proposed benchmark.", "conclusion": "The study contributes a reproducible, open-access management simulator for LLM benchmarking, highlighting the limitations of current LLMs in long-term strategic decision-making."}}
