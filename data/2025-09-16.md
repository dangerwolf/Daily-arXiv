<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 5]
- [cs.CL](#cs.CL) [Total: 13]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.DL](#cs.DL) [Total: 5]
- [cs.RO](#cs.RO) [Total: 2]
- [math.HO](#math.HO) [Total: 2]
- [cs.DB](#cs.DB) [Total: 26]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.SE](#cs.SE) [Total: 43]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.CY](#cs.CY) [Total: 12]
- [cs.MS](#cs.MS) [Total: 1]
- [cs.PL](#cs.PL) [Total: 9]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.OH](#cs.OH) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.HC](#cs.HC) [Total: 32]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [stat.OT](#stat.OT) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Image deidentification in the XNAT ecosystem: use cases and solutions](https://arxiv.org/abs/2504.20657)
*Alex Michie,Simon J Doran*

Main category: cs.CV

TL;DR: 本文详细描述了使用XNAT及其生态系统工具的DICOM数据去识别工作流程，以及其在MIDI-B挑战中的表现和未来的改进方向。


<details>
  <summary>Details</summary>
Motivation: 在学术研究中，对DICOM图像等医疗数据进行去识别化是必不可少的，以保护患者隐私并促进数据共享。本文旨在通过参与MIDI-B挑战，验证并改进其现有的去识别化方法。

Method: 本文采用了一个基于XNAT设施和独立工具的DICOM数据去识别工作流程。最初使用基于规则的方法，随后尝试结合机器学习模型来处理地址数据。

Result: 在MIDI-B挑战测试阶段，初始得分是97.91%，由于技术原因，该分数低于预期。经过反馈和改进后，分数显著提高到99.61%。纯规则方法在移除姓名相关信息方面表现良好，但在处理地址数据时出现失败。初步的机器学习模型实验在移除地址方面部分成功，但对其他自由文本数据过于“激进”，导致整体性能略有下降至99.54%。目前，估计的真实去识别失败率为0.19%。

Conclusion: 基于规则的方法在姓名去识别方面有效，但在地址方面存在不足。机器学习模型需要进一步优化以避免过度去识别。未来的工作将专注于改进地址识别能力以及更好地去除图像像素中烧录的身份信息。

Abstract: XNAT is a server-based data management platform widely used in academia for
curating large databases of DICOM images for research projects. We describe in
detail a deidentification workflow for DICOM data using facilities in XNAT,
together with independent tools in the XNAT "ecosystem". We list different
contexts in which deidentification might be needed, based on our prior
experience. The starting point for participation in the Medical Image
De-Identification Benchmark (MIDI-B) challenge was a set of pre-existing local
methodologies, which were adapted during the validation phase of the challenge.
Our result in the test phase was 97.91\%, considerably lower than our peers,
due largely to an arcane technical incompatibility of our methodology with the
challenge's Synapse platform, which prevented us receiving feedback during the
validation phase. Post-submission, additional discrepancy reports from the
organisers and via the MIDI-B Continuous Benchmarking facility, enabled us to
improve this score significantly to 99.61\%. An entirely rule-based approach
was shown to be capable of removing all name-related information in the test
corpus, but exhibited failures in dealing fully with address data. Initial
experiments using published machine-learning models to remove addresses were
partially successful but showed the models to be "over-aggressive" on other
types of free-text data, leading to a slight overall degradation in performance
to 99.54\%. Future development will therefore focus on improving
address-recognition capabilities, but also on better removal of identifiable
data burned into the image pixels. Several technical aspects relating to the
"answer key" are still under discussion with the challenge organisers, but we
estimate that our percentage of genuine deidentification failures on the MIDI-B
test corpus currently stands at 0.19\%. (Abridged from original for arXiv
submission)

</details>


### [2] [Vision Language Models for Spreadsheet Understanding: Challenges and Opportunities](https://arxiv.org/abs/2405.16234)
*Shiyu Xia,Junyu Xiong,Haoyu Dong,Jianbo Zhao,Yuzhang Tian,Mengyu Zhou,Yeye He,Shi Han,Dongmei Zhang*

Main category: cs.CV

TL;DR: 本文探讨了视觉语言模型（VLM）在电子表格理解方面的能力，提出了自监督挑战和评估指标，发现VLM在光学字符识别（OCR）方面表现良好，但在空间感知和格式识别方面不足。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索和评估视觉语言模型（VLM）在电子表格理解方面的能力，特别是它们在光学字符识别（OCR）、空间感知和视觉格式识别方面的表现，并找出当前的局限性。

Method: 提出三个自监督挑战及相应评估指标，以全面评估VLM在OCR、空间感知和视觉格式识别方面的能力。利用电子表格表格检测任务来评估VLM的整体性能。提出列宽调整、样式更改和地址增强等三种电子表格到图像的设置，并设计了不同的提示变体。为了利用VLM理解文本的优势，在电子表格边界检测中建议解码表格四边界上的单元格值。

Result: VLM展现出有前景的OCR能力，但由于单元格遗漏和错位导致结果不尽如人意。VLM在空间和格式识别技能方面表现不足。

Conclusion: 未来的工作应利用本文提出的方法，在各种设置下生成大量的电子表格-图像对，以增强VLM的电子表格数据理解能力。

Abstract: This paper explores capabilities of Vision Language Models on spreadsheet
comprehension. We propose three self-supervised challenges with corresponding
evaluation metrics to comprehensively evaluate VLMs on Optical Character
Recognition (OCR), spatial perception, and visual format recognition.
Additionally, we utilize the spreadsheet table detection task to assess the
overall performance of VLMs by integrating these challenges. To probe VLMs more
finely, we propose three spreadsheet-to-image settings: column width
adjustment, style change, and address augmentation. We propose variants of
prompts to address the above tasks in different settings. Notably, to leverage
the strengths of VLMs in understanding text rather than two-dimensional
positioning, we propose to decode cell values on the four boundaries of the
table in spreadsheet boundary detection. Our findings reveal that VLMs
demonstrate promising OCR capabilities but produce unsatisfactory results due
to cell omission and misalignment, and they notably exhibit insufficient
spatial and format recognition skills, motivating future work to enhance VLMs'
spreadsheet data comprehension capabilities using our methods to generate
extensive spreadsheet-image pairs in various settings.

</details>


### [3] [High-Throughput Phenotyping using Computer Vision and Machine Learning](https://arxiv.org/abs/2407.06354)
*Vivaan Singhvi,Langalibalele Lunga,Pragya Nidhi,Chris Keum,Varrun Prakash*

Main category: cs.CV

TL;DR: 该研究将高通量表型分析与机器学习相结合，通过OCR读取植物标签，并使用图像分割和机器学习对形态进行分类，预测处理方式。OCR模型准确率为94.31%，形态分类平均准确率为62.82%，处理预测准确率为60.08%。发现EXIF标签缺失关键信息，阻碍了叶片大小和表型关联的评估。


<details>
  <summary>Details</summary>
Motivation: 以往结合深度神经网络和自动化相机的高通量表型研究常缺乏物理标签，导致数据处理和特定性状提取效率低下。本研究旨在通过使用带有物理标签的数据集来解决这一问题。

Method: 本研究使用了包含1,672张杨树图像的数据集，图像带有显示处理、区块、行、位置和基因型的白色物理标签。采用光学字符识别（OCR）读取标签信息；结合图像分割技术和机器学习算法进行形态分类；使用机器学习模型根据分类预测处理方式；并分析编码的EXIF标签以寻找叶片大小和表型之间的关联。

Result: OCR模型在非空文本提取方面的准确率为94.31%。分类模型在识别叶片形状、颜色和褐色斑点程度方面的平均准确率为62.82%。植物处理预测的准确率为60.08%。发现EXIF标签中缺少关键信息，导致无法评估叶片大小以及表型与条件之间的关联。

Conclusion: 本研究成功地将OCR和机器学习应用于高通量表型分析，在标签读取方面取得了高准确率，在分类和处理预测方面也取得了合理准确率。然而，EXIF标签数据中的限制阻碍了对叶片大小和表型关联的全面评估，为未来的研究指明了改进方向。

Abstract: High-throughput phenotyping refers to the non-destructive and efficient
evaluation of plant phenotypes. In recent years, it has been coupled with
machine learning in order to improve the process of phenotyping plants by
increasing efficiency in handling large datasets and developing methods for the
extraction of specific traits. Previous studies have developed methods to
advance these challenges through the application of deep neural networks in
tandem with automated cameras; however, the datasets being studied often
excluded physical labels. In this study, we used a dataset provided by Oak
Ridge National Laboratory with 1,672 images of Populus Trichocarpa with white
labels displaying treatment (control or drought), block, row, position, and
genotype. Optical character recognition (OCR) was used to read these labels on
the plants, image segmentation techniques in conjunction with machine learning
algorithms were used for morphological classifications, machine learning models
were used to predict treatment based on those classifications, and analyzed
encoded EXIF tags were used for the purpose of finding leaf size and
correlations between phenotypes. We found that our OCR model had an accuracy of
94.31% for non-null text extractions, allowing for the information to be
accurately placed in a spreadsheet. Our classification models identified leaf
shape, color, and level of brown splotches with an average accuracy of 62.82%,
and plant treatment with an accuracy of 60.08%. Finally, we identified a few
crucial pieces of information absent from the EXIF tags that prevented the
assessment of the leaf size. There was also missing information that prevented
the assessment of correlations between phenotypes and conditions. However,
future studies could improve upon this to allow for the assessment of these
features.

</details>


### [4] [Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language](https://arxiv.org/abs/2204.00598)
*Andy Zeng,Maria Attarian,Brian Ichter,Krzysztof Choromanski,Adrian Wong,Stefan Welker,Federico Tombari,Aveek Purohit,Michael Ryoo,Vikas Sindhwani,Johnny Lee,Vincent Vanhoucke,Pete Florence*

Main category: cs.CV

TL;DR: Socratic Models (SMs)通过零样本组合多个预训练模型，利用它们不同的领域知识，实现强大的多模态能力并支持新应用，无需微调。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型（如VLM和LM）因训练数据域不同而具备独特能力并存储不同形式的常识知识。这种多样性是共生的，可以被利用。

Method: Socratic Models (SMs) 是一个模块化框架，通过多模态知情提示（multimodal-informed prompting）零样本组合多个预训练模型，使它们能够互相交换信息并获取新的多模态能力，无需微调。

Result: SMs在零样本图像字幕生成和视频到文本检索方面与现有最先进技术具有竞争力，并且支持新的应用，例如：回答关于自我中心视频的自由形式问题、进行多模态辅助对话，以及机器人感知和规划。

Conclusion: 不同预训练模型中知识的多样性是共生的，可以通过Socratic Models (SMs)有效地利用，以零样本方式实现强大的多模态能力和新的应用。

Abstract: Large pretrained (e.g., "foundation") models exhibit distinct capabilities
depending on the domain of data they are trained on. While these domains are
generic, they may only barely overlap. For example, visual-language models
(VLMs) are trained on Internet-scale image captions, but large language models
(LMs) are further trained on Internet-scale text with no images (e.g.,
spreadsheets, SAT questions, code). As a result, these models store different
forms of commonsense knowledge across different domains. In this work, we show
that this diversity is symbiotic, and can be leveraged through Socratic Models
(SMs): a modular framework in which multiple pretrained models may be composed
zero-shot i.e., via multimodal-informed prompting, to exchange information with
each other and capture new multimodal capabilities, without requiring
finetuning. With minimal engineering, SMs are not only competitive with
state-of-the-art zero-shot image captioning and video-to-text retrieval, but
also enable new applications such as (i) answering free-form questions about
egocentric video, (ii) engaging in multimodal assistive dialogue with people
(e.g., for cooking recipes) by interfacing with external APIs and databases
(e.g., web search), and (iii) robot perception and planning.

</details>


### [5] [TableParser: Automatic Table Parsing with Weak Supervision from Spreadsheets](https://arxiv.org/abs/2201.01654)
*Susie Xi Rao,Johannes Rausch,Peter Egger,Ce Zhang*

Main category: cs.CV

TL;DR: 本文提出了TableParser系统，能够高精度地解析原生PDF和扫描图像中的表格。该系统通过领域自适应和基于电子表格的弱监督机制实现。


<details>
  <summary>Details</summary>
Motivation: 表格是存储数据的重要结构，从PDF、图像等多种格式中解析表格结构并提取内容在许多应用中都非常重要。

Method: 开发了TableParser系统，用于解析原生PDF和扫描图像中的表格。通过领域自适应提高工具的效率。创建了TableAnnotator和ExcelAnnotator，构成了基于电子表格的弱监督机制和表格解析的管道。

Result: TableParser系统能够高精度地解析表格。广泛的实验证明了领域自适应的有效性。创建了TableAnnotator和ExcelAnnotator，并与研究社区共享，以促进进一步的研究。

Conclusion: 本文提出了TableParser，一个用于高精度解析PDF和图像中表格的系统，并利用领域自适应和弱监督机制。相关的工具和资源也已共享，以促进该领域未来的研究。

Abstract: Tables have been an ever-existing structure to store data. There exist now
different approaches to store tabular data physically. PDFs, images,
spreadsheets, and CSVs are leading examples. Being able to parse table
structures and extract content bounded by these structures is of high
importance in many applications. In this paper, we devise TableParser, a system
capable of parsing tables in both native PDFs and scanned images with high
precision. We have conducted extensive experiments to show the efficacy of
domain adaptation in developing such a tool. Moreover, we create TableAnnotator
and ExcelAnnotator, which constitute a spreadsheet-based weak supervision
mechanism and a pipeline to enable table parsing. We share these resources with
the research community to facilitate further research in this interesting
direction.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [6] [General Table Question Answering via Answer-Formula Joint Generation](https://arxiv.org/abs/2503.12345)
*Zhongyuan Wang,Richong Zhang,Zhijie Nie,Hangyu Mao*

Main category: cs.CL

TL;DR: 该论文提出使用电子表格公式解决表格问答（TableQA）问题，构建了FormulaQA数据集并提出了TabAF框架，在多个TableQA基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的高级表格问答方法在处理特定问题类型或表格结构时缺乏通用性。电子表格公式作为表格数据操作语言尚未被充分探索用于解决TableQA。

Method: 论文构建了FormulaQA，一个大型的公式标注TableQA数据集。此外，提出了一种通用的表格问答框架TabAF，该框架使用单个大型语言模型主干同时解码答案和公式，以解决多种类型表格上的多任务。

Result: TabAF在相同的模型尺寸下，在WikiTableQuestion、HiTab和TabFact上实现了新的最先进性能，并展示了其通用性和泛化能力。

Conclusion: 该论文成功地探索了将电子表格公式作为解决复杂表格推理的可执行表示，所提出的TabAF框架在多类型表格和任务上表现出卓越的通用性和最先进的性能。

Abstract: Advanced table question answering (TableQA) methods prompt large language
models (LLMs) to generate answer text, SQL query, Python code, or custom
operation, which impressively improve the complex reasoning problems in the
TableQA task. However, these methods lack the versatility to cope with specific
question types or table structures. In contrast, the Spreadsheet Formula, the
widely used and well-defined operation language for tabular data, has not been
thoroughly explored to solve TableQA. In this paper, we first attempt to use
the Formula as the executable representation for solving complex reasoning on
tables with different structures. Specifically, we construct
\texttt{FromulaQA}, a large Formula-annotated TableQA dataset from existing
datasets. In addition, we propose \texttt{TabAF}, a general table answering
framework to solve multiple types of tasks over multiple types of tables
simultaneously, which decodes answers and Formulas with a single LLM backbone.
Extensive experiments demonstrate the versatility and generalization of
\texttt{TabAF}. Under the same model size, \texttt{TabAF} achieves new
state-of-the-art performance on the WikiTableQuestion, HiTab, and TabFact.

</details>


### [7] [An Empirical Study of Validating Synthetic Data for Formula Generation](https://arxiv.org/abs/2407.10657)
*Usneek Singh,José Cambronero,Sumit Gulwani,Aditya Kanade,Anirudh Khatry,Vu Le,Mukul Singh,Gust Verbruggen*

Main category: cs.CL

TL;DR: 大型语言模型(LLMs)在编写电子表格公式方面资源稀缺。本文提出生成合成自然语言话语进行微调，并通过代理目标验证这些合成训练示例的准确性。经验结果表明，验证能提高模型性能，并使其能解决更复杂的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）可以帮助编写电子表格公式，但这些公式的资源稀缺，影响了预训练模型的性能并限制了微调能力。需要解决如何有效利用LLMs生成数据进行微调，并确保生成数据的准确性。

Method: 生成用于微调的合成自然语言话语。使用代理目标（surrogate objectives）来评估这些合成标注的准确性，从而验证这些合成训练示例。

Result: 验证后，在四种模型（2个开源和2个闭源）上，性能均优于使用原始数据。有趣的是，尽管验证倾向于剔除更具挑战性的示例，但经过验证数据微调后，模型能解决问题的复杂性反而增加了。

Conclusion: 通过代理目标验证合成训练示例的准确性，可以显著提高大型语言模型在电子表格公式生成方面的性能，并使其能够解决更复杂的问题，即使这可能意味着剔除部分初始的难题。

Abstract: Large language models (LLMs) can be leveraged to help with writing formulas
in spreadsheets, but resources on these formulas are scarce, impacting both the
base performance of pre-trained models and limiting the ability to fine-tune
them. Given a corpus of formulas, we can use a(nother) model to generate
synthetic natural language utterances for fine-tuning. However, it is important
to validate whether the NL generated by the LLM is indeed accurate to be
beneficial for fine-tuning. In this paper, we provide empirical results on the
impact of validating these synthetic training examples with surrogate
objectives that evaluate the accuracy of the synthetic annotations. We
demonstrate that validation improves performance over raw data across four
models (2 open and 2 closed weight). Interestingly, we show that although
validation tends to prune more challenging examples, it increases the
complexity of problems that models can solve after being fine-tuned on
validated data.

</details>


### [8] [TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios](https://arxiv.org/abs/2403.19318)
*Xiaokang Zhang,Sijia Luo,Bohan Zhang,Zeyao Ma,Jing Zhang,Yang Li,Guanlin Li,Zijun Yao,Kangli Xu,Jinchang Zhou,Daniel Zhang-Li,Jifan Yu,Shu Zhao,Juanzi Li,Jie Tang*

Main category: cs.CL

TL;DR: TableLLM是一个80亿参数的大型语言模型，专门用于处理文档和电子表格中的表格数据操作任务，通过一种包含推理过程扩展和交叉验证的远程监督方法进行训练，并在专用基准测试中表现优于现有的通用和专注于表格数据的LLM。


<details>
  <summary>Details</summary>
Motivation: 需要一个大型语言模型（LLM）能够熟练处理文档或电子表格中嵌入的表格数据操作任务，以应对现实世界的办公场景。

Method: 该研究引入了TableLLM，一个拥有80亿参数的LLM。它提出了一种用于训练的远程监督方法，该方法包含一个推理过程扩展策略（帮助LLM更有效地理解推理模式）和一个交叉验证策略（确保自动生成数据的质量）。为了评估性能，研究者制作了针对文档和电子表格格式的定制基准，并构建了一个组织良好的评估流程。

Result: 彻底的评估结果强调了TableLLM与各种现有通用型和专注于表格数据的LLM相比的优势。

Conclusion: TableLLM是一个强大且有优势的LLM，专门用于表格数据操作，研究团队已经公开了模型检查点、源代码、基准和Web应用程序，供用户互动和使用。

Abstract: We introduce TableLLM, a robust large language model (LLM) with 8 billion
parameters, purpose-built for proficiently handling tabular data manipulation
tasks, whether they are embedded within documents or spreadsheets, catering to
real-world office scenarios. We propose a distant supervision method for
training, which comprises a reasoning process extension strategy, aiding in
training LLMs to understand reasoning patterns more effectively as well as a
cross-way validation strategy, ensuring the quality of the automatically
generated data. To evaluate the performance of TableLLM, we have crafted
benchmarks tailored to address both document and spreadsheet formats as well as
constructed a well-organized evaluation pipeline capable of handling both
scenarios. Thorough evaluations underscore the advantages of TableLLM when
compared to various existing general-purpose and tabular data-focused LLMs. We
have publicly released the model checkpoint, source code, benchmarks, and a web
application for user interaction. Our codes and data are publicly available at
https://github.com/TableLLM/TableLLM.

</details>


### [9] [GiesKaNe: Bridging Past and Present in Grammatical Theory and Practical Application](https://arxiv.org/abs/2502.05113)
*Volker Emmrich*

Main category: cs.CL

TL;DR: 本文探讨了GiesKaNe语料库（一个历史的、句法深度标注的树库）的编译要求，该项目旨在平衡创新与标准，并结合人工和机器辅助过程，利用现有基础设施实现。


<details>
  <summary>Details</summary>
Motivation: GiesKaNe项目旨在创建一个历史的、句法深度标注的树库，连接历史和当代语料库。本文的动机是探讨其编译要求，应对平衡创新与标准以及管理方法复杂性的挑战。

Method: 该项目通过人类专业知识和机器辅助过程的互补结合来管理其方法复杂性。它讨论了分词、规范化、句子定义、标注、句法分析和标注者间一致性等基础主题，并提出了文本口语-书面语连续体上机器辅助分类的新方法以及从现有标注中推导事实标准标注的方法。工作流程基于战略性使用简单电子表格和现有基础设施。

Result: 文章展示了像GiesKaNe这样雄心勃勃的项目可以使用现有研究基础设施（例如简单的电子表格）有效实施，无需专门的标注工具。同时，提出了机器辅助文本分类的新方法和从现有标注中推导事实标准标注的方法。

Conclusion: GiesKaNe项目通过平衡创新与标准、利用人机协作以及利用现有研究基础设施（如简单的电子表格）可以有效实施。它为文本分类和标注标准化提供了新的方法。

Abstract: This article explores the requirements for corpus compilation within the
GiesKaNe project (University of Giessen and Kassel, Syntactic Basic Structures
of New High German). The project is defined by three central characteristics:
it is a reference corpus, a historical corpus, and a syntactically deeply
annotated treebank. As a historical corpus, GiesKaNe aims to establish
connections with both historical and contemporary corpora, ensuring its
relevance across temporal and linguistic contexts. The compilation process
strikes the balance between innovation and adherence to standards, addressing
both internal project goals and the broader interests of the research
community. The methodological complexity of such a project is managed through a
complementary interplay of human expertise and machine-assisted processes. The
article discusses foundational topics such as tokenization, normalization,
sentence definition, tagging, parsing, and inter-annotator agreement, alongside
advanced considerations. These include comparisons between grammatical models,
annotation schemas, and established de facto annotation standards as well as
the integration of human and machine collaboration. Notably, a novel method for
machine-assisted classification of texts along the continuum of conceptual
orality and literacy is proposed, offering new perspectives on text selection.
Furthermore, the article introduces an approach to deriving de facto standard
annotations from existing ones, mediating between standardization and
innovation. In the course of describing the workflow the article demonstrates
that even ambitious projects like GiesKaNe can be effectively implemented using
existing research infrastructure, requiring no specialized annotation tools.
Instead, it is shown that the workflow can be based on the strategic use of a
simple spreadsheet and integrates the capabilities of the existing
infrastructure.

</details>


### [10] [MiMoTable: A Multi-scale Spreadsheet Benchmark with Meta Operations for Table Reasoning](https://arxiv.org/abs/2412.11711)
*Zheng Li,Yang Du,Mao Zheng,Mingyang Song*

Main category: cs.CL

TL;DR: 本文提出了MiMoTable，这是一个用于表格推理的新型多尺度电子表格基准，包含真实的电子表格和新的难度评估元操作标准。实验表明，MiMoTable对大型语言模型（LLM）具有挑战性，并且他们的新标准能有效衡量基准难度。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）表格推理基准与实际应用中的表格和用户问题之间存在显著差距。

Method: 本文提出了MiMoTable基准。MiMoTable包含来自七个领域的真实世界电子表格，并定义了六类元操作作为衡量问题难度的新标准。

Result: Claude-3.5-Sonnet在MiMoTable上取得了77.4%的最佳准确率，表明LLM仍有很大的提升空间。此外，实验证明LLM的性能随基准难度的增加而下降，证实了新标准的有效性。

Conclusion: MiMoTable是一个具有挑战性的新基准，可用于推进LLM在表格推理方面的能力。提出的新元操作标准能有效衡量表格推理任务的难度。

Abstract: Extensive research has been conducted to explore the capability of Large
Language Models (LLMs) for table reasoning and has significantly improved the
performance on existing benchmarks. However, tables and user questions in
real-world applications are more complex and diverse, presenting an unignorable
gap compared to the existing benchmarks. To fill the gap, we propose a
\textbf{M}ult\textbf{i}-scale spreadsheet benchmark with \textbf{M}eta
\textbf{o}perations for \textbf{Table} reasoning, named as MiMoTable.
Specifically, MiMoTable incorporates two key features. First, the tables in
MiMoTable are all spreadsheets used in real-world scenarios, which cover seven
domains and contain different types. Second, we define a new criterion with six
categories of meta operations for measuring the difficulty of each question in
MiMoTable, simultaneously as a new perspective for measuring the difficulty of
the existing benchmarks. Experimental results show that Claude-3.5-Sonnet
achieves the best performance with 77.4\% accuracy, indicating that there is
still significant room to improve for LLMs on MiMoTable. Furthermore, we grade
the difficulty of existing benchmarks according to our new criteria.
Experiments have shown that the performance of LLMs decreases as the difficulty
of benchmarks increases, thereby proving the effectiveness of our proposed new
criterion.

</details>


### [11] [SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation](https://arxiv.org/abs/2406.14991)
*Zeyao Ma,Bohan Zhang,Jing Zhang,Jifan Yu,Xiaokang Zhang,Xiaohan Zhang,Sijia Luo,Xi Wang,Jie Tang*

Main category: cs.CL

TL;DR: 本文介绍了 SpreadsheetBench，一个基于真实场景的、具有挑战性的电子表格操作基准测试，旨在让大型语言模型 (LLMs) 沉浸在电子表格用户的实际工作流程中，并提出了一个更可靠的评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试依赖于合成查询和简化的电子表格文件，无法反映电子表格用户的实际复杂需求。

Method: 创建了 SpreadsheetBench，包含从在线 Excel 论坛收集的 912 个真实问题和相关的、包含多种数据类型的电子表格。提出了一种类似于在线判题平台的更可靠的评估指标，为每条指令创建多个电子表格文件作为测试用例。在单轮和多轮推理设置下对各种 LLMs 进行了综合评估。

Result: 对各种 LLMs 的综合评估显示，最先进的模型与人类表现之间存在巨大差距，突显了该基准测试的难度。

Conclusion: SpreadsheetBench 是一个具有挑战性且真实的基准测试，揭示了当前 LLMs 在处理真实世界电子表格操作方面的局限性。

Abstract: We introduce SpreadsheetBench, a challenging spreadsheet manipulation
benchmark exclusively derived from real-world scenarios, designed to immerse
current large language models (LLMs) in the actual workflow of spreadsheet
users. Unlike existing benchmarks that rely on synthesized queries and
simplified spreadsheet files, SpreadsheetBench is built from 912 real questions
gathered from online Excel forums, which reflect the intricate needs of users.
The associated spreadsheets from the forums contain a variety of tabular data
such as multiple tables, non-standard relational tables, and abundant
non-textual elements. Furthermore, we propose a more reliable evaluation metric
akin to online judge platforms, where multiple spreadsheet files are created as
test cases for each instruction, ensuring the evaluation of robust solutions
capable of handling spreadsheets with varying values. Our comprehensive
evaluation of various LLMs under both single-round and multi-round inference
settings reveals a substantial gap between the state-of-the-art (SOTA) models
and human performance, highlighting the benchmark's difficulty.

</details>


### [12] [NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries](https://arxiv.org/abs/2402.14853)
*Wei Zhao,Zhitao Hou,Siyuan Wu,Yan Gao,Haoyu Dong,Yao Wan,Hongyu Zhang,Yulei Sui,Haidong Zhang*

Main category: cs.CL

TL;DR: 本文介绍了一个名为NL2Formula的新基准任务和数据集，旨在根据自然语言查询生成可执行的电子表格公式，并提出了一个名为fCoder的序列到序列基线实现。


<details>
  <summary>Details</summary>
Motivation: 在电子表格上编写公式对于许多最终用户来说是一项繁琐且容易出错的任务，尤其是在处理复杂操作时。

Method: 引入了NL2Formula基准任务，构建了一个包含70,799对自然语言查询和对应电子表格公式的综合数据集，涵盖21,670个表格和37种公式函数。通过提供一个名为fCoder的序列到序列基线实现来完成NL2Formula任务。

Result: 实验结果验证了fCoder的有效性，证明其性能优于基线模型。此外，还将fCoder与初始的GPT-3.5模型（即text-davinci-003）进行了比较。

Conclusion: 通过深入的错误分析，识别了NL2Formula任务中潜在的挑战，并倡导进一步的研究。

Abstract: Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets,
is a widespread practice among users performing data analysis. However,
crafting formulas on spreadsheets remains a tedious and error-prone task for
many end-users, particularly when dealing with complex operations. To alleviate
the burden associated with writing spreadsheet formulas, this paper introduces
a novel benchmark task called NL2Formula, with the aim to generate executable
formulas that are grounded on a spreadsheet table, given a Natural Language
(NL) query as input. To accomplish this, we construct a comprehensive dataset
consisting of 70,799 paired NL queries and corresponding spreadsheet formulas,
covering 21,670 tables and 37 types of formula functions. We realize the
NL2Formula task by providing a sequence-to-sequence baseline implementation
called fCoder. Experimental results validate the effectiveness of fCoder,
demonstrating its superior performance compared to the baseline models.
Furthermore, we also compare fCoder with an initial GPT-3.5 model (i.e.,
text-davinci-003). Lastly, through in-depth error analysis, we identify
potential challenges in the NL2Formula task and advocate for further
investigation.

</details>


### [13] [InstructExcel: A Benchmark for Natural Language Instruction in Excel](https://arxiv.org/abs/2310.14495)
*Justin Payan,Swaroop Mishra,Mukul Singh,Carina Negreanu,Christian Poelitz,Chitta Baral,Subhro Roy,Rasika Chakravarthy,Benjamin Van Durme,Elnaz Nouri*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）能否根据自然语言指令生成Excel OfficeScripts代码以解决Excel任务。为此，引入了一个新的大规模基准InstructExcel（包含10k+样本，覆盖170+Excel操作）。实验表明，InstructExcel对GPT-4等最先进模型来说是一个困难的基准，但使用GPT-4、提供更多上下文示例和动态提示可以提高性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的演进，它们能够解决越来越复杂的自然语言处理任务，包括在电子表格领域。本研究旨在探讨LLMs是否能够根据自然语言用户指令，生成解决Excel特定任务的代码（Excel OfficeScripts）。

Method: 为实现研究目的，本文引入了一个新的大规模基准测试集InstructExcel。该基准是通过利用Excel的“自动化”功能，从用户操作中自动生成OfficeScripts代码来创建的。InstructExcel包含了超过1万个样本，涵盖了2000个公开可用的Excel电子表格中的170多种Excel操作。研究人员在各种零样本和少样本设置下进行了实验。

Result: 实验结果显示，InstructExcel对于像GPT-4这样的最先进模型来说，是一个具有挑战性的基准。研究观察到，(1) 使用GPT-4而不是GPT-3.5，(2) 提供更多上下文示例，以及 (3) 动态提示，能够帮助提高模型在该基准上的性能。

Conclusion: 大型语言模型可以生成解决Excel任务的代码，但这仍然是一个具有挑战性的任务。通过采用更强大的模型（如GPT-4）、提供更多上下文示例以及使用动态提示等策略，可以有效提高模型在Excel代码生成任务上的表现。

Abstract: With the evolution of Large Language Models (LLMs) we can solve increasingly
more complex NLP tasks across various domains, including spreadsheets. This
work investigates whether LLMs can generate code (Excel OfficeScripts, a
TypeScript API for executing many tasks in Excel) that solves Excel specific
tasks provided via natural language user instructions. To do so we introduce a
new large-scale benchmark, InstructExcel, created by leveraging the 'Automate'
feature in Excel to automatically generate OfficeScripts from users' actions.
Our benchmark includes over 10k samples covering 170+ Excel operations across
2,000 publicly available Excel spreadsheets. Experiments across various
zero-shot and few-shot settings show that InstructExcel is a hard benchmark for
state of the art models like GPT-4. We observe that (1) using GPT-4 over
GPT-3.5, (2) providing more in-context examples, and (3) dynamic prompting can
help improve performance on this benchmark.

</details>


### [14] [Active Learning with Tabular Language Models](https://arxiv.org/abs/2211.04128)
*Martin Ringsquandl,Aneta Koleva*

Main category: cs.CL

TL;DR: 本文研究了在表格语言模型中使用主动学习以减少标注成本的方法，发现在子单元命名实体识别中，带有内置多样性的单元级获取函数能显著降低标注工作量，而强制的表格多样性则不利。


<details>
  <summary>Details</summary>
Motivation: 尽管表格语言模型有进展，但实际应用中，表格数据标注成本高昂且缺乏主动学习与表格语言模型结合的研究。

Method: 在一个真实世界的工业表格语言模型用例中，针对子单元命名实体识别，调查了不同的获取函数。

Result: 结果表明，带有内置多样性的单元级获取函数可以显著减少标注工作量，而强制的表格多样性则有害。

Conclusion: 仍存在关于计算效率和人类标注者视角的基本开放问题。

Abstract: Despite recent advancements in tabular language model research, real-world
applications are still challenging. In industry, there is an abundance of
tables found in spreadsheets, but acquisition of substantial amounts of labels
is expensive, since only experts can annotate the often highly technical and
domain-specific tables. Active learning could potentially reduce labeling
costs, however, so far there are no works related to active learning in
conjunction with tabular language models. In this paper we investigate
different acquisition functions in a real-world industrial tabular language
model use case for sub-cell named entity recognition. Our results show that
cell-level acquisition functions with built-in diversity can significantly
reduce the labeling effort, while enforced table diversity is detrimental. We
further see open fundamental questions concerning computational efficiency and
the perspective of human annotators.

</details>


### [15] [Table-To-Text generation and pre-training with TabT5](https://arxiv.org/abs/2210.09162)
*Ewa Andrejczuk,Julian Martin Eisenschlos,Francesco Piccinno,Syrine Krichene,Yasemin Altun*

Main category: cs.CL

TL;DR: TABT5是一种编码器-解码器模型，它克服了仅编码器模型在表格理解任务上的局限性，并实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的仅编码器Transformer模型（如TAPAS）在表格理解任务中受限于分类任务。需要一种模型能够基于表格和文本输入生成自然语言文本。

Method: TABT5是一个编码器-解码器模型，它包含一个解码器组件，并利用表格特定的嵌入和预训练来处理输入结构。

Result: TABT5在多个领域取得了新的最先进结果，包括：电子表格公式预测（序列准确率提高15%），问答（序列准确率提高2.5%），以及数据到文本生成（BLEU提高2.5%）。

Conclusion: TABT5通过生成基于表格和文本的自然语言文本，克服了仅编码器模型的局限性，并在多项任务中设定了新的最先进基准。

Abstract: Encoder-only transformer models have been successfully applied to different
table understanding tasks, as in TAPAS (Herzig et al., 2020). A major
limitation of these architectures is that they are constrained to
classification-like tasks such as cell selection or entailment detection. We
present TABT5, an encoder-decoder model that generates natural language text
based on tables and textual inputs. TABT5 overcomes the encoder-only limitation
by incorporating a decoder component and leverages the input structure with
table specific embeddings and pre-training. TABT5 achieves new state-of-the-art
results on several domains, including spreadsheet formula prediction with a 15%
increase in sequence accuracy, QA with a 2.5% increase in sequence accuracy and
data-to-text generation with a 2.5% increase in BLEU.

</details>


### [16] [Table Pre-training: A Survey on Model Architectures, Pre-training Objectives, and Downstream Tasks](https://arxiv.org/abs/2201.09745)
*Haoyu Dong,Zhoujun Cheng,Xinyi He,Mengyu Zhou,Anda Zhou,Fan Zhou,Ao Liu,Shi Han,Dongmei Zhang*

Main category: cs.CL

TL;DR: 这篇综述全面回顾了表格预训练框架，包括模型设计、预训练目标和下游任务，并探讨了现有挑战和未来机遇。


<details>
  <summary>Details</summary>
Motivation: 由于网页、电子表格和PDF中表格数量庞大，且文本和图像预训练取得了成功，表格预训练框架应运而生并在各种任务上取得了最先进的成果。因此，有必要对这些框架进行全面回顾。

Method: 本文提供了一份关于表格预训练的模型设计、预训练目标和下游任务的全面综述，并分享了作者对现有挑战和未来机遇的见解和展望。

Result: 本文的结果是系统地总结了表格预训练领域的发展，包括其多样化的模型设计、预训练目标以及在问答、类型识别等下游任务上的应用，并识别了关键的挑战和机遇。

Conclusion: 本综述为表格预训练领域提供了一个全面的概览，有助于研究人员理解当前进展、面临的挑战以及未来的研究方向。

Abstract: Since a vast number of tables can be easily collected from web pages,
spreadsheets, PDFs, and various other document types, a flurry of table
pre-training frameworks have been proposed following the success of text and
images, and they have achieved new state-of-the-arts on various tasks such as
table question answering, table type recognition, column relation
classification, table search, formula prediction, etc. To fully use the
supervision signals in unlabeled tables, a variety of pre-training objectives
have been designed and evaluated, for example, denoising cell values,
predicting numerical relationships, and implicitly executing SQLs. And to best
leverage the characteristics of (semi-)structured tables, various tabular
language models, particularly with specially-designed attention mechanisms,
have been explored. Since tables usually appear and interact with free-form
text, table pre-training usually takes the form of table-text joint
pre-training, which attracts significant research interests from multiple
domains. This survey aims to provide a comprehensive review of different model
designs, pre-training objectives, and downstream tasks for table pre-training,
and we further share our thoughts and vision on existing challenges and future
opportunities.

</details>


### [17] [TableQuery: Querying tabular data with natural language](https://arxiv.org/abs/2202.00454)
*Abhijith Neil Abraham,Fariz Rahman,Damanpreet Kaur*

Main category: cs.CL

TL;DR: TableQuery是一种利用预训练深度学习模型将自然语言查询转换为结构化查询的工具，用于表格数据查询，解决了现有方法需要将整个表格作为输入和重新训练的限制。


<details>
  <summary>Details</summary>
Motivation: 现有的表格数据深度学习问答方法有局限性，例如需要将整个表格作为输入，这不适用于包含数百万行或实时更新的真实世界数据，因为内存限制和频繁序列化数据不切实际。

Method: TableQuery使用预训练的自由文本问答深度学习模型，将自然语言查询转换为结构化查询，这些查询可以直接在数据库或电子表格上运行。此方法无需将整个数据加载到内存中，也无需序列化数据库。

Result: TableQuery消除了将整个数据加载到内存中以及序列化数据库的需要。它不需要重新训练，可以轻松替换性能更好的新型预训练模型。

Conclusion: TableQuery提供了一种高效且灵活的表格数据自然语言查询方法，通过利用预训练的自由文本问答模型，解决了现有方法的内存和实时更新挑战。

Abstract: This paper presents TableQuery, a novel tool for querying tabular data using
deep learning models pre-trained to answer questions on free text. Existing
deep learning methods for question answering on tabular data have various
limitations, such as having to feed the entire table as input into a neural
network model, making them unsuitable for most real-world applications. Since
real-world data might contain millions of rows, it may not entirely fit into
the memory. Moreover, data could be stored in live databases, which are updated
in real-time, and it is impractical to serialize an entire database to a neural
network-friendly format each time it is updated. In TableQuery, we use deep
learning models pre-trained for question answering on free text to convert
natural language queries to structured queries, which can be run against a
database or a spreadsheet. This method eliminates the need for fitting the
entire data into memory as well as serializing databases. Furthermore, deep
learning models pre-trained for question answering on free text are readily
available on platforms such as HuggingFace Model Hub (7). TableQuery does not
require re-training; when a newly trained model for question answering with
better performance is available, it can replace the existing model in
TableQuery.

</details>


### [18] [The Impact of Text Presentation on Translator Performance](https://arxiv.org/abs/2011.05978)
*Samuel Läubli,Patrick Simianer,Joern Wuebker,Geza Kovacs,Rico Sennrich,Spence Green*

Main category: cs.CL

TL;DR: 首次对计算机辅助翻译（CAT）工具的设计选择对译者表现进行对照评估。逐句呈现和上下排列方式能提高文本复制和句内错误识别的速度。然而，对于修订，未分段文本能带来最高的准确性和时间效率。


<details>
  <summary>Details</summary>
Motivation: 目前广泛使用的计算机辅助翻译（CAT）工具将文档分割成句子等片段并以并排、类似电子表格的视图排列。本文旨在首次对照评估这些设计选择对译者表现的影响。

Method: 采用对照评估，在三个实验性文本处理任务中测量速度和准确性。比较了逐句呈现与未分段文本，以及源句和目标句的上下排列与并排排列。

Result: 结果显示，与未分段文本相比，逐句呈现能加快文本复制和句内错误识别。与并排排列相比，源句和目标句的上下排列能加快文本复制。然而，对于修订任务，结果表明呈现未分段文本能带来最高的准确性和时间效率。

Conclusion: 本研究结果对设计CAT工具的最佳实践具有直接的指导意义。

Abstract: Widely used computer-aided translation (CAT) tools divide documents into
segments such as sentences and arrange them in a side-by-side, spreadsheet-like
view. We present the first controlled evaluation of these design choices on
translator performance, measuring speed and accuracy in three experimental text
processing tasks. We find significant evidence that sentence-by-sentence
presentation enables faster text reproduction and within-sentence error
identification compared to unsegmented text, and that a top-and-bottom
arrangement of source and target sentences enables faster text reproduction
compared to a side-by-side arrangement. For revision, on the other hand, our
results suggest that presenting unsegmented text results in the highest
accuracy and time efficiency. Our findings have direct implications for best
practices in designing CAT tools.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [19] [LEI2JSON: Schema-based Validation and Conversion of Livestock Event Information](https://arxiv.org/abs/2310.17414)
*Mahir Habib,Muhammad Ashad Kabir,Lihong Zheng*

Main category: eess.SY

TL;DR: LEI2JSON是一个Google Sheets插件，旨在帮助畜牧生产者将牲畜事件数据标准化为JSON格式，以节省时间和资源。


<details>
  <summary>Details</summary>
Motivation: 畜牧生产者在标准化（转换和验证）牲畜事件数据时常遇到困难。

Method: LEI2JSON通过在Google Sheets中构建带有适当列标题、注释和验证规则的电子表格模板，将数据转换为JSON格式，并根据LEI架构验证输出来实现数据标准化。它支持将JSON格式的数据无缝存储在本地或Google Drive。

Result: 论文进行了广泛的实验评估，以评估该工具的有效性。

Conclusion: LEI2JSON为畜牧生产者提供了一种高效的数据标准化机制，能够显著节省时间和资源，并促进牲畜事件信息的无缝存储。

Abstract: Livestock producers often need help in standardising (i.e., converting and
validating) their livestock event data. This article introduces a novel
solution, LEI2JSON (Livestock Event Information To JSON). The tool is an add-on
for Google Sheets, adhering to the livestock event information (LEI) schema.
The core objective of LEI2JSON is to provide livestock producers with an
efficient mechanism to standardise their data, leading to substantial savings
in time and resources. This is achieved by building the spreadsheet template
with the appropriate column headers, notes, and validation rules, converting
the spreadsheet data into JSON format, and validating the output against the
schema. LEI2JSON facilitates the seamless storage of livestock event
information locally or on Google Drive in JSON. Additionally, we have conducted
an extensive experimental evaluation to assess the effectiveness of the tool.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [20] [Trapped in Transformative Agreements? A Multifaceted Analysis of >1,000 Contracts](https://arxiv.org/abs/2409.20224)
*Laura Rothfritz,W. Benedikt Schmal,Ulrich Herb*

Main category: cs.DL

TL;DR: 该研究分析了学术出版商和研究机构之间的转型协议（TAs），发现尽管这些协议普遍存在，但它们并未将学术界引向完全开放获取的世界，反而使机构“受困”于混合系统，增加了传统出版商的市场支配力，提高了进入壁垒和成本，降低了竞争。


<details>
  <summary>Details</summary>
Motivation: 转型协议（TAs）在学术出版界非常普遍，ESAC倡议数据库中列有1000多个合同。作者旨在利用这一独特数据集进行深入分析，以评估这些协议的实际影响和格局。

Method: 作者通过网络抓取ESAC倡议数据库中的每个合同详情，以大幅扩展ESAC提供的概述表格。在此基础上，他们结合定性和定量方法，对合同特征和TA格局进行了深入分析。

Result: 分析表明，研究机构似乎“受困”于转型协议。这些协议未能成为通向完全开放获取世界的桥梁，反而使学术界陷入混合系统。这赋予了传统（非开放获取）出版商巨大的市场支配力，提高了进入壁垒，降低了竞争，并增加了图书馆和大学的成本。

Conclusion: 转型协议并未有效推动学术界实现完全开放获取，反而巩固了混合出版系统，增强了传统出版商的市场力量，并给研究机构带来了经济负担。

Abstract: Transformative agreements between academic publishers and research
institutions are ubiquitous. The 'Efficiency and Standards for Article Charges'
(ESAC) Initiative lists more than 1,000 contracts in its database. We make use
of this unique dataset by web-scraping the details of every contract to
substantially expand the overview spreadsheet provided by the ESAC Initiative.
Based on that hitherto unused data source, we combine qualitative and
quantitative methods to conduct an in-depth analysis of the contract
characteristics and the TA landscape. Our analysis demonstrates that research
institutions seem to be 'trapped' in transformative agreements. Instead of
being a bridge towards a fully Open Access world, academia is stuck in the
hybrid system. This endows the legacy (non-Open Access) publishing houses with
substantial market power. It raises entry barriers, lowers competition, and
increases costs for libraries and universities.

</details>


### [21] [Ensuring Adherence to Standards in Experiment-Related Metadata Entered Via Spreadsheets](https://arxiv.org/abs/2409.08897)
*Martin J. O'Connor,Josef Hardi,Marcos Martínez-Romero,Sowmya Somasundaram,Brendan Honick,Stephen A. Fisher,Ajay Pillai,Mark A. Musen*

Main category: cs.DL

TL;DR: 该论文描述了一种支持基于电子表格的元数据输入的方法，同时确保符合标准并提供质量控制，通过使用可定制的模板、受控术语和交互式网络工具。


<details>
  <summary>Details</summary>
Motivation: 尽管有复杂的工具辅助数据注释，研究人员通常更喜欢使用电子表格提供元数据，尽管电子表格在确保元数据一致性和符合正式规范方面存在局限性。

Method: 该方法包括用于捕获元数据标准的可定制模板、可直接从电子表格访问的受控术语和本体，以及允许用户快速识别和修复基于电子表格的元数据中错误的交互式网络工具。

Result: 该方法正在生物医学联盟 HuBMAP 中部署，用于定义和收集各种生物测定的元数据。

Conclusion: 该论文提出了一种端到端的方法，支持基于电子表格的元数据输入，同时确保严格遵守基于社区的元数据标准并提供质量控制。

Abstract: Scientists increasingly recognize the importance of providing rich,
standards-adherent metadata to describe their experimental results. Despite the
availability of sophisticated tools to assist in the process of data
annotation, investigators generally seem to prefer to use spreadsheets when
supplying metadata, despite the limitations of spreadsheets in ensuring
metadata consistency and compliance with formal specifications. In this paper,
we describe an end-to-end approach that supports spreadsheet-based entry of
metadata, while ensuring rigorous adherence to community-based metadata
standards and providing quality control. Our methods employ several key
components, including customizable templates that capture metadata standards
and that can inform the spreadsheets that investigators use to author metadata,
controlled terminologies and ontologies for defining metadata values that can
be accessed directly from a spreadsheet, and an interactive Web-based tool that
allows users to rapidly identify and fix errors in their spreadsheet-based
metadata. We demonstrate how this approach is being deployed in a biomedical
consortium known as HuBMAP to define and collect metadata about a wide range of
biological assays.

</details>


### [22] [A Comprehensive Approach to Ensuring Quality in Spreadsheet-Based Metadata](https://arxiv.org/abs/2312.09107)
*Martin J. O'Connor,Marcos Martínez-Romero,Mete Ugur Akdogan,Josef Hardi,Mark A. Musen*

Main category: cs.DL

TL;DR: 本文介绍了一种端到端的方法，支持基于电子表格的元数据输入，并提供严格的合规性和质量控制。该方法采用可定制模板、受控术语支持以及交互式网络工具来识别和修复错误，并已在一个生物医学联盟中得到部署。


<details>
  <summary>Details</summary>
Motivation: 科学家们虽然认识到元数据的重要性，但仍偏好使用电子表格，尽管其在合规性和质量方面存在局限性。现有工具存在学习曲线陡峭和定制化受限等缺点。

Method: 本文提出了一种端到端的方法，支持基于电子表格的元数据输入，并提供严格的合规性和质量控制。其关键策略包括：用于定义元数据的可定制模板、在定义这些模板时对受控术语的集成支持，以及一个交互式网络工具，允许用户快速识别并修复电子表格元数据中的错误。

Result: 该方法正在一个生物医学联盟中部署，用于定义和收集科学实验的元数据。

Conclusion: 本文提出的端到端方法有效解决了基于电子表格的元数据管理中合规性和质量控制的挑战，并通过可定制的模板、受控术语和交互式错误修复工具，提供了一个实用的解决方案，目前已成功应用于生物医学领域。

Abstract: While scientists increasingly recognize the importance of metadata in
describing their data, spreadsheets remain the preferred tool for supplying
this information despite their limitations in ensuring compliance and quality.
Various tools have been developed to address these limitations, but they suffer
from their own shortcomings, such as steep learning curves and limited
customization. In this paper, we describe an end-to-end approach that supports
spreadsheet-based entry of metadata while providing rigorous compliance and
quality control. Our approach employs several key strategies, including
customizable templates for defining metadata, integral support for the use of
controlled terminologies when defining these templates, and an interactive
Web-based tool that allows users to rapidly identify and fix errors in the
spreadsheet-based metadata they supply. We demonstrate how this approach is
being deployed in a biomedical consortium to define and collect metadata about
scientific experiments.

</details>


### [23] [Towards Semantic Interoperability in Historical Research: Documenting Research Data and Knowledge with Synthesis](https://arxiv.org/abs/2107.13957)
*Pavlos Fafalios,Konstantina Konsolaki,Lida Charami,Kostas Petrakis,Manos Paterakis,Dimitris Angelakis,Yannis Tzitzikas,Chrysoula Bekiari,Martin Doerr*

Main category: cs.DL

TL;DR: 本文介绍了一个名为Synthesis的基于网络的协作式文档系统，该系统利用CIDOC-CRM和RDF标准，旨在解决历史科学研究中现有文物文档实践（如电子表格或简单关系数据库）在协作、细节表示、数据结构扩展、多源数据整合以及数据重用方面的诸多问题。


<details>
  <summary>Details</summary>
Motivation: 历史科学研究中，文物文档和相关证据的研究主要使用电子表格或简单的关系数据库来组织信息。然而，这种方式存在多项问题，包括：难以实现大量用户的协作与受控文档、缺乏对推断关系的细节表示、难以扩展底层数据结构并整合来自多个不同来源的数据、以及数据难以在特定研究活动之外重复使用。

Method: 本文描述了Synthesis文档系统。该系统是基于网络的、协作式的，并利用现有的信息文档和发布标准（如CIDOC-CRM、RDF），专注于语义互操作性以及高价值和长期有效数据的生产。

Result: 该系统已被大量历史学家在一个正在进行的艺术史研究项目中成功使用。

Conclusion: Synthesis系统旨在通过提供一个协作、基于网络并符合现有标准的解决方案，支持历史学家应对现有文档实践中的问题，从而实现语义互操作性，并生成具有高价值和长期有效性的数据。

Abstract: A vast area of research in historical science concerns the documentation and
study of artefacts and related evidence. Current practice mostly uses
spreadsheets or simple relational databases to organise the information as rows
with multiple columns of related attributes. This form offers itself for data
analysis and scholarly interpretation, however it also poses problems including
i) the difficulty for collaborative but controlled documentation by a large
number of users, ii) the lack of representation of the details from which the
documented relations are inferred, iii) the difficulty to extend the underlying
data structures as well as to combine and integrate data from multiple and
diverse information sources, and iv) the limitation to reuse the data beyond
the context of a particular research activity. To support historians to cope
with these problems, in this paper we describe the Synthesis documentation
system and its use by a large number of historians in the context of an ongoing
research project in the field of History of Art. The system is Web-based and
collaborative, and makes use of existing standards for information
documentation and publication (CIDOC-CRM, RDF), focusing on semantic
interoperability and the production of data of high value and long-term
validity.

</details>


### [24] [FAST CAT: Collaborative Data Entry and Curation for Semantic Interoperability in Digital Humanities](https://arxiv.org/abs/2105.13733)
*Pavlos Fafalios,Kostas Petrakis,Georgios Samaritakis,Korina Doerr,Athina Kritsotaki,Yannis Tzitzikas,Martin Doerr*

Main category: cs.DL

TL;DR: 本文介绍了FAST CAT，一个针对数字人文和实证研究的协作式辅助数据录入和管理系统，旨在解决传统数据管理工具在描述性和实证科学中面临的局限性。


<details>
  <summary>Details</summary>
Motivation: 描述性和实证科学（如历史学）在数据管理和分析中主要依赖电子表格和关系型数据库。然而，这种做法存在局限性，包括数据高度依赖初始假设、难以用于其他研究、无法表示推断关系的细节以及难以核实原始数据源。

Method: 本文提出了FAST CAT，一个用于数字人文和类似实证研究的协作式辅助数据录入和管理系统。该系统旨在应对现有数据管理实践的问题。

Result: 论文描述了相关的挑战，支持语义互操作性的总体方法，并讨论了FAST CAT在一个名为SeaLiT的欧洲（ERC）海洋历史项目中的应用，该项目研究1850年至1920年间蒸汽船引入地中海地区对经济、社会和人口的影响。

Conclusion: FAST CAT系统通过解决传统数据管理工具的局限性，为数字人文和实证研究提供了改进的数据录入和管理方案，并通过实际项目案例展示了其在语义互操作性方面的潜力。

Abstract: Descriptive and empirical sciences, such as History, are the sciences that
collect, observe and describe phenomena in order to explain them and draw
interpretative conclusions about influences, driving forces and impacts under
given circumstances. Spreadsheet software and relational database management
systems are still the dominant tools for quantitative analysis and overall data
management in these these sciences, allowing researchers to directly analyse
the gathered data and perform scholarly interpretation. However, this current
practice has a set of limitations, including the high dependency of the
collected data on the initial research hypothesis, usually useless for other
research, the lack of representation of the details from which the registered
relations are inferred, and the difficulty to revisit the original data sources
for verification, corrections or improvements. To cope with these problems, in
this paper we present FAST CAT, a collaborative system for assistive data entry
and curation in Digital Humanities and similar forms of empirical research. We
describe the related challenges, the overall methodology we follow for
supporting semantic interoperability, and discuss the use of FAST CAT in the
context of a European (ERC) project of Maritime History, called SeaLiT, which
examines economic, social and demographic impacts of the introduction of
steamboats in the Mediterranean area between the 1850s and the 1920s.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [25] [Using a Catenary Trajectory to Reduce Wellbore Friction in Horizontal Extended Reach Drilling](https://arxiv.org/abs/2309.12317)
*Vu Nguyen*

Main category: cs.RO

TL;DR: 通过悬链线概念减少井筒摩擦。


<details>
  <summary>Details</summary>
Motivation: 井筒摩擦与总成本密切相关，是一个主要问题。悬链线概念可以降低摩擦，但缺乏详细分析。

Method: 采用悬链线形状使钻杆自由悬挂在井筒中，以消除接触和摩擦。通过案例研究比较悬链线轨迹设计与传统二维圆弧设计。计算过程在Excel电子表格中。

Result: 引入案例研究来检验悬链线轨迹设计与传统二维圆弧设计之间的结果。开发了易于使用且可靠的MS Excel电子表格，用于设计延伸井的悬链线井眼轨迹。

Conclusion: 该项目旨在填补悬链线概念在减少井筒摩擦方面的详细分析空白，提供了一种减少扭矩和阻力的设计方法，并开发了实用的Excel工具。

Abstract: Wellbore friction is one of the biggest concerns when drilling due to its
relation to the total cost. The catenary concept was introduced to reduce
wellbore friction, but it requires detailed analyses. This project would fill
this gap. A catenary shape is simply the natural shape of a rope, chain, or
drill string. The drill string will then hang freely inside the wellbore.
Perfectly, there should be no contact between the hole and the string, and thus
no friction. Torque and drag should be minimized this way. A case study is
introduced to examine the outcome between Catenary Trajectory Design and
traditional 2D Arc design. The calculation procedure of Catenary Trajectory and
2D Arc Design can be found in an MS Excel spreadsheet which is easy to use and
reliable for designing catenary well trajectories for extended-reach wells.

</details>


### [26] [Hazard Analysis of Collaborative Automation Systems: A Two-layer Approach based on Supervisory Control and Simulation](https://arxiv.org/abs/2209.12560)
*Tom P. Huck,Yuvaraj Selvaraj,Constantin Cronrath,Christoph Ledermann,Martin Fabian,Bengt Lennartson,Torsten Kröger*

Main category: cs.RO

TL;DR: 本文提出了一种用于安全关键系统的两层模型化危害分析方法，结合了形式化方法（监督控制理论）和仿真，并在人机协作系统上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 当前的危害分析方法（人工推理、检查表、电子表格、基于测试）随着系统复杂性增加而变得不适用，且成本高昂或存在风险。现有的模型化方法各有优缺点。

Method: 采用两层方法：首先，使用监督控制理论从系统的形式化模型中合成不安全行为；然后，将结果输入到仿真中，使用特定领域的风险指标进行详细分析。

Result: 该方法结合了形式化方法的穷举分析和仿真的详细分析的优点，可普遍适用，并在一项工业人机协作系统上展示了其优势。

Conclusion: 本文提出了一种新颖的两层危害分析方法，它结合了形式化方法和仿真，克服了传统方法和单一模型化方法的局限性，从而改善了安全关键系统的分析。

Abstract: Safety critical systems are typically subjected to hazard analysis before
commissioning to identify and analyse potentially hazardous system states that
may arise during operation. Currently, hazard analysis is mainly based on human
reasoning, past experiences, and simple tools such as checklists and
spreadsheets. Increasing system complexity makes such approaches decreasingly
suitable. Furthermore, testing-based hazard analysis is often not suitable due
to high costs or dangers of physical faults. A remedy for this are model-based
hazard analysis methods, which either rely on formal models or on simulation
models, each with their own benefits and drawbacks. This paper proposes a
two-layer approach that combines the benefits of exhaustive analysis using
formal methods with detailed analysis using simulation. Unsafe behaviours that
lead to unsafe states are first synthesised from a formal model of the system
using Supervisory Control Theory. The result is then input to the simulation
where detailed analyses using domain-specific risk metrics are performed.
Though the presented approach is generally applicable, this paper demonstrates
the benefits of the approach on an industrial human-robot collaboration system.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [27] [Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!](https://arxiv.org/abs/2309.02110)
*James P. Dilger*

Main category: math.HO

TL;DR: 通过对Wordle玩家首轮猜测数据的分析，发现每日有4000-10000名玩家作弊，至少1/3的玩家有偏好开局词，且玩家行为易受外部因素影响。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在超越社交媒体帖子、调查和谷歌趋势，通过提供关于Wordle游戏中作弊行为和玩家习惯的可靠定量证据。

Method: 作者从2023年5月至2023年8月收集并整理了Wordle玩家首轮猜测的数据，这些数据可通过复制粘贴轻松导入电子表格。

Result: 1) 每天约有0.2-0.5%的玩家一次性猜中，这表明有4000-10000名玩家作弊。2) 至少1/3的玩家有最喜欢的起始词，并且即使该词曾作为目标词出现，他们也倾向于保持忠诚。3) 2023年8月15日，约30,000名玩家突然改变了他们的起始词，推测是受填字游戏线索的影响。

Conclusion: Wordle玩家行为易受外部因素影响，且通过定量数据分析可以发现游戏中的作弊行为和玩家习惯。

Abstract: Wordle is a popular, online word game offered by the New York Times
(nytimes.com). Currently there are some 2 million players of the English
version worldwide. Players have 6 attempts to guess the daily word (target
word) and after each attempt, the player receives color-coded information about
the correctness and position of each letter in the guess. After either a
successful completion of the puzzle or the final unsuccessful attempt, software
can assess the player's luck and skill using Information Theory and can display
data for the first, second, ..., sixth guesses of a random sample of all
players. Recently, I discovered that the latter data is presented in a format
that can easily be copied and pasted into a spreadsheet. I compiled data on
Wordle players' first guesses from May 2023 - August 2023 and inferred some
interesting information about Wordle players. A) Every day, about 0.2-0.5% of
players solve the puzzle in one attempt. Because the odds of guessing the one
of 2,315 possible target words at random is 0.043%, this implies that 4,000 -
10,000 players cheat by obtaining the target word outside of playing the game!
B) At least 1/3 of the players have a favorite starting word, or cycle through
several. And even though players should be aware that target words are never
repeated, most players appear to remain loyal to their starting word even after
its appearance as a target word. C) On August 15, 2023, about 30,000 players
abruptly changed their starting word, presumably based on a crossword puzzle
clue! Wordle players can be influenced! This study goes beyond social media
postings, surveys, and Google Trends to provide solid, quantitative evidence
about cheating in Wordle.

</details>


### [28] [GeoGebra e situações que envolvem modelação numa abordagem STEAM](https://arxiv.org/abs/1907.02099)
*J. M. D. S. Dos Santos,A. P. Silveira,A. E. S. Trocado*

Main category: math.HO

TL;DR: 该论文介绍了一套基于GeoGebra的教学任务，旨在葡萄牙语国家推广STEAM数学教学方法，主要应用于二维和三维几何建模问题，并计划推广至其他语言。


<details>
  <summary>Details</summary>
Motivation: 在葡萄牙语国家，将GeoGebra互动数学软件融入数学课堂，以实施STEAM教学方法。

Method: 设计了一系列涉及二维和三维几何建模的教学任务，利用GeoGebra的各种功能窗口（如2D、3D、CAS、电子表格），并提供脚本以支持不同熟练程度的用户。这些任务首先用于教师培训，之后将根据需要调整后应用于学生。

Result: 这些任务促进了数学与其他科学和艺术的联系，支持了项目开发，并巩固了相关的数学内容。它们已被整合到葡萄牙语国家GeoGebra培训师课程中，并计划翻译成西班牙语和英语以供全球推广。

Conclusion: 开发的GeoGebra教学任务对于在数学教育中实施STEAM方法、促进跨学科联系具有重要意义，并将进行国际推广。

Abstract: In order to implement a STEAM approach including the use of technology,
namely the use of interactive mathematics software GeoGebra, in mathematics
classes, in the lusophone space, the materials presented here were conceived,
to be implemented in a first phase among teachers. Later, with the necessary
adaptations, these tasks will be applied to the students. The tasks deal with
modeling situations, in two- and three-dimensional geometric problems, in order
to apply GeoGebra software in its analysis to illustrate its capabilities. The
different windows of this software are used, namely the 2D and 3D windows, CAS
window, spreadsheet and extra two dimensional windows in order to study cutting
planes in solids and some surfaces. The tasks are presented so that any user,
regardless of the degree of knowledge they have of the software, can follow
them, being supported in scripts with some indications of the tools and
commands to use. Designed for the teaching and learning of Mathematics, from a
STEAM approach, these tasks allow connections with other Sciences and the Arts,
and allow the development of projects using and consolidating relevant
mathematical contents. These tasks are part of the proposals of activities of
the participants of the Training Courses for Trainers in GeoGebra for
Portuguese Speaking Countries, which from 2019 have an impact on the STEAM
approach. These courses are carried out with the high sponsorship of the
Organization of Ibero-American States for Education, Science and Culture (OEI).
Given the interest that the tasks have for the users of the Iberian space, as
well as their dissemination at a global level, the materials initially
developed in Portuguese language will be adapted for Spanish and English
speakers.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [29] [Consensus-Free Spreadsheet Integration](https://arxiv.org/abs/2209.14457)
*Brandon Baylor,Eric Daimler,James Hansen,Esteban Montero,Ryan Wisnesky*

Main category: cs.DB

TL;DR: 该论文提出了一种利用范畴论方法合并多个电子表格的策略，通过将每个表格公式视为代数理论，值视为模型，重叠部分视为理论和模型态射，然后运用余极限、提升和Kan-扩展等概念来计算一个普遍整合的理论和模型，最终可以表示为一个新的电子表格。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在提供一种合并工程模型的方法，该方法无需模型作者之间达成共识，因为理论和模型态射是语义保持的。

Method: 将每个电子表格的公式表示为代数（方程）理论，将其值表示为其理论的模型。将表格之间的重叠部分表示为理论和模型态射。然后，使用范畴论中的余极限、提升和Kan-扩展构造来计算一个规范通用的集成理论和模型，该模型可以表示为一个电子表格。

Result: 通过一个在一家主要能源公司进行的真实世界油气计算案例研究，展示了该方法，该案例整合了由两位非交互工程师构建的两个不同套管压力测试（MASP）计算电子表格。同时描述了验证重叠映射语义保持以及结果集成表格的保守性/一致性所涉及的自动化定理证明负担。

Conclusion: 文章最后探讨了如何将该方法应用于企业范围内的工程工作规模扩展。

Abstract: We describe a method for merging multiple spreadsheets into one sheet, and/or
exchanging data among the sheets, by expressing each sheet's formulae as an
algebraic (equational) theory and each sheet's values as a model of its theory,
expressing the overlap between the sheets as theory and model morphisms, and
then performing colimit, lifting, and Kan-extension constructions from category
theory to compute a canonically universal integrated theory and model, which
can then be expressed as a spreadsheet. Our motivation is to find methods of
merging engineering models that do not require consensus (agreement) among the
authors of the models being merged, a condition fulfilled by our method because
theory and model morphisms are semantics-preserving. We describe a case study
of this methodology on a real-world oil and gas calculation at a major energy
company, describing the theories and models that arise when integrating two
different casing pressure test (MASP) calculation spreadsheets constructed by
two non-interacting engineers. We also describe the automated theorem proving
burden associated with both verifying the semantics preservation of the overlap
mappings as well as verifying the conservativity/consistency of the resulting
integrated sheet. We conclude with thoughts on how to apply the methodology to
scale engineering efforts across the enterprise.

</details>


### [30] [Data-Semantics-Aware Recommendation of Diverse Pivot Tables](https://arxiv.org/abs/2507.06171)
*Whanhee Cho,Anna Fariha*

Main category: cs.DB

TL;DR: SAGE是一个数据语义感知的系统，旨在推荐多样化、有洞察力且可解释的枢轴表，解决了从大型数据集中识别有用属性组合和多样化推荐的挑战。


<details>
  <summary>Details</summary>
Motivation: 在电子表格中，数据汇总（如使用枢轴表）对于从大型数据集中发现洞察至关重要。然而，识别有用的属性组合以及多样化推荐是一个挑战，尤其是在高维数据集中，传统方法未能充分解决枢轴表多样化问题。

Method: SAGE提出了一个数据语义感知模型来衡量单个枢轴表的效用和一组枢轴表的多样性。它还提出了一种可扩展的贪婪算法，通过利用数据语义来显著减少组合搜索空间，从而有效地选择一组具有高效用的多样化枢轴表。

Result: SAGE在三个真实世界数据集上的广泛实验表明，它优于其他替代方法，并且能够有效地扩展以适应高维数据集。此外，案例研究突出了SAGE在商业软件和大型语言模型（LLMs）上的定性有效性。

Conclusion: SAGE成功地解决了推荐有洞察力、可解释且多样化的枢轴表的挑战，克服了现有方法的不足，并通过利用数据语义提供了高效且高质量的解决方案。

Abstract: Data summarization is essential to discover insights from large datasets. In
a spreadsheets, pivot tables offer a convenient way to summarize tabular data
by computing aggregates over some attributes, grouped by others. However,
identifying attribute combinations that will result in useful pivot tables
remains a challenge, especially for high-dimensional datasets. We formalize the
problem of automatically recommending insightful and interpretable pivot
tables, eliminating the tedious manual process. A crucial aspect of
recommending a set of pivot tables is to diversify them. Traditional works
inadequately address the table-diversification problem, which leads us to
consider the problem of pivot table diversification.
  We present SAGE, a data-semantics-aware system for recommending k-budgeted
diverse pivot tables, overcoming the shortcomings of prior work for top-k
recommendations that cause redundancy. SAGE ensures that each pivot table is
insightful, interpretable, and adaptive to the user's actions and preferences,
while also guaranteeing that the set of pivot tables are different from each
other, offering a diverse recommendation. We make two key technical
contributions: (1) a data-semantics-aware model to measure the utility of a
single pivot table and the diversity of a set of pivot tables, and (2) a
scalable greedy algorithm that can efficiently select a set of diverse pivot
tables of high utility, by leveraging data semantics to significantly reduce
the combinatorial search space. Our extensive experiments on three real-world
datasets show that SAGE outperforms alternative approaches, and efficiently
scales to accommodate high-dimensional datasets. Additionally, we present
several case studies to highlight SAGE's qualitative effectiveness over
commercial software and Large Language Models (LLMs).

</details>


### [31] [A System and Benchmark for LLM-based Q&A on Heterogeneous Data](https://arxiv.org/abs/2409.05735)
*Achille Fokoue,Srideepika Jayaraman,Elham Khabiri,Jeffrey O. Kephart,Yingjie Li,Dhruv Shah,Youssef Drissi,Fenno F. Heath III,Anu Bhamidipaty,Fateh A. Tipu,Robert J. Baseman*

Main category: cs.DB

TL;DR: 本文介绍了siwarex平台，该平台通过引入对数据库和API的自然语言访问来解决工业环境中异构数据源问题，从而扩展了Text-to-SQL应用。


<details>
  <summary>Details</summary>
Motivation: 在许多工业环境中，用户希望通过自然语言访问存储在不同（且可能是孤立的）结构化数据源（如电子表格、数据库、API）中的信息。现有的基于大型语言模型（LLM）的Text-to-SQL应用无法有效处理这种数据源异构性，这使其在实际工业场景中不具实用性。

Method: 本文提出了siwarex平台，旨在实现对数据库和API的无缝自然语言访问，以解决数据源异构性问题。为了验证其有效性，作者扩展了流行的Spider数据集和基准，将部分表格替换为数据检索API。

Result: siwarex平台能够很好地处理数据源的异构性。

Conclusion: siwarex平台成功解决了工业环境中异构数据源的自然语言访问问题，并且修改后的Spider基准将很快向研究社区开放。

Abstract: In many industrial settings, users wish to ask questions whose answers may be
found in structured data sources such as a spreadsheets, databases, APIs, or
combinations thereof. Often, the user doesn't know how to identify or access
the right data source. This problem is compounded even further if multiple (and
potentially siloed) data sources must be assembled to derive the answer.
Recently, various Text-to-SQL applications that leverage Large Language Models
(LLMs) have addressed some of these problems by enabling users to ask questions
in natural language. However, these applications remain impractical in
realistic industrial settings because they fail to cope with the data source
heterogeneity that typifies such environments. In this paper, we address
heterogeneity by introducing the siwarex platform, which enables seamless
natural language access to both databases and APIs. To demonstrate the
effectiveness of siwarex, we extend the popular Spider dataset and benchmark by
replacing some of its tables by data retrieval APIs. We find that siwarex does
a good job of coping with data source heterogeneity. Our modified Spider
benchmark will soon be available to the research community

</details>


### [32] [Auditable and reusable crosswalks for fast, scaled integration of scattered tabular data](https://arxiv.org/abs/2409.01517)
*Gavin Chait*

Main category: cs.DB

TL;DR: 该论文介绍了一个开源的数据整理工具包，旨在生成结构良好且可互操作的数据。


<details>
  <summary>Details</summary>
Motivation: 解决复杂和分散的表格数据需要符合目标模式的问题，并生产结构良好和可互操作的数据。

Method: 该工具包将数据整理分为离散组件，以模式为中心，使用高级顺序脚本进行模式到模式的映射。它以Python包和“无代码”可视化网络应用的形式提供。

Result: 数据通过交叉映射进行转换。一个可视化示例展示了如何将数百个地方议会的分散源数据集成到一个单一数据库中。

Conclusion: 该工具包提供了一种有效的方法来重构符合模式定义的任何数据，从而提高数据结构和互操作性，并通过整合复杂的真实世界数据得到了证明。

Abstract: This paper presents an open-source curatorial toolkit intended to produce
well-structured and interoperable data. Curation is divided into discrete
components, with a schema-centric focus for auditable restructuring of complex
and scattered tabular data to conform to a destination schema. Task separation
allows development of software and analysis without source data being present.
Transformations are captured as high-level sequential scripts describing
schema-to-schema mappings, reducing complexity and resource requirements.
Ultimately, data are transformed, but the objective is that any data meeting a
schema definition can be restructured using a crosswalk. The toolkit is
available both as a Python package, and as a 'no-code' visual web application.
A visual example is presented, derived from a longitudinal study where
scattered source data from hundreds of local councils are integrated into a
single database.

</details>


### [33] [Auto-Formula: Recommend Formulas in Spreadsheets using Contrastive Learning for Table Representations](https://arxiv.org/abs/2404.12608)
*Sibei Chen,Yeye He,Weiwei Cui,Ju Fan,Song Ge,Haidong Zhang,Dongmei Zhang,Surajit Chaudhuri*

Main category: cs.DB

TL;DR: Auto-Formula系统通过对比学习，利用组织内相似电子表格中的现有公式，准确预测用户在目标单元格中可能需要的公式，旨在简化复杂公式的编写。


<details>
  <summary>Details</summary>
Motivation: 尽管电子表格被广泛使用，但非技术用户编写复杂公式仍然具有挑战性，因为他们需要理解复杂的公式语法。

Method: 开发了一个名为Auto-Formula的系统，该系统通过学习和借鉴相似电子表格中已有的公式，并利用受计算机视觉中“相似人脸识别”启发的对比学习技术，来预测用户想要在目标电子表格单元格中编写的公式。

Result: 在从真实企业电子表格中提取的2000多个测试公式上进行了广泛评估，结果表明Auto-Formula系统比其他替代方案更有效。

Conclusion: Auto-Formula系统通过利用现有数据和对比学习有效解决了电子表格中复杂公式编写的挑战，并且表现优于其他替代方案。

Abstract: Spreadsheets are widely recognized as the most popular end-user programming
tools, which blend the power of formula-based computation, with an intuitive
table-based interface. Today, spreadsheets are used by billions of users to
manipulate tables, most of whom are neither database experts nor professional
programmers.
  Despite the success of spreadsheets, authoring complex formulas remains
challenging, as non-technical users need to look up and understand non-trivial
formula syntax. To address this pain point, we leverage the observation that
there is often an abundance of similar-looking spreadsheets in the same
organization, which not only have similar data, but also share similar
computation logic encoded as formulas. We develop an Auto-Formula system that
can accurately predict formulas that users want to author in a target
spreadsheet cell, by learning and adapting formulas that already exist in
similar spreadsheets, using contrastive-learning techniques inspired by
"similar-face recognition" from compute vision.
  Extensive evaluations on over 2K test formulas extracted from real enterprise
spreadsheets show the effectiveness of Auto-Formula over alternatives. Our
benchmark data is available at https://github.com/microsoft/Auto-Formula to
facilitate future research.

</details>


### [34] [Facilitating Digital Agriculture with Simple Databases](https://arxiv.org/abs/2312.06517)
*Dennis Buckmaster,Sami Basir,Hanae Sakata*

Main category: cs.DB

TL;DR: 为农业从业者提供开源、用户友好的Air table数据库模板，帮助他们用简单的电子表格技能有效管理运营数据。


<details>
  <summary>Details</summary>
Motivation: 农业从业者（特别是那些电子表格技能有限的）需要一种简单的方法来管理和分析运营数据，以改善物流、提供背景元数据和增强企业分析。这项工作旨在促进数字农业原理的普及。

Method: 提供结构良好的私人数据库模板（Air table数据库）作为开源资源。这些数据库使用简单的数据验证表格，具有定制应用程序的外观和感觉，并提供一个解释如何构建活动记录数据库的录制研讨会。

Result: 这些模板产生整洁、机器和人类可读、可编辑、可导出用于其他软件分析的运营数据。这些数据可以促进物流、提供背景元数据并改进企业分析。

Conclusion: 这些资源（开源模板和研讨会）可能通过推广和结构化的教育项目促进数字农业原则的融入，帮助农业从业者更好地管理数据。

Abstract: As an on-ramp to databases, we offer several well-structured private database
templates as open source resources for agriculturalists, particularly those
with modest spreadsheet skills. These farmer-oriented Air table databases use
simple data-validated forms, with the look and feel of a customized app, to
yield operational data that is tidy, machine- and human-readable, editable, and
exportable for analysis in other software. Such data can facilitate logistics,
provide contextual metadata, and improve enterprise analysis. A recorded
workshop explaining how to build a database for activity records is presented.
These resources may facilitate infusion of digital agriculture principles
through Extension and structured educational programming.

</details>


### [35] [Streamlining Knowledge Graph Construction with a façade: The SPARQL Anything project](https://arxiv.org/abs/2310.16700)
*Luigi Asprino,Enrico Daga,Justin Dowdy,Paul Mulholland,Aldo Gangemi,Marco Ratta*

Main category: cs.DB

TL;DR: 本文介绍了SPARQL Anything系统，这是一个为知识工程师设计的数据集成框架，它允许通过SPARQL 1.1查询各种异构资源，仿佛它们是RDF数据。文章描述了其设计理念、软件架构、功能特点以及通过用户调查和行业报告评估的价值。


<details>
  <summary>Details</summary>
Motivation: 解决知识工程师在数据集成中如何构建一个有效框架的问题，并提出将异构资源统一视为RDF进行查询的需求。

Method: 本文描述了SPARQL Anything系统的设计理念和软件架构，该系统借鉴了面向对象软件工程中的“façade”概念，并通过重载SERVICE子句实现。此外，通过社区调查和行业实地报告来评估其设计假设的用户价值。

Result: SPARQL Anything支持广泛的文件格式（包括CSV、JSON、XML、Spreadsheets、Markdown、YAML、DOCx、Bibtex），提供灵活的Web API查询、参数化查询和多重转换链。研究报告了其设计理念相对于替代解决方案的用户价值。

Conclusion: SPARQL Anything系统为知识工程师提供了一个强大的数据集成框架，能够高效地查询和整合多种异构数据源，并通过实际应用场景和用户反馈验证了其设计和方法的有效性。

Abstract: What should a data integration framework for knowledge engineers look like?
Recent research on Knowledge Graph construction proposes the design of a
fa\c{c}ade, a notion borrowed from object-oriented software engineering. This
idea is applied to SPARQL Anything, a system that allows querying heterogeneous
resources as-if they were in RDF, in plain SPARQL 1.1, by overloading the
SERVICE clause. SPARQL Anything supports a wide variety of file formats, from
popular ones (CSV, JSON, XML, Spreadsheets) to others that are not supported by
alternative solutions (Markdown, YAML, DOCx, Bibtex). Features include querying
Web APIs with high flexibility, parametrised queries, and chaining multiple
transformations into complex pipelines. In this paper, we describe the design
rationale and software architecture of the SPARQL Anything system. We provide
references to an extensive set of reusable, real-world scenarios from various
application domains. We report on the value-to-users of the founding
assumptions of its design, compared to alternative solutions through a
community survey and a field report from the industry.

</details>


### [36] [DataVinci: Learning Syntactic and Semantic String Repairs](https://arxiv.org/abs/2308.10922)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Gust Verbruggen*

Main category: cs.DB

TL;DR: DataVinci是一种完全无监督的字符串数据错误检测和修复系统，它利用LLM处理语法和语义错误，并结合执行信息进行修复，在性能上超越了现有基线。


<details>
  <summary>Details</summary>
Motivation: 字符串数据在实际数据集中非常常见（例如，Excel中67.6%的值是文本）。成功清洗这些数据可以对用户产生重大影响。现有方法存在局限性，如仅限于错误检测、需要用户输入，或只独立关注语法或语义错误，忽略了字符串常常同时包含这两种类型。

Method: DataVinci是一个完全无监督的字符串数据错误检测和修复系统。它通过学习覆盖列中大多数值的正则表达式模式来检测错误。对于修复，它根据多数模式和从其他列学习到的约束自动推导编辑。为了处理同时包含语法和语义子字符串的情况，DataVinci使用大型语言模型（LLM）来抽象（并重新具体化）字符串的语义部分。此外，对于无法形成多数模式的数据，DataVinci利用现有程序的执行信息来识别和纠正数据修复。

Result: DataVinci在4个现有和新基准上进行评估时，在错误检测和修复方面均优于7个基线系统。

Conclusion: DataVinci提供了一种新颖的、完全无监督的字符串数据清洗方法，有效地解决了语法和语义错误，显著提高了数据质量。

Abstract: String data is common in real-world datasets: 67.6% of values in a sample of
1.8 million real Excel spreadsheets from the web were represented as text.
Systems that successfully clean such string data can have a significant impact
on real users. While prior work has explored errors in string data, proposed
approaches have often been limited to error detection or require that the user
provide annotations, examples, or constraints to fix the errors. Furthermore,
these systems have focused independently on syntactic errors or semantic errors
in strings, but ignore that strings often contain both syntactic and semantic
substrings. We introduce DataVinci, a fully unsupervised string data error
detection and repair system. DataVinci learns regular-expression-based patterns
that cover a majority of values in a column and reports values that do not
satisfy such patterns as data errors. DataVinci can automatically derive edits
to the data error based on the majority patterns and constraints learned over
other columns without the need for further user interaction. To handle strings
with both syntactic and semantic substrings, DataVinci uses an LLM to abstract
(and re-concretize) portions of strings that are semantic prior to learning
majority patterns and deriving edits. Because not all data can result in
majority patterns, DataVinci leverages execution information from an existing
program (which reads the target data) to identify and correct data repairs that
would not otherwise be identified. DataVinci outperforms 7 baselines on both
error detection and repair when evaluated on 4 existing and new benchmarks.

</details>


### [37] [Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples](https://arxiv.org/abs/2307.14565)
*Peng Li,Yeye He,Cong Yan,Yue Wang,Surajit Chaudhuri*

Main category: cs.DB

TL;DR: 现实世界中超过30%的表格不符合关系型标准，这使得使用SQL进行分析变得困难。本文开发了一个名为Auto-Tables的系统，可以自动将非关系型表格转换为标准关系型格式，从而简化数据准备过程。


<details>
  <summary>Details</summary>
Motivation: 关系型表格是数据库的标准，但在实际的电子表格和网络表格中，超过30%的表格不符合这一标准。这导致需要复杂的表格重构转换才能使用基于SQL的分析工具进行查询。手动编程这些转换非常困难，给技术和非技术用户都带来了巨大痛点。

Method: 本文开发了Auto-Tables系统，该系统可以自动合成包含多步转换的管道（使用Python或其他语言），将非关系型表格转换为标准关系型表格，以便进行下游分析。作者还通过收集来自用户电子表格和在线论坛的244个真实测试用例，为这项新任务编制了一个广泛的基准。

Result: 评估表明，Auto-Tables系统能够以交互速度成功地为超过70%的测试用例合成转换，且无需用户输入。

Conclusion: Auto-Tables是一个有效的工具，可以帮助技术和非技术用户准备数据进行分析，解决了非关系型表格转换的难题。

Abstract: Relational tables, where each row corresponds to an entity and each column
corresponds to an attribute, have been the standard for tables in relational
databases. However, such a standard cannot be taken for granted when dealing
with tables "in the wild". Our survey of real spreadsheet-tables and web-tables
shows that over 30% of such tables do not conform to the relational standard,
for which complex table-restructuring transformations are needed before these
tables can be queried easily using SQL-based analytics tools. Unfortunately,
the required transformations are non-trivial to program, which has become a
substantial pain point for technical and non-technical users alike, as
evidenced by large numbers of forum questions in places like StackOverflow and
Excel/Power-BI/Tableau forums.
  We develop an Auto-Tables system that can automatically synthesize pipelines
with multi-step transformations (in Python or other languages), to transform
non-relational tables into standard relational forms for downstream analytics,
obviating the need for users to manually program transformations. We compile an
extensive benchmark for this new task, by collecting 244 real test cases from
user spreadsheets and online forums. Our evaluation suggests that Auto-Tables
can successfully synthesize transformations for over 70% of test cases at
interactive speeds, without requiring any input from users, making this an
effective tool for both technical and non-technical users to prepare data for
analytics.

</details>


### [38] [Efficient and Compact Spreadsheet Formula Graphs](https://arxiv.org/abs/2302.05482)
*Dixin Tang,Fanchao Chen,Christopher De Leon,Tana Wattanawaroon,Jeaseok Yun,Srinivasan Seshadri,Aditya G. Parameswaran*

Main category: cs.DB

TL;DR: TACO框架利用表格局部性压缩电子表格公式图，从而显著提高查询和维护速度。


<details>
  <summary>Details</summary>
Motivation: 电子表格中庞大复杂的公式图导致查询和维护耗时，降低了交互性。

Method: TACO框架利用表格局部性（相邻单元格具有相似公式结构）的四个模式，开发了压缩公式图、直接查询压缩图以及在更新期间增量维护图的算法。

Result: TACO显著减小了公式图的尺寸。在查询公式图方面，TACO相对于基线和商业电子表格系统，分别实现了高达34,972倍和632倍的加速。

Conclusion: TACO框架通过高效压缩公式图，大幅提升了电子表格的查询和维护性能。

Abstract: Spreadsheets are one of the most popular data analysis tools, wherein users
can express computation as formulae alongside data. The ensuing dependencies
are tracked as formula graphs. Efficiently querying and maintaining these
formula graphs is critical for interactivity across multiple settings.
Unfortunately, formula graphs are often large and complex such that querying
and maintaining them is time-consuming, reducing interactivity. We propose
TACO, a framework for efficiently compressing formula graphs, thereby reducing
the time for querying and maintenance. The efficiency of TACO stems from a key
spreadsheet property: tabular locality, which means that cells close to each
other are likely to have similar formula structures. We leverage four such
tabular locality-based patterns and develop algorithms for compressing formula
graphs using these patterns, directly querying the compressed graph without
decompression, and incrementally maintaining the graph during updates. We
integrate TACO into an open-source spreadsheet system and show that TACO can
significantly reduce formula graph sizes. For querying formula graphs, the
speedups of TACO over a baseline implemented in our framework and a commercial
spreadsheet system are up to 34,972x and 632x, respectively.

</details>


### [39] [Sigma Workbook: A Spreadsheet for Cloud Data Warehouses](https://arxiv.org/abs/2204.03128)
*James Gale,Max Seiden,Deepanshu Utkarsh,Jason Frantz,Rob Woollen,Çağatay Demiralp*

Main category: cs.DB

TL;DR: Sigma Workbook 是一个交互式系统，允许业务用户在大规模云数据仓库中进行可视化数据分析，通过类似电子表格的界面和动态构建的SQL查询实现。


<details>
  <summary>Details</summary>
Motivation: 现有用于分析云数据仓库数据的工具在即席转换方面受限，或者对业务用户来说难以使用。

Method: Sigma Workbook 提供了一个类似电子表格的界面，通过直接操作进行分析。它从用户交互中动态构建匹配的SQL查询，并直接在云数据仓库上执行这些查询。

Result: 通过3个实际用例（群组分析、会话化和数据增强），展示了Sigma Workbook的易用性、可伸缩性和表达能力。

Conclusion: Sigma Workbook 使得业务用户可以轻松地在大规模云数据仓库中进行可视化分析，克服了现有工具的局限性，并充分利用了新一代云数据仓库的优势。

Abstract: Cloud data warehouses (CDWs) bring large-scale data and compute power closer
to users in enterprises. However, existing tools for analyzing data in CDWs are
either limited in ad-hoc transformations or difficult to use for business
users. Here we introduce Sigma Workbook, a new interactive system that enables
business users to easily perform a visual analysis of data in CDWs at scale.
For this, Sigma Workbook provides an accessible spreadsheet-like interface for
analysis through direct manipulation. Sigma Workbook dynamically constructs
matching SQL queries from user interactions, building on the versatility and
expressivity of SQL. Constructed queries are directly executed on CDWs,
leveraging the superior characteristics of the new generation CDWs, including
scalability. We demonstrate Sigma Workbook through 3 real-life use cases --
cohort analysis, sessionization, and data augmentation -- and underline
Workbook's ease of use, scalability, and expressivity.

</details>


### [40] [Efficient Specialized Spreadsheet Parsing for Data Science](https://arxiv.org/abs/2202.13189)
*Felix Henze,Haralampos Gavriilidis,Eleni Tzirita Zacharatou,Volker Markl*

Main category: cs.DB

TL;DR: 该论文介绍了一种新颖的解析器，用于将电子表格加载到数据科学环境中（例如R），该解析器比现有方法更快且内存使用量更少。


<details>
  <summary>Details</summary>
Motivation: 目前的电子表格加载方法运行时或内存使用量高，阻碍了在普通系统上进行数据探索。

Method: 本文引入了一种通过紧密耦合解压缩和解析来最小化内存使用的新型解析器。此外，通过优化的电子表格专用解析例程和并行化来减少运行时间。

Result: 将Excel电子表格加载到R环境中的原型比最先进的方法快3倍，内存消耗减少40倍。

Conclusion: 该新方法通过显著提高性能和减少内存使用，使得在普通系统上进行电子表格加载变得实用。

Abstract: Spreadsheets are widely used for data exploration. Since spreadsheet systems
have limited capabilities, users often need to load spreadsheets to other data
science environments to perform advanced analytics. However, current approaches
for spreadsheet loading suffer from either high runtime or memory usage, which
hinders data exploration on commodity systems. To make spreasheet loading
practical on commodity systems, we introduce a novel parser that minimizes
memory usage by tightly coupling decompression and parsing. Furthermore, to
reduce the runtime, we introduce optimized spreadsheet-specific parsing
routines and employ parallelism. To evaluate our approach, we implement a
prototype for loading Excel spreadsheets into R environments. Our evaluation
shows that our novel approach is up to 3x faster while consuming up to 40x less
memory than state-of-the-art approaches.

</details>


### [41] [Spread2RML: Constructing Knowledge Graphs by Predicting RML Mappings on Messy Spreadsheets](https://arxiv.org/abs/2110.12829)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: Spread2RML 利用模板和启发式方法预测混乱电子表格的 RML 映射，以减少映射创建时间。


<details>
  <summary>Details</summary>
Motivation: 由于电子表格数据模型复杂且混乱，使用 RDF 映射语言 (RML) 将其映射到 RDF 知识图谱非常耗时。

Method: Spread2RML 通过一套可扩展的 RML 对象映射模板，根据启发式方法为每个列预测 RML 映射。

Result: 在从非常混乱的合成数据到整洁度较低的 data.gov 电子表格的三个数据集上，获得了初步的有希望的结果，尤其是在全自动处理混乱数据方面。

Conclusion: 该方法在自动处理混乱电子表格数据到 RDF 的映射方面显示出前景。

Abstract: The RDF Mapping Language (RML) allows to map semi-structured data to RDF
knowledge graphs. Besides CSV, JSON and XML, this also includes the mapping of
spreadsheet tables. Since spreadsheets have a complex data model and can become
rather messy, their mapping creation tends to be very time consuming. In order
to reduce such efforts, this paper presents Spread2RML which predicts RML
mappings on messy spreadsheets. This is done with an extensible set of RML
object map templates which are applied for each column based on heuristics. In
our evaluation, three datasets are used ranging from very messy synthetic data
to spreadsheets from data.gov which are less messy. We obtained first promising
results especially with regard to our approach being fully automatic and
dealing with rather messy data.

</details>


### [42] [Supercomputing Enabled Deployable Analytics for Disaster Response](https://arxiv.org/abs/2108.11525)
*Kaira Samuel,Jeremy Kepner,Michael Jones,Lauren Milechin,Vijay Gadepally,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Michael Houle,Anna Klein,Victor Lopez,Julie Mullen,Andrew Prout,Albert Reuther,Antonio Rosa,Sid Samsi,Charles Yee,Peter Michaleas*

Main category: cs.DB

TL;DR: 为急救人员提供预计算的地理空间人口普查数据文件（如Google Earth和Excel），以应对网络受限和软件安全要求下的紧急情况分析，该方法已在COVID-19大流行应对中进行测试。


<details>
  <summary>Details</summary>
Motivation: 急救人员和一线工作人员需要高级分析能力，但受限于网络访问和软件安全要求，无法使用标准的基于云的微服务分析平台。他们需要一个离线且能在各种旧硬件上运行的解决方案。

Method: 使用MIT SuperCloud预计算各种分析结果，生成数千个Google Earth (kml) 和 Microsoft Excel (xlsx) 文件。这些文件包含按县分类的每个普查区的人口统计数据（总人口、15岁以下、65岁以上、中位年龄）。Excel界面包含坐标单位转换功能，Google Earth文件探索了多种绝对和相对人口密度颜色地图以提供直观界面。

Result: 成功创建了数千个Google Earth和Microsoft Excel文件，实现了人口普查数据的快速映射。这为紧急响应人员提供了强大的工具，可以在几分钟内使用MIT SuperCloud生成新的分析数据。

Conclusion: 预计算并以用户友好的文件格式（KML、XLSX）分发地理空间人口普查数据，是支持紧急响应人员进行高级分析的有效且强大的方法，尤其适用于资源受限的环境，可提高应急准备水平。

Abstract: First responders and other forward deployed essential workers can benefit
from advanced analytics. Limited network access and software security
requirements prevent the usage of standard cloud based microservice analytic
platforms that are typically used in industry. One solution is to precompute a
wide range of analytics as files that can be used with standard preinstalled
software that does not require network access or additional software and can
run on a wide range of legacy hardware. In response to the COVID-19 pandemic,
this approach was tested for providing geo-spatial census data to allow quick
analysis of demographic data for better responding to emergencies. These data
were processed using the MIT SuperCloud to create several thousand Google Earth
and Microsoft Excel files representative of many advanced analytics. The fast
mapping of census data using Google Earth and Microsoft Excel has the potential
to give emergency responders a powerful tool to improve emergency preparedness.
Our approach displays relevant census data (total population, population under
15, population over 65, median age) per census block, sorted by county, through
a Microsoft Excel spreadsheet (xlsx file) and Google Earth map (kml file). The
spreadsheet interface includes features that allow users to convert between
different longitude and latitude coordinate units. For the Google Earth files,
a variety of absolute and relative colors maps of population density have been
explored to provide an intuitive and meaningful interface. Using several
hundred cores on the MIT SuperCloud, new analytics can be generated in a few
minutes.

</details>


### [43] [Table2Charts: Recommending Charts by Learning Shared Table Representations](https://arxiv.org/abs/2008.11015)
*Mengyu Zhou,Qingtao Li,Xinyi He,Yuejiang Li,Yibo Liu,Wei Ji,Shi Han,Yining Chen,Daxin Jiang,Dongmei Zhang*

Main category: cs.DB

TL;DR: Table2Charts框架通过深度Q学习和启发式搜索，从大规模表格-图表对中学习模式，实现表格到序列的生成，在图表推荐任务中显著优于现有系统，召回率翻倍。


<details>
  <summary>Details</summary>
Motivation: 在实际中推荐常用图表面临效率、数据不平衡和表格上下文等挑战。

Method: 提出Table2Charts框架，利用深度Q学习与复制机制和启发式搜索，进行表格到序列的生成，每个序列遵循一个图表模板，从大规模（表格，图表）对语料库中学习共同模式。

Result: Table2Charts能够学习表格字段的共享表示，使不同图表类型的推荐任务相互增强；在多类型任务中，召回率R@3=0.61和R@1=0.43（召回率翻倍），并在人工评估中优于其他图表推荐系统。

Conclusion: Table2Charts是一个有效且优越的图表推荐系统，能够克服现有挑战并提供更准确的推荐。

Abstract: It is common for people to create different types of charts to explore a
multi-dimensional dataset (table). However, to recommend commonly composed
charts in real world, one should take the challenges of efficiency, imbalanced
data and table context into consideration. In this paper, we propose
Table2Charts framework which learns common patterns from a large corpus of
(table, charts) pairs. Based on deep Q-learning with copying mechanism and
heuristic searching, Table2Charts does table-to-sequence generation, where each
sequence follows a chart template. On a large spreadsheet corpus with 165k
tables and 266k charts, we show that Table2Charts could learn a shared
representation of table fields so that recommendation tasks on different chart
types could mutually enhance each other. Table2Charts outperforms other chart
recommendation systems in both multi-type task (with doubled recall numbers
R@3=0.61 and R@1=0.43) and human evaluations.

</details>


### [44] [Sigma Worksheet: Interactive Construction of OLAP Queries](https://arxiv.org/abs/2012.00697)
*James Gale,Max Seiden,Gretchen Atwood,Jason Frantz,Rob Woollen,Çağatay Demiralp*

Main category: cs.DB

TL;DR: Sigma Worksheet是一个新的交互式系统，它为云数据仓库(CDW)中的数据提供特设的可视化分析。它通过熟悉的电子表格界面让业务用户轻松操作，动态生成高效的SQL查询，并在CDW上执行。评估表明它易于使用、学习，能提高用户生产力，并且查询性能与基准相当。


<details>
  <summary>Details</summary>
Motivation: 新一代云数据仓库虽然提供了大量数据和计算能力，但现有分析工具在特设转换方面受限或难以供业务用户使用，阻碍了用户进行大规模交互式数据分析以改进决策。

Method: Sigma Worksheet提供了一个易于访问的类似电子表格的界面进行数据分析，通过用户交互动态构建匹配的SQL查询，并直接在CDWs上执行。通过两个实际用例（群组分析和会话化）展示其表达能力，使用TPC-H基准测试查询性能，并通过100人调查和70人半结构化访谈评估其在部署中的有用性。

Result: Sigma Worksheet的表达能力通过实际用例得到验证，其生成的SQL查询性能与基准查询相当。用户调查和访谈结果显示，Sigma Worksheet更易于使用和学习，提高了用户的工作效率。研究还表明可以通过提供指导来进一步改善用户体验。

Conclusion: Sigma Worksheet成功地为云数据仓库中的特设可视化分析提供了解决方案，它易于使用和学习，提高了业务用户的工作效率，并且性能良好，未来可以通过提供用户指导来进一步优化用户体验。

Abstract: The new generation of cloud data warehouses (CDWs) brings large amounts of
data and compute power closer to users in enterprises. The ability to directly
access the warehouse data, interactively analyze and explore it at scale can
empower users to improve their decision making cycles. However, existing tools
for analyzing data in CDWs are either limited in ad-hoc transformations or
difficult to use for business users, the largest user segment in enterprises.
Here we introduce Sigma Worksheet, a new interactive system that enables users
to easily perform ad-hoc visual analysis of data in CDWs at scale. For this,
Sigma Worksheet provides an accessible spreadsheet-like interface for data
analysis through direct manipulation. Sigma Worksheet dynamically constructs
matching SQL queries from user interactions on this familiar interface,
building on the versatility and expressivity of SQL. Sigma Worksheet executes
constructed queries directly on CDWs, leveraging the superior characteristics
of the new generation CDWs, including scalability. To evaluate Sigma Worksheet,
we first demonstrate its expressivity through two real life use cases, cohort
analysis and sessionization. We then measure the performance of the Worksheet
generated queries with a set of experiments using the TPC-H benchmark. Results
show the performance of our compiled SQL queries is comparable to that of the
reference queries of the benchmark. Finally, to assess the usefulness of Sigma
Worksheet in deployment, we elicit feedback through a 100-person survey
followed by a semi-structured interview study with 70 participants. We find
that Sigma Worksheet is easier to use and learn, improving the productivity of
users. Our findings also suggest Sigma Worksheet can further improve user
experience by providing guidance to users at various steps of data analysis.

</details>


### [45] [Mapping Spreadsheets to RDF: Supporting Excel in RML](https://arxiv.org/abs/2104.13600)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: 该论文扩展了RML Mapper以支持Microsoft Excel电子表格文件到RDF图的映射，包括访问单元格元数据和实验性功能，并提供在线演示和公开代码。


<details>
  <summary>Details</summary>
Motivation: RML及其现有实现忽略了广泛使用的电子表格格式（Microsoft Excel），尽管它们支持CSV等表格数据。

Method: 作者扩展了RML Mapper以支持Microsoft Excel电子表格文件。他们还提供了一个交互式在线演示和实验性功能。

Result: 扩展后的RML Mapper现在支持Microsoft Excel电子表格文件，允许在典型的RML映射中访问电子表格单元格的各种元数据，并包含一些实验性功能。

Conclusion: 该扩展弥补了RML在电子表格格式支持方面的空白，使其在从Excel文件映射到RDF图方面更加通用。该实现已公开可用。

Abstract: The RDF Mapping Language (RML) enables, among other formats, the mapping of
tabular data as Comma-Separated Values (CSV) files to RDF graphs.
Unfortunately, the widely used spreadsheet format is currently neglected by its
specification and well-known implementations. Therefore, we extended one of the
tools which is RML Mapper to support Microsoft Excel spreadsheet files and
demonstrate its capabilities in an interactive online demo. Our approach allows
to access various meta data of spreadsheet cells in typical RML maps. Some
experimental features for more specific use cases are also provided. The
implementation code is publicly available in a GitHub fork.

</details>


### [46] [Dataset Generation Patterns for Evaluating Knowledge Graph Construction](https://arxiv.org/abs/2104.13576)
*Markus Schröder,Christian Jilek,Andreas Dengel*

Main category: cs.DB

TL;DR: 由于保密性限制，真实标注的企业和个人数据集难以发布，但它们对评估工业场景中的知识图谱构建方法至关重要。本文提出了一种合成数据生成方法，通过发现知识工作者处理数据时的习惯模式来模仿真实数据集。作者从工业领域的真实电子表格中推导出了11种独特模式，并展示了一个名为Data Sprout的生成器，能够重现这些模式以生成逼真的合成数据。


<details>
  <summary>Details</summary>
Motivation: 由于保密性问题，用于评估工业场景下知识图谱构建方法的真实、标注数据集难以公开。为了解决这个问题，需要能够生成尽可能真实的合成数据。

Method: 基于知识工作者在数据生产和管理中存在特定习惯的假设，可以发现数据生成模式。作者从真实的工业电子表格中推导出了11种不同的模式。然后，他们开发了一个名为Data Sprout的生成器，能够重现这些模式。

Result: Data Sprout生成器能够重现这11种派生模式。论文描述了生成器如何总体上生成电子表格以及已实现的模式所产生的改变效果。

Conclusion: 本文成功演示了一种通过识别和重现真实世界电子表格中发现的模式来生成逼真合成数据的方法，从而解决了数据共享中因保密性导致知识图谱评估数据匮乏的问题。

Abstract: Confidentiality hinders the publication of authentic, labeled datasets of
personal and enterprise data, although they could be useful for evaluating
knowledge graph construction approaches in industrial scenarios. Therefore, our
plan is to synthetically generate such data in a way that it appears as
authentic as possible. Based on our assumption that knowledge workers have
certain habits when they produce or manage data, generation patterns could be
discovered which can be utilized by data generators to imitate real datasets.
In this paper, we initially derived 11 distinct patterns found in real
spreadsheets from industry and demonstrate a suitable generator called Data
Sprout that is able to reproduce them. We describe how the generator produces
spreadsheets in general and what altering effects the implemented patterns
have.

</details>


### [47] [Interactively Constructing Knowledge Graphs from Messy User-Generated Spreadsheets](https://arxiv.org/abs/2103.03537)
*Markus Schröder,Christian Jilek,Michael Schulze,Andreas Dengel*

Main category: cs.DB

TL;DR: 该论文提出了一种交互式方法，通过批量注释单元格，将非结构化电子表格数据转换为知识图谱，解决了从混乱电子表格构建知识图谱的挑战。


<details>
  <summary>Details</summary>
Motivation: 电子表格中的自由填写导致内容非结构化，机器难以解释。当数据“混乱”时，将其转换为知识图谱等结构化形式极具挑战性。

Method: 提出了一种交互式方法，包括图形用户界面，使知识工程师能够批量注释电子表格单元格，然后根据注释构建知识图谱。

Result: 使用来自工业场景的五个电子表格，构建了一个包含25k三元组的图谱。与现有最先进的RDF映射语言（RML）方法进行了比较，突出了本方法的贡献。

Conclusion: 本论文提供了一种交互式的、基于GUI的解决方案，用于将混乱的电子表格数据转换为知识图谱，并证明了其相对于现有方法的优势。

Abstract: When spreadsheets are filled freely by knowledge workers, they can contain
rather unstructured content. For humans and especially machines it becomes
difficult to interpret such data properly. Therefore, spreadsheets are often
converted to a more explicit, formal and structured form, for example, to a
knowledge graph. However, if a data maintenance strategy has been missing and
user-generated data becomes "messy", the construction of knowledge graphs will
be a challenging task. In this paper, we catalog several of those challenges
and propose an interactive approach to solve them. Our approach includes a
graphical user interface which enables knowledge engineers to bulk-annotate
spreadsheet cells with extracted information. Based on the cells' annotations a
knowledge graph is ultimately formed. Using five spreadsheets from an
industrial scenario, we built a 25k-triple graph during our evaluation. We
compared our method with the state-of-the-art RDF Mapping Language (RML)
attempt. The comparison highlights contributions of our approach.

</details>


### [48] [Leam: An Interactive System for In-situ Visual Text Analysis](https://arxiv.org/abs/2009.03520)
*Sajjadur Rahman,Peter Griggs,Çağatay Demiralp*

Main category: cs.DB

TL;DR: 该论文提出了Leam系统，一个结合了计算笔记本、电子表格和可视化工具的文本分析系统，旨在解决现有系统在数据异构性、可重用性、可复现性等方面的挑战，并具有交互式用户界面、新的数据模型和富有表现力的代数。


<details>
  <summary>Details</summary>
Motivation: 现有的文本分析系统无法解决数据异构性、溯源、工作流可重用性和可复现性以及与既定实践的兼容性等挑战，而网络上生成的数字文本的规模和可用性正在增加。

Method: 本文提出了Leam系统，它将文本分析过程视为一个单一的连续体，结合了计算笔记本、电子表格和可视化工具的优点。Leam具有一个用于运行文本分析工作流的交互式用户界面、一个用于管理多种原子和复合数据类型的新数据模型，以及一个能够捕获代表文本分析各个阶段的各种操作并实现系统不同组件（包括数据、代码和可视化）之间协调的富有表现力的代数。

Result: 我们报告了Leam开发的当前进展，并通过使用示例展示了它的实用性。

Conclusion: 最后，我们概述了Leam的一些增强功能，并确定了开发交互式可视化文本分析系统的几个研究方向。

Abstract: With the increase in scale and availability of digital text generated on the
web, enterprises such as online retailers and aggregators often use text
analytics to mine and analyze the data to improve their services and products
alike. Text data analysis is an iterative, non-linear process with diverse
workflows spanning multiple stages, from data cleaning to visualization.
Existing text analytics systems usually accommodate a subset of these stages
and often fail to address challenges related to data heterogeneity, provenance,
workflow reusability and reproducibility, and compatibility with established
practices. Based on a set of design considerations we derive from these
challenges, we propose Leam, a system that treats the text analysis process as
a single continuum by combining advantages of computational notebooks,
spreadsheets, and visualization tools. Leam features an interactive user
interface for running text analysis workflows, a new data model for managing
multiple atomic and composite data types, and an expressive algebra that
captures diverse sets of operations representing various stages of text
analysis and enables coordination among different components of the system,
including data, code, and visualizations. We report our current progress in
Leam development while demonstrating its usefulness with usage examples.
Finally, we outline a number of enhancements to Leam and identify several
research directions for developing an interactive visual text analysis system.

</details>


### [49] [ObjTables: structured spreadsheets that promote data quality, reuse, and integration](https://arxiv.org/abs/2005.05227)
*Jonathan R. Karr,Wolfram Liebermeister,Arthur P. Goldberg,John A. P. Sekar,Bilal Shaikh*

Main category: cs.DB

TL;DR: ObjTables是一个工具包，它通过结合电子表格与模式和对象关系映射系统，使电子表格既能被人类理解也能被机器读取，从而促进数据重用和元分析。


<details>
  <summary>Details</summary>
Motivation: 科学中的一个核心挑战是理解系统行为如何从复杂网络中产生，这通常需要聚合、重用和整合异构信息。补充性电子表格是关键数据源，但由于缺乏定义对象、关系和属性的模式，它们通常难以重新分析。

Method: 我们开发了ObjTables，这是一个将电子表格与模式和对象关系映射系统结合起来的工具包。它包含模式格式、指示电子表格和列所代表的类和属性的标记、针对科学信息的多种数据类型，以及用于读写、验证、比较、合并、修订和分析电子表格的高级软件。

Result: ObjTables使电子表格更易于重用，从而能够进行前所未有的二次元分析。通过简化新数据类型新格式及相关软件的构建，ObjTables还可以加速新兴科学领域的发展。

Conclusion: ObjTables通过提供一种结构化的方式来定义数据，提高了电子表格的可重用性和分析能力，从而推动了科学元分析和数据管理。

Abstract: A central challenge in science is to understand how systems behaviors emerge
from complex networks. This often requires aggregating, reusing, and
integrating heterogeneous information. Supplementary spreadsheets to articles
are a key data source. Spreadsheets are popular because they are easy to read
and write. However, spreadsheets are often difficult to reanalyze because they
capture data ad hoc without schemas that define the objects, relationships, and
attributes that they represent. To help researchers reuse and compose
spreadsheets, we developed ObjTables, a toolkit that makes spreadsheets human-
and machine-readable by combining spreadsheets with schemas and an
object-relational mapping system. ObjTables includes a format for schemas;
markup for indicating the class and attribute represented by each spreadsheet
and column; numerous data types for scientific information; and high-level
software for using schemas to read, write, validate, compare, merge, revision,
and analyze spreadsheets. By making spreadsheets easier to reuse, ObjTables
could enable unprecedented secondary meta-analyses. By making it easy to build
new formats and associated software for new types of data, ObjTables can also
accelerate emerging scientific fields.

</details>


### [50] [Needles in the 'Sheet'stack: Augmented Analytics to get Insights from Spreadsheets](https://arxiv.org/abs/2006.08224)
*Medha Atre,Anand Deshpande,Reshma Godse,Pooja Deokar,Sandip Moharir,Dhruva Ray,Akshay Chitlangia,Trupti Phadnis,Yugansh Goyal*

Main category: cs.DB

TL;DR: 该论文提出了一种商业智能（BI）工具，旨在帮助不熟悉数据库模式或缺乏现代分析资源的用户，通过分析定期电子表格来获取洞察，且无需预先了解数据库模式或数据信息。


<details>
  <summary>Details</summary>
Motivation: 现有的商业智能工具在数据库分析方面已取得长足发展并提供即时洞察或可视化查询探索，但本演示旨在解决那些不熟悉数据库模式或没有资源进行当代分析的用户需求，通过检查定期电子表格来提供洞察。

Method: 该方法通过检查不同报告（视图）的定期电子表格来提供洞察，无需事先了解数据库或报告的模式，也无需数据信息。

Result: 此演示展示了一个解决方案，能够无需数据库模式或数据信息的先验知识，即可从定期电子表格中提供洞察。

Conclusion: 该解决方案的目标用户是那些不熟悉数据库模式或没有资源进行现代分析的用户。

Abstract: Business intelligence (BI) tools for database analytics have come a long way
and nowadays also provide ready insights or visual query explorations, e.g.
QuickInsights by Microsoft Power BI, SpotIQ by ThoughtSpot, Zenvisage, etc. In
this demo, we focus on providing insights by examining periodic spreadsheets of
different reports (aka views), without prior knowledge of the schema of the
database or reports, or data information. Such a solution is targeted at users
without the familiarity with the database schema or resources to conduct
analytics in the contemporary way.

</details>


### [51] [A Billion Updates per Second Using 30,000 Hierarchical In-Memory D4M Databases](https://arxiv.org/abs/1902.00846)
*Jeremy Kepner,Vijay Gadepally,Lauren Milechin,Siddharth Samsi,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Micheal Houle,Micheal Jones,Anne Klein,Peter Michaleas,Julie Mullen,Andrew Prout,Antonio Rosa,Charles Yee,Albert Reuther*

Main category: cs.DB

TL;DR: 该论文提出了一种利用D4M库中的关联数组对大规模流网络数据进行高性能更新和分析的方法，通过分层实现，在MIT SuperCloud上达到了每秒19亿次更新的速率。


<details>
  <summary>Details</summary>
Motivation: 分析大规模网络需要对图表示进行高性能流式更新。

Method: 使用结合了电子表格、数据库、矩阵和图属性的关联数组来表示和分析流网络数据。D4M库实现了关联数组，并采用分层实现来优化内存层次结构的性能，以处理大规模关联数组的流式更新。

Result: 在MIT SuperCloud上的1,100个服务器节点上运行34,000个分层D4M关联数组实例，实现了每秒1,900,000,000次更新的持续速率。

Conclusion: 这种能力使得MIT SuperCloud能够分析极其庞大的流网络数据集。

Abstract: Analyzing large scale networks requires high performance streaming updates of
graph representations of these data. Associative arrays are mathematical
objects combining properties of spreadsheets, databases, matrices, and graphs,
and are well-suited for representing and analyzing streaming network data. The
Dynamic Distributed Dimensional Data Model (D4M) library implements associative
arrays in a variety of languages (Python, Julia, and Matlab/Octave) and
provides a lightweight in-memory database. Associative arrays are designed for
block updates. Streaming updates to a large associative array requires a
hierarchical implementation to optimize the performance of the memory
hierarchy. Running 34,000 instances of a hierarchical D4M associative arrays on
1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of
1,900,000,000 updates per second. This capability allows the MIT SuperCloud to
analyze extremely large streaming network data sets.

</details>


### [52] [Towards Semantically Enhanced Data Understanding](https://arxiv.org/abs/1806.04952)
*Markus Schröder,Christian Jilek,Jörn Hees,Andreas Dengel*

Main category: cs.DB

TL;DR: 提出了一种单一的语义模型，将数据与其文档相互关联，以改善数据科学家的数据理解和利用，并支持其他应用。


<details>
  <summary>Details</summary>
Motivation: 在机器学习领域，数据理解需要大量文档，但现有文档与数据分离，导致查找开销大，且其他支持应用难以利用非结构化数据。

Method: 提出了一种使用单一语义模型的方法，该模型将数据与其文档相互关联。

Result: 数据科学家可以直接查找数据相关信息；文档始终引用数据；模型可用于搜索、比较、集成或可视化数据等其他方法。并展示了一个早期原型。

Conclusion: 所提出的语义模型通过将数据和文档互联，提高了数据理解的效率和可用性，对数据科学家和其他应用均有益。

Abstract: In the field of machine learning, data understanding is the practice of
getting initial insights in unknown datasets. Such knowledge-intensive tasks
require a lot of documentation, which is necessary for data scientists to grasp
the meaning of the data. Usually, documentation is separate from the data in
various external documents, diagrams, spreadsheets and tools which causes
considerable look up overhead. Moreover, other supporting applications are not
able to consume and utilize such unstructured data. That is why we propose a
methodology that uses a single semantic model that interlinks data with its
documentation. Hence, data scientists are able to directly look up the
connected information about the data by simply following links. Equally, they
can browse the documentation which always refers to the data. Furthermore, the
model can be used by other approaches providing additional support, like
searching, comparing, integrating or visualizing data. To showcase our approach
we also demonstrate an early prototype.

</details>


### [53] [WebRelate: Integrating Web Data with Spreadsheets using Examples](https://arxiv.org/abs/1711.05787)
*Jeevana Priya Inala,Rishabh Singh*

Main category: cs.DB

TL;DR: WebRelate是一个系统，它使用输入-输出示例将半结构化网络数据与关系数据集成。它将任务分解为URL学习和依赖于输入的网络数据提取。


<details>
  <summary>Details</summary>
Motivation: 网络源与关系数据之间的数据集成面临挑战，因为网站不提供直接的表格数据接口，用户需要编写复杂的脚本来提取数据，而这往往超出他们编程专业知识。

Method: WebRelate通过示例学习字符串转换程序来生成URL（URL学习），并通过示例学习程序来从相应网页中提取数据（输入依赖的网络提取）。它设计了表达性领域特定语言（DSLs）和高效的合成算法。

Result: WebRelate在88个真实世界网络数据集成任务上进行了评估，结果表明，在大多数任务中，WebRelate可以在几秒钟内使用一个示例学习到所需的程序。

Conclusion: WebRelate通过基于示例的学习简化了网络数据集成过程，有效地解决了网络源与关系数据集成中的挑战，使复杂任务对用户更易于操作。

Abstract: Data integration between web sources and relational data is a key challenge
faced by data scientists and spreadsheet users. There are two main challenges
in programmatically joining web data with relational data. First, most websites
do not expose a direct interface to obtain tabular data, so the user needs to
formulate a logic to get to different webpages for each input row in the
relational table. Second, after reaching the desired webpage, the user needs to
write complex scripts to extract the relevant data, which is often conditioned
on the input data. Since many data scientists and end-users come from diverse
backgrounds, writing such complex regular-expression based logical scripts to
perform data integration tasks is unfortunately often beyond their programming
expertise.
  We present WebRelate, a system that allows users to join semi-structured web
data with relational data in spreadsheets using input-output examples.
WebRelate decomposes the web data integration task into two sub-tasks of i) URL
learning and ii) input-dependent web extraction. The first sub-task generates
the URLs for the webpages containing the desired data for all rows in the
relational table. WebRelate achieves this by learning a string transformation
program using a few example URLs. The second sub-task uses examples of desired
data to be extracted from the corresponding webpages and learns a program to
extract the data for the other rows. We design expressive domain-specific
languages for URL generation and web data extraction, and present efficient
synthesis algorithms for learning programs in these DSLs from few input-output
examples. We evaluate WebRelate on 88 real-world web data integration tasks
taken from online help forums and Excel product team, and show that WebRelate
can learn the desired programs within few seconds using only 1 example for the
majority of the tasks.

</details>


### [54] [Towards a Holistic Integration of Spreadsheets with Databases: A Scalable Storage Engine for Presentational Data Management](https://arxiv.org/abs/1708.06712)
*Mangesh Bendre,Vipul Venkataraman,Xinyan Zhou,Kevin Chang,Aditya Parameswaran*

Main category: cs.DB

TL;DR: 该论文提出DataSpread，将电子表格作为前端界面与数据库作为后端数据存储进行集成，旨在为电子表格提供可伸缩性，为数据库提供交互性，并为此开发了一个存储引擎，优化了数据表示和位置访问。


<details>
  <summary>Details</summary>
Motivation: 电子表格缺乏可伸缩性，而数据库系统缺乏交互性。论文旨在通过将两者结合来弥补这些不足。

Method: 论文首先对电子表格的使用进行了广泛调查，以确定存储引擎的功能需求。然后开发了一套灵活表示电子表格数据的机制，并提出了一种有效的方法来识别最优表示。此外，还通过位置访问机制扩展了这些机制，以实现恒定时间访问和修改性能，避免了级联更新问题。

Result: 在典型的电子表格和操作工作负载下，存储空间减少了20%，公式评估时间减少了50%。

Conclusion: 这项工作是实现“演示数据管理”(PDM)愿景的第一步，通过优化数据表示和位置访问，在电子表格与数据库集成中实现了存储和性能的显著改进。

Abstract: Spreadsheet software is the tool of choice for interactive ad-hoc data
management, with adoption by billions of users. However, spreadsheets are not
scalable, unlike database systems. On the other hand, database systems, while
highly scalable, do not support interactivity as a first-class primitive. We
are developing DataSpread, to holistically integrate spreadsheets as a
front-end interface with databases as a back-end datastore, providing
scalability to spreadsheets, and interactivity to databases, an integration we
term presentational data management (PDM). In this paper, we make a first step
towards this vision: developing a storage engine for PDM, studying how to
flexibly represent spreadsheet data within a database and how to support and
maintain access by position. We first conduct an extensive survey of
spreadsheet use to motivate our functional requirements for a storage engine
for PDM. We develop a natural set of mechanisms for flexibly representing
spreadsheet data and demonstrate that identifying the optimal representation is
NP-Hard; however, we develop an efficient approach to identify the optimal
representation from an important and intuitive subclass of representations. We
extend our mechanisms with positional access mechanisms that don't suffer from
cascading update issues, leading to constant time access and modification
performance. We evaluate these representations on a workload of typical
spreadsheets and spreadsheet operations, providing up to 20% reduction in
storage, and up to 50% reduction in formula evaluation time.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [55] [Hillview: A trillion-cell spreadsheet for big data](https://arxiv.org/abs/1907.04827)
*Mihai Budiu,Parikshit Gopalan,Lalith Suresh,Udi Wieder,Han Kruiger,Marcos K. Aguilera*

Main category: cs.DC

TL;DR: Hillview是一个分布式电子表格，通过引入可视化草图（vizketches）来处理非常大的数据集，实现了高效的数据可视化和交互式探索。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理单个机器无法应对的超大规模数据集，数据分析师需要快速、多维度地探索信息并灵活切换可视化。

Method: Hillview引入了可视化草图（vizketches），它结合了数据摘要的算法技术和高效渲染的计算机图形学原理。

Result: Hillview在八台服务器上运行时，能够导航和可视化数百亿行和数万亿单元格的数据集，远超现有竞争系统的能力。

Conclusion: 可视化草图（vizketches）通过并行计算、减少通信、提供渐进式可视化和精确的准确性保证，有效地扩展了电子表格的性能。

Abstract: Hillview is a distributed spreadsheet for browsing very large datasets that
cannot be handled by a single machine. As a spreadsheet, Hillview provides a
high degree of interactivity that permits data analysts to explore information
quickly along many dimensions while switching visualizations on a whim. To
provide the required responsiveness, Hillview introduces visualization
sketches, or vizketches, as a simple idea to produce compact data
visualizations. Vizketches combine algorithmic techniques for data
summarization with computer graphics principles for efficient rendering. While
simple, vizketches are effective at scaling the spreadsheet by parallelizing
computation, reducing communication, providing progressive visualizations, and
offering precise accuracy guarantees. Using Hillview running on eight servers,
we can navigate and visualize datasets of tens of billions of rows and
trillions of cells, much beyond the published capabilities of competing
systems.

</details>


### [56] [Streaming 1.9 Billion Hypersparse Network Updates per Second with D4M](https://arxiv.org/abs/1907.04217)
*Jeremy Kepner,Vijay Gadepally,Lauren Milechin,Siddharth Samsi,William Arcand,David Bestor,William Bergeron,Chansup Byun,Matthew Hubbell,Michael Houle,Michael Jones,Anne Klein,Peter Michaleas,Julie Mullen,Andrew Prout,Antonio Rosa,Charles Yee,Albert Reuther*

Main category: cs.DC

TL;DR: D4M库实现了关联数组以分析网络数据。本文描述了分层关联数组的设计和性能优化，显著降低了内存压力并提高了更新速率，在分布式环境中实现了高吞吐量。


<details>
  <summary>Details</summary>
Motivation: D4M关联数组的流式更新对内存层次结构造成巨大压力，需要减少内存压力并提高更新速率。

Method: 通过设计并实现分层关联数组，优化了内存压力和更新速率，其参数可调。

Result: 单个实例每秒更新超过40,000次。在1,100个服务器节点上，34,000个D4M实例实现了每秒1,900,000,000次的持续更新速率。

Conclusion: 分层关联数组显著提高了D4M库的更新性能和可伸缩性，使其能够分析极大的流式网络数据集。

Abstract: The Dynamic Distributed Dimensional Data Model (D4M) library implements
associative arrays in a variety of languages (Python, Julia, and Matlab/Octave)
and provides a lightweight in-memory database implementation of hypersparse
arrays that are ideal for analyzing many types of network data. D4M relies on
associative arrays which combine properties of spreadsheets, databases,
matrices, graphs, and networks, while providing rigorous mathematical
guarantees, such as linearity. Streaming updates of D4M associative arrays put
enormous pressure on the memory hierarchy. This work describes the design and
performance optimization of an implementation of hierarchical associative
arrays that reduces memory pressure and dramatically increases the update rate
into an associative array. The parameters of hierarchical associative arrays
rely on controlling the number of entries in each level in the hierarchy before
an update is cascaded. The parameters are easily tunable to achieve optimal
performance for a variety of applications. Hierarchical arrays achieve over
40,000 updates per second in a single instance. Scaling to 34,000 instances of
hierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud
achieved a sustained update rate of 1,900,000,000 updates per second. This
capability allows the MIT SuperCloud to analyze extremely large streaming
network data sets.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [57] [TableTalk: Scaffolding Spreadsheet Development with a Language Agent](https://arxiv.org/abs/2502.09787)
*Jenny T. Liang,Aayush Kumar,Yasharth Bajpai,Sumit Gulwani,Vu Le,Chris Parnin,Arjun Radhakrishna,Ashish Tiwari,Emerson Murphy-Hill,Guastavo Soares*

Main category: cs.SE

TL;DR: TableTalk是一种基于大型语言模型的电子表格编程智能体，它通过支架、灵活性和增量性设计原则，帮助用户创建高质量电子表格，显著降低认知负荷并减少思考时间。


<details>
  <summary>Details</summary>
Motivation: 电子表格编程具有挑战性，需要专业的编程知识和问题解决能力。大型语言模型代理在电子表格创建方面显示出潜力。

Method: 本文提出了TableTalk，一个基于七名电子表格程序员和85个Excel模板研究结果的智能体。它遵循支架、灵活性和增量性三个设计原则，通过结构化计划引导程序员，提供三个潜在的下一步，并使用预定义工具逐步构建电子表格。

Result: 在20名程序员的实验中，TableTalk生成的电子表格质量更高（2.3倍更受青睐），并使认知负荷和思考时间减少了12.6%。

Conclusion: 本文提出了代理式电子表格编程工具的设计指南，并讨论了其对电子表格编程、终端用户编程、AI辅助编程以及人机协作的影响。

Abstract: Spreadsheet programming is challenging. Programmers use spreadsheet
programming knowledge (e.g., formulas) and problem-solving skills to combine
actions into complex tasks. Advancements in large language models have
introduced language agents that observe, plan, and perform tasks, showing
promise for spreadsheet creation. We present TableTalk, a spreadsheet
programming agent embodying three design principles -- scaffolding,
flexibility, and incrementality -- derived from studies with seven spreadsheet
programmers and 85 Excel templates. TableTalk guides programmers through
structured plans based on professional workflows, generating three potential
next steps to adapt plans to programmer needs. It uses pre-defined tools to
generate spreadsheet components and incrementally build spreadsheets. In a
study with 20 programmers, TableTalk produced higher-quality spreadsheets 2.3
times more likely to be preferred than the baseline. It reduced cognitive load
and thinking time by 12.6%. From this, we derive design guidelines for agentic
spreadsheet programming tools and discuss implications on spreadsheet
programming, end-user programming, AI-assisted programming, and human-agent
collaboration.

</details>


### [58] [Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE](https://arxiv.org/abs/2506.17330)
*Simon Thorne*

Main category: cs.SE

TL;DR: 本研究引入了一个全面的基准框架（FLARE）来评估大型语言模型（LLMs）在电子表格任务中的表现。结果显示，LLMs在简单任务中表现出色，但在复杂的、多步骤操作中经常失败，突显了将符号推理能力整合到LLM架构中的必要性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在电子表格相关任务中的有效性尚未得到充分探索，尽管它们在其他领域展现出显著能力。

Method: 研究引入了一个用于评估领先LLMs在电子表格函数、公式生成和数据操作任务中表现的综合基准框架（FLARE）。该基准涵盖从基本公式创建到复杂的真实世界电子表格场景的任务。

Result: LLMs在简单的任务中表现出熟练，但在复杂的、多步骤操作中常常失败，经常产生看似合理但实际上不正确的输出。

Conclusion: 当前LLMs在处理需要精确逻辑推理的电子表格任务方面存在局限性，需要将符号推理能力整合到LLM架构中。FLARE被引入以评估LLM在真实世界电子表格逻辑、审计和推理任务中的表现。

Abstract: Large Language Models (LLMs) have demonstrated some significant capabilities
across various domains; however, their effectiveness in spreadsheet related
tasks remains underexplored. This study introduces a foundation for a
comprehensive benchmark framework to evaluate the performance of leading LLMs
in executing spreadsheet functions, formula generation and data manipulation
tasks. The benchmark encompasses tasks ranging from basic formula creation to
complex, real world spreadsheet scenarios. Our findings reveal that while LLMs
exhibit proficiency in straightforward tasks, they often falter in complex,
multi step operations, frequently producing plausible yet incorrect outputs.
These results underscore the limitations of current LLMs in handling
spreadsheet tasks that require precise logical reasoning and highlight the need
for integrating symbolic reasoning capabilities into LLM architectures. To
support this, we introduce FLARE (Formula Logic, Auditing, Reasoning and
Evaluation) a new benchmark for evaluating LLM performance on real-world
spreadsheet logic, auditing, and reasoning tasks.

</details>


### [59] [Overcoming Vision Language Model Challenges in Diagram Understanding: A Proof-of-Concept with XML-Driven Large Language Models Solutions](https://arxiv.org/abs/2502.04389)
*Shue Shiinoki,Ryo Koshihara,Hayato Motegi,Masumi Morishige*

Main category: cs.SE

TL;DR: 该研究提出了一种文本驱动的方法，通过直接从可编辑的源文件（如xlsx、pptx、docx）中提取图表元素作为文本元数据，然后将其输入到大型语言模型（LLM）中进行分析，从而绕过视觉语言模型（VLM）的视觉识别局限性，实现了对图表结构的更准确理解，以回答业务相关问题。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型（VLM）在图像理解任务方面取得了进展，但在识别和提取业务文档中图表的结构和关系方面仍面临挑战。

Method: 本研究提出了一种文本驱动的方法。它不依赖VLM的视觉识别能力，而是利用可编辑的源文件（如xlsx、pptx或docx），将图表元素（如形状、线条、注释）作为文本元数据进行提取。在概念验证中，从xlsx系统设计文档中提取图表信息，并将提取的形状数据转换为文本输入给大型语言模型（LLM），以分析关系并生成业务相关问题的答案。

Result: 与基于VLM的方法相比，所提出的文本驱动框架在需要详细理解图表结构的问题上产生了更准确的答案。该方法不仅限于测试的.xlsx文件，还可以扩展到Office pptx和docx等其他具有源文件的文档中的图表。

Conclusion: 研究结果表明，通过从原始源文件直接进行文本提取来规避VLM的限制是可行的。通过LLM实现强大的图表理解，为现实世界业务场景中提高工作流程效率和信息分析提供了一条有前景的途径。

Abstract: Diagrams play a crucial role in visually conveying complex relationships and
processes within business documentation. Despite recent advances in
Vision-Language Models (VLMs) for various image understanding tasks, accurately
identifying and extracting the structures and relationships depicted in
diagrams continues to pose significant challenges. This study addresses these
challenges by proposing a text-driven approach that bypasses reliance on VLMs'
visual recognition capabilities. Instead, it utilizes the editable source
files--such as xlsx, pptx or docx--where diagram elements (e.g., shapes, lines,
annotations) are preserved as textual metadata. In our proof-of-concept, we
extracted diagram information from xlsx-based system design documents and
transformed the extracted shape data into textual input for Large Language
Models (LLMs). This approach allowed the LLM to analyze relationships and
generate responses to business-oriented questions without the bottleneck of
image-based processing. Experimental comparisons with a VLM-based method
demonstrated that the proposed text-driven framework yielded more accurate
answers for questions requiring detailed comprehension of diagram
structures.The results obtained in this study are not limited to the tested
.xlsx files but can also be extended to diagrams in other documents with source
files, such as Office pptx and docx formats. These findings highlight the
feasibility of circumventing VLM constraints through direct textual extraction
from original source files. By enabling robust diagram understanding through
LLMs, our method offers a promising path toward enhanced workflow efficiency
and information analysis in real-world business scenarios.

</details>


### [60] [On the Workflows and Smells of Leaderboard Operations (LBOps): An Exploratory Study of Foundation Model Leaderboards](https://arxiv.org/abs/2407.04065)
*Zhimin Zhao,Abdul Ali Bangash,Filipe Roseiro Côgo,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文旨在通过分析1045个FM排行榜的运作方式，识别出五种工作流模式和八种排行榜异味，以提高基础模型评估和选择的透明度、问责制和协作性。


<details>
  <summary>Details</summary>
Motivation: 基础模型（如LLMs）在软件工程任务中展现出卓越适应性，导致FM排行榜成为团队选择最佳第三方FM的关键工具。然而，缺乏标准化的评估和比较指南威胁到FM排行榜的透明度，并限制了利益相关者有效选择FM的能力。

Method: 研究收集了来自GitHub、Hugging Face Spaces、Papers With Code、电子表格和独立平台等五个来源的1045个FM排行榜，审查其文档并与排行榜运营者直接沟通。通过卡片分类和协商一致，识别出五种独特的工作流模式，并开发了一个捕捉关键组件及其交互的领域模型。随后，识别出八种独特的排行榜异味（“leaderboard smells”）。

Result: 研究识别了五种独特的工作流模式和一个领域模型，捕捉了这些工作流中的关键组件及其交互。此外，还识别出八种独特的排行榜异味。

Conclusion: 通过减轻这些排行榜异味，软件工程团队可以提高当前LBOps实践的透明度、问责制和协作性，从而为FM的比较和选择建立一个更健壮和负责任的生态系统。

Abstract: Foundation models (FM), such as large language models (LLMs), which are
large-scale machine learning (ML) models, have demonstrated remarkable
adaptability in various downstream software engineering (SE) tasks, such as
code completion, code understanding, and software development. As a result, FM
leaderboards have become essential tools for SE teams to compare and select the
best third-party FMs for their specific products and purposes. However, the
lack of standardized guidelines for FM evaluation and comparison threatens the
transparency of FM leaderboards and limits stakeholders' ability to perform
effective FM selection. As a first step towards addressing this challenge, our
research focuses on understanding how these FM leaderboards operate in
real-world scenarios ("leaderboard operations") and identifying potential
pitfalls and areas for improvement ("leaderboard smells"). In this regard, we
collect up to 1,045 FM leaderboards from five different sources: GitHub,
Hugging Face Spaces, Papers With Code, spreadsheet and independent platform, to
examine their documentation and engage in direct communication with leaderboard
operators to understand their workflows. Through card sorting and negotiated
agreement, we identify five distinct workflow patterns and develop a domain
model that captures the key components and their interactions within these
workflows. We then identify eight unique types of leaderboard smells in LBOps.
By mitigating these smells, SE teams can improve transparency, accountability,
and collaboration in current LBOps practices, fostering a more robust and
responsible ecosystem for FM comparison and selection.

</details>


### [61] [MaxMind: A Memory Loop Network to Enhance Software Productivity based on Large Language Models](https://arxiv.org/abs/2408.03841)
*Yuchen Dong,XiaoXiang Fang,Yuchen Hu,Renshuang Jiang,Zhe Jiang*

Main category: cs.SE

TL;DR: 该论文提出了一种名为MaxMind的模型，通过将外部记忆模型演变为记忆循环网络，并结合知识精度分割的RAG机制，来解决大型语言模型（LLMs）在软件操作和工具生成（SOTG）中忽视实时任务经验和知识价值区分的问题。实验表明，MaxMind能显著提高任务成功率（每轮约3%-6%）和任务执行效率（高达25%）。


<details>
  <summary>Details</summary>
Motivation: 当前关于大型语言模型（LLMs）应用于自动化软件操作和工具生成（SOTG）的研究，忽视了将实时任务经验转化为系统记忆的重要性，以及区分现有知识价值以供未来参考的需求。

Method: 将外部记忆模型演变为记忆循环网络，用于及时记忆和经验参考。通过知识精度分割增强RAG机制，以基于价值区分利用记忆。设计了MaxMind模型用于SOTG，并开发了MaxMind4Sheet电子表格处理系统进行演示。

Result: 任务记忆的积累和循环利用使任务成功率稳步提高，每轮改进率约为3%-6%。记忆循环还能将系统任务执行效率提高多达25%。通过记忆转移，可以解决LLMs在处理专业任务时面临的再训练问题。

Conclusion: MaxMind在增强大型语言模型系统在软件操作和工具生成（SOTG）中的能力和生产力方面具有巨大潜力。

Abstract: The application of large language models to facilitate automated software
operations and tool generation (SOTG), thus augmenting software productivity,
mirrors the early stages of human evolution when the ability to create and use
tools accelerated the progress of civilization. These complex tasks require AI
to continuously summarize and improve. Current research often overlooks the
importance of converting real-time task experiences into system memory and
differentiating the value of existing knowledge for future reference. This
paper addresses these issues by evolving external memory models into
Memory-Loop Networks for timely memorization and experience referencing. We
also enhance a RAG mechanism with knowledge precision segmentation to utilize
memory based on value differentiation, and design the MaxMind model for SOTG
accordingly.To demonstrate our approach, we developed MaxMind4Sheet, an
electronic spreadsheet processing system aligned with the MaxMind philosophy.
Comparative experiments with SheetCopilot have demonstrated that the
accumulation and recycling of task memories lead to a steady enhancement in
task success rate, with an improvement rate of approximately 3%-6% per round in
this implementation example. Note that as the memories continue to grow, this
cumulative improvement may be substantial. The inclusion of memory recycling
can also boost the system's task execution efficiency by up to 25%, and it can
address the retraining issue faced by LLMs when handling specialized tasks
through memories transfer.These suggest that MaxMind has significant potential
to enhance the capabilities and productivity of LLM systems in SOTG.

</details>


### [62] [Will Dynamic Arrays finally change the way Models are built?](https://arxiv.org/abs/2006.14706)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: 电子表格因其直观性而广受欢迎，但也容易出错。本文探讨了在更专业的开发环境中使用Excel的新动态数组，以取代传统技术，从而提高解决方案的完整性并减少错误。


<details>
  <summary>Details</summary>
Motivation: 电子表格虽然直观且广泛使用，但其临时性使其极易出错，这使得人们对其是否适用于严肃的分析和建模任务产生疑问。以前的研究曾尝试使用CSE数组公式提高透明度，但这种方法开发起来很麻烦。

Method: 本文研究了在更专业的开发环境中使用新的动态数组来取代传统技术，以提高解决方案的完整性。

Result: 完全动态的模型需要较少的人工干预来保持更新，因此有可能减少随之而来的错误和风险。

Conclusion: 在专业的电子表格开发环境中采用新的动态数组，可以提高解决方案的完整性，并通过减少人工干预来降低错误和风险。

Abstract: Spreadsheets offer a supremely successful and intuitive means of processing
and exchanging numerical content. Its intuitive ad-hoc nature makes it hugely
popular for use in diverse areas including business and engineering, yet these
very same characteristics make it extraordinarily error-prone; many would
question whether it is suitable for serious analysis or modelling tasks. A
previous EuSpRIG paper examined the role of Names in increasing solution
transparency and providing a readable notation to forge links with the problem
domain. Extensive use was made of CSE array formulas, but it is acknowledged
that their use makes spreadsheet development a distinctly cumbersome task.
Since that time, the new dynamic arrays have been introduced and array
calculation is now the default mode of operation for Excel. This paper examines
the thesis that their adoption within a more professional development
environment could replace traditional techniques where solution integrity is
important. A major advantage of fully dynamic models is that they require less
manual intervention to keep them updated and so have the potential to reduce
the attendant errors and risk.

</details>


### [63] [A Structured Approach to the development of Solutions in Excel](https://arxiv.org/abs/1704.01142)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: 本文探讨了如何通过使用非常规技术，在电子表格中构建结构化的解决方案策略，以解决传统电子表格解决方案缺乏结构导致可扩展性差和错误增多的问题。


<details>
  <summary>Details</summary>
Motivation: 电子表格为非专业用户提供了强大的数字处理平台，但其默认解决方案通常缺乏超越单单元格公式的结构，导致在扩展时错误增多。

Method: 论文考虑使用有争议或鲜为人知的技术，创建一种连贯的解决方案策略，其中问题通过一系列类似于编程语言步骤的公式来解决。

Result: 通过引入结构化元素，旨在使电子表格解决方案能够像传统代码一样实现可扩展性，并减少错误。

Conclusion: 本文提出了一种通过类似编程语言步骤的公式序列解决问题的方法，旨在为电子表格解决方案引入连贯的结构和更好的可扩展性。

Abstract: Spreadsheets offer a supremely successful democratisation platform, placing
the manipulation and presentation of numbers within the grasp of users that
have little or no mathematical expertise or IT experience. What appears to be
almost completely lacking within a "normal" solution built using Excel default
settings is the deployment of any structure that extends beyond a single-cell
formula. The structural elements that allow conventional code to scale without
escalating errors appear to be absent. This paper considers the use of
controversial or lesser-used techniques to create a coherent solution strategy
in which the problem is solved by a sequence of formulas resembling the steps
of a programmed language.

</details>


### [64] [Spreadsheet-based Configuration of Families of Real-Time Specifications](https://arxiv.org/abs/2310.20395)
*José Proença,David Pereira,Giann Spilere Nandi,Sina Borrami,Jonas Melchert*

Main category: cs.SE

TL;DR: 该工作通过利用形式模型和需求的变异性，使用Excel表格配置，并由原型工具自动生成实例和运行模型检查器，从而简化了实时系统变异的模型检查。


<details>
  <summary>Details</summary>
Motivation: 实时系统模型检查复杂，需要权衡细节和状态爆炸。本研究旨在通过利用形式模型和需求的变异性，促进实时规范变体的模型检查，并源于与Alstom公司的合作，在VALU3S欧洲项目中有具体用例。

Method: 利用形式模型和需求的变异性。使用MS Excel电子表格配置形式规范的变异性，并通过原型工具自动处理这些表格以生成实例并运行模型检查器。提议通过对有效特征组合进行分析来扩展先前的工作，同时保持基于电子表格的简单接口。

Result: 该工作促进了实时规范变异的模型检查。开发了一个原型工具，可以从Excel表格自动生成实例并运行模型检查器。该方法允许在保持简单接口的同时，对有效特征组合进行分析。

Conclusion: 本研究提出了一种模型检查可变实时系统的方法和工具，通过直观的电子表格接口进行配置，并扩展了分析有效特征组合的能力，有效解决了状态爆炸的复杂性。

Abstract: Model checking real-time systems is complex, and requires a careful trade-off
between including enough detail to be useful and not too much detail to avoid
state explosion. This work exploits variability of the formal model being
analysed and the requirements being checked, to facilitate the model-checking
of variations of real-time specifications. This work results from the
collaboration between academics and Alstom, a railway company with a concrete
use-case, in the context of the VALU3S European project. The configuration of
the variability of the formal specifications is described in MS Excel
spreadsheets with a particular structure, making it easy to use also by
developers. These spreadsheets are processed automatically by our prototype
tool that generates instances and runs the model checker. We propose the
extension of our previous work by exploiting analysis over valid combination of
features, while preserving the simplicity of a spreadsheet-based interface with
the model checker.

</details>


### [65] [SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models](https://arxiv.org/abs/2305.19308)
*Hongxin Li,Jingran Su,Yuntao Chen,Qing Li,Zhaoxiang Zhang*

Main category: cs.SE

TL;DR: 本文提出了一个名为 SheetCopilot 的代理，它利用大型语言模型（LLMs）和自然语言指令来自动化电子表格任务，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 计算机终端用户在处理表格数据和项目时间表等日常任务上花费了大量时间，这些任务重复且容易出错，但大多数用户缺乏自动化技能。大型语言模型的出现使得用自然语言指导软件成为可能。

Method: 我们提出了 SheetCopilot 代理，它接受自然语言任务并控制电子表格以满足要求。我们定义了一组原子操作作为电子表格软件功能的抽象，并设计了一个基于状态机的任务规划框架，使 LLMs 能够稳健地与电子表格交互。我们还整理了一个包含221个电子表格控制任务的数据集，并建立了全自动评估流程。

Result: SheetCopilot 在单次生成中正确完成了 44.3% 的任务，大幅优于强大的代码生成基线。

Conclusion: SheetCopilot 代理成功利用 LLMs 实现了电子表格任务的自然语言自动化，展示了其在软件控制任务中的强大能力和潜力。

Abstract: Computer end users have spent billions of hours completing daily tasks like
tabular data processing and project timeline scheduling. Most of these tasks
are repetitive and error-prone, yet most end users lack the skill to automate
these burdensome works. With the advent of large language models (LLMs),
directing software with natural language user requests become a reachable goal.
In this work, we propose a SheetCopilot agent that takes natural language task
and control spreadsheet to fulfill the requirements. We propose a set of atomic
actions as an abstraction of spreadsheet software functionalities. We further
design a state machine-based task planning framework for LLMs to robustly
interact with spreadsheets. We curate a representative dataset containing 221
spreadsheet control tasks and establish a fully automated evaluation pipeline
for rigorously benchmarking the ability of LLMs in software control tasks. Our
SheetCopilot correctly completes 44.3\% of tasks for a single generation,
outperforming the strong code generation baseline by a wide margin. Our project
page:https://sheetcopilot.github.io/.

</details>


### [66] [Excel as a Turing-complete Functional Programming Environment](https://arxiv.org/abs/2309.00115)
*Peter Bartholomew*

Main category: cs.SE

TL;DR: Excel的动态数组（2018年升级）正在改变电子表格的构建方式，使其更接近于正式编程，本文探讨了新兴趋势及其对风险的潜在影响。


<details>
  <summary>Details</summary>
Motivation: 展示由于Excel动态数组的升级，传统的电子表格实践如何能被更接近正式编程的新方法所取代。

Method: 本文将讨论Excel社区内先锋工作所产生的一些新兴趋势。

Result: 本文将展示构建电子表格解决方案的全新、截然不同的方法，这些方法更类似于正式编程。一些趋势正在出现，但其采用程度和对风险的影响尚不清楚。

Conclusion: Excel的动态数组正在促使电子表格解决方案向更正式的编程方向转变，本文将讨论新兴趋势和潜在影响，特别是对风险的影响，尽管最终影响尚不明确。

Abstract: Since the calculation engine of Excel was the subject of a major upgrade to
accommodate Dynamic Arrays in 2018 there has been a series of seismic changes
to the art of building spreadsheet solutions. This paper will show the ad-hoc
end user practices of traditional spreadsheets can be replaced by radically
different approaches that have far more in common with formal programming. It
is too early to guess the extent to which the new functionality will be adopted
by the business and engineering communities and the impact that may have upon
risk. Nevertheless, some trends are emerging from pioneering work within the
Excel community which we will discuss here.

</details>


### [67] [A Use Case-Engineering Resources Taxonomy for Analytical Spreadsheet Models](https://arxiv.org/abs/2309.00104)
*Thomas A. Grossman,Vijay Mehrotra*

Main category: cs.SE

TL;DR: 本文提出了一种分析型电子表格模型的九类分类法，考虑了用例和开发资源。它区分了分析解决方案和工业质量模型，有助于理解指导方针、错误、风险以及电子表格随时间的变化。


<details>
  <summary>Details</summary>
Motivation: 本文旨在扩展先前的分类法，识别九种类型的电子表格模型，并连接不同的研究文献，以区分“分析解决方案”和“工业级分析电子表格模型”。

Method: 本文扩展了之前的三类分类法，识别了九种电子表格模型。它探讨了每种模型的性质，提出了定义，将其与文献关联起来，并假设了它们可能产生的方式。

Result: 本文识别了文献中出现的九种类型的电子表格模型。该分类法有助于识别最有效的电子表格开发指南，为审视电子表格错误和风险提供了视角，并为理解电子表格如何随时间变化提供了结构。

Conclusion: 该分类法为许多有趣的研究问题打开了大门，包括其自身的完善。

Abstract: This paper presents a taxonomy for analytical spreadsheet models. It
considers both the use case that a spreadsheet is meant to serve, and the
engineering resources devoted to its development. We extend a previous
three-type taxonomy, to identify nine types of spreadsheet models, that
encompass the many analytical spreadsheet models seen in the literature. We
connect disparate research literature to distinguish between an "analytical
solution" and an "industrial-quality analytical spreadsheet model". We explore
the nature of each of the nine types, propose definitions for some, relate them
to the literature, and hypothesize on how they might arise. The taxonomy aids
in identifying where various spreadsheet development guidelines are most
useful, provides a lens for viewing spreadsheet errors and risk, and offers a
structure for understanding how spreadsheets change over time. This taxonomy
opens the door to many interesting research questions, including refinements to
itself.

</details>


### [68] [Experimenting with ChatGPT for Spreadsheet Formula Generation: Evidence of Risk in AI Generated Spreadsheets](https://arxiv.org/abs/2309.00095)
*Simon Thorne*

Main category: cs.SE

TL;DR: 本文探讨了大型语言模型（LLMs），特别是ChatGPT，在通过解释英文句子生成电子表格公式方面的能力和局限性。它展示了在特定条件下ChatGPT的准确性，但也揭示了在信息有限或问题复杂时其推理能力和准确性的下降。


<details>
  <summary>Details</summary>
Motivation: 探索ChatGPT在需要推断、推理和解决问题的场景中，生成有效电子表格公式和相关计算输出的能力。

Method: 通过ChatGPT进行了一系列实验。

Result: 在某些情况下，ChatGPT可以生成正确的电子表格公式并提供正确的推理、演绎和推断。然而，当信息有限、不确定或问题过于复杂时，ChatGPT的准确性及其推理、推断和演绎能力会下降，甚至可能导致虚假陈述和“幻觉”。

Conclusion: ChatGPT在特定条件下能有效地生成电子表格公式，但面临信息不足或复杂问题时，其性能会显著下降，并可能产生错误信息。

Abstract: Large Language Models (LLM) have become sophisticated enough that complex
computer programs can be created through interpretation of plain English
sentences and implemented in a variety of modern languages such as Python, Java
Script, C++ and Spreadsheets. These tools are powerful and relatively accurate
and therefore provide broad access to computer programming regardless of the
background or knowledge of the individual using them. This paper presents a
series of experiments with ChatGPT to explore the tool's ability to produce
valid spreadsheet formulae and related computational outputs in situations
where ChatGPT has to deduce, infer and problem solve the answer. The results
show that in certain circumstances, ChatGPT can produce correct spreadsheet
formulae with correct reasoning, deduction and inference. However, when
information is limited, uncertain or the problem is too complex, the accuracy
of ChatGPT breaks down as does its ability to reason, infer and deduce. This
can also result in false statements and "hallucinations" that all subvert the
process of creating spreadsheet formulae.

</details>


### [69] [Demonstration of CORNET: A System For Learning Spreadsheet Formatting Rules By Example](https://arxiv.org/abs/2308.07357)
*Mukul Singh,Jose Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Gust Verbruggen*

Main category: cs.SE

TL;DR: CORNET是一个系统，它能根据用户提供的示例，自动学习并生成电子表格的条件格式规则。


<details>
  <summary>Details</summary>
Motivation: 用户需要手动编写条件格式规则，这既耗时又复杂。

Method: CORNET结合了归纳式程序合成、基于半监督聚类和迭代决策树学习的符号规则枚举，以及一个神经网络排序器。

Result: CORNET作为Microsoft Excel的插件，用户提供一两个格式化单元格作为示例后，CORNET能生成格式规则建议。

Conclusion: CORNET系统能够根据用户示例自动学习并生成准确的条件格式规则，从而简化了电子表格的数据管理和分析任务。

Abstract: Data management and analysis tasks are often carried out using spreadsheet
software. A popular feature in most spreadsheet platforms is the ability to
define data-dependent formatting rules. These rules can express actions such as
"color red all entries in a column that are negative" or "bold all rows not
containing error or failure." Unfortunately, users who want to exercise this
functionality need to manually write these conditional formatting (CF) rules.
We introduce CORNET, a system that automatically learns such conditional
formatting rules from user examples. CORNET takes inspiration from inductive
program synthesis and combines symbolic rule enumeration, based on
semi-supervised clustering and iterative decision tree learning, with a neural
ranker to produce accurate conditional formatting rules. In this demonstration,
we show CORNET in action as a simple add-in to Microsoft Excel. After the user
provides one or two formatted cells as examples, CORNET generates formatting
rule suggestions for the user to apply to the spreadsheet.

</details>


### [70] [Excel Spreadsheet Analyzer](https://arxiv.org/abs/2211.06333)
*Amir Nassereldine,Patrick Chen,Jinjun Xiong*

Main category: cs.SE

TL;DR: 该论文提出了一种工具，用于将电子表格转换为科学编程语言（如 Python），同时保留单元格之间的依赖关系，从而实现在 Python 中进行数据分析。


<details>
  <summary>Details</summary>
Motivation: 电子表格在数值分析中广泛使用，但数据科学家正转向使用 Python 等科学编程语言。在将电子表格数据导入 Python 时，会丢失公式和单元格依赖等关键信息。

Method: 提出了一种创建电子表格抽象中间表示（AIR）的工具。这种 AIR 有助于将电子表格数据传输到科学编程语言，同时保留数据间的依赖关系。此外，还在该工具之上构建了一个用于在 Python 中进行数据分析的 Python 库。

Result: 该工具创建了电子表格的抽象中间表示（AIR），能够将电子表格数据转移到 Python 中，同时保留了单元格的相互依赖信息。在此基础上构建的 Python 库使得能够在 Python 中执行数据分析。

Conclusion: 通过创建抽象中间表示（AIR），该工具及其配套的 Python 库弥补了电子表格与科学编程语言之间的鸿沟，使得在 Python 中进行数据分析成为可能，同时保留了关键的电子表格信息。

Abstract: Spreadsheets are widely used in various fields to do large numerical
analysis. While several companies have relied on spreadsheets for decades, data
scientists are going in the direction of using scientific programming languages
such as python to do their data analysis due to the support, community, and
vast amount of libraries. While using python to analyze a company's
spreadsheets, some information such as the formulas and dependencies of a cell
are lost. We propose a tool that creates an abstract intermediate
representation (AIR) of a spreadsheet. This representation facilitates the
transfer from spreadsheets into scientific programming languages while
preserving inter-dependency information about data. In addition to that, we
build a python library on top of our tool to perform some data analysis in
python.

</details>


### [71] [Exploring Spreadsheet Use and Practices in a Technologically Constrained Setting](https://arxiv.org/abs/2201.07696)
*Khwima Mckinley Mkamanga,Simon Thorne*

Main category: cs.SE

TL;DR: 本文探讨了电子表格在马拉维一家水务公司业务运营中的影响，指出电子表格在实现业务自动化方面发挥了作用，但同时也带来了高风险，需要改进管理、技术和人为因素方面的政策。


<details>
  <summary>Details</summary>
Motivation: 旨在探讨电子表格对马拉维一家水务公司业务运营的影响，并通过影响管理电子表格潜在风险的新方法来帮助定义未来的电子表格使用。

Method: 研究重点是该水务公司内部电子表格的使用范围和生命周期，以及组织政策和治理。

Result: 研究发现，电子表格在该组织中普及促进了业务自动化，但也因管理、技术和人为因素问题导致了高风险。

Conclusion: 研究结论是，在许多方面，例如实施管理电子表格开发流程和采用的综合政策和法规方面，仍有很大的改进空间。

Abstract: This paper explores the impacts of spreadsheets on business operations in a
water utility parastatal in Malawi, Sub-Saharan Africa. The organisation is a
typical example of a semi-government body operating in a technologically
underdeveloped country. The study focused on spreadsheet scope of use and life
cycle as well as organisational policy and governance. The results will help
define future spreadsheet usage by influencing new approaches for managing
potential risks associated with spreadsheets in the organization. Generally,
findings indicate that the proliferation of spreadsheets in the organization
has provided an enabling environment for business automation. The paper also
highlights management, technological and human factor issues contributing to
high risks associated with the pervasive spreadsheet use. The conclusions drawn
from the research confirms that there is ample room for improvement in many
areas such as implementation of comprehensive policies and regulations
governing spreadsheet development processes and adoption.

</details>


### [72] [Methodology for Assessing the State of the Practice for Domain X](https://arxiv.org/abs/2110.11575)
*Spencer Smith,Jacques Carette,Peter Michalski,Ao Dong,Olu Owojaiye*

Main category: cs.SE

TL;DR: 该论文提出了一种评估研究软件领域软件开发实践现状的方法论，通过分析工件、工具、原则、痛点和改进措施来理解当前实践。


<details>
  <summary>Details</summary>
Motivation: 为了改进研究软件的开发方法和工具，首先需要了解当前的实践现状。

Method: 该方法论包括以下步骤：确定领域、识别并筛选约30个软件包、收集源代码和文档、收集仓库数据（如星级、问题数、代码行数）、填写包含108个问题的测量模板、访谈开发者（20个问题）、使用层次分析法（AHP）对软件进行排名，并分析数据以回答关键问题。整个过程需要领域专家参与，预计耗时173人时。

Result: 论文描述了一种评估研究软件开发实践状态的综合方法论，包括测量模板和AHP工具，并估计了完成一次评估所需的时间成本。

Conclusion: 所提出的方法论提供了一种系统化的方式来评估研究软件领域的开发实践现状，从而为未来的改进提供依据。

Abstract: To improve software development methods and tools for research software, we
first need to understand the current state of the practice. Therefore, we have
developed a methodology for assessing the state of the software development
practices for a given research software domain. For each domain we wish to
answer questions such as: i) What artifacts (documents, code, test cases, etc.)
are present? ii) What tools are used? iii) What principles, process and
methodologies are used? iv) What are the pain points for developers? v) What
actions are used to improve qualities like maintainability and reproducibility?
To answer these questions, our methodology prescribes the following steps: i)
Identify the domain; ii) Identify a list of candidate software packages; iii)
Filter the list to a length of about 30 packages; iv) Gather source code and
documentation for each package; v) Collect repository related data on each
software package, like number of stars, number of open issues, number of lines
of code; vi) Fill in the measurement template (the template consists of 108
questions to assess 9 qualities (including the qualities of installability,
usability and visibility)); vii) Interview developers (the interview consists
of 20 questions and takes about an hour); viii) Rank the software using the
Analytic Hierarchy Process (AHP); and, ix) Analyze the data to answer the
questions posed above. A domain expert should be engaged throughout the
process, to ensure that implicit information about the domain is properly
represented and to assist with conducting an analysis of the commonalities and
variabilities between the 30 selected packages. Using our methodology,
spreadsheet templates and AHP tool, we estimate (based on our experience with
using the process) the time to complete an assessment for a given domain at 173
person hours.

</details>


### [73] [Agile Elicitation of Scalability Requirements for Open Systems: A Case Study](https://arxiv.org/abs/2108.00567)
*Gunnar Brataas,Antonio Martini,Geir Kjetil Hanssen,Georg Ræder*

Main category: cs.SE

TL;DR: 该论文提出了ScrumScale模型，一个基于电子表格的轻量级工具，用于在敏捷软件开发中系统地获取可伸缩性需求。通过开放银行案例研究验证，该模型对TietoEVRY公司而言非常有效。


<details>
  <summary>Details</summary>
Motivation: 敏捷软件开发中获取可伸缩性需求过程复杂且在现有研究中描述不足。开放银行案例研究中，传统银行系统开放后面临的可伸缩性挑战也凸显了这一需求。

Method: 本研究采用设计科学方法，阐明了ScrumScale模型（一个简单的电子表格）背后的可伸缩性概念，并利用了协调理论。通过TietoEVRY公司的开放银行项目进行了案例研究。

Result: 在TietoEVRY公司的开放银行案例研究中，利益相关者花费55小时获取需求。TietoEVRY认为ScrumScale模型提供了一种系统化的方法来生成可伸缩性需求，并且在与其他利益相关者的对话中也带来了显著优势。

Conclusion: ScrumScale模型是一种有效且系统化的工具，用于在敏捷软件开发（尤其是在开放银行等复杂场景中）中获取可伸缩性需求，并能改善利益相关者之间的沟通。

Abstract: Eliciting scalability requirements during agile software development is
complicated and poorly described in previous research. This article presents a
lightweight artifact for eliciting scalability requirements during agile
software development: the ScrumScale model. The ScrumScale model is a simple
spreadsheet. The scalability concepts underlying the ScrumScale model are
clarified in this design science research, which also utilizes coordination
theory. This paper describes the open banking case study, where a legacy
banking system becomes open. This challenges the scalability of this legacy
system. The first step in understanding this challenge is to elicit the new
scalability requirements. In the open banking case study, key stakeholders from
TietoEVRY spent 55 hours eliciting TietoEVRY's open banking project's
scalability requirements. According to TietoEVRY, the ScrumScale model provided
a systematic way of producing scalability requirements. For TietoEVRY, the
scalability concepts behind the ScrumScale model also offered significant
advantages in dialogues with other stakeholders.

</details>


### [74] [SpreadsheetCoder: Formula Prediction from Semi-structured Context](https://arxiv.org/abs/2106.15339)
*Xinyun Chen,Petros Maniatis,Rishabh Singh,Charles Sutton,Hanjun Dai,Max Lin,Denny Zhou*

Main category: cs.SE

TL;DR: SpreadsheetCoder是一种基于BERT的模型，它利用表格上下文（包括表头和半结构化表格数据）来预测电子表格公式，显著提高了预测准确性并能辅助更多用户。


<details>
  <summary>Details</summary>
Motivation: 以往的电子表格公式预测方法没有充分利用真实的表格上下文，例如表格结构和表头信息，导致预测能力受限。

Method: 提出了SpreadsheetCoder，一个基于BERT的模型架构，以行和列两种格式表示表格上下文。模型在一个大型电子表格数据集上进行训练。

Result: SpreadsheetCoder实现了42.51%的top-1预测准确率，相比不使用丰富表格上下文的基线模型有显著提升。与基于规则的系统相比，SpreadsheetCoder在Google Sheets上辅助用户编写公式的比例提高了82%。

Conclusion: 通过利用表格上下文，SpreadsheetCoder显著改进了电子表格公式的预测，为用户编写公式提供了更有效的辅助。

Abstract: Spreadsheet formula prediction has been an important program synthesis
problem with many real-world applications. Previous works typically utilize
input-output examples as the specification for spreadsheet formula synthesis,
where each input-output pair simulates a separate row in the spreadsheet.
However, this formulation does not fully capture the rich context in real-world
spreadsheets. First, spreadsheet data entries are organized as tables, thus
rows and columns are not necessarily independent from each other. In addition,
many spreadsheet tables include headers, which provide high-level descriptions
of the cell data. However, previous synthesis approaches do not consider
headers as part of the specification. In this work, we present the first
approach for synthesizing spreadsheet formulas from tabular context, which
includes both headers and semi-structured tabular data. In particular, we
propose SpreadsheetCoder, a BERT-based model architecture to represent the
tabular context in both row-based and column-based formats. We train our model
on a large dataset of spreadsheets, and demonstrate that SpreadsheetCoder
achieves top-1 prediction accuracy of 42.51%, which is a considerable
improvement over baselines that do not employ rich tabular context. Compared to
the rule-based system, SpreadsheetCoder assists 82% more users in composing
formulas on Google Sheets.

</details>


### [75] [From webtables to datatables](https://arxiv.org/abs/2006.14694)
*Mária Csernoch*

Main category: cs.SE

TL;DR: 本文详述了LOL论坛表格的转换过程，并提出了两种解决方案：一种在文字处理器中，另一种在电子表格应用中。


<details>
  <summary>Details</summary>
Motivation: Webtables是教授电子表格技能、解决实际问题和培养计算思维能力的绝佳资源，有助于发展计算思维技能。

Method: 详述了LOL论坛（英雄联盟）表格的转换过程算法，并提供了两种解决方案：一种在文字处理器中实现，另一种纯粹在电子表格应用程序中实现。

Result: 提供了在文字处理器和电子表格应用程序中的两种LOL论坛表格转换解决方案。

Conclusion: 为进一步的讨论、发明其他解决方案和组合它们提供了空间。

Abstract: Webtables -- tables and table-like structures on webpages -- are excellent
sources for teaching spreadsheeting, in commercial and professional
organisations by utilizing and developing knowledge-transfer items, presenting
and handling various real-world problems and solutions, discussing and
debugging, and in general, developing and utilizing computational thinking
skills. In the present paper the conversion process of one of the LOL Boards
(League of Legends, Riot Games Inc. 2019) is detailed. After presenting the
algorithm of the conversion, two solutions are offered -- one in a word
processor, the other purely in a spreadsheet application -- leaving space for
discussions, inventing other solutions and combining them.

</details>


### [76] [Implementation Strategies for Multidimensional Spreadsheets](https://arxiv.org/abs/2006.05814)
*Paul Mireault*

Main category: cs.SE

TL;DR: 分析了经验丰富的Excel开发者在二维Excel环境中实现多维变量的不同策略。


<details>
  <summary>Details</summary>
Motivation: 了解Excel开发者如何在二维环境中处理多维数据，并识别不同的实现策略。

Method: 邀请经验丰富的Excel开发者参与一个实现多维变量电子表格的挑战，并分析他们的电子表格。

Result: 识别出两种策略：多数参与者使用三维或四维变量在Excel二维平面上的投影；少数参与者使用数据库方法，将多维变量表示为带主键的数据集表，这种方法能带来更简单的公式。

Conclusion: 在Excel中处理多维变量时，数据库方法能导致更简单的公式。

Abstract: Seasoned Excel developers were invited to participate in a challenge to
implement a spreadsheet with multi-dimensional variables. We analyzed their
spreadsheet to see the different implement strategies employed. We identified
two strategies: most participants used a projection of three or
four-dimensional variables on the two-dimensional plane used by Excel. A few
participants used a database approach where the multi-dimensional variables are
presented in the form of a dataset table with the appropriate primary key. This
approach leads to simpler formulas.

</details>


### [77] [Abstracting spreadsheet data flow through hypergraph redrawing](https://arxiv.org/abs/2006.04794)
*David Birch,Nicolai Stawinoga,Jack Binks,Bruno Nicoletti,Paul Kelly*

Main category: cs.SE

TL;DR: 该论文提出通过重新定义单元格的边界来提高电子表格的抽象级别，将单元格表示为超图边，以减少错误并更好地反映最终用户的思维模型。


<details>
  <summary>Details</summary>
Motivation: 传统电子表格因抽象级别低而容易出错，用户被迫从低级单元格构建数据模型，且链接隐藏。

Method: 将电子表格转换为细粒度图，操作符和值作为节点。通过在操作符/数据节点集周围绘制边界“墙”，将“单元格”表示为超图边。通过公共子表达式识别和子树同构检测向量（数组）操作来阐述此方法。

Result: 论文提出了一种通过重新定义单元格边界来提高电子表格抽象级别的方法，并通过公共子表达式识别和应用子树同构检测向量操作来阐述了这一方法。

Conclusion: 研究人员应寻求重新绘制单元格边界的技术，以创建更高层次的“单元格”，更真实地代表最终用户的真实世界/思维模型，从而减少错误。

Abstract: We believe the error prone nature of traditional spreadsheets is due to their
low level of abstraction. End user programmers are forced to construct their
data models from low level cells which we define as "a data container or
manipulator linked by user-intent to model their world and positioned to
reflect its structure". Spreadsheet cells are limited in what they may contain
(scalar values) and the links between them are inherently hidden. This paper
proposes a method of raising the level of abstraction of spreadsheets by
"redrawing the boundary" of the cell. To expose the hidden linkage structure we
transform spreadsheets into fine-grained graphs with operators and values as
nodes. "cells" are then represented as hypergraph edges by drawing a boundary
"wall" around a set of operator/data nodes. To extend what cells may contain
and to create a higher level model of the spreadsheet we propose that
researchers should seek techniques to redraw these boundaries to create higher
level "cells" which will more faithfully represent the end-user's real
world/mental model. We illustrate this approach via common sub-expression
identification and the application of sub-tree isomorphisms for the detection
of vector (array) operations.

</details>


### [78] [Comprehensive review for common types of errors using spreadsheets](https://arxiv.org/abs/1912.09209)
*Ali Aburas*

Main category: cs.SE

TL;DR: 电子表格被广泛使用但容易出错。本文综述了用于预防、检测和纠正电子表格错误的方法，对最新研究进行了分类，并识别了最终用户常犯的错误。


<details>
  <summary>Details</summary>
Motivation: 电子表格在各种组织中被广泛使用，但其中包含大量错误，因此研究人员开发了旨在预防、检测和纠正电子表格错误的工具。

Method: 本文采用综合综述的方法，描述和分类了在电子表格中发现和修复错误的方法。它讨论了最新研究方法的定义、工作原理以及它们在电子表格中可以发现的错误类型。本文还探讨了最终用户在电子表格中常犯的错误类型。

Result: 本文讨论并分类了用于发现和修复电子表格错误的最新研究方法，详细介绍了它们的定义、工作原理和可以识别的错误类型。它还识别了最终用户常犯的错误。

Conclusion: 本文对发现和修复电子表格错误的方法进行了全面综述和分类，并分析了最终用户常犯的错误。

Abstract: Thanks to their flexibility and capability to perform different tasks and
organize data in the best form and format, spreadsheets are widely used in
different organizations and by different end users. Many business organizations
rely on spreadsheets to fulfill their various tasks. On the other hand, the
number of spreadsheets that contain errors are very high, thus researchers have
developed different tools aimed at the prevention, detection, and correction of
errors in spreadsheets. This research work is a comprehensive review that
describes and classifies approaches on finding and fixing errors in
spreadsheets. The paper discusses up-to-date research work approaches in terms
of definition, how they work, and kinds of errors they can find in
spreadsheets. The paper looks also for the kinds of errors that end users
commonly make in spreadsheets.

</details>


### [79] [A Coding-free Software Framework of Developing Web Data Management Systems](https://arxiv.org/abs/1910.05685)
*Can Yang,Shiying Pan,Runmin Li,Yu Liu,Lizhang Peng*

Main category: cs.SE

TL;DR: 本文提出了一种基于SaaS架构的无代码方法和平台，用于非程序员构建数据管理系统。


<details>
  <summary>Details</summary>
Motivation: 越来越多的企业希望在云端部署数据管理系统，但非程序员开发此类系统仍面临困难。SaaS的发展为无代码开发带来了更多可能性。

Method: 本文提出了一套无代码构建数据管理系统的理论和方法，包括一个实际应用平台、一套构建方法和一套数据交换接口。通过抽象数据管理系统的共性特征，设计了一个通用Web平台来快速生成和发布定制的系统实例。此外，还提出了一种使用特定需求表（在电子表格中）开发数据管理系统的方法，平台通过解析表格模型并在运行阶段实现目标系统来将需求表映射为系统实例。

Result: 作者实现了所提出的框架并将其部署在Web上。

Conclusion: 实证结果证明了该无代码方法在开发Web数据管理系统中的可行性和可用性。

Abstract: More and more enterprises recently intend to deploy data management systems
in the cloud. Due to the professionalism of software development, it has still
been difficult for non-programmers to develop this kind of systems, even a
small one. However, the development of SaaS brings forth the more feasibility
of coding-free software development than before. Based on the SaaS
architecture, this paper presents a set of theory and method for coding-free
construction of a data management system, on which our contributions involve in
a practical application platform, a set of construction method and a set of
interface on data exchange. By abstracting the common features of data
management systems, we design a universal web platform to quickly generate and
publish customized system instances. Moreover, we propose a kind of method to
develop a data management system using a specific requirements table in
spreadsheet. The corresponding platform maps the requirements table into a
system instance through parsing the table model and implementing the objective
system in the running stage. Finally, we implement the proposed framework and
deploy it on web. The empirical result demonstrates the feasibility and
availability of the coding-free method in developing web data management
systems.

</details>


### [80] [Structured Spreadsheet Modelling and Implementation with Multiple Dimensions -- Part 2: Implementation](https://arxiv.org/abs/1909.00891)
*Paul Mireault*

Main category: cs.SE

TL;DR: 本文提供了实现多维问题（如产品、区域、行业和月份）的精确步骤，以生成易于维护的电子表格。这是对第一部分中提出的概念模型的延续。


<details>
  <summary>Details</summary>
Motivation: 本文旨在提供一个实用的方法来解决多维问题，这些问题在第一部分中已通过公式图和公式列表进行了概念建模。主要目标是确保实现后的电子表格易于维护。

Method: 本文提出的方法是提供实现多维问题的精确步骤。

Result: 通过遵循本文提供的步骤，可以生成一个易于维护的电子表格，用于处理多维问题。

Conclusion: 本文总结了将多维问题从概念模型转化为可维护的电子表格实现的实用指南。

Abstract: In Part 1, we showed how to develop a conceptual model of a problem involving
variables of multiple dimensions, like Products, Regions, Sectors and Months.
The conceptual model is presented as a Formula Diagram, giving a global view of
the interaction between all the variables, and a Formula List, giving a precise
view of the interaction between the variables. In this paper, we present
precise steps to implement a multi-dimensional problem in a way that will
produce a spreadsheet that is easy to maintain

</details>


### [81] [On the Refinement of Spreadsheet Smells by means of Structure Information](https://arxiv.org/abs/1810.04542)
*Patrick Koch,Birgit Hofer,Franz Wotawa*

Main category: cs.SE

TL;DR: 该论文通过利用推断出的结构信息，改进了电子表格异味检测方法，减少了错误和冗余报告，并引入了新的检测技术。


<details>
  <summary>Details</summary>
Motivation: 电子表格用户通常意识不到糟糕设计带来的风险。现有的异味检测技术存在缺陷，导致错误或冗余的报告，这可能会让用户不堪重负。

Method: 提出通过利用推断的结构信息来改进电子表格异味检测。具体方法包括：首先，提供静态分析方法来推断相关单元格的簇和块；其次，通过三个示例改进现有异味检测，这些改进结合了单元格组和计算块的信息；最后，提出了三种利用推断出的电子表格结构的新型异味检测技术。

Result: 经验评估表明，这些改进成功减少了错误和冗余报告的异味数量，并且新引入的异味揭示了新的缺陷。

Conclusion: 通过利用推断出的结构信息，本文提出的方法（包括改进现有异味检测和引入新型异味检测）提高了电子表格异味检测的准确性和全面性。

Abstract: Spreadsheet users are often unaware of the risks imposed by poorly designed
spreadsheets. One way to assess spreadsheet quality is to detect smells which
attempt to identify parts of spreadsheets that are hard to comprehend or
maintain and which are more likely to be the root source of bugs.
Unfortunately, current spreadsheet smell detection techniques suffer from a
number of drawbacks that lead to incorrect or redundant smell reports. For
example, the same quality issue is often reported for every copy of a cell,
which may overwhelm users. To deal with these issues, we propose to refine
spreadsheet smells by exploiting inferred structural information for smell
detection. We therefore first provide a detailed description of our static
analysis approach to infer clusters and blocks of related cells. We then
elaborate on how to improve existing smells by providing three example
refinements of existing smells that incorporate information about cell groups
and computation blocks. Furthermore, we propose three novel smell detection
techniques that make use of the inferred spreadsheet structures. Empirical
evaluation of the proposed techniques suggests that the refinements
successfully reduce the number of incorrectly and redundantly reported smells,
and novel deficits are revealed by the newly introduced smells.

</details>


### [82] [Now You're Thinking With Structures: A Concept for Structure-based Interactions with Spreadsheets](https://arxiv.org/abs/1809.03435)
*Patrick Koch*

Main category: cs.SE

TL;DR: 通过结构感知的方法提升电子表格的理解和交互，从而提高效率和质量。


<details>
  <summary>Details</summary>
Motivation: 复杂的电子表格难以理解和修改；需要更高阶的心智模型来认知复杂系统。

Method: 提出一种结构感知的电子表格理解和交互概念，基于结构推断，通过结构信息丰富可视化、增强用户操作、以及主动修改电子表格整体结构而非单个单元格。目前已实现结构推断和可视化工具。

Result: 意图系统将作为附加功能层增强现有电子表格工具。已实现结构推断和可视化工具，并计划未来引入主动和反应式交互机制，提供结构感知功能作为插件。

Conclusion: 提供结构感知的电子表格思考和交互工具将提高用户生产力和电子表格整体质量。

Abstract: Spreadsheets are the go-to tool for computerized calculation and modelling,
but are hard to comprehend and adapt after reaching a certain complexity. In
general, cognition of complex systems is facilitated by having a higher order
mental model of the system in question to work with. We therefore present a
concept for structure-aware understanding of and interaction with spreadsheets
that extends previous work on structure inference in the domain. Following this
concept, structural information is used to enrich visualizations, reactively
enhance traditional user actions, and provide tools to proactively alter the
overall spreadsheet makeup instead of individual cells The intended systems
should, in first approximation, not replace common spreadsheet tools, but
provide an additional layer of functionality alongside the established
interface. In ongoing work, we therefore implemented a tool for structure
inference and visualization along the common spreadsheet layout. Based on this
framework, we plan to introduce the envisioned proactive and reactive
interaction mechanics, and finally provide structure-aware unctionality as an
add-in for common spreadsheet processors. We believe that providing the tools
for thinking about and interacting with spreadsheets in this manner will
benefit users both in terms of productivity and overall spreadsheet quality.

</details>


### [83] [Typed Table Transformations](https://arxiv.org/abs/1809.02746)
*Martin Erwig*

Main category: cs.SE

TL;DR: 基于行和列类型的新的电子表格转换方法。


<details>
  <summary>Details</summary>
Motivation: 电子表格通常带有标签，这些标签构成了数据类型，控制着值在表中的位置。

Method: 提出一种基于行和列类型转换的电子表格转换新方法。

Result: 阐释了基于类型的表格构建和转换的基本思想。

Conclusion: 提出了一系列未来工作中需要解决的研究问题。

Abstract: Spreadsheet tables are often labeled, and these labels effectively constitute
types for the data in the table. In such cases tables can be considered to be
built from typed data where the placement of values within the table is
controlled by the types used for rows and columns. We present a new approach to
the transformations of spreadsheet tables that is based on transformations of
row and column types. We illustrate the basic idea of type-based table
construction and transformation and lay out a series of research questions that
should be addressed in future work.

</details>


### [84] [Implementing WHERE and ORDER BY as spreadsheet formulas](https://arxiv.org/abs/1809.00025)
*Paul Mireault*

Main category: cs.SE

TL;DR: 本文开发了电子表格公式来实现在SQL的WHERE和ORDER BY子句中选择和排序数据的功能，以解决现有电子表格工具无法自动响应计算值变化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的电子表格工具（如筛选、排序、查询或数据透视表）在选择和排序数据时，无法自动响应计算值的变化，而SQL的WHERE和ORDER BY子句可以做到这一点，因此存在一个需求来弥补电子表格在这方面的不足。

Method: 本文开发了实现SQL WHERE和ORDER BY子句功能的电子表格公式。

Result: 摘要中未明确提及研究结果，但暗示所开发的公式将克服现有工具的缺点。

Conclusion: 摘要未提供明确的结论，但其目标是为电子表格提供类似SQL WHERE和ORDER BY子句的自动响应功能。

Abstract: The WHERE and ORDER BY clauses of the SQL SELECT statement select a subset of
rows in the result of a database query and present the result in the specified
order. In a spreadsheet program like Microsoft Excel, one could use the filter
and sort buttons, or use its Query or its Pivot Table tools to achieve a
similar effect. The disadvantage of using those tools is that they don't react
automatically to changes in the calculated values of the spreadsheet. In this
paper, we develop spreadsheet formulas that implement SQL's WHERE and ORDER BY
clauses.

</details>


### [85] [The use of Charts, Pivot Tables, and Array Formulas in two Popular Spreadsheet Corpora](https://arxiv.org/abs/1808.10642)
*Bas Jansen,Felienne Hermans*

Main category: cs.SE

TL;DR: 电子表格在工业中广泛使用但容易出错，尤其是图表、数据透视表和数组公式等非公式结构研究不足。本文分析了Enron和EUSES两个常用电子表格语料库中这些结构的使用情况，以提高对电子表格质量的理解。


<details>
  <summary>Details</summary>
Motivation: 公司依赖电子表格做决策，但电子表格容易出错，可能导致决策失误和金钱损失。现有研究主要集中在公式上，而图表、数据透视表和数组公式等其他重要结构的使用情况几乎没有研究。为了提高电子表格质量，迫切需要理解这些结构如何被使用。

Method: 本文分析了Enron和EUSES两个流行的电子表格语料库，研究其中图表、数据透视表和数组公式的使用情况。

Result: （抽象中未直接给出具体结果，但预计结果将是关于Enron和EUSES语料库中图表、数据透视表和数组公式使用情况的分析。）

Conclusion: 通过分析这些在现有研究中被忽视的电子表格结构，本文旨在更全面地理解电子表格的使用，从而为提高电子表格质量做出贡献。

Abstract: The use of spreadsheets in industry is widespread. Companies base decisions
on information coming from spreadsheets. Unfortunately, spreadsheets are
error-prone and this increases the risk that companies base their decisions on
inaccurate information, which can lead to incorrect decisions and loss of
money. In general, spreadsheet research is aimed to reduce the error-proneness
of spreadsheets. Most research is concentrated on the use of formulas. However,
there are other constructions in spreadsheets, like charts, pivot tables, and
array formulas, that are also used to present decision support information to
the user. There is almost no research about how these constructions are used.
To improve spreadsheet quality it is important to understand how spreadsheets
are used and to obtain a complete understanding, the use of charts, pivot
tables, and array formulas should be included in research. In this paper, we
analyze two popular spreadsheet corpora: Enron and EUSES on the use of the
aforementioned constructions.

</details>


### [86] [Asheetoxy: A Taxonomy for Classifying Negative Spreadsheet-related Phenomena](https://arxiv.org/abs/1808.10231)
*Daniel Kulesz,Stefan Wagner*

Main category: cs.SE

TL;DR: 该论文提出了一种名为 Asheetoxy 的简单、以现象为导向的电子表格问题分类法，避免了“错误”这一模糊术语，并表明非专业人士也能轻松使用。


<details>
  <summary>Details</summary>
Motivation: 电子表格在许多组织中扮演着关键角色，但由于错误常常导致糟糕的决策。现有关于电子表格错误的分类法存在术语模糊、需要深入了解底层流程和用户“思维状态”等问题，无法有效分类公开电子表格语料库中的错误现象，阻碍了研究人员之间的讨论。

Method: 我们提出了 Asheetoxy，一种简单且以现象为导向的分类法，完全避免了“错误”这个有问题的术语。

Result: 一项有 7 名参与者的初步研究表明，即使是非电子表格研究人员也能使用 Asheetoxy 类似地对真实世界的电子表格现象进行分类。

Conclusion: Asheetoxy 提供了一种简单、以现象为导向且易于使用的电子表格问题分类法，通过避免模糊术语并简化不同用户的分类，解决了先前工作的局限性。

Abstract: Spreadsheets (sometimes also called Excel programs) are powerful tools which
play a business-critical role in many organizations. However, due to faulty
spreadsheets many bad decisions have been taken in recent years. Since then, a
number of researchers have been studying spreadsheet errors. However, one issue
that hinders discussion among researchers and professionals is the lack of a
commonly accepted taxonomy.
  Albeit a number of taxonomies for spreadsheet errors have been proposed in
previous work, a major issue is that they use the term error that itself is
already ambiguous. Furthermore, to apply most existing taxonomies, detailed
knowledge about the underlying process and knowledge about the "brain state" of
the acting spreadsheet users is required. Due to these limitations, known
error-like phenomena in freely available spreadsheet corpora cannot be
classified with these taxonomies.
  We propose Asheetoxy, a simple and phenomenon-oriented taxonomy that avoids
the problematic term error altogether. An initial study with 7 participants
indicates that even non-spreadsheet researchers similarly classify real-world
spreadsheet phenomena using Asheetoxy.

</details>


### [87] [Proceedings of the 5th International Workshop on Software Engineering Methods in Spreadsheets (SEMS'18)](https://arxiv.org/abs/1808.09174)
*Birgit Hofer,Jorge Mendes*

Main category: cs.SE

TL;DR: 该文本是2018年10月1日在葡萄牙里斯本举行的第五届软件工程方法在电子表格国际研讨会（SEMS'18）的会议记录摘要，该研讨会与2018年IEEE可视化语言和以人为本计算研讨会（VL/HCC）同期举行。


<details>
  <summary>Details</summary>
Motivation: 该文本描述的是一个研讨会，而非研究论文，因此不包含论文的动机。

Method: 该文本描述的是一个研讨会，而非研究论文，因此不包含论文的方法。

Result: 该文本描述的是一个研讨会，而非研究论文，因此不包含论文的结果。

Conclusion: 该文本描述的是一个研讨会，而非研究论文，因此不包含论文的结论。

Abstract: Proceedings of the 5th International Workshop on Software Engineering Methods
in Spreadsheets (SEMS'18), held on October 1st, 2018, in Lisbon, Portugal, and
co-located with the 2018 IEEE Symposium on Visual Languages and Human-Centric
Computing (VL/HCC).

</details>


### [88] [Combining Spreadsheet Smells for Improved Fault Prediction](https://arxiv.org/abs/1805.10493)
*Patrick Koch,Konstantin Schekotihin,Dietmar Jannach,Birgit Hofer,Franz Wotawa*

Main category: cs.SE

TL;DR: 该论文提出了一种基于机器学习的方法（使用AdaBoost集成分类器）来结合现有的电子表格“代码异味”的预测能力，以提高电子表格故障预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 电子表格在组织中广泛用于业务计算和决策，其错误可能导致严重的业务影响。尽管已将软件工程中的“代码异味”概念应用于电子表格以进行故障预测，但现有研究发现单个“代码异味”的预测能力有限。

Method: 本文提出了一种基于机器学习的方法，该方法通过使用AdaBoost集成分类器来结合单个“代码异味”的预测。

Result: 在包含真实世界电子表格故障的两个公共数据集上的实验表明，该方法在故障预测准确性方面有显著改进。

Conclusion: 通过AdaBoost集成分类器结合单个电子表格“代码异味”的预测，可以显著提高电子表格故障预测的准确性。

Abstract: Spreadsheets are commonly used in organizations as a programming tool for
business-related calculations and decision making. Since faults in spreadsheets
can have severe business impacts, a number of approaches from general software
engineering have been applied to spreadsheets in recent years, among them the
concept of code smells. Smells can in particular be used for the task of fault
prediction. An analysis of existing spreadsheet smells, however, revealed that
the predictive power of individual smells can be limited. In this work we
therefore propose a machine learning based approach which combines the
predictions of individual smells by using an AdaBoost ensemble classifier.
Experiments on two public datasets containing real-world spreadsheet faults
show significant improvements in terms of fault prediction accuracy.

</details>


### [89] [An Easy & Collaborative RDF Data Entry Method using the Spreadsheet Metaphor](https://arxiv.org/abs/1804.04175)
*Markus Schröder,Christian Jilek,Jörn Hees,Sven Hertling,Andreas Dengel*

Main category: cs.SE

TL;DR: 本文提出了一种基于网络的零配置电子表格编辑器，可以将电子表格条目同时转换为RDF语句，使不同水平的用户都能轻松创建语义数据。


<details>
  <summary>Details</summary>
Motivation: 知识工作者广泛使用电子表格，但定义RDF语句对他们来说并不容易。本文旨在弥合这一差距，使语义数据创建更加便捷。

Method: 本文提出了一种易于使用、零配置的基于网络的电子表格编辑器，可以将电子表格输入同时转换为RDF语句。它侧重于从空知识库开始，增量填充实例数据。

Result: 用户研究表明，与传统方法相比，参与者能够以更短的时间创建更多的语句，并且质量相似甚至显著优于传统方法。

Conclusion: 该编辑器成功地使各种用户能够轻松创建语义数据，在效率和质量方面优于其他方法。

Abstract: Spreadsheets are widely used by knowledge workers, especially in the
industrial sector. Their methodology enables a well understood, easy and fast
possibility to enter data. As filling out a spreadsheet is more accessible to
common knowledge workers than defining RDF statements, in this paper, we
propose an easy-to-use, zero-configuration, web-based spreadsheet editor that
simultaneously transfers spreadsheet entries into RDF statements. It enables
various kinds of users to easily create semantic data whether they are RDF
experts or novices. The typical scenario we address focuses on creating
instance data starting with an empty knowledge base that is filled
incrementally. In a user study, participants were able to create more
statements in shorter time, having similar or even significantly outperforming
quality, compared to other approaches.

</details>


### [90] [Structured Spreadsheet Modelling and Implementation with Multiple Dimensions - Part 1: Modelling](https://arxiv.org/abs/1801.09777)
*Paul Mireault*

Main category: cs.SE

TL;DR: 这篇论文讨论了在日常模型中维度的重要性，尤其提到了时间维度和第二维度的表示方法。


<details>
  <summary>Details</summary>
Motivation: 这篇论文的动机是探讨如何在电子表格中有效地表示和处理多维数据，尤其是在时间维度之外的第二维度。

Method: 该论文似乎通过在工作表中重复公式块或创建多个具有相同结构的工作表来表示第二维度，但具体方法细节未在摘要中详细说明。

Result: 摘要中没有明确说明研究结果。

Conclusion: 摘要中没有明确说明研究结论。

Abstract: Dimensions are an integral part of many models we use every day. Without
thinking about it, we frequently use the time dimension: many financial and
accounting spreadsheets have columns representing months or years. Representing
a second dimension is often done by repeating blocs of formulas in a worksheet
or creating multiple worksheets with the same structure.

</details>


### [91] [Mitigating Spreadsheet Risk in Complex Multi-Dimensional Models in Excel](https://arxiv.org/abs/1802.01640)
*Steve Litt*

Main category: cs.SE

TL;DR: Microsoft Excel 在复杂多维模型中普遍使用但易出错。本文提出“PivotModel”解决方案，旨在利用Excel功能，类似于数据透视表，以降低电子表格风险。


<details>
  <summary>Details</summary>
Motivation: Microsoft Excel 电子表格虽然功能强大、灵活易用，但手动操作多且易出错，导致公司难以控制电子表格风险，尤其是在需要复杂算法、层级和数据库回写等功能的“复杂多维模型”中。

Method: 解决方案被称为“PivotModel”，其工作方式类似于数据透视表，但旨在利用Microsoft Excel平台的强大功能。它旨在处理需要复杂功能（如高级算法、复杂层级和数据库回写）的“复杂”应用，并提供“多维”能力（如报告、数据输入表单和临时分析）以减轻电子表格风险。

Result: 本文提出并描述了一种名为“PivotModel”的解决方案设计，该方案旨在通过利用Microsoft Excel的强大功能，以类似于数据透视表的方式，减轻复杂多维模型中的电子表格风险。它详细说明了针对复杂应用和多维需求的具体功能。

Conclusion: “PivotModel”被提出作为一种有效的方法，通过利用Microsoft Excel的强大功能，以结构化和稳健的方式，类似于数据透视表，来减轻复杂多维模型中的电子表格风险。

Abstract: Microsoft Excel is the most ubiquitous analytical tool ever built. Companies
around the world leverage it for its power, flexibility and ease of use.
However, spreadsheets are manually intensive and prone to error, making it
difficult for companies to control spreadsheet risk. The following solution is
designed to mitigate spreadsheet risk for a set of problems commonly addressed
in a spreadsheet defined as "complex multi-dimensional models". "Complex"
referring to certain types of applications that require functionality such as
sophisticated algorithms, challenging hierarchies and database write-back (i.e.
planning, forecasting, etc.) and "multi-dimensional" referring to providing
capabilities such as reporting, data input forms and ad hoc analysis on the
different attributes associated with the resulting model. The solution is
defined as a "PivotModel" because it works similarly to a PivotTable but is
designed to leverage the robust capabilities of the Microsoft Excel platform.

</details>


### [92] [Proposed Spreadsheet Transparency Definition and Measures](https://arxiv.org/abs/1802.01628)
*Craig Hatmaker*

Main category: cs.SE

TL;DR: 本文提出了一种针对电子表格建模透明度的定义，以解决金融模型透明度定义不明确和缺乏衡量标准的问题。


<details>
  <summary>Details</summary>
Motivation: 审计师要求财务模型具有透明度，但目前对“透明度”没有精确的共识定义和衡量标准，这导致无法客观评估和比较建模方法。

Method: 本文提出了一种针对电子表格建模透明度的定义。

Result: 所提出的定义足够具体，可以创建衡量标准和自动化工具，帮助审计师确定模型是否符合透明度要求，并使建模者能够客观比较不同的电子表格建模方法。

Conclusion: 本文提出的定义能够帮助审计师和建模者客观地评估和比较电子表格建模的透明度。

Abstract: Auditors demand financial models be transparent yet no consensus exists on
what that means precisely. Without a clear modeling transparency definition we
cannot know when our models are "transparent". The financial modeling community
debates which methods are more or less transparent as though transparency is a
quantifiable entity yet no measures exist. Without a transparency measure
modelers cannot objectively evaluate methods and know which improves model
transparency.
  This paper proposes a definition for spreadsheet modeling transparency that
is specific enough to create measures and automation tools for auditors to
determine if a model meets transparency requirements. The definition also
provides modelers the ability to objectively compare spreadsheet modeling
methods to select which best meets their goals.

</details>


### [93] [Alternative Spreadsheet Model Designs for an Operations Management Model Embedded in a Periodic Business Process](https://arxiv.org/abs/1802.00484)
*Thomas A. Grossman,Vijay Mehrotra,Mouwafac Sidaoui*

Main category: cs.SE

TL;DR: 本文评估了三种不同的电子表格实现（数据驱动、规范和表格驱动技术设计），用于一个常用的供应链和分销规划运营管理模型。研究发现，技术设计在修改新数据和结构元素方面更高效，错误风险更低。


<details>
  <summary>Details</summary>
Motivation: 供应链和分销规划中常用的运营管理模型需要周期性修改和重复使用，因此寻找更好的电子表格实现方法以提高准确性、修改效率、分析能力和可移植性是必要的。

Method: 研究评估了三种电子表格实现：数据驱动设计、规范设计和新颖的表格驱动技术设计。评估标准包括准确性、修改便利性、分析能力、可移植性以及所需的用户培训和技术复杂程度。

Result: 数据驱动设计揭示了新手建模者不佳的电子表格实践。技术设计无需手动编辑单元格公式即可修改新数据和结构元素，从而加快修改速度并降低错误风险。技术设计还有潜力应用于其他类别的模型。

Conclusion: 表格驱动技术设计在模型修改和错误风险降低方面表现出优越性，并具有广泛的应用潜力。研究也指出了未来的研究方向。

Abstract: We present a widely-used operations management model used in supply and
distribution planning, that is typically embedded in a periodic business
process that necessitates model modification and reuse. We consider three
alternative spreadsheet implementations, a data-driven design, a canonical
(textbook) design, and a novel (table-driven) technical design. We evaluate
each regarding suitability for accuracy, modification, analysis, and transfer.
We consider the degree of training and technical sophistication required to
utilize each design. The data-driven design provides insight into poor
spreadsheet practices by na\"ive modelers. The technical design can be modified
for new data and new structural elements without manual writing or editing of
cell formulas, thus speeding modification and reducing risk of error. The
technical design has potential for use with other classes of models. We
identify opportunities for future research.

</details>


### [94] [Mitigating Spreadsheet Model Risk with Python Open Source Infrastructure](https://arxiv.org/abs/1801.09771)
*Oliver Beavers*

Main category: cs.SE

TL;DR: 表格模型开发缺乏传统软件工程工具和协议，导致错误率高。本文提出利用Python开源软件包开发可重现的审计工具和“预言机”，用于测试和审计电子表格计算。


<details>
  <summary>Details</summary>
Motivation: 电子表格模型类似于软件，但开发人员并非软件工程师，缺乏传统软件工程工具和协议导致结果错误率高。

Method: 使用免费的、开源的Python软件包，为电子表格建模专业人员开发可重现的审计工具。

Result: 使利益相关者能够开发明确定义的模型“预言机”，用于测试和审计电子表格计算。

Conclusion: 为电子表格建模专业人员开发可重现的审计工具奠定基础，以提高电子表格计算的准确性和可审计性。

Abstract: Across an aggregation of EuSpRIG presentation papers, two maxims hold true:
spreadsheets models are akin to software, yet spreadsheet developers are not
software engineers. As such, the lack of traditional software engineering tools
and protocols invites a higher rate of error in the end result. This paper lays
ground work for spreadsheet modelling professionals to develop reproducible
audit tools using freely available, open source packages built with the Python
programming language, enabling stakeholders to develop clearly defined model
"oracles" with which to test and audit spreadsheet calculations against.

</details>


### [95] [Structuring Spreadsheets with the "Lish" Data Model](https://arxiv.org/abs/1801.08603)
*Alan Hall,Michel Wermelinger,Tony Hirst,Santi Phithakkitnukoon*

Main category: cs.SE

TL;DR: 本文介绍了一种名为“lish”的新型数据模型，旨在替代传统电子表格网格，通过捕捉更高层次的结构，同时保持电子表格的简洁性，从而提高理解力并减少错误。


<details>
  <summary>Details</summary>
Motivation: 传统电子表格虽然灵活，但其独立单元格缺乏对整体结构的认知，这阻碍了理解并增加了公式复制的风险，从而提高了出错的可能性。

Method: 本文提出了一种名为“lish”的数据模型，它将单元格组织成嵌套列表，用户可以在其中选择使用模板来原型化重复结构。这些模板元素是工作表的正式成员，并且本身可以包含内部结构。

Result: 一个小型的演示应用程序展示了“lish”的运行情况。

Conclusion: “lish”模型通过捕获更高层次的结构，同时保持简洁性，为电子表格提供了一种替代方案，有望减少错误并提高理解力。

Abstract: A spreadsheet is remarkably flexible in representing various forms of
structured data, but the individual cells have no knowledge of the larger
structures of which they may form a part. This can hamper comprehension and
increase formula replication, increasing the risk of error on both scores. We
explore a novel data model (called the "lish") that could form an alternative
to the traditional grid in a spreadsheet-like environment. Its aim is to
capture some of these higher structures while preserving the simplicity that
makes a spreadsheet so attractive. It is based on cells organised into nested
lists, in each of which the user may optionally employ a template to prototype
repeating structures. These template elements can be likened to the marginal
"cells" in the borders of a traditional worksheet, but are proper members of
the sheet and may themselves contain internal structure. A small demonstration
application shows the "lish" in operation.

</details>


### [96] [Automated Refactoring of Nested-IF Formulae in Spreadsheets](https://arxiv.org/abs/1712.09797)
*Jie Zhang,Shi Han,Dan Hao,Lu Zhang,Dongmei Zhang*

Main category: cs.SE

TL;DR: 该论文提出了一种基于AST的自动化方法，用于重构电子表格中的嵌套IF公式，这些公式难以阅读且容易出错。该方法在真实数据集上成功重构了99%以上的此类公式，显著降低了嵌套级别，并且用户更喜欢重构后的版本。


<details>
  <summary>Details</summary>
Motivation: 电子表格中的嵌套IF表达式可读性低，认知成本高，且容易出错。终端用户缺乏编程知识来解决这些问题，目前也没有有效的自动化方法。

Method: 该方法是一种基于AST的自动化重构方法，分两步进行：1）检测并消除AST上的逻辑冗余；2）识别分散的高级语义，并使用简洁的内置函数重新组合语法。

Result: 该方法在包含68,000多个电子表格和2700万个嵌套IF公式的真实语料库中，解决了99%以上嵌套IF公式的代码异味问题。超过50%的重构将嵌套IF的嵌套级别减少了一半以上。一项涉及49名参与者的调查显示，大多数情况下参与者更喜欢重构后的公式，并认同这种自动化重构方法的必要性和帮助性。

Conclusion: 所提出的自动化AST重构方法能有效改善电子表格中嵌套IF公式的可读性并降低其复杂性，并受到用户的欢迎。

Abstract: Spreadsheets are the most popular end-user programming software, where
formulae act like programs and also have smells. One well recognized common
smell of spreadsheet formulae is nest-IF expressions, which have low
readability and high cognitive cost for users, and are error-prone during reuse
or maintenance. However, end users usually lack essential programming language
knowledge and skills to tackle or even realize the problem. The previous
research work has made very initial attempts in this aspect, while no effective
and automated approach is currently available.
  This paper firstly proposes an AST-based automated approach to systematically
refactoring nest-IF formulae. The general idea is two-fold. First, we detect
and remove logic redundancy on the AST. Second, we identify higher-level
semantics that have been fragmented and scattered, and reassemble the syntax
using concise built-in functions. A comprehensive evaluation has been conducted
against a real-world spreadsheet corpus, which is collected in a leading IT
company for research purpose. The results with over 68,000 spreadsheets with 27
million nest-IF formulae reveal that our approach is able to relieve the smell
of over 99\% of nest-IF formulae. Over 50% of the refactorings have reduced
nesting levels of the nest-IFs by more than a half. In addition, a survey
involving 49 participants indicates that for most cases the participants prefer
the refactored formulae, and agree on that such automated refactoring approach
is necessary and helpful.

</details>


### [97] [Spreadsheet Guardian: An Approach to Protecting Semantic Correctness throughout the Evolution of Spreadsheets](https://arxiv.org/abs/1612.03813)
*Daniel Kulesz,Verena Käfer,Stefan Wagner*

Main category: cs.SE

TL;DR: 本文介绍了一种名为“Spreadsheet Guardian”的方法，它通过将测试规则的定义与执行分离，旨在检测和防止电子表格在协作维护过程中出现的语义错误。该方法作为Microsoft Excel的一个插件实现，并通过用户评估证明其易学易用，并能帮助用户更准确地评估其电子表格的正确性。


<details>
  <summary>Details</summary>
Motivation: 电子表格在许多组织中扮演着关键业务角色，但其错误常导致糟糕的决策。当前，电子表格的协作维护缺乏确保其正确性的支持。

Method: 我们开发了“Spreadsheet Guardian”方法，它将电子表格测试规则的规范与执行分离。该方法通过自动执行用户定义的测试规则来检测语义故障，并保护所有协作的电子表格用户在维护过程中不会引入故障。该技术已作为Microsoft Excel的插件实现。

Result: 该测试技术通过对29名最终用户和42名计算机科学学生的两次实证评估进行了评估。结果表明该技术易于学习和应用。此外，使用该技术“保护”的电子表格的参与者在维护完成后，对其电子表格正确性的认识比仅使用基于静态分析技术的“经典”非交互式测试规则的参与者更为实际。

Conclusion: Spreadsheet Guardian 对业务关键型电子表格很有用。

Abstract: Spreadsheets are powerful tools which play a business-critical role in many
organizations. However, many bad decisions taken due to faulty spreadsheets
show that these tools need serious quality assurance. Furthermore, while
collaboration on spreadsheets for maintenance tasks is common, there has been
almost no support for ensuring that the spreadsheets remain correct during this
process.
  We have developed an approach named Spreadsheet Guardian which separates the
specification of spreadsheet test rules from their execution. By automatically
executing user-defined test rules, our approach is able to detect semantic
faults. It also protects all collaborating spreadsheet users from introducing
faults during maintenance, even if only few end-users specify test rules. To
evaluate Spreadsheet Guardian, we implemented a representative testing
technique as an add-in for Microsoft Excel.
  We evaluated the testing technique in two empirical evaluations with 29
end-users and 42 computer science students. The results indicate that the
technique is easy to learn and to apply. Furthermore, after finishing
maintenance, participants with spreadsheets "protected" by the technique are
more realistic about the correctness of their spreadsheets than participants
who employ only "classic", non-interactive test rules based on static analysis
techniques. Hence, we believe Spreadsheet Guardian can be of use for
business-critical spreadsheets.

</details>


### [98] [On Evidence-based Risk Management in Requirements Engineering](https://arxiv.org/abs/1707.00144)
*Daniel Méndez Fernández,Michaela Tießler,Marcos Kalinowski,Michael Felderer,Marco Kuhrmann*

Main category: cs.SE

TL;DR: 该论文提出并验证了一种基于证据的方法，利用来自228家公司的跨公司调查数据，通过构建概率网络来评估需求工程（RE）中特定于上下文的风险，并在6家公司进行了初步验证。


<details>
  <summary>Details</summary>
Motivation: 由于对上下文的敏感性，需求工程（RE）中的问题难以有效控制，阻碍了有效的风险管理。目前，关于特定上下文的RE现象的经验知识仍然很少，而这对于RE中有效的上下文敏感风险管理是必不可少的。

Method: 该研究使用来自228家公司的调查数据，构建了一个概率网络来预测特定上下文的RE现象。该方法通过电子表格实现，以支持轻量级的风险评估。

Result: 在6家公司进行的初步验证结果增强了研究人员的信心，表明该方法提高了对RE中个体风险因素的认识，并且反馈有助于将该方法推广到实践中。

Conclusion: 该论文提出的基于证据的方法，利用跨公司数据和概率网络，能够有效评估特定上下文的RE风险，并具有在实践中推广的潜力。

Abstract: Background: The sensitivity of Requirements Engineering (RE) to the context
makes it difficult to efficiently control problems therein, thus, hampering an
effective risk management devoted to allow for early corrective or even
preventive measures. Problem: There is still little empirical knowledge about
context-specific RE phenomena which would be necessary for an effective
context- sensitive risk management in RE. Goal: We propose and validate an
evidence-based approach to assess risks in RE using cross-company data about
problems, causes and effects. Research Method: We use survey data from 228
companies and build a probabilistic network that supports the forecast of
context-specific RE phenomena. We implement this approach using spreadsheets to
support a light-weight risk assessment. Results: Our results from an initial
validation in 6 companies strengthen our confidence that the approach increases
the awareness for individual risk factors in RE, and the feedback further
allows for disseminating our approach into practice.

</details>


### [99] [Tabula: A Language to Model Spreadsheet Tables](https://arxiv.org/abs/1707.02833)
*Jorge Mendes,João Saraiva*

Main category: cs.SE

TL;DR: 本文介绍了Tabula，一种新的电子表格建模语言，它扩展了现有模型，增加了类型约束、带重复的嵌套类等功能，并包含一个双向转换引擎，以保证模型或电子表格更新后的同步。


<details>
  <summary>Details</summary>
Motivation: 电子表格灵活但易出错，现有电子表格模型语言表达能力有限，无法建模许多真实世界电子表格的特性。

Method: 引入了建模语言Tabula，它通过类型约束和带重复的嵌套类等特性扩展了之前的电子表格模型。Tabula比其他模型更具表达力，并且可以进一步扩展更多功能。此外，Tabula包含一个双向转换引擎。

Result: Tabula比其他模型更具表达力且可扩展。其双向转换引擎可确保模型或电子表格更新后的同步。

Conclusion: Tabula提供了一个更具表达力和可扩展性的电子表格建模解决方案，提高了错误预防能力并确保了同步。

Abstract: Spreadsheets provide a flexible and easy to use software development
environment, but that leads to error proneness. Work has been done to prevent
errors in spreadsheets, including using models to specify distinct parts of a
spreadsheet as it is done with model-driven software development. Previous
model languages for spreadsheets offer a limited expressiveness, and cannot
model several features present in most real world spreadsheets.
  In this paper, the modeling language Tabula is introduced. It extends
previous spreadsheet models with features like type constraints and nested
classes with repetitions. Tabula is not only more expressive than other models
but it can also be extended with more features. Moreover, Tabula includes a
bidirectional transformation engine that guarantees synchronization after an
update either in the model or spreadsheet.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [100] [Using the Abstract Computer Architecture Description Language to Model AI Hardware Accelerators](https://arxiv.org/abs/2402.00069)
*Mika Markus Müller,Alexander Richard Manfred Borst,Konstantin Lübeck,Alexander Louis-Ferdinand Jung,Oliver Bringmann*

Main category: cs.AR

TL;DR: 本论文介绍了抽象计算机架构描述语言（ACADL），用于建模AI硬件加速器，将深度神经网络（DNNs）映射到加速器上，并通过时序模拟评估其性能。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络（DNNs）的普及，AI硬件加速器的需求日益增长。然而，在众多供应商提供的参数化加速器中，为特定产品选择并配置合适的加速器面临挑战。现有比较方法（如数据表、电子表格或缓慢的黑盒模拟器）通常只能提供粗略的性能理解，使得比较不同加速器设计方案变得复杂。

Method: 论文提出使用抽象计算机架构描述语言（ACADL）来形式化描述计算机架构，以建立AI硬件加速器模型。具体方法包括使用ACADL描述加速器，将DNNs映射到这些ACADL描述的加速器上，并解释时序模拟的语义以获取性能结果。

Result: 论文演示了如何使用ACADL来建模AI硬件加速器、将DNNs映射到这些加速器上，以及通过解释时序模拟语义来收集性能结果。

Conclusion: ACADL提供了一种简洁的形式化方法来描述计算机架构，有助于在不同抽象级别上交流计算机架构并推断性能特征。通过使用ACADL，可以有效地对AI硬件加速器进行建模、DNNs映射和性能模拟，从而解决选择和比较加速器的挑战。

Abstract: Artificial Intelligence (AI) has witnessed remarkable growth, particularly
through the proliferation of Deep Neural Networks (DNNs). These powerful models
drive technological advancements across various domains. However, to harness
their potential in real-world applications, specialized hardware accelerators
are essential. This demand has sparked a market for parameterizable AI hardware
accelerators offered by different vendors.
  Manufacturers of AI-integrated products face a critical challenge: selecting
an accelerator that aligns with their product's performance requirements. The
decision involves choosing the right hardware and configuring a suitable set of
parameters. However, comparing different accelerator design alternatives
remains a complex task. Often, engineers rely on data sheets, spreadsheet
calculations, or slow black-box simulators, which only offer a coarse
understanding of the performance characteristics.
  The Abstract Computer Architecture Description Language (ACADL) is a concise
formalization of computer architecture block diagrams, which helps to
communicate computer architecture on different abstraction levels and allows
for inferring performance characteristics. In this paper, we demonstrate how to
use the ACADL to model AI hardware accelerators, use their ACADL description to
map DNNs onto them, and explain the timing simulation semantics to gather
performance results.

</details>


### [101] [Online Model Swapping in Architectural Simulation](https://arxiv.org/abs/2012.01571)
*Patrick Lavin,Jeffrey Young,Rich Vuduc,Jonathan Beard*

Main category: cs.AR

TL;DR: 该论文提出了一种在线方法，通过监控仿真行为并自动用更简单的统计近似模型替换详细模型，以弥补简单仿真和详细仿真之间的差距。在SVE-Cachesim中实现的L1D缓存替换案例显示，在模拟周期计数中只有8%的误差，但90%以上的模拟使用了近似模型，计算量减少了2到8倍。


<details>
  <summary>Details</summary>
Motivation: 随着系统和应用程序日益复杂，详细仿真所需时间不断增加，导致设计迭代变慢。为了快速迭代，架构师常被迫使用更简单的模型，如电子表格。然而，从简单模型迁移到详细模型，或凭直觉选择简单模型，都可能导致比直接运行详细模型更高的成本或复杂性。

Method: 本文提出一种通过在线监控仿真行为，并自动用更简单的统计近似模型替换详细模型的方法，以弥合简单仿真和详细仿真之间的差距。该方法在开源模拟器SVE-Cachesim中实现，用于替换存储层次结构中的一级数据缓存（L1D）。

Result: 该技术能够处理非平凡的用例，包括时间不变和时间变化的统计数据，以及下游副作用（例如，L1D过滤二级缓存的访问）。在模拟周期计数中仅有8%的误差，却有超过90%的模拟使用了近似缓存模型，并且这些更简单的模型在每次“执行”中所需的计算量减少了2到8倍。

Conclusion: 该工作证明了其方法具有加速模拟的潜力，通过自动替换详细模型为统计近似模型，在复杂、时间敏感的用例中实现了显著的计算效率提升，且保持了可接受的精度。

Abstract: As systems and applications grow more complex, detailed simulation takes an
ever increasing amount of time. The prospect of increased simulation time
resulting in slower design iteration forces architects to use simpler models,
such as spreadsheets, when they want to iterate quickly on a design. However,
the task of migrating from a simple simulation to one with more detail often
requires multiple executions to find where simple models could be effective,
which could be more expensive than running the detailed model in the first
place. Also, architects must often rely on intuition to choose these simpler
models, further complicating the problem.
  In this work, we present a method of bridging the gap between simple and
detailed simulation by monitoring simulation behavior online and automatically
swapping out detailed models with simpler statistical approximations. We
demonstrate the potential of our methodology by implementing it in the
open-source simulator SVE-Cachesim to swap out the level one data cache (L1D)
within a memory hierarchy. This proof of concept demonstrates that our
technique can handle a non-trivial use-case in not just approximation of local
time-invariant statistics, but also those that vary with time (e.g., the L1D is
a form of a time-series function), and downstream side-effects (e.g., the L1D
filters accesses for the level two cache). Our simulation swaps out the
built-in cache model with only an 8% error in the simulated cycle count while
using the approximated cache models for over 90% of the simulation, and our
simpler models require two to eight times less computation per "execution" of
the model

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [102] [Billion-files File Systems (BfFS): A Comparison](https://arxiv.org/abs/2408.01805)
*Sohail Shaikh*

Main category: cs.PF

TL;DR: 这篇论文分析了EXT4、XFS、BtrFS、ZFS和F2FS等流行的Linux文件系统在处理大量文件时的性能和局限性。研究通过创建、存储和读取十亿个文件来评估读写吞吐量、存储块使用、磁盘空间利用率以及性能下降等指标。


<details>
  <summary>Details</summary>
Motivation: 随着数据量的指数级增长，需要快速处理数据并减少传输延迟，因此研究本地文件系统的工作原理、性能和局限性变得至关重要。

Method: 研究通过创建、存储和读取十亿个文件来分析EXT4、XFS、BtrFS、ZFS和F2FS等文件系统。同时，捕获并分析了读写吞吐量、存储块使用、磁盘空间利用率、开销以及文件系统在大量文件和文件夹创建过程中及之后的性能下降等指标。

Result: 论文捕获并分析了这些文件系统的读写吞吐量、存储块使用、磁盘空间利用率、开销以及在处理大量文件时的性能下降情况，为系统设计者和集成商提供了有用的指标。

Conclusion: 这项研究为系统设计者和集成商提供了关于流行Linux文件系统在处理海量文件时的性能和局限性的宝贵见解。

Abstract: As the volume of data being produced is increasing at an exponential rate
that needs to be processed quickly, it is reasonable that the data needs to be
available very close to the compute devices to reduce transfer latency. Due to
this need, local filesystems are getting close attention to understand their
inner workings, performance, and more importantly their limitations. This study
analyzes few popular Linux filesystems: EXT4, XFS, BtrFS, ZFS, and F2FS by
creating, storing, and then reading back one billion files from the local
filesystem. The study also captured and analyzed read/write throughput, storage
blocks usage, disk space utilization and overheads, and other metrics useful
for system designers and integrators. Furthermore, the study explored other
side effects such as filesystem performance degradation during and after these
large numbers of files and folders are created.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [103] [Excel: Automated Ledger or Analytics IDE?](https://arxiv.org/abs/2409.12976)
*Andrew Kumiega*

Main category: cs.CY

TL;DR: Excel已演变为分析领域的集成开发环境（IDE），拥有数据库、OLAP引擎和多种编程语言等功能。这种转变需要扩展当前的风险框架以管理其独特的风险。


<details>
  <summary>Details</summary>
Motivation: 由于Excel从简单的账本自动化工具缓慢演变为分析IDE，许多人未能意识到其作为IDE的全面功能和日益增长的风险。因此，有必要建立一个全面的风险框架来管理这个独特的开发环境。

Method: 本文将解释如何扩展当前针对电子表格的风险框架，以管理将Excel用作分析IDE所带来的日益增长的风险。

Result: 抽象指出Excel已作为一个分析IDE运作，集成了各种复杂的工具（数据库、OLAP、统计语言等）于一个低代码环境中，而当前的电子表格风险框架未能充分解决这些问题。

Conclusion: 认识到Excel向分析IDE的转变，建立一个全面的风险框架来管理其独特的开发环境和相关风险变得至关重要。

Abstract: Since the inception of VisiCalc over four decades ago, spreadsheets have
undergone a gradual transformation, evolving from simple ledger automation
tools to the current state of Excel, which can be described as an Integrated
Development Environment (IDE) for analytics. The slow evolution of Excel from
an automation tool for ledgers to an IDE for analytics explains why many people
have not noticed that Excel includes a fully functional database, an OLAP
Engine, multiple statistical programming languages, multiple third-party
software libraries, dynamic charts, and real time data connectors. The
simplicity of accessing these multiple tools is a low-code framework controlled
from the Excel tool that is effectively an IDE. Once we acknowledge Excel's
shift from a desk top application to an IDE for analytics, the importance of
establishing a comprehensive risk framework for managing this distinctive
development environment becomes clear. In this paper we will explain how the
current risk framework for spreadsheets needs to be expanded to manage the
growing risks of using Excel as an IDE for analytics.

</details>


### [104] [Improving Feedback from Automated Reviews of Student Spreadsheets](https://arxiv.org/abs/2311.10728)
*Sören Aguirre Reid,Frank Kammer,Jonas-Ian Kuche,Pia-Doreen Ritzke,Markus Siepermann,Max Stephan,Armin Wagenknecht*

Main category: cs.CY

TL;DR: 开发了一个智能辅导系统（ITS），用于自动评估Excel作业并提供个性化、分级的反馈，该反馈基于值匹配、公式分析和质量评估，从而提高了学生的提交质量。


<details>
  <summary>Details</summary>
Motivation: 电子表格是最终用户最广泛使用的工具之一，但用于评估电子表格作业的数字化解决方案在教学环境中仍然稀缺。

Method: 开发了一个智能辅导系统（ITS），该系统通过值匹配、公式的详细分析和解决方案的质量评估来审查学生的Excel提交。为了考虑学生的学习水平，该系统开发了反馈级别，通过使用不同的分析逐步提供更多关于错误的信息。

Result: 较高水平的反馈导致了更高比例的正确提交，并且学生认为这些反馈易于理解和有帮助。

Conclusion: 所开发的ITS能够有效评估Excel作业并提供有益的反馈，从而提高学生的学习成果。

Abstract: Spreadsheets are one of the most widely used tools for end users. As a
result, spreadsheets such as Excel are now included in many curricula. However,
digital solutions for assessing spreadsheet assignments are still scarce in the
teaching context. Therefore, we have developed an Intelligent Tutoring System
(ITS) to review students' Excel submissions and provide individualized feedback
automatically. Although the lecturer only needs to provide one reference
solution, the students' submissions are analyzed automatically in several ways:
value matching, detailed analysis of the formulas, and quality assessment of
the solution. To take the students' learning level into account, we have
developed feedback levels for an ITS that provide gradually more information
about the error by using one of the different analyses. Feedback at a higher
level has been shown to lead to a higher percentage of correct submissions and
was also perceived as well understandable and helpful by the students.

</details>


### [105] [How Beaufort, Neumann and Gates met? Subject integration with spreadsheeting](https://arxiv.org/abs/2309.12353)
*Maria Csernoch,Julia Csernoch*

Main category: cs.CY

TL;DR: 本文提出了一种新颖的方法，通过将传统纸质问题转化为数字环境，支持学科整合和数字图式构建，并在八年级行动研究中发现学生的内容知识和数字技能发展更有效。


<details>
  <summary>Details</summary>
Motivation: 计算思维应成为继读、写、算之后的第四项基本技能，但要形成数字问题解决的图式仍有很长的路要走。

Method: 本文基于 Beaufort 等级，提出了一种新颖的方法来支持学科整合和数字图式构建，通过在八年级行动研究中，将一个传统的纸质问题和数据检索过程进行转化。

Result: 研究发现，学生的内容知识和数字技能比在传统课本和去语境化的数字环境中发展得更有效。

Conclusion: 本文提出的方法可以应用于任何通过数字环境能更有效解决的纸质问题，并且在学科和信息学中都能以各种形式构建图式。

Abstract: Computational thinking should be the fourth fundamental skill, along with
reading, writing, and arithmetic (3R). To reach the level where computational
thinking skills, especially digital problem solving have their own schemata,
there is a long way to go. In the present paper, a novel approach is detailed
to support subject integration and building digital schemata, on the well-known
Beaufort scale. The conversion of a traditional, paper-based problem and a data
retrieval process are presented within the frame of a Grade 8 action research
study. It is found that both students content knowledge and their digital
skills developed more efficiently than in traditional course book and
decontextualized digital environments. Furthermore, the method presented here
can be adapted to any paper-based problems whose solutions would be more
effective in a digital environment and which offer various forms for building
schemata both in the subject matter and informatics.

</details>


### [106] [A Simpler Method for Understanding Emergency Shelter Access Patterns](https://arxiv.org/abs/2210.13619)
*Geoffrey G. Messier*

Main category: cs.CY

TL;DR: 简化访问指标（SAM）是一种新的方法，用于衡量紧急避难所客户的脆弱性，通过分析访问模式。它易于非技术人员操作，使用较少数据就能实时反映外部因素对避难所访问模式的影响，并提供与传统聚类分析相似的结果。


<details>
  <summary>Details</summary>
Motivation: 为避难所运营者提供一种直观、易于实施的方式来理解客户访问模式，从而评估客户脆弱性，且无需专业技术人员和大量数据。

Method: 提出简化访问指标（SAM），并利用北美大型避难所的客户数据进行验证。通过比较SAM与传统过渡性、偶发性和长期性客户聚类分析的结果来证明其有效性。使用九年的避难所客户数据，通过SAM生成时间线，展示住房优先计划和COVID-19封锁对人们访问避难所方式的影响。

Result: SAM能产生与传统聚类分析相似的结果，但所需数据更少，因此能够实时反映外部因素（如住房优先计划和COVID-19封锁）对避难所访问模式的影响。SAM的“软”输出可以直接作为衡量脆弱性的指标，超越了传统的客户分类标签。

Conclusion: 简化访问指标（SAM）为避难所工作人员提供了一种实用、直观的工具，用于理解客户脆弱性和访问模式，它以更少的数据提供实时洞察，并直接衡量脆弱性。

Abstract: The Simplified Access Metric (SAM) is a new approach for characterizing
emergency shelter access patterns as a measure of shelter client vulnerability.
The goal of SAM is to provide shelter operators with an intuitive way to
understand access patterns that can be implemented by non-technical staff using
spreadsheet operations. Client data from a large North American shelter will be
used to demonstrate that SAM produces similar results to traditional
transitional, episodic and chronic client cluster analysis. Since SAM requires
less data than cluster analysis, it is also able to generate a real time
picture of how shelter access patterns are affected by external factors.
Timelines generated from nine years of shelter client data using SAM
demonstrate the impact of Housing First programming and the COVID-19 lockdown
on how people access shelter. Finally, SAM allows shelter staff to move beyond
assigning transitional, episodic and chronic labels and instead use the "soft"
output of SAM directly as a measure of vulnerability.

</details>


### [107] [Long-Term Mentoring for Computer Science Researchers](https://arxiv.org/abs/2208.04738)
*Emily Ruppel,Sihang Liu,Elba Garza,Sukyoung Ryu,Alexandra Silva,Talia Ringer*

Main category: cs.CY

TL;DR: 本文介绍了编程语言（PL）和计算机体系结构（CA）领域创建并实施的两个长期指导项目：SIGPLAN-M和CALM。这些项目旨在解决社区中建立持久联系的难题，并取得了显著成效，希望激励计算机科学领域更广泛地推广长期指导计划。


<details>
  <summary>Details</summary>
Motivation: 疫情初期，编程语言和计算机体系结构研究领域的领导者发现，社区中新成员难以建立持久的联系，而现有的短期指导项目不足以解决这一问题，因此需要建立长期指导项目。

Method: 计算机体系结构领域采用科学方法，为社区范围内的长期指导提供了证据支持，并成立了CALM项目。编程语言领域则由一人自发发起了一个非官方的长期指导项目，该项目后来于2021年1月发展成为官方的跨机构SIGPLAN-M项目。两个项目的领导者分享了他们的设计、影响和挑战。

Result: SIGPLAN-M项目已覆盖41个国家的328名受指导者和234名指导者，受指导者称其为“改变人生”和“职业生涯的救星”。CALM项目目前处于试点阶段，有来自7个国家的13名指导者和21名受指导者，也获得了非常积极的反馈。领导者们分享了他们的设计、影响和遇到的挑战。

Conclusion: 这些长期指导项目产生了强大的积极影响。作者希望通过分享他们的经验，能够促进计算机科学领域内更大范围的长期指导工作的开展。

Abstract: Early in the pandemic, we -- leaders in the research areas of programming
languages (PL) and computer architecture (CA) -- realized that we had a
problem: the only way to form new lasting connections in the community was to
already have lasting connections in the community. Both of our academic
communities had wonderful short-term mentoring programs to address this
problem, but it was clear that we needed long-term mentoring programs.
  Those of us in CA approached this scientifically, making an evidence-backed
case for community-wide long-term mentoring. In the meantime, one of us in PL
had impulsively launched an unofficial long-term mentoring program, founded on
chaos and spreadsheets. In January 2021, the latter grew to an official
cross-institutional long-term mentoring program called SIGPLAN-M; in January
2022, the former grew to Computer Architecture Long-term Mentoring (CALM).
  The impacts have been strong: SIGPLAN-M reaches 328 mentees and 234 mentors
across 41 countries, and mentees have described it as "life changing" and "a
career saver." And while CALM is in its pilot phase -- with 13 mentors and 21
mentees across 7 countries -- it has received very positive feedback. The
leaders of SIGPLAN-M and CALM shared our designs, impacts, and challenges along
the way. Now, we wish to share those with you. We hope this will kick-start a
larger long-term mentoring effort across all of computer science.

</details>


### [108] [Optimization Helps Scheduling Nursing Staff at the Long-Term Care Homes of the City of Toronto](https://arxiv.org/abs/2102.09461)
*Manion Anderson,Merve Bodur,Scott Rathwell,Vahid Sarhangian*

Main category: cs.CY

TL;DR: 该论文描述了一个基于电子表格的排班工具，采用分层优化模型，旨在自动化多伦多长期护理机构的护理人员排班，纳入护士偏好，并减少缺勤率。该工具显著缩短了排班时间，并提高了偏好满足度。


<details>
  <summary>Details</summary>
Motivation: 多伦多市长期护理院及服务部在护理人员排班方面面临日益严峻的挑战，并且兼职护士的缺勤率很高。

Method: 开发了一个基于电子表格的排班工具。其核心是一个分层优化模型，该模型在满足最大可能需求的同时，生成具有最高总偏好得分的可行排班表，并遵守复杂的资历要求。

Result: 该工具在多伦多一家拥有391张床位的护理院中实施。它将排班时间从手动方式的数十小时缩短到不到一小时。此外，排班表成功地考虑了偏好，平均超过94%的分配班次被评为最受欢迎。

Conclusion: 该排班工具成功实现了护理人员排班的自动化，纳入了护士偏好，显著缩短了排班时间，并可能有助于解决长期护理机构的缺勤问题。

Abstract: The City of Toronto Long Term Care Homes & Services (LTCH&S) division is one
of the largest providers of long-term care in the Canadian province of Ontario,
providing care to 2,640 residents at 10 homes across Toronto. Our collaboration
with LTCH&S was initiated to facilitate the increasingly challenging task of
scheduling nursing staff and reduce high absenteeism rate observed among the
part-time nurses. We developed a spreadsheet-based scheduling tool to automate
the generation of schedules and incorporate nurses' preferences for different
shifts into the schedules. At the core of the scheduling tool is a hierarchical
optimization model that generates a feasible schedule with the highest total
preference score while satisfying the maximum possible demand. Feasible
schedules had to abide by a set of complex seniority requirements which
prioritized more senior nurses when allocating the available shifts. Our
scheduling tool was implemented in a 391-bed home in Toronto. The tool allowed
nursing managers to generate feasible schedules within a fraction of an hour,
in contrast to the status-quo manual approach which could took up to tens of
hours. In addition, the schedules successfully accounted for preferences with
on average above 94% of the allocated shifts ranked as most preferred.

</details>


### [109] [Developing Excel Thought Leadership](https://arxiv.org/abs/2006.04793)
*David Lyford-Smith*

Main category: cs.CY

TL;DR: ICAEW在五年内开发了三份关于电子表格最佳实践的指导文件。本文将回顾这些文件的历史、关键经验，并讨论其制定过程如何帮助ICAEW发展其在该领域的地位。


<details>
  <summary>Details</summary>
Motivation: 描述ICAEW在电子表格使用和工作环境方面开发的三份指导文件的历史、关键经验，以及这些文件的制定过程如何帮助ICAEW发展其在该领域的地位。

Method: 通过回顾这三份文件的历史，总结每份文件的关键经验教训，并讨论其制定过程如何帮助ICAEW发展其在该领域的地位来进行分析。

Result: 本文将呈现这三份指导文件的历史回顾，总结它们各自提供的关键经验教训，并分析其制定过程如何积极地帮助ICAEW在该领域发展其定位。

Conclusion: 本文将总结ICAEW在电子表格最佳实践领域的工作及其指导文件的价值，以及制定过程对该组织在该领域发展的重要性。

Abstract: Over a period of five years, the Institute of Chartered Accountants in
England and Wales (ICAEW) has developed a suite of three 'thought leadership'
papers surrounding good practice in spreadsheet use and spreadsheet work
environments. We will review the history of these three papers, the key lessons
which each has to teach, and discuss how the process of making them has helped
ICAEW to develop its position in the field.

</details>


### [110] [A Case Study of Spreadsheet Use within the Finance and Academic Registry units within a Higher Education Institution](https://arxiv.org/abs/1909.07462)
*Simon Thorne,Jamie Hancock*

Main category: cs.CY

TL;DR: 本文对英国一所高等教育机构的电子表格使用情况进行了案例研究，分析了其重要性、培训、目的、技术、规模和共享，发现电子表格使用量巨大，且组织需要明确的指导方针以确保数据完整性和效率。


<details>
  <summary>Details</summary>
Motivation: 了解和探讨高等教育机构内部电子表格的使用情况，以发现其对准确报告、数据完整性和效率的影响。

Method: 在英国一所高等教育机构的两个部门（学术注册处和财务部）进行了一项案例研究，从多个维度探索了电子表格的使用情况。

Result: 创建和使用了大量的电子表格；电子表格开发人员的特征与其他研究相似；迫切需要明确的电子表格开发原则和指南，以确保数据完整性、减少重复工作并优化使用。

Conclusion: 该机构需要建立明确的电子表格开发原则和指南，以提高数据完整性，减少重复工作，并优化电子表格对机构目标的利用。

Abstract: This paper presents the findings of a case study of spreadsheet use in a
higher education institution in the UK. The paper considers the use of
spreadsheets in two units of the organisation, academic registry and finance.
Spreadsheet use is explored in terms of importance, training, experience,
purpose, techniques deployed, size of spreadsheets created and sharing of
spreadsheets. The implications of the results are then considered in terms of
accurate reporting to external funding bodies such the funding councils,
internal data integrity and internal data efficiencies. The results show a
large volume of spreadsheets being created and used, that the profile of
spreadsheet developers is typical of other studies of spreadsheet use and the
need for the organisation to have clear principles and guidelines for the
development of spreadsheet models in the organisation to ensure data integrity,
reduce duplication of effort and to optimise the use of spreadsheets to meet
the institutions goals.

</details>


### [111] [Computer Simulation of Neural Networks Using Spreadsheets: The Dawn of the Age of Camelot](https://arxiv.org/abs/1807.00018)
*Serhiy O. Semerikov,Illia O. Teplytskyi,Yuliia V. Yechkalo,Arnold E. Kiv*

Main category: cs.CY

TL;DR: 该文章探讨了在电子表格环境中进行神经网络计算机模拟的培训方法，回顾了相关方法、计算神经科学的历史背景，并指出了掌握相关技能的关键模型。


<details>
  <summary>Details</summary>
Motivation: 有必要开发在电子表格环境中进行神经网络计算机模拟的培训方法。

Method: 作者对电子表格在模拟人工神经网络中的应用进行了系统回顾，并区分了解决电子表格环境中网络计算机模拟培训问题的基本方法。通过分析1890-1950年的文献，研究确定了《数学生物物理学公报》、其创始人Nicolas Rashevsky及围绕该期刊的科学界在创建和发展计算神经科学模型和方法中的作用。同时，确定了创建神经网络的心理物理学基础、神经计算的数学基础和神经工程方法，并讨论了Walter Pitts在结合描述性和定量训练理论中的作用。

Result: 文章区分了在电子表格环境中解决网络计算机模拟培训问题的基本方法。确定了《数学生物物理学公报》、Nicolas Rashevsky及其科学界在计算神经科学发展中的作用。识别了创建神经网络的心理物理学基础、数学基础和神经工程方法。讨论了Walter Pitts的角色。结果表明，要在电子表格环境中掌握神经模拟能力，应掌握基于历史和遗传方法的模型。指出了三组有前景的模型：Rashevsky的连续两因素模型、McCulloch和Pitts的离散模型以及Householder和Landahl的离散连续模型。

Conclusion: 要在电子表格环境中掌握神经模拟能力，必须掌握基于历史和遗传方法论的模型，其中Rashevsky、McCulloch-Pitts以及Householder和Landahl的三组特定模型在方法开发方面前景广阔。文章还强调了计算神经科学的历史发展和各种基于电子表格的模拟方法。

Abstract: The article substantiates the necessity to develop training methods of
computer simulation of neural networks in the spreadsheet environment. The
systematic review of their application to simulating artificial neural networks
is performed. The authors distinguish basic approaches to solving the problem
of network computer simulation training in the spreadsheet environment, joint
application of spreadsheets and tools of neural network simulation, application
of third-party add-ins to spreadsheets, development of macros using the
embedded languages of spreadsheets; use of standard spreadsheet add-ins for
non-linear optimization, creation of neural networks in the spreadsheet
environment without add-ins and macros. After analyzing a collection of
writings of 1890-1950, the research determines the role of the scientific
journal "Bulletin of Mathematical Biophysics", its founder Nicolas Rashevsky
and the scientific community around the journal in creating and developing
models and methods of computational neuroscience. There are identified
psychophysical basics of creating neural networks, mathematical foundations of
neural computing and methods of neuroengineering (image recognition, in
particular). The role of Walter Pitts in combining the descriptive and
quantitative theories of training is discussed. It is shown that to acquire
neural simulation competences in the spreadsheet environment, one should master
the models based on the historical and genetic approach. It is indicated that
there are three groups of models, which are promising in terms of developing
corresponding methods - the continuous two-factor model of Rashevsky, the
discrete model of McCulloch and Pitts, and the discrete-continuous models of
Householder and Landahl.

</details>


### [112] [Edu-Edition Spreadsheet Competency Framework](https://arxiv.org/abs/1802.00496)
*Maria Csernoch,Piroska Biró*

Main category: cs.CY

TL;DR: 本文介绍了电子表格能力框架的教育版（E2SCF），旨在尽早培养学生的电子表格能力，强调数学能力、计算机支持的实际问题解决、双向知识传递、数据和错误分析以及编程方面。


<details>
  <summary>Details</summary>
Motivation: 财务专业人士的电子表格能力培养应尽早从教育阶段开始，并得到专家教师的支持，以提高效率。

Method: E2SCF的核心特点是“高数学能力的计算机辅助真实世界问题解决”。该方法从培训初期就基于双向知识传递、数据和错误分析与处理以及电子表格的编程方面。

Result: E2SCF旨在帮助基础和普通用户建立扎实的电子表格知识，并培养可迁移的问题解决技能和能力。

Conclusion: E2SCF是一个教育框架，旨在通过早期教育和全面的方法（包括专家支持、数学能力、真实世界问题解决和编程）来培养强大的电子表格技能和可迁移的问题解决能力。

Abstract: Based on the Spreadsheet Competency Framework for finance professionals, in
the present paper we introduce the Edu-Edition of the Spreadsheet Competency
Framework (E2SCF). We claim that building spreadsheet competences should start
in education, as early as possible, and this process is a lot more effective if
support arrives from expert teachers. The main feature of E2SCF is high
mathability computer-supported real world problem solving. This approach is
based on - from the very beginning of training - a two-directional knowledge
transfer, data and error analysis and handling, and the programming aspect of
spreadsheets. Based on these features, E2SCF is set up for basic and general
users to build up firm spreadsheet knowledge and to develop transferable
problem solving skills and competences.

</details>


### [113] [The Future of Spreadsheets in the Big Data Era](https://arxiv.org/abs/1801.10231)
*David Birch,David Lyford-Smith,Yike Guo*

Main category: cs.CY

TL;DR: 这篇论文探讨了电子表格技术在过去三十年中的成功，分析了大数据、终端用户计算和移动计算等因素如何推动其演变，并记录了行业专家对电子表格未来方向、变化趋势以及对用户影响的看法，最后提出了进一步研究的关键方向。


<details>
  <summary>Details</summary>
Motivation: 尽管电子表格在过去三十年间取得了巨大成功且应用广泛，但其核心技术基本未变。随着大数据、终端用户计算和移动计算的兴起，有必要探讨电子表格技术的未来发展方向。

Method: 本文记录了一个研讨会的观点，该研讨会汇集了学术界和工业界人士，旨在探讨电子表格技术的未来方向及其对用户的影响。

Result: 论文记录了参与者关于电子表格成功的原因、驱动变化的趋势、可能的变革方向的看法，并提出了电子表格演进和使用方面的关键研究方向，以及这些趋势对终端用户的影响。

Conclusion: 论文总结了电子表格技术未来演变和使用的主要研究方向，并强调了这些趋势对终端用户的重要意义。

Abstract: The humble spreadsheet is the most widely used data storage, manipulation and
modelling tool. Its ubiquity over the past 30 years has seen its successful
application in every area of life. Surprisingly the spreadsheet has remained
fundamentally unchanged over the past three decades. As spreadsheet technology
enters its 4th decade a number of drivers of change are beginning to impact
upon the spreadsheet. The rise of Big Data, increased end-user computing and
mobile computing will undoubtedly increasingly shape the evolution and use of
spreadsheet technology.
  To explore the future of spreadsheet technology a workshop was convened with
the aim of "bringing together academia and industry to examine the future
direction of spreadsheet technology and the consequences for users". This paper
records the views of the participants on the reasons for the success of the
spreadsheet, the trends driving change and the likely directions of change for
the spreadsheet. We then set out key directions for further research in the
evolution and use of spreadsheets. Finally we look at the implications of these
trends for the end users who after all are the reason for the remarkable
success of the spreadsheet.

</details>


### [114] [The Role of Spreadsheets in Clinical Decision Support: A Survey of the Medical Algorithms Company User Community](https://arxiv.org/abs/1801.07782)
*Simon Thorne*

Main category: cs.CY

TL;DR: 本文对来自Medical Algorithms Company网站的临床决策支持系统（CDSS）用户进行了小型范围调查，以了解CDSS对临床实践的影响。


<details>
  <summary>Details</summary>
Motivation: 了解CDSS如何影响临床实践。

Method: 对Medical Algorithms Company网站的CDSS用户进行了小型范围调查，分析并讨论了结果，并与其他类似研究进行了比较。

Result: Medical Algorithms Company提供的CDSS被临床专业人员在各种环境中用作操作工具以及研究和参考工具。这些工具的初始逻辑是在电子表格上制定，然后才在数据库中实现。

Conclusion: 本文描述了CDSS的逻辑开发过程，并检查了部分调查结果，有助于更广泛地理解CDSS对临床实践的影响。

Abstract: This paper presents and discusses the results of a small scoping survey of
Clinical Decision Support System (CDSS) users from the Medical Algorithms
Company website which hosts 24,000 different CDSS. These results are analysed,
discussed, and compared with other similar studies and contribute to the wider
understanding of how CDSS impact on clinical practice. The results show that
CDSS provided by Medal are being used by clinical professionals in a variety of
settings, both as an operational tool and as a research and reference tool.
Whilst these tools are implemented and executed in a database, the initial
logic is worked out on a spreadsheet. The paper describes that process and
examines some of the results of the survey.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [115] [Mathematics of Digital Hyperspace](https://arxiv.org/abs/2103.15203)
*Jeremy Kepner,Timothy Davis,Vijay Gadepally,Hayden Jananthan,Lauren Milechin*

Main category: cs.MS

TL;DR: 这篇论文提出了一个新颖的数学概念“半链接”（semilink），它结合了成对的半环，旨在增强GraphBLAS标准，使其能够更有效地处理数字超空间中的非结构化数据。通过引入半链接和基于键的索引，GraphBLAS可以成为一种更丰富的关联数组代数，并替代电子表格、数据库表和以数据为中心的操作系统，从而改善非结构化数据的导航。


<details>
  <summary>Details</summary>
Motivation: 当前的数字环境充斥着大量的非结构化数据，这些数据形成了“数字超空间”，其无定形的数据流挑战了标准的类型和维度概念。现有的工具难以有效地处理这些非结构化数据。

Method: 论文探索了一种名为“半链接”的新颖数学概念，它通过结合成对的半环来提供图分析、数据库操作和机器学习所需的基本运算。作者建议将半链接与基于键的索引（例如指向字符串的指针）添加到GraphBLAS标准中。

Result: 半链接能够为图分析、数据库操作和机器学习提供必要的运算。通过添加基于键的索引和半链接，GraphBLAS可以成为一种更丰富的关联数组代数。

Conclusion: 增强后的GraphBLAS（结合了半链接和基于键的索引）可以作为电子表格、数据库表和以数据为中心的操作系统的一个即插即用替代品，从而大大提升在数字超空间中导航非结构化数据的能力。

Abstract: Social media, e-commerce, streaming video, e-mail, cloud documents, web
pages, traffic flows, and network packets fill vast digital lakes, rivers, and
oceans that we each navigate daily. This digital hyperspace is an amorphous
flow of data supported by continuous streams that stretch standard concepts of
type and dimension. The unstructured data of digital hyperspace can be
elegantly represented, traversed, and transformed via the mathematics of
hypergraphs, hypersparse matrices, and associative array algebra. This paper
explores a novel mathematical concept, the semilink, that combines pairs of
semirings to provide the essential operations for graph analytics, database
operations, and machine learning. The GraphBLAS standard currently supports
hypergraphs, hypersparse matrices, the mathematics required for semilinks, and
seamlessly performs graph, network, and matrix operations. With the addition of
key based indices (such as pointers to strings) and semilinks, GraphBLAS can
become a richer associative array algebra and be a plug-in replacement for
spreadsheets, database tables, and data centric operating systems, enhancing
the navigation of unstructured data found in digital hyperspace.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [116] [Is spreadsheet syntax better than numeric indexing for cell selection?](https://arxiv.org/abs/2505.23296)
*Philip Heltweg,Dirk Riehle,Georg-Daniel Schwarz*

Main category: cs.PL

TL;DR: 该论文比较了数据工程中单元格选择的数字索引和电子表格式语法，发现电子表格式语法能减少错误并加快编写速度。


<details>
  <summary>Details</summary>
Motivation: 数据工程中选择单元格是常见任务，存在多种表达方式（如数字索引和电子表格式语法），本文旨在比较这些方法的有效性。

Method: 进行了一项大规模的对照实验，学生参与者作为数据从业者的代表，比较了数字索引和电子表格式语法在代码阅读和编写的速度及正确性方面的表现。

Result: 结果显示，在阅读代码时，参与者使用电子表格式语法犯的错误更少。在编写代码时，与数字语法相比，使用电子表格语法犯的错误更少且速度更快。

Conclusion: 领域特定语法，如数据工程中的电子表格语法，对于未来支持没有软件工程背景的从业者的工具来说，是一种值得探索的有前景的替代方案。

Abstract: Selecting a subset of cells is a common task in data engineering, for
example, to remove errors or select only specific parts of a table. Multiple
approaches to express this selection exist. One option is numeric indexing,
commonly found in general programming languages, where a tuple of numbers
identifies the cell. Alternatively, the separate dimensions can be referred to
using different enumeration schemes like "A1" for the first cell, commonly
found in software such as spreadsheet systems.
  In a large-scale controlled experiment with student participants as proxy for
data practitioners, we compare the two options with respect to speed and
correctness of reading and writing code.
  The results show that, when reading code, participants make less mistakes
using spreadsheet-style syntax. Additionally, when writing code, they make
fewer mistakes and are faster when using spreadsheet syntax compared to numeric
syntax.
  From this, a domain-specific syntax, such as spreadsheet syntax for data
engineering, appears to be a promising alternative to explore in future tools
to support practitioners without a software engineering background.

</details>


### [117] [Solving Data-centric Tasks using Large Language Models](https://arxiv.org/abs/2402.11734)
*Shraddha Barke,Christian Poelitz,Carina Suzana Negreanu,Benjamin Zorn,José Cambronero,Andrew D. Gordon,Vu Le,Elnaz Nouri,Nadia Polikarpova,Advait Sarkar,Brian Slininger,Neil Toronto,Jack Williams*

Main category: cs.PL

TL;DR: 大型语言模型正在取代StackOverflow等帮助论坛，特别对非专业程序员和终端用户处理数据中心任务有帮助。然而，仅靠自然语言描述难以解决此类任务，需要包含数据。本文提出了一个真实世界NL-to-代码任务的数据集，并引入了一种“先聚类后选择”的提示技术，通过添加最具代表性的数据行来提高LLM在处理表格数据时的性能，尤其是在输入表格语法变化大时优于随机选择。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在取代StackOverflow等帮助论坛方面表现出色，尤其对非专业程序员和终端用户处理数据中心任务（如电子表格操作和数据整理）非常有用。然而，如果意图仅通过自然语言描述而不包含数据，这些任务很难解决。因此，关键问题是如何决定在提示中包含多少数据以及哪些数据。

Method: 1. 创建一个从StackOverflow帖子中挖掘的真实世界NL-to-代码任务数据集，该数据集涉及表格数据操作。
2. 引入一种“先聚类后选择”（cluster-then-select）的提示技术，该技术将输入数据中最具代表性的行添加到LLM的提示中。

Result: 1. 实验表明，LLM的性能确实对提示中传递的数据量敏感。
2. 对于输入表格中存在大量句法变化的任务，本文提出的“先聚类后选择”技术优于随机选择基线。

Conclusion: 本研究通过创建数据集和提出“先聚类后选择”的提示技术，有效地解决了在数据中心任务中LLM提示如何选择数据的问题，并证明了该技术在提高LLM性能方面的有效性，尤其是在处理具有复杂语法变化的表格数据时。

Abstract: Large language models (LLMs) are rapidly replacing help forums like
StackOverflow, and are especially helpful for non-professional programmers and
end users. These users are often interested in data-centric tasks, such as
spreadsheet manipulation and data wrangling, which are hard to solve if the
intent is only communicated using a natural-language description, without
including the data. But how do we decide how much data and which data to
include in the prompt? This paper makes two contributions towards answering
this question. First, we create a dataset of real-world NL-to-code tasks
manipulating tabular data, mined from StackOverflow posts. Second, we introduce
a cluster-then-select prompting technique, which adds the most representative
rows from the input data to the LLM prompt. Our experiments show that LLM
performance is indeed sensitive to the amount of data passed in the prompt, and
that for tasks with a lot of syntactic variation in the input table, our
cluster-then-select technique outperforms a random selection baseline.

</details>


### [118] [FLAME: A small language model for spreadsheet formulas](https://arxiv.org/abs/2301.13779)
*Harshit Joshi,Abishai Ebenezer,José Cambronero,Sumit Gulwani,Aditya Kanade,Vu Le,Ivan Radiček,Gust Verbruggen*

Main category: cs.PL

TL;DR: FLAME是一个专门针对Excel公式训练的transformer模型，它通过利用领域洞察，在显著更小的模型尺寸（60M参数）和更少的数据量下，超越了大型通用模型在公式修复、补全和检索任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在电子表格公式编写辅助方面存在训练成本高昂和部署困难的问题，因为它们模型庞大（数十亿参数）。

Method: 该研究提出了FLAME模型，一个专门针对Excel公式训练的transformer模型。它通过草图去重（sketch deduplication）来整理训练数据集，引入了Excel专用的公式分词器，并使用领域特定的掩码跨度预测和噪声自编码作为预训练目标。

Result: 在公式修复和补全任务中，FLAME在14项评估设置中的10项表现优于Davinci (175B) 和 Cushman (12B) 版本的Codex以及CodeT5 (220M) 等更大的模型。在公式检索方面，FLAME也优于CodeT5、CodeBERT和GraphCodeBERT。

Conclusion: 通过利用领域洞察，FLAME模型在保持竞争性性能的同时，显著减小了模型规模并减少了训练数据量，解决了大型语言模型在电子表格公式辅助方面的挑战。

Abstract: Spreadsheets are a vital tool for end-user data management. Using large
language models for formula authoring assistance in these environments can be
difficult, as these models are expensive to train and challenging to deploy due
to their size (up to billions of parameters). We present FLAME, a
transformer-based model trained exclusively on Excel formulas that leverages
domain insights to achieve competitive performance while being substantially
smaller (60M parameters) and training on two orders of magnitude less data. We
curate a training dataset using sketch deduplication, introduce an
Excel-specific formula tokenizer, and use domain-specific versions of masked
span prediction and noisy auto-encoding as pre-training objectives. We evaluate
FLAME on formula repair, formula completion, and similarity-based formula
retrieval. FLAME can outperform much larger models, such as the Davinci (175B)
and Cushman (12B) variants of Codex and CodeT5 (220M), in 10 of 14 evaluation
settings for the repair and completion tasks. For formula retrieval, FLAME
outperforms CodeT5, CodeBERT, and GraphCodeBERT.

</details>


### [119] [Enhanced Spreadsheet Computing with Finite-Domain Constraint Satisfaction](https://arxiv.org/abs/2203.16346)
*Ezana N. Beyenne,Hai-Feng Guo*

Main category: cs.PL

TL;DR: 这篇论文扩展了电子表格的计算范式，使其能够解决约束满足问题，通过在一个可视化环境中支持有限域约束求解，并构建了一种电子表格特定的约束语言，从而简化了许多基于约束的应用的开发。


<details>
  <summary>Details</summary>
Motivation: 电子表格是现代社会中最广泛使用的计算工具之一，但由于其单向数据流，主要局限于簿记类应用，无法很好地解决约束满足问题。

Method: 论文提出了一种增强的电子表格系统，该系统在一个可视化环境中良好地支持有限域约束求解。此外，还构建了一种电子表格特定的约束语言，供普通用户以声明式和可扩展的方式指定数据单元格之间的约束。

Result: 新的电子表格系统显著简化了使用可视化表格界面开发许多基于约束的应用。文中提供了示例以说明扩展电子表格范式的可用性和实用性。

Conclusion: 通过扩展电子表格范式以解决约束满足问题，本研究显著提升了电子表格在更广泛应用领域的可用性和实用性。

Abstract: The spreadsheet application is among the most widely used computing tools in
modern society. It provides excellent usability and usefulness, and it easily
enables a non-programmer to perform programming-like tasks in a visual tabular
"pen and paper" approach. However, spreadsheets are mostly limited to
bookkeeping-like applications due to their mono-directional data flow. This
paper shows how the spreadsheet computing paradigm is extended to break this
limitation for solving constraint satisfaction problems. We present an enhanced
spreadsheet system where finite-domain constraint solving is well supported in
a visual environment. Furthermore, a spreadsheet-specific constraint language
is constructed for general users to specify constraints among data cells in a
declarative and scalable way. The new spreadsheet system significantly
simplifies the development of many constraint-based applications using a visual
tabular interface. Examples are given to illustrate the usability and
usefulness of the extended spreadsheet paradigm.
  KEYWORDS: Spreadsheet computing, Finite-domain constraint satisfaction,
Constraint logic programming

</details>


### [120] [Typed Image-based Programming with Structure Editing](https://arxiv.org/abs/2110.08993)
*Jonathan Edwards,Tomas Petricek*

Main category: cs.PL

TL;DR: 该论文提出通过类型和结构编辑在基于图像的编程中实现协作，主要关注持久数据的模式变更，并利用静态类型和结构编辑来捕获和适应这些变更。其主要技术贡献是实现结构编辑的版本控制理论。


<details>
  <summary>Details</summary>
Motivation: 基于图像的编程系统（如Smalltalk）在简化方面有优势，但由于缺乏协作和部署支持，其商业成功受限。本文旨在解决这一问题。

Method: 论文提出通过类型和结构编辑在基于图像的编程中实现协作。它专注于持久数据模式变更的问题，利用静态类型来表达和执行这些变更，并通过结构编辑来捕获类型定义的变化，从而自动调整数据。主要技术贡献是提出了一个实现结构编辑版本控制的理论。

Result: 论文提出了一个实现结构编辑版本控制的理论，主要聚焦于类型编辑。

Conclusion: 如果这种方法能够扩展到整个编程体验，它将为基于图像的编程中的协作提供一种新方式。

Abstract: Many beloved programming systems are image-based: self-contained worlds that
persist both code and data in a single file. Examples include Smalltalk, LISP,
HyperCard, Flash, and spreadsheets. Image-based programming avoids much of the
complexity of modern programming technology stacks and encourages more casual
and exploratory programming. However conventional file-based programming has
better support for collaboration and deployment. These problems have been
blamed for the limited commercial success of Smalltalk. We propose to enable
collaboration in image-based programming via types and structure editing.
  We focus on the problem of schema change on persistent data. We turn to
static types, which paradoxically require more schema change but also provide a
mechanism to express and execute those changes. To determine those changes we
turn to structure editing, so that we can capture changes in type definitions
with sufficient fidelity to automatically adapt the data to suit. We conjecture
that typical schema changes can be handled through structure editing of static
types.
  That positions us to tackle collaboration with what could be called version
control for structure editing. We present a theory realizing this idea, which
is our main technical contribution. While we focus here on editing types, if we
can extend the approach to cover the entire programming experience then it
would offer a new way to collaborate in image-based programming.

</details>


### [121] [ExceLint: Automatically Finding Spreadsheet Formula Errors](https://arxiv.org/abs/1901.11100)
*Daniel W. Barowy,Emery D. Berger,Benjamin Zorn*

Main category: cs.PL

TL;DR: 该论文介绍了ExceLint，一个用于Excel的静态分析工具，它利用电子表格的矩形特性和信息论来发现公式错误。实验证明ExceLint快速有效，并且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 电子表格是广泛使用的编程环境，尤其在金融等领域，错误可能导致灾难性后果，因此需要找到电子表格公式中的错误。

Method: 论文提出了一种专门用于发现电子表格公式错误的静态分析方法。该方法直接利用了电子表格的矩形特性，并采用信息论方法来识别对附近矩形区域造成“意外”干扰的公式。将此方法实现为Microsoft Excel的ExceLint工具。

Result: ExceLint快速有效：在包含70个电子表格的语料库中，ExceLint平均每个电子表格耗时5秒，并且其性能显著优于最先进的分析方法。

Conclusion: ExceLint为查找电子表格中的公式错误提供了一种有效且高效的解决方案，相对于现有方法有了显著改进。

Abstract: Spreadsheets are one of the most widely used programming environments, and
are widely deployed in domains like finance where errors can have catastrophic
consequences. We present a static analysis specifically designed to find
spreadsheet formula errors. Our analysis directly leverages the rectangular
character of spreadsheets. It uses an information-theoretic approach to
identify formulas that are especially surprising disruptions to nearby
rectangular regions. We present ExceLint, an implementation of our static
analysis for Microsoft Excel. We demonstrate that ExceLint is fast and
effective: across a corpus of 70 spreadsheets, ExceLint takes a median of 5
seconds per spreadsheet, and it significantly outperforms the state of the art
analysis.

</details>


### [122] [Synthesizing Bijective Lenses](https://arxiv.org/abs/1710.03248)
*Anders Miltner,Kathleen Fisher,Benjamin C. Pierce,David Walker,Steve Zdancewic*

Main category: cs.PL

TL;DR: Optician是一款工具，它通过结合正则表达式和少量具体示例，自动合成双向字符串转换器，解决了手动编写或使用领域特定语言的繁琐和错误。


<details>
  <summary>Details</summary>
Motivation: 在现代软件系统中，双向数据转换（如序列化器、数据库视图）频繁出现。手动构建这些转换既繁琐又容易出错。虽然领域特定语言可以解决此问题，但它们通常难以编程，需要程序员管理复杂的类型系统中的细节。

Method: 本文提出了Optician工具，它通过接收两个代表数据格式的普通正则表达式和少量示例作为输入，合成一个在Boomerang（一种基于透镜理论的双向语言）中良好类型的程序。该方法克服了巨大的程序搜索空间，并在具有丰富类型等价关系（正则表达式理论）的语言上下文中操作。它合成等价语言的术语并将其转换为透镜语言，并证明了合成算法的正确性。

Result: Optician将合成问题从难以处理的解决方案转变为高效的解决方案。该工具在包含39个示例的基准测试套件上进行了评估，其中包括来自Flash Fill和Augeas等其他数据管理系统的实际示例，并证明了其有效性。

Conclusion: Optician提供了一种高效且用户友好的方式来自动合成双向字符串转换器，显著减少了手动工作的复杂性和错误率，并提高了编程效率。

Abstract: Bidirectional transformations between different data representations occur
frequently in modern software systems. They appear as serializers and
deserializers, as database views and view updaters, and more. Manually building
bidirectional transformations---by writing two separate functions that are
intended to be inverses---is tedious and error prone. A better approach is to
use a domain-specific language in which both directions can be written as a
single expression. However, these domain-specific languages can be difficult to
program in, requiring programmers to manage fiddly details while working in a
complex type system.
  To solve this, we present Optician, a tool for type-directed synthesis of
bijective string transformers. The inputs to Optician are two ordinary regular
expressions representing two data formats and a few concrete examples for
disambiguation. The output is a well-typed program in Boomerang (a
bidirectional language based on the theory of lenses). The main technical
challenge involves navigating the vast program search space efficiently enough.
Unlike most prior work on type-directed synthesis, our system operates in the
context of a language with a rich equivalence relation on types (the theory of
regular expressions). We synthesize terms of a equivalent language and convert
those generated terms into our lens language. We prove the correctness of our
synthesis algorithm. We also demonstrate empirically that our new language
changes the synthesis problem from one that admits intractable solutions to one
that admits highly efficient solutions. We evaluate Optician on a benchmark
suite of 39 examples including both microbenchmarks and realistic examples
derived from other data management systems including Flash Fill, a tool for
synthesizing string transformations in spreadsheets, and Augeas, a tool for
bidirectional processing of Linux system configuration files.

</details>


### [123] [Active Learning of Input Grammars](https://arxiv.org/abs/1708.08731)
*Matthias Höschele,Alexander Kampmann,Andreas Zeller*

Main category: cs.PL

TL;DR: 该论文提出 AUTOGRAM 原型，通过少量样本输入推断程序输入的上下文无关文法，这些文法准确且可读，可直接用于测试生成器进行全面的自动化测试。


<details>
  <summary>Details</summary>
Motivation: 精确了解程序输入的格式是系统测试的必要前提。

Method: 1. 跟踪输入的数据流，将共享相同数据流的输入片段聚合成词法和句法实体；2. 根据相关的变量和函数标识符为这些实体命名；3. 通过成员查询系统地泛化生成规则。

Result: 只需最少的样本输入，即可获得反映有效输入结构的人类可读的上下文无关文法。AUTOGRAM 原型在 URL、电子表格或配置文件等输入上获得的输入文法既准确又非常易读，并且可以直接输入到测试生成器中进行全面的自动化测试。

Conclusion: AUTOGRAM 原型能够从最少的样本中推断出准确且可读的输入文法，从而促进全面的自动化测试。

Abstract: Knowing the precise format of a program's input is a necessary prerequisite
for systematic testing. Given a program and a small set of sample inputs, we
(1) track the data flow of inputs to aggregate input fragments that share the
same data flow through program execution into lexical and syntactic entities;
(2) assign these entities names that are based on the associated variable and
function identifiers; and (3) systematically generalize production rules by
means of membership queries. As a result, we need only a minimal set of sample
inputs to obtain human-readable context-free grammars that reflect valid input
structure. In our evaluation on inputs like URLs, spreadsheets, or
configuration files, our AUTOGRAM prototype obtains input grammars that are
both accurate and very readable - and that can be directly fed into test
generators for comprehensive automated testing.

</details>


### [124] [Synthesis of Data Completion Scripts using Finite Tree Automata](https://arxiv.org/abs/1707.01469)
*Xinyu Wang,Isil Dillig,Rishabh Singh*

Main category: cs.PL

TL;DR: 本文提出了一种基于PBE和轻量级草图的综合技术，用于自动化表格数据中的数据补全任务，通过结合空间和关系推理的领域特定语言以及基于有限树自动机（FTA）的新型版本空间学习算法，能够从输入-输出示例中合成程序。


<details>
  <summary>Details</summary>
Motivation: 在表格数据应用中，数据补全（如缺失值插补或派生数据计算）是一项常见任务，但终端用户和数据科学家通常因缺乏编程专业知识而难以完成。

Method: 提出了一种使用编程示例（PBE）和轻量级草图的综合技术，通过一个领域特定语言（DSL）结合空间和关系推理，并利用基于有限树自动机（FTA）的新型版本空间学习算法，从输入-输出示例中合成程序。

Result: 将所提出的方法实现为一个名为DACE的工具，并在来自在线帮助论坛的84个基准测试上进行了评估。通过与现有合成器（PROSE和SKETCH）的比较，展示了该方法的优势。

Conclusion: 基于FTA的版本空间学习算法能带来更紧凑的表示，并实现示例一致程序之间更好的共享，从而有效自动化数据补全任务。

Abstract: In application domains that store data in a tabular format, a common task is
to fill the values of some cells using values stored in other cells. For
instance, such data completion tasks arise in the context of missing value
imputation in data science and derived data computation in spreadsheets and
relational databases. Unfortunately, end-users and data scientists typically
struggle with many data completion tasks that require non-trivial programming
expertise. This paper presents a synthesis technique for automating data
completion tasks using programming-by-example (PBE) and a very lightweight
sketching approach. Given a formula sketch (e.g., AVG($?_1$, $?_2$)) and a few
input-output examples for each hole, our technique synthesizes a program to
automate the desired data completion task. Towards this goal, we propose a
domain-specific language (DSL) that combines spatial and relational reasoning
over tabular data and a novel synthesis algorithm that can generate DSL
programs that are consistent with the input-output examples. The key technical
novelty of our approach is a new version space learning algorithm that is based
on finite tree automata (FTA). The use of FTAs in the learning algorithm leads
to a more compact representation that allows more sharing between programs that
are consistent with the examples. We have implemented the proposed approach in
a tool called DACE and evaluate it on 84 benchmarks taken from online help
forums. We also illustrate the advantages of our approach by comparing our
technique against two existing synthesizers, namely PROSE and SKETCH.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [125] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: 当前AI主要关注像素、词汇和音素，但实际世界和企业最有价值的数据是关系型的。这篇论文解释了关系学习未能普及的原因，并探讨了如何使其获得应有的地位。


<details>
  <summary>Details</summary>
Motivation: 人工智能系统主要建模像素、词汇和音素，但世界实际上由具有属性和关系实体构成。企业中最有价值的数据通常是电子表格和数据库等关系型格式，而非传统的感知数据。尽管存在关系学习领域，但它尚未普及，这构成了本文探讨的动机。

Method: 本文通过分析解释了关系学习未能普及的原因（除了少数受限情况），并提出了使其达到应有地位所需采取的措施。

Result: 关系学习目前并未普及全球，仅在少数关系受限的情况下有所应用。

Conclusion: 需要采取具体措施来提升关系学习的地位，使其在人工智能领域发挥应有的作用。

Abstract: Artificial intelligence seems to be taking over the world with systems that
model pixels, words, and phonemes. The world is arguably made up, not of
pixels, words, and phonemes but of entities (objects, things, including events)
with properties and relations among them. Surely we should model these, not the
perception or description of them. You might suspect that concentrating on
modeling words and pixels is because all of the (valuable) data in the world is
in terms of text and images. If you look into almost any company you will find
their most valuable data is in spreadsheets, databases and other relational
formats. These are not the form that are studied in introductory machine
learning, but are full of product numbers, student numbers, transaction numbers
and other identifiers that can't be interpreted naively as numbers. The field
that studies this sort of data has various names including relational learning,
statistical relational AI, and many others. This paper explains why relational
learning is not taking over the world -- except in a few cases with restricted
relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


### [126] [MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.05587)
*Junjie Xing,Yeye He,Mengyu Zhou,Haoyu Dong,Shi Han,Lingjiao Chen,Dongmei Zhang,Surajit Chaudhuri,H. V. Jagadish*

Main category: cs.AI

TL;DR: MMTU 是一个包含 30K 问题和 25 个真实世界表格任务的大规模基准测试，旨在评估模型理解、推理和操作专业级表格的能力，结果显示当前领先模型仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 表格在许多真实世界应用中扮演关键角色，但传统上需要专业用户操作。尽管大型语言模型在处理表格方面取得了进展，但相关能力的全面基准测试仍然有限，现有评估范围狭窄，未能覆盖专业用户面临的广泛实际任务，这限制了该领域的理解和模型发展。

Method: 引入 MMTU，一个大规模基准测试，包含超过 30K 个问题，涵盖 25 个真实世界的表格任务。这些任务取材于数十年的表格数据计算机科学研究，专注于专业用户面临的复杂表格任务，旨在全面评估模型在专家级水平上理解、推理和操作真实表格的能力。

Result: MMTU 要求模型具备表格理解、推理和编码等综合技能，这对当前的领先模型（如 OpenAI o4-mini 和 DeepSeek R1）仍然具有挑战性，它们的得分仅在 60% 左右，表明有显著的改进空间。

Conclusion: MMTU 在评估中揭示了关键发现，并有望推动结构化数据处理和分析基础模型的理解和开发方面的进一步进展。

Abstract: Tables and table-based use cases play a crucial role in many important
real-world applications, such as spreadsheets, databases, and computational
notebooks, which traditionally require expert-level users like data engineers,
data analysts, and database administrators to operate. Although LLMs have shown
remarkable progress in working with tables (e.g., in spreadsheet and database
copilot scenarios), comprehensive benchmarking of such capabilities remains
limited. In contrast to an extensive and growing list of NLP benchmarks,
evaluations of table-related tasks are scarce, and narrowly focus on tasks like
NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks
that professional users face. This gap limits our understanding and model
progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 30K
questions across 25 real-world table tasks, designed to comprehensively
evaluate models ability to understand, reason, and manipulate real tables at
the expert-level. These tasks are drawn from decades' worth of computer science
research on tabular data, with a focus on complex table tasks faced by
professional users. We show that MMTU require a combination of skills --
including table understanding, reasoning, and coding -- that remain challenging
for today's frontier models, where even frontier reasoning models like OpenAI
o4-mini and DeepSeek R1 score only around 60%, suggesting significant room for
improvement. We highlight key findings in our evaluation using MMTU and hope
that this benchmark drives further advances in understanding and developing
foundation models for structured data processing and analysis. Our code and
data are available at https://github.com/MMTU-Benchmark/MMTU and
https://huggingface.co/datasets/MMTU-benchmark/MMTU.

</details>


### [127] [Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models](https://arxiv.org/abs/2505.23667)
*Lang Cao,Jingxian Xu,Hanbing Liu,Jinyu Wang,Mengyu Zhou,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种名为 Formula Tuning (Fortune) 的强化学习框架，用于训练大型语言模型 (LMs) 生成可执行的电子表格公式，以解决表格数据上的问答问题。该方法通过使用二元答案正确性作为奖励信号，减少了对监督公式标注的依赖，显著提高了LMs在多步数值和符号推理任务上的表现，甚至使7B模型超越了OpenAI o1。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理表格数据（尤其是在复杂场景下）的数值或符号推理方面仍然存在困难。电子表格公式作为一种强大且富有表达力的符号操作媒介，其所蕴含的丰富推理模式尚未得到充分利用。

Method: Formula Tuning (Fortune) 是一种强化学习 (RL) 框架，旨在训练LMs生成用于通用表格数据问答的可执行电子表格公式。它通过使用二元答案正确性作为奖励信号来学习公式推导，从而减少了对监督公式标注的依赖。

Result: Formula Tuning 显著增强了LMs的性能，特别是在多步数值和符号推理任务上。实验表明，该方法使一个7B模型在表格理解方面超越了OpenAI o1，并在七个表格推理基准上展示了其有效性。

Conclusion: 公式驱动的强化学习有潜力推动大型语言模型中符号表格推理的发展。

Abstract: Tables are a fundamental structure for organizing and analyzing data, making
effective table understanding a critical capability for intelligent systems.
While large language models (LMs) demonstrate strong general reasoning
abilities, they continue to struggle with accurate numerical or symbolic
reasoning over tabular data, especially in complex scenarios. Spreadsheet
formulas provide a powerful and expressive medium for representing executable
symbolic operations, encoding rich reasoning patterns that remain largely
underutilized. In this paper, we propose Formula Tuning (Fortune), a
reinforcement learning (RL) framework that trains LMs to generate executable
spreadsheet formulas for question answering over general tabular data. Formula
Tuning reduces the reliance on supervised formula annotations by using binary
answer correctness as a reward signal, guiding the model to learn formula
derivation through reasoning. We provide a theoretical analysis of its
advantages and demonstrate its effectiveness through extensive experiments on
seven table reasoning benchmarks. Formula Tuning substantially enhances LM
performance, particularly on multi-step numerical and symbolic reasoning tasks,
enabling a 7B model to outperform OpenAI o1 on table understanding. This
highlights the potential of formula-driven RL to advance symbolic table
reasoning in LMs.

</details>


### [128] [The AI Risk Repository: A Comprehensive Meta-Review, Database, and Taxonomy of Risks From Artificial Intelligence](https://arxiv.org/abs/2408.12622)
*Peter Slattery,Alexander K. Saeri,Emily A. C. Grundy,Jess Graham,Michael Noetel,Risto Uuk,James Dao,Soroush Pour,Stephen Casper,Neil Thompson*

Main category: cs.AI

TL;DR: 该论文创建了一个AI风险知识库，包含从43种分类法中提取的777种风险，并通过因果因素和风险领域进行分类，旨在为AI风险讨论和管理提供一个共同的参考框架。


<details>
  <summary>Details</summary>
Motivation: 当前对AI风险缺乏共识，阻碍了对AI风险的全面讨论、研究和应对。

Method: 通过对分类法和其他AI风险结构化分类进行系统审查，进行专家咨询，并采用最佳拟合框架合成法开发了两个整体分类法（因果分类法和领域分类法）。

Result: 建立了一个AI风险知识库，包含从43种分类法中提取的777种风险，并通过高级别因果分类法（实体、意图、时机）和中级别领域分类法（7个领域，23个子领域）进行分类，可通过网站和在线电子表格访问、修改和更新。

Conclusion: 该AI风险知识库是第一个严格整理、分析和提取AI风险框架的公开、全面、可扩展和分类的风险数据库，为更协调、连贯和完整的AI系统风险定义、审计和管理方法奠定了基础。

Abstract: The risks posed by Artificial Intelligence (AI) are of considerable concern
to academics, auditors, policymakers, AI companies, and the public. However, a
lack of shared understanding of AI risks can impede our ability to
comprehensively discuss, research, and react to them. This paper addresses this
gap by creating an AI Risk Repository to serve as a common frame of reference.
This comprises a living database of 777 risks extracted from 43 taxonomies,
which can be filtered based on two overarching taxonomies and easily accessed,
modified, and updated via our website and online spreadsheets. We construct our
Repository with a systematic review of taxonomies and other structured
classifications of AI risk followed by an expert consultation. We develop our
taxonomies of AI risk using a best-fit framework synthesis. Our high-level
Causal Taxonomy of AI Risks classifies each risk by its causal factors (1)
Entity: Human, AI; (2) Intentionality: Intentional, Unintentional; and (3)
Timing: Pre-deployment; Post-deployment. Our mid-level Domain Taxonomy of AI
Risks classifies risks into seven AI risk domains: (1) Discrimination &
toxicity, (2) Privacy & security, (3) Misinformation, (4) Malicious actors &
misuse, (5) Human-computer interaction, (6) Socioeconomic & environmental, and
(7) AI system safety, failures, & limitations. These are further divided into
23 subdomains. The AI Risk Repository is, to our knowledge, the first attempt
to rigorously curate, analyze, and extract AI risk frameworks into a publicly
accessible, comprehensive, extensible, and categorized risk database. This
creates a foundation for a more coordinated, coherent, and complete approach to
defining, auditing, and managing the risks posed by AI systems.

</details>


### [129] [SpreadsheetLLM: Encoding Spreadsheets for Large Language Models](https://arxiv.org/abs/2407.09025)
*Haoyu Dong,Jianbo Zhao,Yuzhang Tian,Junyu Xiong,Shiyu Xia,Mengyu Zhou,Yun Lin,José Cambronero,Yeye He,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: SpreadsheetLLM 引入了一种高效的编码方法，包括 SheetCompressor 框架和 Chain of Spreadsheet，以优化大型语言模型对电子表格的理解和推理能力，并在表格检测和问答任务中取得了显著的性能提升和压缩效果。


<details>
  <summary>Details</summary>
Motivation: 电子表格的二维网格、灵活布局和多样的格式对大型语言模型（LLMs）构成了重大挑战，限制了LLMs在电子表格上的理解和推理能力。

Method: 该论文提出了 SpreadsheetLLM，包含：1. 初始的简单序列化方法，结合单元格地址、值和格式。2. SheetCompressor 编码框架，通过结构锚点压缩、逆向索引转换和数据格式感知聚合三个模块有效压缩电子表格。3. 针对下游任务，提出了 Chain of Spreadsheet。

Result: SheetCompressor 在电子表格表格检测任务中，在 GPT4 的上下文学习设置下，性能比简单方法提高了25.6%。经过 SheetCompressor 微调的 LLM 平均压缩比达到25倍，并在 F1 分数上达到了78.9%的最新水平，超越现有最佳模型12.3%。SpreadsheetLLM 在各种电子表格任务中表现出高效性，并在新的电子表格问答任务中得到验证。

Conclusion: SpreadsheetLLM 通过其创新的编码方法（尤其是 SheetCompressor）和针对下游任务的 Chain of Spreadsheet，有效解决了 LLMs 处理电子表格的挑战，显著提升了其理解和推理能力，并在多项任务中取得了最先进的性能。

Abstract: Spreadsheets are characterized by their extensive two-dimensional grids,
flexible layouts, and varied formatting options, which pose significant
challenges for large language models (LLMs). In response, we introduce
SpreadsheetLLM, pioneering an efficient encoding method designed to unleash and
optimize LLMs' powerful understanding and reasoning capability on spreadsheets.
Initially, we propose a vanilla serialization approach that incorporates cell
addresses, values, and formats. However, this approach was limited by LLMs'
token constraints, making it impractical for most applications. To tackle this
challenge, we develop SheetCompressor, an innovative encoding framework that
compresses spreadsheets effectively for LLMs. It comprises three modules:
structural-anchor-based compression, inverse index translation, and
data-format-aware aggregation. It significantly improves performance in the
spreadsheet table detection task, outperforming the vanilla approach by 25.6%
in GPT4's in-context learning setting. Moreover, fine-tuned LLM with
SheetCompressor has an average compression ratio of 25 times, and achieves a
state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%.
Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet
understanding and validate it in a new and demanding spreadsheet QA task. We
methodically leverage the inherent layout and structure of spreadsheets,
demonstrating that SpreadsheetLLM is highly effective across a variety of
spreadsheet tasks.

</details>


### [130] [SheetAgent: Towards A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models](https://arxiv.org/abs/2403.03636)
*Yibin Chen,Yifu Yuan,Zeyu Zhang,Yan Zheng,Jinyi Liu,Fei Ni,Jianye Hao,Hangyu Mao,Fuzheng Zhang*

Main category: cs.AI

TL;DR: 该论文引入了SheetRM，一个用于复杂电子表格任务的基准，以及SheetAgent，一个基于LLM的自主代理，包含Planner、Informer和Retriever模块，并在电子表格操作中显示出显著的性能改进。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在电子表格操作方面尚未解决包含推理挑战的复杂和现实任务（例如，需要多步骤推理和模糊要求的长周期操作）。该研究旨在弥合与实际需求之间的差距。

Method: 提出SheetRM，一个针对长周期和多类别任务的基准，这些任务涉及由现实挑战引起的依赖推理的操作。提出了SheetAgent，一个自主代理，由Planner、Informer和Retriever三个协同模块组成，利用大型语言模型通过迭代任务推理和反思，在无需人工交互的情况下实现高级推理和准确的电子表格操作。

Result: SheetAgent在多个基准测试中比基线模型的通过率提高了20-40%，在电子表格操作中实现了更高的精度，并展示了卓越的表格推理能力。

Conclusion: SheetAgent显著提高了电子表格操作和表格推理能力，超越了基线模型，有效地解决了现实世界中复杂的电子表格任务。

Abstract: Spreadsheets are ubiquitous across the World Wide Web, playing a critical
role in enhancing work efficiency across various domains. Large language model
(LLM) has been recently attempted for automatic spreadsheet manipulation but
has not yet been investigated in complicated and realistic tasks where
reasoning challenges exist (e.g., long horizon manipulation with multi-step
reasoning and ambiguous requirements). To bridge the gap with the real-world
requirements, we introduce SheetRM, a benchmark featuring long-horizon and
multi-category tasks with reasoning-dependent manipulation caused by real-life
challenges. To mitigate the above challenges, we further propose SheetAgent, a
novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of
three collaborative modules: Planner, Informer, and Retriever, achieving both
advanced reasoning and accurate manipulation over spreadsheets without human
interaction through iterative task reasoning and reflection. Extensive
experiments demonstrate that SheetAgent delivers 20--40\% pass rate
improvements on multiple benchmarks over baselines, achieving enhanced
precision in spreadsheet manipulation and demonstrating superior table
reasoning abilities. More details and visualizations are available at the
project website: https://sheetagent.github.io/. The datasets and source code
are available at https://anonymous.4open.science/r/SheetAgent.

</details>


### [131] [Large Language Model for Table Processing: A Survey](https://arxiv.org/abs/2402.05121)
*Weizheng Lu,Jing Zhang,Ju Fan,Zihao Fu,Yueguo Chen,Xiaoyong Du*

Main category: cs.AI

TL;DR: 这篇综述全面概述了使用大型语言模型（LLMs）和视觉语言模型（VLMs）处理表格相关任务的现状，涵盖了从传统任务到新兴领域，并讨论了训练技术、提示工程及现有挑战。


<details>
  <summary>Details</summary>
Motivation: 自动化表格相关任务（如数据库查询、电子表格操作、网页表格问答、图像表格信息提取）具有显著的社会效益，并受到了学术界和工业界的广泛关注。

Method: 本文作为一篇综述，全面审视了表格相关任务的用户场景和技术方面。它涵盖了表格问答等传统任务以及电子表格操作和表格数据分析等新兴领域。文章总结了专门用于表格处理的LLMs和VLMs的训练技术，并讨论了提示工程（特别是LLM驱动的智能体）在各种表格任务中的应用。

Result: 本综述提供了表格相关任务的全面概览，包括用户场景和技术方面；总结了LLMs和VLMs在表格处理中的训练技术；探讨了提示工程（尤其是LLM驱动的智能体）的应用。

Conclusion: 文章指出了当前面临的几个挑战，包括服务时用户输入的多样性以及使用思维链时思考速度慢的问题。

Abstract: Tables, typically two-dimensional and structured to store large amounts of
data, are essential in daily activities like database queries, spreadsheet
manipulations, web table question answering, and image table information
extraction. Automating these table-centric tasks with Large Language Models
(LLMs) or Visual Language Models (VLMs) offers significant public benefits,
garnering interest from academia and industry. This survey provides a
comprehensive overview of table-related tasks, examining both user scenarios
and technical aspects. It covers traditional tasks like table question
answering as well as emerging fields such as spreadsheet manipulation and table
data analysis. We summarize the training techniques for LLMs and VLMs tailored
for table processing. Additionally, we discuss prompt engineering, particularly
the use of LLM-powered agents, for various table-related tasks. Finally, we
highlight several challenges, including diverse user input when serving and
slow thinking using chain-of-thought.

</details>


### [132] [FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language](https://arxiv.org/abs/2310.17306)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Elnaz Nouri,Mohammad Raza,Gust Verbruggen*

Main category: cs.AI

TL;DR: FormaT5是一个基于Transformer的模型，可以通过自然语言描述和目标表格生成条件格式（CF）规则，并通过预测占位符解决描述不足的问题，在CF任务上超越了其他神经方法。


<details>
  <summary>Details</summary>
Motivation: 电子表格软件允许用户通过编写数据相关的条件格式（CF）规则来自动格式化表格。然而，编写这些规则对于用户来说往往具有挑战性，因为它需要用户理解和实现底层逻辑。

Method: 我们提出了FormaT5，一个基于Transformer的模型，可以根据目标表格和所需格式逻辑的自然语言描述生成CF规则。为了解决描述不足和最小化参数错误的问题，FormaT5通过弃权目标学习预测占位符。这些占位符可以由第二个模型填充，或者当有应格式化行的示例时，由一个按示例编程系统填充。

Result: 我们创建了一个包含1053个CF任务的广泛基准来评估FormaT5，这些任务包含从四个不同来源收集的真实世界描述。弃权和填充使FormaT5在我们的基准测试中优于8种不同的神经方法，无论是否有示例。我们的结果说明了构建领域特定学习系统的价值。

Conclusion: FormaT5通过其独特的占位符预测和填充机制，能够有效地从不足的自然语言描述中生成条件格式规则，并在真实世界的任务中表现出色，证明了领域特定学习系统的有效性。

Abstract: Formatting is an important property in tables for visualization,
presentation, and analysis. Spreadsheet software allows users to automatically
format their tables by writing data-dependent conditional formatting (CF)
rules. Writing such rules is often challenging for users as it requires them to
understand and implement the underlying logic. We present FormaT5, a
transformer-based model that can generate a CF rule given the target table and
a natural language description of the desired formatting logic. We find that
user descriptions for these tasks are often under-specified or ambiguous,
making it harder for code generation systems to accurately learn the desired
rule in a single step. To tackle this problem of under-specification and
minimise argument errors, FormaT5 learns to predict placeholders though an
abstention objective. These placeholders can then be filled by a second model
or, when examples of rows that should be formatted are available, by a
programming-by-example system. To evaluate FormaT5 on diverse and real
scenarios, we create an extensive benchmark of 1053 CF tasks, containing
real-world descriptions collected from four different sources. We release our
benchmarks to encourage research in this area. Abstention and filling allow
FormaT5 to outperform 8 different neural approaches on our benchmarks, both
with and without examples. Our results illustrate the value of building
domain-specific learning systems.

</details>


### [133] [Don't Treat the Symptom, Find the Cause! Efficient Artificial-Intelligence Methods for (Interactive) Debugging](https://arxiv.org/abs/2306.12850)
*Patrick Rodler*

Main category: cs.AI

TL;DR: 模型化诊断是应对现代复杂系统故障的关键方法，旨在检测、定位并修复异常行为系统中的故障，从而最大程度地减少损害。


<details>
  <summary>Details</summary>
Motivation: 现代系统日益复杂且对人们的生活至关重要，系统故障的可能性及其负面影响巨大。因此，最大程度地减少故障造成的损害（停机时间、修复成本）成为一项重要需求。

Method: 模型化诊断是一种独立于领域的通用方法，它利用知识表示、自动推理、启发式问题解决、智能搜索、优化、随机过程、统计学、不确定性决策、机器学习以及微积分、组合学和集合论等技术，来检测、定位和修复异常行为系统中的故障。本论文将介绍该领域、指出主要挑战并讨论解决这些问题的研究方法。

Result: 摘要中没有明确说明已取得的具体结果，但指出论文将讨论作者研究中解决模型化诊断挑战的精选方法。

Conclusion: 模型化诊断是管理复杂系统故障的原则性解决方案。本论文旨在深入探讨这一领域、其挑战以及研究贡献。

Abstract: In the modern world, we are permanently using, leveraging, interacting with,
and relying upon systems of ever higher sophistication, ranging from our cars,
recommender systems in e-commerce, and networks when we go online, to
integrated circuits when using our PCs and smartphones, the power grid to
ensure our energy supply, security-critical software when accessing our bank
accounts, and spreadsheets for financial planning and decision making. The
complexity of these systems coupled with our high dependency on them implies
both a non-negligible likelihood of system failures, and a high potential that
such failures have significant negative effects on our everyday life. For that
reason, it is a vital requirement to keep the harm of emerging failures to a
minimum, which means minimizing the system downtime as well as the cost of
system repair. This is where model-based diagnosis comes into play.
  Model-based diagnosis is a principled, domain-independent approach that can
be generally applied to troubleshoot systems of a wide variety of types,
including all the ones mentioned above, and many more. It exploits and
orchestrates i.a. techniques for knowledge representation, automated reasoning,
heuristic problem solving, intelligent search, optimization, stochastics,
statistics, decision making under uncertainty, machine learning, as well as
calculus, combinatorics and set theory to detect, localize, and fix faults in
abnormally behaving systems.
  In this thesis, we will give an introduction to the topic of model-based
diagnosis, point out the major challenges in the field, and discuss a selection
of approaches from our research addressing these issues.

</details>


### [134] [CORNET: Learning Table Formatting Rules By Example](https://arxiv.org/abs/2208.06032)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Carina Negreanu,Mohammad Raza,Gust Verbruggen*

Main category: cs.AI

TL;DR: CORNET是一个从用户示例中自动学习表格条件格式规则的系统，它结合了符号规则枚举和神经排序器，在真实数据集上表现出色，能够学习准确且更简洁的规则。


<details>
  <summary>Details</summary>
Motivation: 用户编写电子表格条件格式规则面临挑战，因为这需要了解底层规则语言和数据逻辑。

Method: CORNET系统结合了符号规则枚举（symbolic rule enumeration）和神经排序器（neural ranker）来学习条件格式规则。

Result: CORNET能够准确地学习规则，并且比用户手写的规则更短，还能在用户手动格式化的电子表格中发现规则。

Conclusion: CORNET成功解决了从用户示例中自动学习条件格式规则的问题，提高了电子表格格式化的效率和准确性。

Abstract: Spreadsheets are widely used for table manipulation and presentation.
Stylistic formatting of these tables is an important property for both
presentation and analysis. As a result, popular spreadsheet software, such as
Excel, supports automatically formatting tables based on rules. Unfortunately,
writing such formatting rules can be challenging for users as it requires
knowledge of the underlying rule language and data logic. We present CORNET, a
system that tackles the novel problem of automatically learning such formatting
rules from user examples in the form of formatted cells. CORNET takes
inspiration from advances in inductive programming and combines symbolic rule
enumeration with a neural ranker to learn conditional formatting rules. To
motivate and evaluate our approach, we extracted tables with over 450K unique
formatting rules from a corpus of over 1.8M real worksheets. Since we are the
first to introduce conditional formatting, we compare CORNET to a wide range of
symbolic and neural baselines adapted from related domains. Our results show
that CORNET accurately learns rules across varying evaluation setups.
Additionally, we show that CORNET finds shorter rules than those that a user
has written and discovers rules in spreadsheets that users have manually
formatted.

</details>


### [135] [Named Entity Recognition in Industrial Tables using Tabular Language Models](https://arxiv.org/abs/2209.14812)
*Aneta Koleva,Martin Ringsquandl,Mark Buckley,Rakebul Hasan,Volker Tresp*

Main category: cs.AI

TL;DR: 本文研究了如何将专门的基于Transformer的表格数据模型应用于工业命名实体识别（NER）问题。针对缺少标记数据的问题，开发了一种基于知识图谱的表格数据增强策略，显著提高了低资源场景下的性能。实验证实，表格Transformer优于其他基线模型，并且其表格归纳偏置对于模型收敛至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管表格数据在工业中普遍存在，但表格Transformer的应用仍然缺乏。本文旨在研究如何将这些模型应用于工业命名实体识别（NER）问题，其中实体在表格结构的电子表格中提及。电子表格的高度技术性以及标记数据的缺乏对微调基于Transformer的模型提出了重大挑战。

Method: 开发了一种基于可用领域特定知识图谱的表格数据增强策略。研究了与将表格视为线性序列相比，表格结构作为归纳偏置的优势。

Result: 开发的表格数据增强策略显著提高了低资源场景下的性能。实验证实，表格Transformer优于其他基线模型，并且其表格归纳偏置对于基于Transformer模型的收敛至关重要。

Conclusion: 专门的表格Transformer模型，结合领域特定的数据增强策略，可以有效应用于工业命名实体识别问题，并且其表格归纳偏置对于模型的性能和收敛至关重要。

Abstract: Specialized transformer-based models for encoding tabular data have gained
interest in academia. Although tabular data is omnipresent in industry,
applications of table transformers are still missing. In this paper, we study
how these models can be applied to an industrial Named Entity Recognition (NER)
problem where the entities are mentioned in tabular-structured spreadsheets.
The highly technical nature of spreadsheets as well as the lack of labeled data
present major challenges for fine-tuning transformer-based models. Therefore,
we develop a dedicated table data augmentation strategy based on available
domain-specific knowledge graphs. We show that this boosts performance in our
low-resource scenario considerably. Further, we investigate the benefits of
tabular structure as inductive bias compared to tables as linearized sequences.
Our experiments confirm that a table transformer outperforms other baselines
and that its tabular inductive bias is vital for convergence of
transformer-based models.

</details>


### [136] [Spreadsheet computing with Finite Domain Constraint Enhancements](https://arxiv.org/abs/2203.10944)
*Ezana N. Beyenne*

Main category: cs.AI

TL;DR: 该论文通过整合有限约束求解器，扩展了电子表格计算范式，以克服其单向数据流的限制，并解决约束满足问题。


<details>
  <summary>Details</summary>
Motivation: 电子表格应用程序虽然易用且对非程序员友好，但由于其单向数据流，仅限于簿记类任务。

Method: 该论文提出了一个框架，将有限约束求解器与电子表格计算范式无缝结合。该框架允许单元格绑定到有限域或约束，提供约束求解接口，并通过引入特定于电子表格的约束来提高大型电子表格应用程序的可伸缩性。

Result: 扩展后的电子表格范式能够克服单向数据流的限制，解决约束满足问题，并通过示例证明了其可用性和实用性。

Conclusion: 通过将约束求解器集成到电子表格计算中，该论文成功扩展了电子表格范式，使其能够解决约束满足问题，从而超越了传统的簿记功能。

Abstract: Spreadsheet computing is one of the more popular computing methodologies in
today's modern society. The spreadsheet application's ease of use and
usefulness has enabled non-programmers to perform programming-like tasks in a
familiar setting modeled after the tabular "pen and paper" approach. However,
spreadsheet applications are limited to bookkeeping-like tasks due to their
single-direction data flow. This thesis demonstrates an extension of the
spreadsheet computing paradigm in overcoming this limitation to solve
constraint satisfaction problems. We present a framework seamlessly
incorporating a finite constraint solver with the spreadsheet computing
paradigm. This framework allows the individual cells in the spreadsheet to be
attached to either a finite domain or a constraint specifying the relationship
among the cells. The framework provides an interface for constraint solving and
further enhances the spreadsheet computing paradigm by providing a set of
spreadsheet-specific constraints that will aid in controlling the scalability
of large spreadsheet applications implementations. Finally, we provide examples
to demonstrate the usability and usefulness of the extended spreadsheet
paradigm.
  Keywords: Spreadsheet computing, Constraint Logic Programming, Constraint
satisfaction, Domain-Specific language, Excel, SWI Prolog, C#

</details>


### [137] [Human-Machine Collaboration for Democratizing Data Science](https://arxiv.org/abs/2004.11113)
*Clément Gautrais,Yann Dauxais,Stefano Teso,Samuel Kolb,Gust Verbruggen,Luc De Raedt*

Main category: cs.AI

TL;DR: VisualSynth是一个人机协作系统，它允许用户通过在电子表格中涂色来执行和自动化数据分析任务，从而实现数据科学的普及。


<details>
  <summary>Details</summary>
Motivation: 少数人拥有数据科学专业知识，但每个人都希望分析他们的数据。本文旨在普及数据科学。

Method: VisualSynth依赖用户提供彩色草图，即对电子表格的一部分进行着色，以部分指定数据科学任务，然后使用人工智能技术确定并执行这些任务。

Result: VisualSynth允许用户在标准电子表格软件中进行交互，以执行和自动化各种数据分析任务，包括数据整理、数据选择、聚类、约束学习、预测建模和自动补全。

Conclusion: VisualSynth通过允许用户在电子表格中通过简单的交互（彩色草图）执行复杂的数据科学任务，从而普及了数据科学。

Abstract: Everybody wants to analyse their data, but only few posses the data science
expertise to to this. Motivated by this observation we introduce a novel
framework and system \textsc{VisualSynth} for human-machine collaboration in
data science.
  It wants to democratize data science by allowing users to interact with
standard spreadsheet software in order to perform and automate various data
analysis tasks ranging from data wrangling, data selection, clustering,
constraint learning, predictive modeling and auto-completion.
\textsc{VisualSynth} relies on the user providing colored sketches, i.e.,
coloring parts of the spreadsheet, to partially specify data science tasks,
which are then determined and executed using artificial intelligence
techniques.

</details>


### [138] [Automated Discovery of Data Transformations for Robotic Process Automation](https://arxiv.org/abs/2001.01007)
*Volodymyr Leno,Marlon Dumas,Marcello La Rosa,Fabrizio Maria Maggi,Artem Polyvyanyy*

Main category: cs.AI

TL;DR: 本文提出了一种分析UI日志的方法，用于发现用户在不同应用间进行重复数据传输的RPA自动化例程，并引入了两种优化以提高效率。


<details>
  <summary>Details</summary>
Motivation: 为了充分利用RPA的机会，企业需要发现哪些特定的例程可以自动化。本文旨在通过分析用户交互（UI）日志，发现用户在电子表格或网页表单之间传输数据的重复性例程。

Method: 将数据传输问题映射为通过示例发现数据转换的问题。针对现有最先进技术在直接应用时计算效率低下的问题，提出了两种优化方案，这些优化利用了UI日志中的信息以及数据传输通常涉及分别复制字母和数字令牌的事实。

Result: 所提出的方法及其优化方案，使用复制真实生活中重复数据传输例程的UI日志进行了评估。

Conclusion: 本文提供了一种优化且高效的方法，可以从UI日志中发现跨应用程序的数据传输例程，从而帮助企业更好地识别和利用RPA的自动化潜力。

Abstract: Robotic Process Automation (RPA) is a technology for automating repetitive
routines consisting of sequences of user interactions with one or more
applications. In order to fully exploit the opportunities opened by RPA,
companies need to discover which specific routines may be automated, and how.
In this setting, this paper addresses the problem of analyzing User Interaction
(UI) logs in order to discover routines where a user transfers data from one
spreadsheet or (Web) form to another. The paper maps this problem to that of
discovering data transformations by example - a problem for which several
techniques are available. The paper shows that a naive application of a
state-of-the-art technique for data transformation discovery is computationally
inefficient. Accordingly, the paper proposes two optimizations that take
advantage of the information in the UI log and the fact that data transfers
across applications typically involve copying alphabetic and numeric tokens
separately. The proposed approach and its optimizations are evaluated using UI
logs that replicate a real-life repetitive data transfer routine.

</details>


### [139] [Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples](https://arxiv.org/abs/1804.01186)
*Ashwin Kalyan,Abhishek Mohta,Oleksandr Polozov,Dhruv Batra,Prateek Jain,Sumit Gulwani*

Main category: cs.AI

TL;DR: 该论文提出了一种名为神经引导演绎搜索（NGDS）的混合合成技术，它结合了符号逻辑和统计模型的优点，用于从少量输入输出示例中合成程序，旨在提高合成速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的程序合成系统要么过度依赖手动工程的演绎逻辑，要么需要大量数据的纯统计模型，导致在具有挑战性的基准测试上无法实现实时合成。

Method: NGDS是一种混合合成技术，它结合了符号逻辑技术和统计模型的优点。它有效地利用演绎搜索框架将神经组件的学习问题简化为监督学习设置，并允许在稀疏的真实世界数据上进行训练，同时利用强大的循环神经网络编码器。

Result: 该方法在真实客户场景中进行了评估，与最先进的系统相比，能够合成准确的程序，并将速度提高多达12倍。

Conclusion: NGDS是一种有效的混合合成技术，能够从少量示例中高效准确地合成用户意图的程序，解决了现有系统在速度和数据需求方面的挑战。

Abstract: Synthesizing user-intended programs from a small number of input-output
examples is a challenging problem with several important applications like
spreadsheet manipulation, data wrangling and code refactoring. Existing
synthesis systems either completely rely on deductive logic techniques that are
extensively hand-engineered or on purely statistical models that need massive
amounts of data, and in general fail to provide real-time synthesis on
challenging benchmarks. In this work, we propose Neural Guided Deductive Search
(NGDS), a hybrid synthesis technique that combines the best of both symbolic
logic techniques and statistical models. Thus, it produces programs that
satisfy the provided specifications by construction and generalize well on
unseen examples, similar to data-driven systems. Our technique effectively
utilizes the deductive search framework to reduce the learning problem of the
neural component to a simple supervised learning setup. Further, this allows us
to both train on sparingly available real-world data and still leverage
powerful recurrent neural network encoders. We demonstrate the effectiveness of
our method by evaluating on real-world customer scenarios by synthesizing
accurate programs with up to 12x speed-up compared to state-of-the-art systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [140] [Reducing Aleatoric and Epistemic Uncertainty through Multi-modal Data Acquisition](https://arxiv.org/abs/2501.18268)
*Arthur Hoarau,Benjamin Quost,Sébastien Destercke,Willem Waegeman*

Main category: cs.LG

TL;DR: 本文提出一种创新的多模态数据采集框架，挑战了传统不确定性（认知可减、偶然不可减）的假设，提出偶然不确定性随模态增加而减少，认知不确定性随观测增加而减少，并通过在样本量和模态两个方向采样来解耦不确定性。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统在处理多模态数据时面临不确定性解耦的挑战，传统的认知不确定性（随数据量增加而减少）和偶然不确定性（不可约减）的假设在多模态背景下不再适用。

Method: 论文引入了一个创新的数据采集框架，该框架通过在样本量和数据模态两个方向采样，使不确定性解耦并支持可操作的决策。其核心假设是偶然不确定性随模态数量增加而减少，而认知不确定性随观测数量增加而减少。该框架整合了主动学习、主动特征获取和不确定性量化思想。

Result: 作者在两个多模态数据集上提供了概念验证实现，成功展示了他们的数据采集框架。

Conclusion: 通过在样本量和数据模态两个方向上进行数据采集，可以有效地解耦多模态数据中的不确定性，并指出偶然不确定性可以通过增加数据模态的数量来降低，从而为AI系统提供更可靠的预测。

Abstract: To generate accurate and reliable predictions, modern AI systems need to
combine data from multiple modalities, such as text, images, audio,
spreadsheets, and time series. Multi-modal data introduces new opportunities
and challenges for disentangling uncertainty: it is commonly assumed in the
machine learning community that epistemic uncertainty can be reduced by
collecting more data, while aleatoric uncertainty is irreducible. However, this
assumption is challenged in modern AI systems when information is obtained from
different modalities. This paper introduces an innovative data acquisition
framework where uncertainty disentanglement leads to actionable decisions,
allowing sampling in two directions: sample size and data modality. The main
hypothesis is that aleatoric uncertainty decreases as the number of modalities
increases, while epistemic uncertainty decreases by collecting more
observations. We provide proof-of-concept implementations on two multi-modal
datasets to showcase our data acquisition framework, which combines ideas from
active learning, active feature acquisition and uncertainty quantification.

</details>


### [141] [Large Scale Transfer Learning for Tabular Data via Language Modeling](https://arxiv.org/abs/2406.12031)
*Josh Gardner,Juan C. Perdomo,Ludwig Schmidt*

Main category: cs.LG

TL;DR: TabuLa-8B是一个用于表格预测的语言模型，在零样本和少样本设置下，其准确性显著高于现有模型，弥补了基础模型在表格领域应用的空白。


<details>
  <summary>Details</summary>
Motivation: 当前的预训练模型在语言和计算机视觉领域取得了巨大成功，但在表格数据预测方面影响力不足，本文旨在缩小这一差距。

Method: 本文从TabLib语料库中提取了一个包含21亿行和400万个独特表格的高质量训练数据集，并提出了表格数据过滤和质量控制方法。然后，利用新颖的打包和注意力机制，对Llama 3-8B大型语言模型进行微调，以实现表格数据预测（分类和分箱回归）。

Result: TabuLa-8B在329个数据集的测试中，零样本准确率比随机猜测高出15个百分点以上，这是现有最先进表格预测模型（如XGBoost、TabPFN）无法达到的。在少样本（1-32样本）设置下，TabuLa-8B比XGBoost和TabPFN模型准确率高出5-15个百分点，即使后者在相同或多达16倍的数据上进行训练。

Conclusion: TabuLa-8B模型证明了大型语言模型在表格数据预测领域的巨大潜力，尤其是在零样本和少样本场景下，显著优于传统模型，为该领域带来了新的范式。

Abstract: Tabular data -- structured, heterogeneous, spreadsheet-style data with rows
and columns -- is widely used in practice across many domains. However, while
recent foundation models have reduced the need for developing task-specific
datasets and predictors in domains such as language modeling and computer
vision, this transfer learning paradigm has not had similar impact in the
tabular domain. In this work, we seek to narrow this gap and present TabuLa-8B,
a language model for tabular prediction. We define a process for extracting a
large, high-quality training dataset from the TabLib corpus, proposing methods
for tabular data filtering and quality control. Using the resulting dataset,
which comprises over 2.1B rows from over 4M unique tables, we fine-tune a Llama
3-8B large language model (LLM) for tabular data prediction (classification and
binned regression) using a novel packing and attention scheme for tabular
prediction. Through evaluation across a test suite of 329 datasets, we find
that TabuLa-8B has zero-shot accuracy on unseen tables that is over 15
percentage points (pp) higher than random guessing, a feat that is not possible
with existing state-of-the-art tabular prediction models (e.g. XGBoost,
TabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the
target datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN
models that are explicitly trained on equal, or even up to 16x more data. We
release our model, code, and data along with the publication of this paper.

</details>


### [142] [Generating tabular datasets under differential privacy](https://arxiv.org/abs/2308.14784)
*Gianluca Truda*

Main category: cs.LG

TL;DR: 本文介绍了TableDiffusion，第一个用于表格数据合成的差分隐私扩散模型。它解决了生成对抗网络（GANs）在差分隐私下合成敏感表格数据时遇到的模式崩溃和训练不稳定等问题，并在质量-隐私权衡方面实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习需要高质量、可访问的训练数据，特别是来自生物医学和金融领域的敏感表格数据。合成数据生成有潜力解锁这些敏感数据，但传统生成模型存在隐私泄露风险。差分隐私（DP）可以缓解这个问题，但会在数据质量和隐私之间产生权衡。现有的基于GAN的差分隐私表格数据合成方法存在训练不稳定和模式崩溃的问题，尤其是在隐私约束和复杂的表格数据模态下更为严重。

Method: 本文优化了生成模型的质量-隐私权衡。作者实现了利用注意力机制学习可逆表格表示的新型端到端模型。此外，还引入了TableDiffusion，这是首个用于表格数据合成的差分隐私扩散模型。通过预测添加的噪声，TableDiffusion绕过了重建混合类型表格数据的挑战。

Result: TableDiffusion生成了更高保真度的合成数据集，避免了模式崩溃问题，并在隐私化表格数据合成方面取得了最先进的性能。扩散范式在数据和隐私效率方面都优于对抗范式。

Conclusion: TableDiffusion作为一种新颖的差分隐私扩散模型，为敏感表格数据合成提供了卓越的解决方案。它在保持相同隐私保证的同时，生成了更高质量的数据，并在保真度、模式崩溃避免和效率方面超越了现有的基于GAN的方法。

Abstract: Machine Learning (ML) is accelerating progress across fields and industries,
but relies on accessible and high-quality training data. Some of the most
important datasets are found in biomedical and financial domains in the form of
spreadsheets and relational databases. But this tabular data is often sensitive
in nature. Synthetic data generation offers the potential to unlock sensitive
data, but generative models tend to memorise and regurgitate training data,
which undermines the privacy goal. To remedy this, researchers have
incorporated the mathematical framework of Differential Privacy (DP) into the
training process of deep neural networks. But this creates a trade-off between
the quality and privacy of the resulting data. Generative Adversarial Networks
(GANs) are the dominant paradigm for synthesising tabular data under DP, but
suffer from unstable adversarial training and mode collapse, which are
exacerbated by the privacy constraints and challenging tabular data modality.
This work optimises the quality-privacy trade-off of generative models,
producing higher quality tabular datasets with the same privacy guarantees. We
implement novel end-to-end models that leverage attention mechanisms to learn
reversible tabular representations. We also introduce TableDiffusion, the first
differentially-private diffusion model for tabular data synthesis. Our
experiments show that TableDiffusion produces higher-fidelity synthetic
datasets, avoids the mode collapse problem, and achieves state-of-the-art
performance on privatised tabular data synthesis. By implementing
TableDiffusion to predict the added noise, we enabled it to bypass the
challenges of reconstructing mixed-type tabular data. Overall, the diffusion
paradigm proves vastly more data and privacy efficient than the adversarial
paradigm, due to augmented re-use of each data batch and a smoother iterative
training process.

</details>


### [143] [Smart Metro: Deep Learning Approaches to Forecasting the MRT Line 3 Ridership](https://arxiv.org/abs/2304.07303)
*Jayrald Empino,Jean Allyson Junsay,Mary Grace Verzon,Mideth Abisado,Shekinah Lor Huyo-a,Gabriel Avelino Sampedro*

Main category: cs.LG

TL;DR: 本研究旨在通过时间序列预测分析，预测马尼拉大都会捷运3号线（MRT3）的每日客流量，以帮助通勤者规划行程并协助交通部（DOTr）进行预测。


<details>
  <summary>Details</summary>
Motivation: 马尼拉大都会捷运3号线（MRT3）的每日客流量受多种因素影响，预测极具挑战性。通勤者缺乏有效规划行程所需的信息，且交通部（DOTr）当前依赖难以分析的电子表格历史数据。

Method: 本研究提出了一种用于预测每日客流量的时间序列预测方法。

Result: 摘要中未明确提及结果。

Conclusion: 摘要中未明确提及结论。

Abstract: Since its establishment in 1999, the Metro Rail Transit Line 3 (MRT3) has
served as a transportation option for numerous passengers in Metro Manila,
Philippines. The Philippine government's transportation department records more
than a thousand people using the MRT3 daily and forecasting the daily passenger
count may be rather challenging. The MRT3's daily ridership fluctuates owing to
variables such as holidays, working days, and other unexpected issues.
Commuters do not know how many other commuters are on their route on a given
day, which may hinder their ability to plan an efficient itinerary. Currently,
the DOTr depends on spreadsheets containing historical data, which might be
challenging to examine. This study presents a time series prediction of daily
traffic to anticipate future attendance at a particular station on specific
days.

</details>


### [144] [Adversarial Networks and Machine Learning for File Classification](https://arxiv.org/abs/2301.11964)
*Ken St. Germain,Josh Angichiodo*

Main category: cs.LG

TL;DR: 该论文提出使用对抗训练的机器学习神经网络（SGAN）来确定文件类型，即使文件扩展名或文件头被混淆，在11种不同类型的文件中实现了97.6%的准确率，并且在监督样本较少的情况下表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 在司法调查中，正确识别文件类型至关重要，但文件所有者可能会隐藏或混淆文件类型。

Method: 该论文提出了一种半监督生成对抗网络（SGAN）来确定文件的真实类型。他们还将SGAN与传统的独立神经网络和其他三种机器学习算法进行了比较。

Result: SGAN在11种不同类型的文件分类中达到了97.6%的准确率。对抗训练的网络被证明是最精确的文件分类器，尤其是在监督样本较少的情况下。

Conclusion: 对抗训练的SGAN是一种有效且精确的文件类型分类方法，即使在文件被混淆和监督数据有限的情况下也能表现出色。

Abstract: Correctly identifying the type of file under examination is a critical part
of a forensic investigation. The file type alone suggests the embedded content,
such as a picture, video, manuscript, spreadsheet, etc. In cases where a system
owner might desire to keep their files inaccessible or file type concealed, we
propose using an adversarially-trained machine learning neural network to
determine a file's true type even if the extension or file header is obfuscated
to complicate its discovery. Our semi-supervised generative adversarial network
(SGAN) achieved 97.6% accuracy in classifying files across 11 different types.
We also compared our network against a traditional standalone neural network
and three other machine learning algorithms. The adversarially-trained network
proved to be the most precise file classifier especially in scenarios with few
supervised samples available. Our implementation of a file classifier using an
SGAN is implemented on GitHub (https://ksaintg.github.io/SGAN-File-Classier).

</details>


### [145] [TabularNet: A Neural Network Architecture for Understanding Semantic Structures of Tabular Data](https://arxiv.org/abs/2106.03096)
*Lun Du,Fei Gao,Xu Chen,Ran Jia,Junshan Wang,Jiang Zhang,Shi Han,Dongmei Zhang*

Main category: cs.LG

TL;DR: 提出TabularNet，一个同时提取表格空间和关系信息的神经网络架构，在表格数据理解任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 表格数据理解中的现有研究多使用CNN建模空间信息，但忽略了单元格之间更丰富的关系信息（如层次和并列关系），自动理解表格固有语义结构仍是一个关键问题。

Method: 提出TabularNet神经网络架构，其空间编码器利用行/列级池化和Bi-GRU捕获统计信息和局部位置相关性；关系编码器基于WordNet树设计图构建方法并采用GCN，关注单元格间的层次和并列关系。该架构可作为多任务的统一神经网络骨干。

Result: 在两个真实世界电子表格数据集上的三个分类任务中，TabularNet的实验结果表明其优于现有的先进基线方法。

Conclusion: TabularNet能够有效同时提取表格的空间和关系信息，在表格数据理解任务中展现出卓越的性能。

Abstract: Tabular data are ubiquitous for the widespread applications of tables and
hence have attracted the attention of researchers to extract underlying
information. One of the critical problems in mining tabular data is how to
understand their inherent semantic structures automatically. Existing studies
typically adopt Convolutional Neural Network (CNN) to model the spatial
information of tabular structures yet ignore more diverse relational
information between cells, such as the hierarchical and paratactic
relationships. To simultaneously extract spatial and relational information
from tables, we propose a novel neural network architecture, TabularNet. The
spatial encoder of TabularNet utilizes the row/column-level Pooling and the
Bidirectional Gated Recurrent Unit (Bi-GRU) to capture statistical information
and local positional correlation, respectively. For relational information, we
design a new graph construction method based on the WordNet tree and adopt a
Graph Convolutional Network (GCN) based encoder that focuses on the
hierarchical and paratactic relationships between cells. Our neural network
architecture can be a unified neural backbone for different understanding tasks
and utilized in a multitask scenario. We conduct extensive experiments on three
classification tasks with two real-world spreadsheet data sets, and the results
demonstrate the effectiveness of our proposed TabularNet over state-of-the-art
baselines.

</details>


### [146] [Visual Backpropagation](https://arxiv.org/abs/1906.04011)
*Roy S. Freedman*

Main category: cs.LG

TL;DR: 本文提出“可视化反向传播”方法，通过声明式函数编程规范，在电子表格中实现反向传播，无需宏或外部代码，利用数组公式和手动计算，其计算顺序类似于脉动阵列。


<details>
  <summary>Details</summary>
Motivation: 旨在展示如何将反向传播的声明式函数编程规范，转化为电子表格中可视化且透明的实现。

Method: “可视化反向传播”方法，通过电子表格中的数组工作表公式和手动计算实现，其计算顺序类似于脉动阵列。不使用隐藏宏、用户自定义函数、循环、赋值语句或外部程序链接。

Result: 通过一个标准的回归问题，将“可视化反向传播”解决方案与TensorFlow (Python) 解决方案进行比较。

Conclusion: 该方法在电子表格中提供了一种可视化且透明的反向传播实现方式，无需传统编程语言的复杂性，提供了一种新颖且易于理解的替代方案。

Abstract: We show how a declarative functional programming specification of
backpropagation yields a visual and transparent implementation within
spreadsheets. We call our method Visual Backpropagation. This backpropagation
implementation exploits array worksheet formulas, manual calculation, and has a
sequential order of computation similar to the processing of a systolic array.
The implementation uses no hidden macros nor user-defined functions; there are
no loops, assignment statements, or links to any procedural programs written in
conventional languages. As an illustration, we compare a Visual Backpropagation
solution to a Tensorflow (Python) solution on a standard regression problem.

</details>


### [147] [MOLTE: a Modular Optimal Learning Testing Environment](https://arxiv.org/abs/1709.04553)
*Yingfei Wang,Warren Powell*

Main category: cs.LG

TL;DR: 本文介绍了MOLTE，一个基于Matlab的模块化、优化学习测试环境，用于对贝叶斯排序和选择问题、随机多臂赌博机或序贯实验设计问题中的学习算法进行实证测试，支持并行计算，并提供易于使用的界面。


<details>
  <summary>Details</summary>
Motivation: 目前学习算法的实证测试相对较少，作者旨在提供一个工具，使研究人员能够进行更全面的测试，并解决最优学习文献中被忽视的调优和先验构建问题。

Method: 本文引入了MOLTE，一个基于Matlab的模拟器。它使用.m模块来表示学习策略和问题，通过电子表格界面指导问题和策略的选择，并包含不同的图形度量。MOLTE旨在兼容并行计算。

Result: MOLTE被提供为一个易于使用的工具，使得研究社区能够进行更全面、更广泛的算法和测试问题测试。作者通过一系列策略比较演示了MOLTE的功能，并解决了最优学习文献中被忽视的调优和先验构建问题。

Conclusion: MOLTE旨在为研究人员提供一个轻松的环境，以研究最优学习中涉及的有趣问题。

Abstract: We address the relative paucity of empirical testing of learning algorithms
(of any type) by introducing a new public-domain, Modular, Optimal Learning
Testing Environment (MOLTE) for Bayesian ranking and selection problem,
stochastic bandits or sequential experimental design problems. The Matlab-based
simulator allows the comparison of a number of learning policies (represented
as a series of .m modules) in the context of a wide range of problems (each
represented in its own .m module) which makes it easy to add new algorithms and
new test problems. State-of-the-art policies and various problem classes are
provided in the package. The choice of problems and policies is guided through
a spreadsheet-based interface. Different graphical metrics are included. MOLTE
is designed to be compatible with parallel computing to scale up from local
desktop to clusters and clouds. We offer MOLTE as an easy-to-use tool for the
research community that will make it possible to perform much more
comprehensive testing, spanning a broader selection of algorithms and test
problems. We demonstrate the capabilities of MOLTE through a series of
comparisons of policies on a starter library of test problems. We also address
the problem of tuning and constructing priors that have been largely overlooked
in optimal learning literature. We envision MOLTE as a modest spur to provide
researchers an easy environment to study interesting questions involved in
optimal learning.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [148] [FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining](https://arxiv.org/abs/2109.07323)
*Zhoujun Cheng,Haoyu Dong,Ran Jia,Pengfei Wu,Shi Han,Fan Cheng,Dongmei Zhang*

Main category: cs.IR

TL;DR: FORTAP利用电子表格公式进行表格预训练，以提高数值推理能力，并在单元格类型分类和公式预测任务上达到了最先进的水平。


<details>
  <summary>Details</summary>
Motivation: 表格中的数值推理仍然是一个挑战，而电子表格公式提供了强大的监督信号，且大量专家制作的公式可在网络上获取。

Method: FORTAP是首个利用大规模电子表格公式语料库进行数值推理感知表格预训练的方法。它设计了两个公式预训练任务，以明确指导模型学习半结构化表格中的数值引用和计算。

Result: FORTAP在单元格类型分类和公式预测这两个代表性下游任务上取得了最先进的成果。

Conclusion: 数值推理感知的预训练方法具有巨大潜力。

Abstract: Tables store rich numerical data, but numerical reasoning over tables is
still a challenge. In this paper, we find that the spreadsheet formula, which
performs calculations on numerical values in tables, is naturally a strong
supervision of numerical reasoning. More importantly, large amounts of
spreadsheets with expert-made formulae are available on the web and can be
obtained easily. FORTAP is the first method for numerical-reasoning-aware table
pretraining by leveraging large corpus of spreadsheet formulae. We design two
formula pretraining tasks to explicitly guide FORTAP to learn numerical
reference and calculation in semi-structured tables. FORTAP achieves
state-of-the-art results on two representative downstream tasks, cell type
classification and formula prediction, showing great potential of
numerical-reasoning-aware pretraining.

</details>


### [149] [Detecting Layout Templates in Complex Multiregion Files](https://arxiv.org/abs/2109.06630)
*Gerardo Vitagliano,Lan Jiang,Felix Naumann*

Main category: cs.IR

TL;DR: Mondrian方法通过将电子表格视为图像，聚类元素以形成区域，并比较布局的图表示，来识别并提取电子表格集合中多个独立的区域及其布局模板。


<details>
  <summary>Details</summary>
Motivation: 电子表格因其灵活的结构导致自动化分析困难，尤其是在单个文件中存在多个独立区域（“多区域”文件）的情况下。研究的动机是为了解决实践者在多区域文件中识别和提取这些区域以及在文件间寻找共同布局的难题，从而实现自动化分析。

Method: Mondrian方法包含三个阶段：1) 将每个文件渲染成图像并检查可能构成区域的元素；2) 使用聚类算法将识别出的元素分组以形成区域；3) 将每个文件布局表示为图，并与其他布局进行比较以找到布局模板。

Result: 该方法在两个真实企业电子表格语料库上，在检测每个文件内可靠区域边界方面表现出最佳性能，并且能够正确识别文件间重复出现的布局，优于最先进的表格识别算法。

Conclusion: Mondrian方法有效自动化了电子表格中多区域数据及其布局模板的识别和提取，显著提升了自动化分析能力。

Abstract: Spreadsheets are among the most commonly used file formats for data
management, distribution, and analysis. Their widespread employment makes it
easy to gather large collections of data, but their flexible canvas-based
structure makes automated analysis difficult without heavy preparation. One of
the common problems that practitioners face is the presence of multiple,
independent regions in a single spreadsheet, possibly separated by repeated
empty cells. We define such files as "multiregion" files. In collections of
various spreadsheets, we can observe that some share the same layout. We
present the Mondrian approach to automatically identify layout templates across
multiple files and systematically extract the corresponding regions. Our
approach is composed of three phases: first, each file is rendered as an image
and inspected for elements that could form regions; then, using a clustering
algorithm, the identified elements are grouped to form regions; finally, every
file layout is represented as a graph and compared with others to find layout
templates. We compare our method to state-of-the-art table recognition
algorithms on two corpora of real-world enterprise spreadsheets. Our approach
shows the best performances in detecting reliable region boundaries within each
file and can correctly identify recurring layouts across files.

</details>


### [150] [TUTA: Tree-based Transformers for Generally Structured Table Pre-training](https://arxiv.org/abs/2010.12537)
*Zhiruo Wang,Haoyu Dong,Ran Jia,Jia Li,Zhiyi Fu,Shi Han,Dongmei Zhang*

Main category: cs.IR

TL;DR: TUTA是一个统一的预训练架构，利用结构感知机制理解通用结构化表格，并在多个任务中取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 现有的表格理解工作主要关注关系型表格，而忽视了其他常见的表格结构。

Method: 本文提出了TUTA，一种用于理解通用结构化表格的统一预训练架构。通过引入以下机制增强了Transformer：1. 设计了一种统一的基于树的结构（双维度坐标树）来描述表格的空间和层次信息。2. 提出了基于树的注意力机制和位置嵌入。3. 设计了三个渐进式预训练目标，以在token、单元格和表格级别实现表示。TUTA在大量的无标签网络和电子表格上进行预训练，并在单元格类型分类和表格类型分类两个任务上进行微调。

Result: TUTA表现出高效性，在五个广泛研究的数据集上达到了最先进的水平。

Conclusion: TUTA通过新颖的结构感知机制和渐进式预训练，为理解通用结构化表格提供了一种统一且有效的方法，成功地整合了空间、层次和语义信息。

Abstract: Tables are widely used with various structures to organize and present data.
Recent attempts on table understanding mainly focus on relational tables, yet
overlook to other common table structures. In this paper, we propose TUTA, a
unified pre-training architecture for understanding generally structured
tables. Noticing that understanding a table requires spatial, hierarchical, and
semantic information, we enhance transformers with three novel structure-aware
mechanisms. First, we devise a unified tree-based structure, called a
bi-dimensional coordinate tree, to describe both the spatial and hierarchical
information of generally structured tables. Upon this, we propose tree-based
attention and position embedding to better capture the spatial and hierarchical
information. Moreover, we devise three progressive pre-training objectives to
enable representations at the token, cell, and table levels. We pre-train TUTA
on a wide range of unlabeled web and spreadsheet tables and fine-tune it on two
critical tasks in the field of table structure understanding: cell type
classification and table type classification. Experiments show that TUTA is
highly effective, achieving state-of-the-art on five widely-studied datasets.

</details>


### [151] [TableSense: Spreadsheet Table Detection with Convolutional Neural Networks](https://arxiv.org/abs/2106.13500)
*Haoyu Dong,Shijie Liu,Shi Han,Zhouyu Fu,Dongmei Zhang*

Main category: cs.IR

TL;DR: 该论文提出了TableSense，一个用于电子表格表格检测的新型端到端卷积神经网络框架，在EoB-2指标上取得了91.3%的召回率和86.5%的精确度。


<details>
  <summary>Details</summary>
Motivation: 自动表格检测是电子表格数据智能的关键技术和初始步骤，但由于电子表格上表格结构和布局的多样性，检测任务面临挑战。

Method: TableSense框架包括三个部分：1. 有效的单元格特征化方案；2. 增强的卷积神经网络模型以满足精确表格边界检测的领域特定要求；3. 有效的不确定性度量指导主动学习的智能采样算法，从而高效构建了包含22,176个表格（来自10,220个电子表格）的训练数据集。

Result: TableSense在EoB-2指标上取得了91.3%的召回率和86.5%的精确度，这比商品电子表格工具中使用的当前检测算法和计算机视觉领域最先进的卷积神经网络都有显著改进。

Conclusion: TableSense是一种高效的电子表格表格检测框架。

Abstract: Spreadsheet table detection is the task of detecting all tables on a given
sheet and locating their respective ranges. Automatic table detection is a key
enabling technique and an initial step in spreadsheet data intelligence.
However, the detection task is challenged by the diversity of table structures
and table layouts on the spreadsheet. Considering the analogy between a cell
matrix as spreadsheet and a pixel matrix as image, and encouraged by the
successful application of Convolutional Neural Networks (CNN) in computer
vision, we have developed TableSense, a novel end-to-end framework for
spreadsheet table detection. First, we devise an effective cell featurization
scheme to better leverage the rich information in each cell; second, we develop
an enhanced convolutional neural network model for table detection to meet the
domain-specific requirement on precise table boundary detection; third, we
propose an effective uncertainty metric to guide an active learning based smart
sampling algorithm, which enables the efficient build-up of a training dataset
with 22,176 tables on 10,220 sheets with broad coverage of diverse table
structures and layouts. Our evaluation shows that TableSense is highly
effective with 91.3\% recall and 86.5\% precision in EoB-2 metric, a
significant improvement over both the current detection algorithm that are used
in commodity spreadsheet tools and state-of-the-art convolutional neural
networks in computer vision.

</details>


### [152] [Recommending Related Tables](https://arxiv.org/abs/1907.03595)
*Shuo Zhang,Krisztian Balog*

Main category: cs.IR

TL;DR: 本文介绍并解决推荐相关表格的任务，通过在多个语义空间中表示表格元素并使用判别学习模型组合元素级相似性来计算表格相似度，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 表格是组织和操作数据的强大工具，电子表格程序是最流行的计算机应用之一。本文旨在解决推荐相关表格的任务，即给定一个输入表格，识别并返回一个相关表格的排序列表，例如为电子表格用户主动提供网络上相关结构化内容的推荐。

Method: 核心任务是计算一对表格之间的相似性。作者开发了一个理论上合理的方法来执行表格匹配，其方法基于在多个语义空间中表示表格元素，然后使用判别学习模型组合元素级相似性。

Result: 使用一个专门构建的维基百科表格测试集，证明了所提出的方法提供了最先进的性能。

Conclusion: 所提出的表格匹配方法能够有效地推荐相关表格，并在实际应用中展现出卓越的性能。

Abstract: Tables are an extremely powerful visual and interactive tool for structuring
and manipulating data, making spreadsheet programs one of the most popular
computer applications. In this paper we introduce and address the task of
recommending related tables: given an input table, identifying and returning a
ranked list of relevant tables. One of the many possible application scenarios
for this task is to provide users of a spreadsheet program proactively with
recommendations for related structured content on the Web. At its core, the
related table recommendation task boils down to computing the similarity
between a pair of tables. We develop a theoretically sound framework for
performing table matching. Our approach hinges on the idea of representing
table elements in multiple semantic spaces, and then combining element-level
similarities using a discriminative learning model. Using a purpose-built test
collection from Wikipedia tables, we demonstrate that the proposed approach
delivers state-of-the-art performance.

</details>


### [153] [SmartTable: A Spreadsheet Program with Intelligent Assistance](https://arxiv.org/abs/1805.06353)
*Shuo Zhang,Vugar Abdul Zada,Krisztian Balog*

Main category: cs.IR

TL;DR: SmartTable是一个在线表格应用，提供智能辅助功能，用于填充关系型表格的行和列，已开源并在线可用。


<details>
  <summary>Details</summary>
Motivation: 介绍SmartTable，一个具备智能辅助功能的在线表格应用，专注于关系型表格的数据填充（行）和属性扩展（列）。

Method: 开发并实现了SmartTable应用，并将其作为开源项目发布。

Result: SmartTable应用已上线并作为开源项目发布，可在http://smarttable.cc访问。

Conclusion: 成功开发并发布了SmartTable，一个针对关系型表格提供智能辅助填充行和列的在线表格应用，该应用已开源并可在线访问。

Abstract: We introduce SmartTable, an online spreadsheet application that is equipped
with intelligent assistance capabilities. With a focus on relational tables,
describing entities along with their attributes, we offer assistance in two
flavors: (i) for populating the table with additional entities (rows) and (ii)
for extending it with additional entity attributes (columns). We provide
details of our implementation, which is also released as open source. The
application is available at http://smarttable.cc.

</details>


### [154] [EntiTables: Smart Assistance for Entity-Focused Tables](https://arxiv.org/abs/1708.08721)
*Shuo Zhang,Krisztian Balog*

Main category: cs.IR

TL;DR: 该论文旨在为电子表格程序提供智能辅助功能，专注于实体表格的行实例填充和列标题填充。通过开发生成概率模型，并结合知识库和大型表格语料库进行估计，实验结果表明其方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为电子表格程序配备智能辅助功能，特别是针对以实体为中心的表格。

Method: 开发了针对行实例填充和列标题填充任务的生成概率模型。模型的组件估计考虑了知识库和大型表格语料库。

Result: 实验评估模拟了用户输入内容的各个阶段，结果表明模型的组件是互补的，并且所提出的方法优于现有文献中的方法。

Conclusion: 开发的生成概率模型在为实体表格提供智能辅助方面表现出色，其性能优于现有方法，模型的各个组件也相互补充。

Abstract: Tables are among the most powerful and practical tools for organizing and
working with data. Our motivation is to equip spreadsheet programs with smart
assistance capabilities. We concentrate on one particular family of tables,
namely, tables with an entity focus. We introduce and focus on two specific
tasks: populating rows with additional instances (entities) and populating
columns with new headings. We develop generative probabilistic models for both
tasks. For estimating the components of these models, we consider a knowledge
base as well as a large table corpus. Our experimental evaluation simulates the
various stages of the user entering content into an actual table. A detailed
analysis of the results shows that the models' components are complimentary and
that our methods outperform existing approaches from the literature.

</details>


<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [155] [Real-time stock analysis for blending recipes in industrial plants](https://arxiv.org/abs/1909.02960)
*Florin Zamfir,Nicolae Paraschiv,Emil Pricop*

Main category: cs.OH

TL;DR: 本文介绍了一个C#应用程序，用于实时库存分析和优化混合过程中的生产决策，以取代易出错的Excel表格。


<details>
  <summary>Details</summary>
Motivation: 许多公司使用Excel电子表格进行库存记录和数据计算，但这些表格难以理解、跟踪，且存在用户意外修改或删除公式的风险。对于已知配方的混合过程，需要一个集中式的应用程序来辅助操作员进行实时库存分析和决策。

Method: 论文提出并实现了一系列算法，用于识别混合过程所需成分、确定现有成分可生产的成品数量以及确定成品的最佳数量。这些算法被集成到一个用C#实现的实时库存分析应用程序中。

Result: 开发了一个易于操作的C#实时库存分析应用程序，该应用程序集成了规划算法，能够帮助操作员逐步构建结果，以实现最佳的混合生产决策。

Conclusion: 所开发的C#应用程序及其集成的规划算法，通过提供一个集中式、实时的分析工具，有效解决了基于Excel的库存管理问题，并协助操作员做出最佳的混合决策。

Abstract: Many companies use Excel spreadsheets to keep stock records and to calculate
process-specific data. These spreadsheets are often hard to understand and
track. And if the user does not protect them, there is a risk that the user
randomly changes or erase formulas. The paper focuses on the stocks of products
used in a blending process with a known recipe. Developing an application that
can bring this data in a centralized form and that can assist the operator in
decide is a necessity. When a programmer implements an application that uses
data from plants he needs to consider one fundamental aspect as reading
real-time data from the process. The real-time stock analysis application takes
into account all the above elements. The application is easy to use by an
operator in the command room of installation because of the planning algorithms
integrated into it. The algorithms proposed and implemented in this paper have
well-defined goals: identifying the ingredients needed to achieve the blending
process for required quantities, determine the quantities of the finished
product that can be made with the existing ingredients and determine the
optimum quantities of the finished product. The application implemented in C#
intensively uses these algorithms and gives the user the ability to build the
result step by step.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [156] [A CNN toolbox for skin cancer classification](https://arxiv.org/abs/1908.08187)
*Fabrizio Nunnari,Daniel Sonntag*

Main category: eess.IV

TL;DR: 本文介绍了一个用于皮肤癌分类深度神经网络配置的软件工具箱，该工具箱允许开发者快速设置网络架构和超参数，并使非技术用户也能通过简单的界面探索配置设置，初步结果量化了图像增强、分辨率和重缩放滤波器对检测性能和训练时间的影响。


<details>
  <summary>Details</summary>
Motivation: 在皮肤癌分类领域，配置深度神经网络复杂且耗时，本文旨在提供一个工具箱来简化CNN架构和超参数的设置，并让非技术用户也能轻松探索不同的配置，以适应不同的数据集。

Method: 该工具箱提供了一个软件架构，允许开发者快速设置新的CNN架构和超参数配置。同时，它还包含一个用户界面，通过简单的电子表格形式，使非技术用户能够探索不同的配置设置。

Result: 初步结果在黑色素瘤检测的背景下，使用两个CNN量化了图像增强、图像分辨率和重缩放滤波器对整体检测性能和训练时间的影响。

Conclusion: 该软件工具箱成功简化了皮肤癌分类中深度神经网络的配置过程，并为不同用户提供了便利。初步研究揭示了图像处理参数对模型性能和训练效率的关键影响。

Abstract: We describe a software toolbox for the configuration of deep neural networks
in the domain of skin cancer classification. The implemented software
architecture allows developers to quickly set up new convolutional neural
network (CNN) architectures and hyper-parameter configurations. At the same
time, the user interface, manageable as a simple spreadsheet, allows
non-technical users to explore different configuration settings that need to be
explored when switching to different data sets. In future versions, meta
leaning frameworks can be added, or AutoML systems that continuously improve
over time. Preliminary results, conducted with two CNNs in the context melanoma
detection on dermoscopic images, quantify the impact of image augmentation,
image resolution, and rescaling filter on the overall detection performance and
training time.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [157] [An Email Attachment is Worth a Thousand Words, or Is It?](https://arxiv.org/abs/1709.00362)
*Gregory Tsipenyuk,Jon Crowcroft*

Main category: cs.SI

TL;DR: 该论文提出了一种新的社交网络分析（SNA）方法，通过使用用户之间共享的附件作为网络中的边，以更深入地了解电子邮件档案中的社会结构。因为附件代表关系的“亲密性”且占据了邮件档案的大部分存储空间。该方法在Enron邮件语料库上进行了验证，并发现其生成的员工相似性群体与公司的组织结构图一致。


<details>
  <summary>Details</summary>
Motivation: 现有的基于电子邮件档案的社交网络分析（SNA）方法主要关注邮件头字段（如发件人、收件人）或邮件内容中的实体（如关键词、URL），但往往忽略了附件。然而，附件不仅代表了关系强度的“亲密性”表现，而且统计分析显示附件占据了电子邮件档案80-90%的磁盘空间，这意味着大部分数据在现有SNA中被忽视了。因此，本文旨在利用附件这一丰富且未充分利用的信息源来提供对社会结构更深入的洞察。

Method: 该研究从Enron邮件语料库中提取了通信网络和共享附件网络。然后，对这两种网络进行了度中心性、介数中心性、接近中心性和特征向量中心性等指标的分析，并比较了它们之间的差异。最后，利用最近邻算法为五名Enron员工生成了相似性群体，并与Enron的组织结构图进行对比以验证方法的有效性。

Result: 统计分析表明，在收集到的私人电子邮件档案和Enron邮件语料库中，附件平均贡献了80-90%的磁盘空间使用量。通过最近邻算法为五名Enron员工生成的相似性群体与Enron的组织结构图保持一致。

Conclusion: 通过将用户间共享的附件作为社交网络分析的边，可以为理解电子邮件档案中的社会结构提供更深入的洞察。这种方法通过其生成的相似性群体与组织结构图的一致性得到了验证，表明附件是分析组织内部人际关系的重要且有效的指标。

Abstract: There is an extensive body of research on Social Network Analysis (SNA) based
on the email archive. The network used in the analysis is generally extracted
either by capturing the email communication in From, To, Cc and Bcc email
header fields or by the entities contained in the email message. In the latter
case, the entities could be, for instance, the bag of words, url's, names,
phones, etc. It could also include the textual content of attachments, for
instance Microsoft Word documents, excel spreadsheets, or Adobe pdfs. The nodes
in this network represent users and entities. The edges represent communication
between users and relations to the entities. We suggest taking a different
approach to the network extraction and use attachments shared between users as
the edges. The motivation for this is two-fold. First, attachments represent
the "intimacy" manifestation of the relation's strength. Second, the
statistical analysis of private email archives that we collected and Enron
email corpus shows that the attachments contribute in average around 80-90% to
the archive's disk-space usage, which means that most of the data is presently
ignored in the SNA of email archives. Consequently, we hypothesize that this
approach might provide more insight into the social structure of the email
archive. We extract the communication and shared attachments networks from
Enron email corpus. We further analyze degree, betweenness, closeness, and
eigenvector centrality measures in both networks and review the differences and
what can be learned from them. We use nearest neighbor algorithm to generate
similarity groups for five Enron employees. The groups are consistent with
Enron's organizational chart, which validates our approach.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [158] [The Paradox of Spreadsheet Self-Efficacy: Social Incentives for Informal Knowledge Sharing in End-User Programming](https://arxiv.org/abs/2408.08068)
*Qing,Xia,Advait Sarkar,Duncan P. Brumby,Anna Cox*

Main category: cs.HC

TL;DR: 本研究探讨了个人（自我效能）、社会（声誉收益、同事间信任）和软件相关（编码努力）变量如何影响电子表格知识分享意愿。发现高自我效能和声誉收益预期能提高分享意愿，而编码努力会降低分享意愿。


<details>
  <summary>Details</summary>
Motivation: 旨在更好地理解个人、社会和软件相关变量如何影响电子表格知识分享意愿，尤其对终端用户程序员获取专业知识至关重要。

Method: 对100名行政和财务领域的电子表格用户进行了调查，并使用多元回归分析。

Result: 结果显示，高水平的电子表格自我效能和分享带来的声誉收益预期能预测更高的知识分享意愿；而认为知识编码费力的个体则表现出较低的知识分享意愿。此外，用户在一般电子表格熟练度上的自我效能感较低，但在工作相关情境中的自我效能感较高。

Conclusion: 研究结果表明，认识并设计考虑这些社会和个人变量，可以帮助避免有经验的个体不必要地避免分享，这对电子表格设计具有启示意义。

Abstract: Informal Knowledge Sharing (KS) is vital for end-user programmers to gain
expertise. To better understand how personal (self-efficacy), social
(reputational gains, trust between colleagues), and software-related
(codification effort) variables influence spreadsheet KS intention, we
conducted a multiple regressions analysis based on survey data from spreadsheet
users (n=100) in administrative and finance roles. We found that high levels of
spreadsheet self-efficacy and a perception that sharing would result in
reputational gains predicted higher KS intention, but individuals who found
knowledge codification effortful showed lower KS intention. We also observed
that regardless of occupation, users tended to report a lower sense of
self-efficacy in their general spreadsheet proficiency, despite also reporting
high self-efficacy in spreadsheet use for job-related contexts. Our findings
suggest that acknowledging and designing for these social and personal
variables can help avoid situations where experienced individuals refrain
unnecessarily from sharing, with implications for spreadsheet design.

</details>


### [159] [Buckaroo: A Direct Manipulation Visual Data Wrangler](https://arxiv.org/abs/2507.16073)
*Annabelle Warner,Andrew McNutt,Paul Rosen,El Kindi Rezig*

Main category: cs.HC

TL;DR: 本文介绍了一个名为 Buckaroo 的可视化系统，旨在简化数据整理工作。它能自动发现数据异常、推荐整理操作，并允许用户通过可视化方式直接操作数据，支持迭代修正。


<details>
  <summary>Details</summary>
Motivation: 数据整理（data wrangling）是数据科学开发中最耗时（占项目时间的80%）且易出错的阶段。传统方法如手动编码或使用电子表格既费力又容易导致数据不准确，影响后续任务质量。

Method: Buckaroo 是一个可视化系统，它具备三个主要功能：1) 自动识别并推荐存在异常的“有趣”数据组进行检查；2) 建议用户可选择的整理操作来修复异常；3) 允许用户通过可视化操作数据，即时显示操作效果并支持撤销/重做，以适应数据整理的迭代性质。

Result: Buckaroo 解决了数据整理中的挑战，通过可视化发现数据差异并允许即时修正，提高了数据质量，并减少了数据科学项目在数据整理阶段的时间消耗。

Conclusion: Buckaroo 通过其创新的可视化和交互功能，提供了一种更高效、更不易出错的数据整理方法，显著提升了数据整理的质量和效率。

Abstract: Preparing datasets -- a critical phase known as data wrangling -- constitutes
the dominant phase of data science development, consuming upwards of 80% of the
total project time. This phase encompasses a myriad of tasks: parsing data,
restructuring it for analysis, repairing inaccuracies, merging sources,
eliminating duplicates, and ensuring overall data integrity. Traditional
approaches, typically through manual coding in languages such as Python or
using spreadsheets, are not only laborious but also error-prone. These issues
range from missing entries and formatting inconsistencies to data type
inaccuracies, all of which can affect the quality of downstream tasks if not
properly corrected. To address these challenges, we present Buckaroo, a
visualization system to highlight discrepancies in data and enable on-the-spot
corrections through direct manipulations of visual objects. Buckaroo (1)
automatically finds "interesting" data groups that exhibit anomalies compared
to the rest of the groups and recommends them for inspection; (2) suggests
wrangling actions that the user can choose to repair the anomalies; and (3)
allows users to visually manipulate their data by displaying the effects of
their wrangling actions and offering the ability to undo or redo these actions,
which supports the iterative nature of data wrangling. A video companion is
available at https://youtu.be/iXdCYbvpQVE

</details>


### [160] [SheetMind: An End-to-End LLM-Powered Multi-Agent Framework for Spreadsheet Automation](https://arxiv.org/abs/2506.12339)
*Ruiyan Zhu,Xi Cheng,Ke Liu,Brian Zhu,Daniel Jin,Neeraj Parihar,Zhoutian Xu,Oliver Gao*

Main category: cs.HC

TL;DR: SheetMind是一个由大型语言模型驱动的模块化多智能体框架，通过自然语言指令实现电子表格自动化。它在单步任务中实现了80%的成功率，在多步指令中实现了约70%的成功率。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机是实现通过自然语言指令进行电子表格自动化，无需脚本或公式知识。

Method: SheetMind包含三个专门的智能体：管理器智能体分解任务，行动智能体使用BNF语法将指令转化为结构化命令，反射智能体验证对齐。它作为Google Sheets的Workspace扩展集成。

Result: 在基准数据集上的实验表明，在单步任务上成功率为80%，在多步指令上成功率约为70%，优于消融和基线变体。

Conclusion: 多智能体分解和基于语法的执行对于连接自然语言和电子表格功能非常有效。

Abstract: We present SheetMind, a modular multi-agent framework powered by large
language models (LLMs) for spreadsheet automation via natural language
instructions. The system comprises three specialized agents: a Manager Agent
that decomposes complex user instructions into subtasks; an Action Agent that
translates these into structured commands using a Backus Naur Form (BNF)
grammar; and a Reflection Agent that validates alignment between generated
actions and the user's original intent. Integrated into Google Sheets via a
Workspace extension, SheetMind supports real-time interaction without requiring
scripting or formula knowledge. Experiments on benchmark datasets demonstrate
an 80 percent success rate on single step tasks and approximately 70 percent on
multi step instructions, outperforming ablated and baseline variants. Our
results highlight the effectiveness of multi agent decomposition and grammar
based execution for bridging natural language and spreadsheet functionalities.

</details>


### [161] ["How do you even know that stuff?": Barriers to expertise sharing among spreadsheet users](https://arxiv.org/abs/2506.09216)
*Qing,Xia,Advait Sarkar,Duncan Brumby,Anna Cox*

Main category: cs.HC

TL;DR: 电子表格专家在知识共享方面遇到困难，这归因于社会规范、自我评价以及对协作的担忧，凸显了软件设计在初始学习性和长期协作学习之间的紧张关系。


<details>
  <summary>Details</summary>
Motivation: 电子表格协作提供了学习和专业知识共享的宝贵机会，但专家们往往未能传播他们的知识，这导致组织内部重要技术技能的流失。本文旨在探讨社会规范和信念如何影响知识共享行为。

Method: 对来自两个不同样本的31位专业电子表格用户进行了半结构化访谈。

Result: 电子表格提供者在将高度个性化的策略适应主观标准和评估共享的适当社会时机方面面临挑战。此外，对自身电子表格专业知识的冲突性自我评价、对知识价值的轻视性规范信念以及对协作可能带来的潜在中断的担忧，会进一步阻碍共享。

Conclusion: 这些发现反映了主要为初始学习性而设计的特性丰富软件中长期学习的挑战。本文提供了设计启示以应对这种张力，并展示了技术设计和社会动态之间的复杂互动如何塑造特性丰富软件中的协作学习行为。

Abstract: Spreadsheet collaboration provides valuable opportunities for learning and
expertise sharing between colleagues. Sharing expertise is essential for the
retention of important technical skillsets within organisations, but previous
studies suggest that spreadsheet experts often fail to disseminate their
knowledge to others. We suggest that social norms and beliefs surrounding the
value of spreadsheet use significantly influence user engagement in sharing
behaviours. To explore this, we conducted 31 semi-structured interviews with
professional spreadsheet users from two separate samples. We found that
spreadsheet providers face challenges in adapting highly personalised
strategies to often subjective standards and evaluating the appropriate social
timing of sharing. In addition, conflicted self-evaluations of one's
spreadsheet expertise, dismissive normative beliefs about the value of this
knowledge, and concerns about the potential disruptions associated with
collaboration can further deter sharing. We suggest these observations reflect
the challenges of long-term learning in feature-rich software designed
primarily with initial learnability in mind. We therefore provide implications
for design to navigate this tension. Overall, our findings demonstrate how the
complex interaction between technology design and social dynamics can shape
collaborative learning behaviours in the context of feature-rich software.

</details>


### [162] [Prompting in the Dark: Assessing Human Performance in Prompt Engineering for Data Labeling When Gold Labels Are Absent](https://arxiv.org/abs/2502.11267)
*Zeyu He,Saniya Naphade,Ting-Hao 'Kenneth' Huang*

Main category: cs.HC

TL;DR: 在没有黄金标准标签的情况下，用户在提示工程中表现不佳，自动优化工具也面临困难。


<details>
  <summary>Details</summary>
Motivation: 探究用户在缺乏黄金标准标签的情况下，对大型语言模型（LLMs）进行提示工程的能力，特别是在“摸黑提示”场景中，即用户迭代地提示LLMs来标注数据，而没有使用手动标注的基准。

Method: 开发了PromptingSheet（一个Google Sheets插件），使用户能够通过电子表格编写、修改和迭代地标注数据。通过一项有20名参与者参与的研究进行调查。

Result: 研究发现“摸黑提示”非常不可靠——在四次或更多次迭代后，只有9名参与者的标注准确性有所提高。像DSPy这样的自动化提示优化工具在黄金标签很少的情况下也表现不佳。

Conclusion: 研究结果强调了黄金标签的重要性，以及在人类提示工程中自动化支持的需求和风险，为未来的工具设计提供了见解。

Abstract: Millions of users prompt large language models (LLMs) for various tasks, but
how good are people at prompt engineering? Do users actually get closer to
their desired outcome over multiple iterations of their prompts? These
questions are crucial when no gold-standard labels are available to measure
progress. This paper investigates a scenario in LLM-powered data labeling,
"prompting in the dark," where users iteratively prompt LLMs to label data
without using manually-labeled benchmarks. We developed PromptingSheet, a
Google Sheets add-on that enables users to compose, revise, and iteratively
label data through spreadsheets. Through a study with 20 participants, we found
that prompting in the dark was highly unreliable -- only 9 participants
improved labeling accuracy after four or more iterations. Automated prompt
optimization tools like DSPy also struggled when few gold labels were
available. Our findings highlight the importance of gold labels and the needs,
as well as the risks, of automated support in human prompt engineering,
providing insights for future tool design.

</details>


### [163] [When Copilot Becomes Autopilot: Generative AI's Critical Risk to Knowledge Work and a Critical Solution](https://arxiv.org/abs/2412.15030)
*Advait Sarkar,Xiaotong,Xu,Neil Toronto,Ian Drosos,Christian Poelitz*

Main category: cs.HC

TL;DR: 生成式AI在知识工作中带来机遇的同时，也存在“幻觉”和削弱人类批判性思维的风险。本文提出应设计AI界面以促进批判性思维，并展示了一个原型系统，该系统利用生成式AI提出筛选标准，并生成“启发性批判”来评估AI的建议，从而探索了AI辅助知识工作中的批判性思维工具设计空间。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在知识工作中（例如电子表格）带来便利的同时，其“幻觉”特性以及可能削弱人类批判性思维的能力构成重要风险。最大的风险是随着更多工作被委托给AI，人类整体性和严谨性评估问题及其解决方案的能力会下降。因此，有必要设计能培养和鼓励批判性思维的生成式AI系统界面。

Method: 本文讨论了一个用于电子表格中“批判性筛选”活动的原型系统。该系统使用生成式AI来建议筛选标准，并应用这些标准对电子表格中的行进行排序。此外，它还生成“启发性批判”（provocations），即短文本片段，用于批评AI生成的标准，突出其风险、缺点和替代方案。

Result: 该原型系统开辟了一个丰富且完全未被探索的设计空间，用于现代AI辅助知识工作中的批判性思维工具。它展示了AI作为批评者或“启发者”的潜力。

Conclusion: 解决生成式AI风险的关键在于设计能够促进批判性思维的AI系统界面。该原型系统通过生成批判性标准和“启发性批判”，为AI作为批评者或启发者的角色提供了新的研究方向，包括启发性批判出现的时间、形式、内容以及潜在的设计权衡。

Abstract: Generative AI, with its tendency to "hallucinate" incorrect results, may pose
a risk to knowledge work by introducing errors. On the other hand, it may also
provide unprecedented opportunities for users, particularly non-experts, to
learn and apply advanced software features and greatly increase the scope and
complexity of tasks they can successfully achieve.
  As an example of a complex knowledge workflow that is subject to risks and
opportunities from generative AI, we consider the spreadsheet. AI
hallucinations are an important challenge, but they are not the greatest risk
posed by generative AI to spreadsheet workflows. Rather, as more work can be
safely delegated to AI, the risk is that human critical thinking -- the ability
to holistically and rigorously evaluate a problem and its solutions -- is
degraded in the process. The solution is to design the interfaces of generative
AI systems deliberately to foster and encourage critical thinking in knowledge
work, building primarily on a long history of research on critical thinking
tools for education.
  We discuss a prototype system for the activity of critical shortlisting in
spreadsheets. The system uses generative AI to suggest shortlisting criteria
and applies these criteria to sort rows in a spreadsheet. It also generates
"provocations": short text snippets that critique the AI-generated criteria,
highlighting risks, shortcomings, and alternatives. Our prototype opens up a
rich and completely unexplored design space of critical thinking tools for
modern AI-assisted knowledge work. We outline a research agenda for AI as a
critic or provocateur, including questions about where and when provocations
should appear, their form and content, and potential design trade-offs.

</details>


### [164] [Understanding and Evaluating Trust in Generative AI and Large Language Models for Spreadsheets](https://arxiv.org/abs/2412.14062)
*Simon Thorne*

Main category: cs.HC

TL;DR: 生成式AI和LLMs在自动化电子表格公式创建方面具有潜力，但由于幻觉和偏差等问题，其输出的信任度值得关注。本文提出了一个基于透明度（可解释性、可见性）和可靠性（一致性、准确性、伦理考虑）的框架来评估公式的信任度，并探讨了幻觉、训练数据偏差等驱动因素，以及不信任技术带来的后果。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和大型语言模型（LLMs）在自动化电子表格公式创建方面前景广阔，但由于幻觉、偏见和用户技能差异，其输出不能被假定为准确或值得信赖。

Method: 本文提出了一个信任度框架，基于评估公式的透明度（通过可解释性和可见性）和可靠性（通过可靠性及伦理考量，如偏见和公平性）来衡量。文章还研究了幻觉、训练数据偏见和构建不当的提示等因素对这些指标的影响。

Result: 提出了一种评估生成式AI生成电子表格公式信任度的框架，该框架考虑了透明度（可解释性、可见性）和可靠性（一致性、准确性、伦理考量），并分析了导致不信任的驱动因素，如幻觉和训练数据偏差。

Conclusion: 为了解决生成式AI在电子表格公式创建中存在的信任问题，需要一个全面的框架来评估其透明度和可靠性，并理解导致不信任的各种驱动因素及其潜在后果。

Abstract: Generative AI and Large Language Models (LLMs) hold promise for automating
spreadsheet formula creation. However, due to hallucinations, bias and variable
user skill, outputs obtained from generative AI cannot be assumed to be
accurate or trustworthy. To address these challenges, a trustworthiness
framework is proposed based on evaluating the transparency and dependability of
the formula. The transparency of the formula is explored through explainability
(understanding the formula's reasoning) and visibility (inspecting the
underlying algorithms). The dependability of the generated formula is evaluated
in terms of reliability (consistency and accuracy) and ethical considerations
(bias and fairness). The paper also examines the drivers to these metrics in
the form of hallucinations, training data bias and poorly constructed prompts.
Finally, examples of mistrust in technology are considered and the consequences
explored.

</details>


### [165] [Dynamic Prompt Middleware: Contextual Prompt Refinement Controls for Comprehension Tasks](https://arxiv.org/abs/2412.02357)
*Ian Drosos,Jack Williams,Advait Sarkar,Nicholas Wilson*

Main category: cs.HC

TL;DR: 本文探讨了生成式AI的提示中间件，特别是动态和静态提示细化控制（PRC）。研究发现，动态PRC在生成AI解释方面提供了更好的用户控制和体验。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的有效提示对许多用户来说具有挑战性，尤其是在表达理解任务的上下文时。现有的提示中间件有助于提示构建，但用户仍难以充分控制以获得符合其偏好的AI响应。

Method: 1. 进行了一项形成性调查（n=38），研究用户对AI生成解释的控制需求。2. 实现了两种提示中间件方法：动态提示细化控制（Dynamic PRC）和静态提示细化控制（Static PRC）。3. 通过一项对照用户研究（n=16）评估了这两种方法。

Result: 1. 用户偏爱动态PRC方法。2. 动态PRC提供了更多控制，降低了提供上下文的障碍，并鼓励任务的探索和反思。3. 推理不同生成控件对最终输出的影响仍然具有挑战性。

Conclusion: 动态提示中间件可以通过提供更大的控制并引导用户获得更好的AI响应来改善生成式AI工作流的用户体验。

Abstract: Effective prompting of generative AI is challenging for many users,
particularly in expressing context for comprehension tasks such as explaining
spreadsheet formulas, Python code, and text passages. Prompt middleware aims to
address this barrier by assisting in prompt construction, but barriers remain
for users in expressing adequate control so that they can receive AI-responses
that match their preferences.
  We conduct a formative survey (n=38) investigating user needs for control
over AI-generated explanations in comprehension tasks, which uncovers a
trade-off between standardized but predictable support for prompting, and
adaptive but unpredictable support tailored to the user and task. To explore
this trade-off, we implement two prompt middleware approaches: Dynamic Prompt
Refinement Control (Dynamic PRC) and Static Prompt Refinement Control (Static
PRC). The Dynamic PRC approach generates context-specific UI elements that
provide prompt refinements based on the user's prompt and user needs from the
AI, while the Static PRC approach offers a preset list of generally applicable
refinements.
  We evaluate these two approaches with a controlled user study (n=16) to
assess the impact of these approaches on user control of AI responses for
crafting better explanations. Results show a preference for the Dynamic PRC
approach as it afforded more control, lowered barriers to providing context,
and encouraged exploration and reflection of the tasks, but that reasoning
about the effects of different generated controls on the final output remains
challenging. Drawing on participant feedback, we discuss design implications
for future Dynamic PRC systems that enhance user control of AI responses. Our
findings suggest that dynamic prompt middleware can improve the user experience
of generative AI workflows by affording greater control and guide users to a
better AI response.

</details>


### [166] [Subject integration with spreadsheets -- Ignoring education is the greatest risk ever](https://arxiv.org/abs/2409.12975)
*Mária Csernoch,Ádám Gulácsi,Júlia Csernoch*

Main category: cs.HC

TL;DR: 在TPACK框架下，本文探讨了如何通过科目整合，利用电子表格解决三年级传统任务，以实现在学校中引入有意义的数字化，并培养计算思维能力。


<details>
  <summary>Details</summary>
Motivation: 在学校中引入有意义的数字化和信息化；缩小“严肃信息学”与“数字素养”之间的差距；将信息学课程情境化。

Method: 详细说明了如何通过电子表格解决三个传统的小学三年级任务。

Result: 展示了教师和学生可以发展哪些技能、能力和计算机科学知识。同时，揭示了分析、理解、规划和讨论任务与电子表格操作本身同等重要，该过程对学生未来的职业准备至关重要。

Conclusion: 通过如电子表格等工具进行学科整合，可以培养学生未来职业所需的关键技能，强调了计算思维过程与数字工具使用同样重要。

Abstract: Within the framework of Technological Pedagogical and Content Knowledge,
subject integration is one possible solution for the introduction of meaningful
digitalization and digitization in schools. This process incorporates that any
school subject can be taught with digital support, informatics (computer)
classes can be contextualized, and the gap between 'serious informatics' and
'digital literacy' can be minimized. The present paper details how three
traditional Grade 3 tasks can be solved in spreadsheets, what skills,
competencies, and computer science knowledge of both teachers and students can
be developed. The solutions also reveal that analysing, understanding,
planning, and discussing tasks is as important as the activity in the
spreadsheets, which process plays a crucial role in the preparation of students
for their future jobs.

</details>


### [167] [Exploring Higher Education Competencies through Spreadsheet Self-Assessment and Time](https://arxiv.org/abs/2409.12974)
*Maria Csernoch,Judit T. Kiss,Viktor Takács,Domicián Máté*

Main category: cs.HC

TL;DR: 大学生普遍高估自己的电子表格能力，并且在数字环境中需要更多时间才能达到与纸质任务相同的分数，这挑战了“数字原住民”的假设。


<details>
  <summary>Details</summary>
Motivation: 旨在通过自我评估和实际问题解决练习，探讨高等教育学生的电子表格能力和可靠性。并检验数字原住民在Excel中表现优于纸质任务的假设。

Method: 通过自我评估和解决实际问题（包括纸质和Excel任务）进行研究。

Result: 学生往往会不准确地评估自己的电子表格能力（倾向于高估）。学生在数字环境中获得相同高分所需的时间至少是纸质任务的两倍。最初关于Excel表现更好的假设未被证实。

Conclusion: 强调了在高等教育背景下，尤其是在技术驱动的学科中，准确的自我评估在数字技能发展和时间管理中的重要性，并质疑了数字原住民无需计算机科学教育的假设。

Abstract: The present paper aims to explore higher education students' spreadsheet
competencies and reliability through self-assessment and real-world
problem-solving practices. Digital natives alleged skills and competences
allowed us to hypothesize that students perform better in Excel than on paper,
but the findings cannot confirm this hypothesis. However, our results indicate
that students tend to inaccurately assess their spreadsheet competencies
compared to their actual performance in both paper-based and Excel tasks. It
has also be found that students need at least twice as much time to achieve the
same high scores in the digital environment as they do on paper. The results
violated the widely accepted assumption that digital native students do not
need computer science education, since they are born with it. This study
highlights the importance of accurate self-assessment in digital skill
development and time management within higher education contexts, particularly
in technology-driven disciplines.

</details>


### [168] [Data Guards: Challenges and Solutions for Fostering Trust in Data](https://arxiv.org/abs/2407.14042)
*Nicole Sultanum,Dennis Bromley,Michael Correll*

Main category: cs.HC

TL;DR: 这篇论文探讨了数据驱动决策中数据有效性面临的威胁，以及如何建立数据信任的问题。通过访谈数据生产者和消费者，论文发现数据验证和核实的需求，并提出了“数据卫士”的概念来增强数据制品的信任度。


<details>
  <summary>Details</summary>
Motivation: 数据驱动决策面临着数据有效性的诸多威胁，因此，使用数据（尤其是新的或不熟悉的数据）需要一定程度的信任或验证。这篇论文旨在探讨如何建立这种信任。

Method: 本文通过对数据制品（如电子表格、图表和仪表盘）的生产者和消费者进行一系列访谈，以了解建立数据信任的策略和障碍。

Result: 研究发现，在数据验证和核实方面存在反复出现的需求，但缺乏现有标准，尤其是在数据消费者中。

Conclusion: 因此，本文提出了一套“数据卫士”：旨在培养对数据制品信任的方法和工具。

Abstract: From dirty data to intentional deception, there are many threats to the
validity of data-driven decisions. Making use of data, especially new or
unfamiliar data, therefore requires a degree of trust or verification. How is
this trust established? In this paper, we present the results of a series of
interviews with both producers and consumers of data artifacts (outputs of data
ecosystems like spreadsheets, charts, and dashboards) aimed at understanding
strategies and obstacles to building trust in data. We find a recurring need,
but lack of existing standards, for data validation and verification,
especially among data consumers. We therefore propose a set of data guards:
methods and tools for fostering trust in data artifacts.

</details>


### [169] [Smart Portable Computer](https://arxiv.org/abs/2405.05292)
*Niladri Das*

Main category: cs.HC

TL;DR: “便携式智能电脑”是一款紧凑、节能、经济且性能媲美台式机的设备，旨在解决疫情期间获取昂贵且配置不足电脑的难题，并提供无缝的桌面体验，支持编程和办公。


<details>
  <summary>Details</summary>
Motivation: 在 COVID-19 疫情期间，组织和学校转向虚拟平台，学生难以获得价格合理且配置充足的电脑（起价 15,000 印度卢比却性能不足），同时依赖笔记本电脑工作的人也觉得传统方式笨重。

Method: 提出并设计了一种“便携式智能电脑”，它具有与传统台式机相当的速度和性能，但体积紧凑、节能且成本效益高。该设备能够提供无缝的桌面体验，支持文档编辑、多标签浏览、电子表格管理、演示文稿创建，并支持 Python、C、C++ 等编程语言以及 Keil 和 Xilinx 等编译器。

Result: 该“便携式智能电脑”成功地以紧凑、节能且经济高效的方式，提供了与传统台式机相媲美的速度和性能。它能够为用户提供无缝的桌面体验，满足日常办公需求，并支持多种编程语言和编译器，满足程序员的需求。

Conclusion: “便携式智能电脑”代表了计算领域的未来，通过提供一种创新、高效且经济的解决方案，有效应对了疫情期间及未来对高性能、便携式计算设备的需求。

Abstract: Amidst the COVID-19 pandemic, with many organizations, schools, colleges, and
universities transitioning to virtual platforms, students encountered
difficulties in acquiring PCs such as desktops or laptops. The starting prices,
around 15,000 INR, often failed to offer adequate system specifications, posing
a challenge for consumers. Additionally, those reliant on laptops for work
found the conventional approach cumbersome. Enter the "Portable Smart
Computer," a leap into the future of computing. This innovative device boasts
speed and performance comparable to traditional desktops but in a compact,
energy-efficient, and cost-effective package. It delivers a seamless desktop
experience, whether one is editing documents, browsing multiple tabs, managing
spreadsheets, or creating presentations. Moreover, it supports programming
languages like Python, C, C++, as well as compilers such as Keil and Xilinx,
catering to the needs of programmers.

</details>


### [170] ["My toxic trait is thinking I'll remember this": gaps in the learner experience of video tutorials for feature-rich software](https://arxiv.org/abs/2404.07114)
*Ian Drosos,Advait Sarkar,Andrew D. Gordon*

Main category: cs.HC

TL;DR: 该论文调查了在使用如Excel等功能丰富的软件视频教程时，学习者遇到的阻碍学习的“差距”。通过分析用户评论和采访创作者，该研究开发了这些差距的理论和分类法，并提出了旨在解决这些差距的设计方案。


<details>
  <summary>Details</summary>
Motivation: 视频教程是学习的流行媒介，但学习者在观看和跟进时会遇到阻碍学习的“差距”。本研究旨在探究在功能丰富的软件视频教程中遇到的这些差距。

Method: 1. 收集并分析来自YouTube、TikTok和Instagram上43位创作者发布的90个Microsoft Excel视频教程的360条观看者评论，以识别和分类差距。 2. 对8位极具影响力的教程创作者进行情境访谈，了解观看者遇到的差距以及创作者如何应对。 3. 深入了解创作者的创作过程和挫折。 4. 向创作者展示两种旨在解决评论分析中发现的差距的设计方案，以获取反馈和替代设计思路。

Result: 研究开发了一套关于视频教程中阻碍学习的“差距”的理论和分类法。通过分析观看者评论，识别了这些差距如何构成学习障碍。同时，获得了创作者在制作视频教程时的创作过程和挫折感方面的见解，并提出了旨在解决这些差距的设计方案。

Conclusion: 该研究有助于理解在复杂软件视频教程中阻碍学习的障碍，并为解决这些障碍提供了潜在的设计解决方案。

Abstract: Video tutorials are a popular medium for informal and formal learning.
However, when learners attempt to view and follow along with these tutorials,
they encounter what we call gaps, that is, issues that can prevent learning. We
examine the gaps encountered by users of video tutorials for feature-rich
software, such as spreadsheets. We develop a theory and taxonomy of such gaps,
identifying how they act as barriers to learning, by collecting and analyzing
360 viewer comments from 90 Microsoft Excel video tutorials published by 43
creators across YouTube, TikTok, and Instagram. We conducted contextual
interviews with 8 highly influential tutorial creators to investigate the gaps
their viewers experience and how they address them. Further, we obtain insights
into their creative process and frustrations when creating video tutorials.
Finally, we present creators with two designs that aim to address gaps
identified in the comment analysis for feedback and alternative design ideas.

</details>


### [171] [Supporting Annotators with Affordances for Efficiently Labeling Conversational Data](https://arxiv.org/abs/2403.07762)
*Austin Z. Henley,David Piorkowski*

Main category: cs.HC

TL;DR: 该论文介绍了一种名为 CAL 的新型数据标注界面，旨在减少相比传统方法（如电子表格）所需的标注工作量和繁琐性。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统需要大量正确标注的数据，但众包标注耗时且昂贵。

Method: 作者设计了 CAL 界面，其特点包括阻止不合适的标签选择、在需要时引导用户选择合适的标签、将标注文档整合到界面中以及提供高效查看先前标签的方法。他们实现了 CAL 并进行了一项用户研究，将其与标准电子表格进行了比较。

Result: 研究发现，使用 CAL 的用户报告认知负荷较低，任务时间没有增加，用户认为 CAL 更易于使用，并且用户更喜欢 CAL 而非电子表格。

Conclusion: CAL 是一种有效的数据标注界面，它在不增加任务时间的情况下，提升了用户体验（更低的认知负荷、更易用、更受青睐）。

Abstract: Without well-labeled ground truth data, machine learning-based systems would
not be as ubiquitous as they are today, but these systems rely on substantial
amounts of correctly labeled data. Unfortunately, crowdsourced labeling is time
consuming and expensive. To address the concerns of effort and tedium, we
designed CAL, a novel interface to aid in data labeling. We made several key
design decisions for CAL, which include preventing inapt labels from being
selected, guiding users in selecting an appropriate label when they need
assistance, incorporating labeling documentation into the interface, and
providing an efficient means to view previous labels. We implemented a
production-quality implementation of CAL and report a user-study evaluation
that compares CAL to a standard spreadsheet. Key findings of our study include
users using CAL reported lower cognitive load, did not increase task time,
users rated CAL to be easier to use, and users preferred CAL over the
spreadsheet.

</details>


### [172] [Prompting for Discovery: Flexible Sense-Making for AI Art-Making with Dreamsheets](https://arxiv.org/abs/2310.09985)
*Shm Garanganao Almeda,J. D. Zamfirescu-Pereira,Kyu Won Kim,Pradeep Mani Rathnam,Bjoern Hartmann*

Main category: cs.HC

TL;DR: DreamSheets是一个基于LLM功能的表格界面，用于辅助提示词构建和同步显示生成结果，旨在帮助用户在文本到图像（TTI）模型的设计空间中进行探索。


<details>
  <summary>Details</summary>
Motivation: 文本到图像（TTI）模型的设计空间探索面临巨大且不透明的输出空间，微小的提示词调整可能导致截然不同的图像。因此，需要有接口来可靠地引导用户进行提示词空间探索，以获得有趣的结果。

Method: 本文设计了一个名为DreamSheets的探索工具，它在一个电子表格界面中集成LLM功能，用于辅助提示词构建和同步显示生成结果。其灵活的布局和新颖的生成功能支持用户自定义工作流程的实验。通过一项初步实验室研究和一项针对五位专业艺术家的长期研究，揭示了参与者应对TTI设计空间探索挑战的策略以及所需界面功能，例如使用文本生成来定义局部探索“轴”。

Result: 两项研究（一项初步实验室研究和一项针对五位专业艺术家的长期研究）揭示了参与者在应对TTI设计空间探索挑战时所使用的策略，以及支持这些策略所需的界面功能，例如使用文本生成来定义局部“探索轴”。

Conclusion: 作者将这些见解提炼成一个UI模型，以指导未来的界面设计。

Abstract: Design space exploration (DSE) for Text-to-Image (TTI) models entails
navigating a vast, opaque space of possible image outputs, through a
commensurately vast input space of hyperparameters and prompt text. Minor
adjustments to prompt input can surface unexpectedly disparate images. How can
interfaces support end-users in reliably steering prompt-space explorations
towards interesting results? Our design probe, DreamSheets, supports
exploration strategies with LLM-based functions for assisted prompt
construction and simultaneous display of generated results, hosted in a
spreadsheet interface. The flexible layout and novel generative functions
enable experimentation with user-defined workflows. Two studies, a preliminary
lab study and a longitudinal study with five expert artists, revealed a set of
strategies participants use to tackle the challenges of TTI design space
exploration, and the interface features required to support them - like using
text-generation to define local "axes" of exploration. We distill these
insights into a UI mockup to guide future interfaces.

</details>


### [173] [Does Using ChatGPT Result in Human Cognitive Augmentation?](https://arxiv.org/abs/2401.11042)
*Ron Fulbright,Miranda Morrison*

Main category: cs.HC

TL;DR: 本论文调查了使用 ChatGPT 是否能增强人类认知能力，发现并非所有任务都能实现认知增强，甚至可能产生负面影响。


<details>
  <summary>Details</summary>
Motivation: 人类使用工具能提升认知表现，例如计算器。然而，近期无监督深度机器学习已在某些领域超越人类。新型认知系统 ChatGPT 出现后，有必要研究其对人类认知增强的影响。

Method: 通过两个实验，比较使用 ChatGPT 和不使用 ChatGPT 生成的响应。

Result: 使用 ChatGPT 并不总能带来认知增强，在某些类型任务中，它尚未能取代人类的判断、辨别和评估。甚至观察到 ChatGPT 导致用户被误导，产生负面认知增强。

Conclusion: ChatGPT 并不总能增强人类认知，也未能取代人类在特定任务中的判断力，有时甚至会产生负面影响。

Abstract: Human cognitive performance is enhanced by the use of tools. For example, a
human can produce a much greater, and more accurate, volume of mathematical
calculation in a unit of time using a calculator or a spreadsheet application
on a computer. Such tools have taken over the burden of lower level cognitive
grunt work but the human still serves the role of the expert performing higher
level thinking and reasoning. Recently, however, unsupervised, deep, machine
learning has produced cognitive systems able to outperform humans in several
domains. When humans use these tools in a human cog ensemble, the cognitive
ability of the human is augmented. In some cases, even non experts can achieve,
and even exceed, the performance of experts in a particular domain, synthetic
expertise. A new cognitive system, ChatGPT, has burst onto the scene during the
past year. This paper investigates human cognitive augmentation due to using
ChatGPT by presenting the results of two experiments comparing responses
created using ChatGPT with results created not using ChatGPT. We find using
ChatGPT does not always result in cognitive augmentation and does not yet
replace human judgement, discernment, and evaluation in certain types of tasks.
In fact, ChatGPT was observed to result in misleading users resulting in
negative cognitive augmentation.

</details>


### [174] [Co-audit: tools to help humans double-check AI-generated content](https://arxiv.org/abs/2310.01297)
*Andrew D. Gordon,Carina Negreanu,José Cambronero,Rasika Chakravarthy,Ian Drosos,Hao Fang,Bhaskar Mitra,Hannah Richardson,Advait Sarkar,Stephanie Simmons,Jack Williams,Ben Zorn*

Main category: cs.HC

TL;DR: 该论文讨论了协同审计工具，旨在帮助用户检查AI生成的内容，尤其是复杂输出（如电子表格），并阐述了其重要性、原则和挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容日益复杂，用户难以核实其正确性。协同审计工具应运而生，以辅助用户检查AI输出，补充了提示工程技术。

Method: 论文描述了针对由生成模型驱动的电子表格计算的协同审计工具的最新研究。解释了协同审计体验对生成式AI应用的重要性（尤其是在质量关键且错误后果严重的情况下），并提出了一系列初步的协同审计原则，以及研究挑战。

Result: 论文指出，在质量至关重要且错误后果严重的生成式AI应用中，协同审计体验是必不可少的。它提出了协同审计的初步原则列表。

Conclusion: 协同审计工具对于确保复杂AI生成内容的质量和正确性至关重要，特别是在电子表格计算等错误会产生严重后果的应用中。未来需要进一步研究以完善原则和应对挑战。

Abstract: Users are increasingly being warned to check AI-generated content for
correctness. Still, as LLMs (and other generative models) generate more complex
output, such as summaries, tables, or code, it becomes harder for the user to
audit or evaluate the output for quality or correctness. Hence, we are seeing
the emergence of tool-assisted experiences to help the user double-check a
piece of AI-generated content. We refer to these as co-audit tools. Co-audit
tools complement prompt engineering techniques: one helps the user construct
the input prompt, while the other helps them check the output response. As a
specific example, this paper describes recent research on co-audit tools for
spreadsheet computations powered by generative models. We explain why co-audit
experiences are essential for any application of generative AI where quality is
important and errors are consequential (as is common in spreadsheet
computations). We propose a preliminary list of principles for co-audit, and
outline research challenges.

</details>


### [175] ["What It Wants Me To Say": Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models](https://arxiv.org/abs/2304.06597)
*Michael Xieyang Liu,Advait Sarkar,Carina Negreanu,Ben Zorn,Jack Williams,Neil Toronto,Andrew D. Gordon*

Main category: cs.HC

TL;DR: 代码生成大型语言模型（LLMs）在为非专业用户生成代码时面临抽象匹配的挑战。本文提出“接地抽象匹配”方法，通过将生成的代码翻译回自然语言来帮助用户理解如何有效地构建查询，以在电子表格中进行数据分析。研究表明，该方法能提高用户对模型能力和所需语言的理解。


<details>
  <summary>Details</summary>
Motivation: 代码生成大型语言模型可以将自然语言翻译成代码，但非专业用户难以学习如何有效地使用自然语言提示来指导代码生成。这种挑战被称为“抽象匹配”，本文在通过Codex生成Python代码进行电子表格数据分析的背景下探讨了这个问题。

Method: 本文提出“接地抽象匹配”方法，通过将生成的Python代码翻译回“系统且可预测的自然语言表达”来弥合抽象鸿沟。作者进行了一项包含24名参与者的组间、有声思维研究，将这种接地方法与基于现有查询框架原则的非接地方法进行了比较。

Result: 研究发现，接地方法提高了最终用户对代码生成模型的范围、能力以及有效使用该模型所需语言的理解。

Conclusion: 接地抽象匹配通过将代码翻译回自然语言，帮助非专业用户更好地理解和有效地使用代码生成模型。

Abstract: Code-generating large language models translate natural language into code.
However, only a small portion of the infinite space of naturalistic utterances
is effective at guiding code generation. For non-expert end-user programmers,
learning this is the challenge of abstraction matching. We examine this
challenge in the specific context of data analysis in spreadsheets, in a system
that maps the users natural language query to Python code using the Codex
generator, executes the code, and shows the result. We propose grounded
abstraction matching, which bridges the abstraction gap by translating the code
back into a systematic and predictable naturalistic utterance. In a
between-subjects, think-aloud study (n=24), we compare grounded abstraction
matching to an ungrounded alternative based on previously established query
framing principles. We find that the grounded approach improves end-users'
understanding of the scope and capabilities of the code-generating model, and
the kind of language needed to use it effectively.

</details>


### [176] [Team OS's System for Dialogue Robot Competition 2022](https://arxiv.org/abs/2210.09928)
*Yuki Kubo,Ryo Yanagimoto,Hayato Futase,Mikio Nakano,Zhaojie Luo,Kazunori Komatani*

Main category: cs.HC

TL;DR: 本文介绍了一个名为OSbot的对话机器人系统，该系统专为2022年对话机器人竞赛开发，其对话流程基于手动状态转换，并利用关键词提取和情感分析。该系统在初赛中获得了第三名。


<details>
  <summary>Details</summary>
Motivation: 本文的目的是描述OSbot，一个为2022年对话机器人竞赛开发的对话机器人系统。

Method: 对话流程基于手动描述的状态转换；转换条件利用关键词提取和情感分析；通过电子表格管理转换以便于查看和编辑；关键词提取基于命名实体提取和预定义关键词集；情感分析是基于文本的，使用SVM并用多模态对话语料库Hazumi进行训练；使用日志功能快速检查和编辑对话流程。

Result: 该系统在竞赛的预赛中获得了第三名。

Conclusion: OSbot是一个使用状态转换、关键词提取和情感分析的对话机器人系统，在2022年对话机器人竞赛的预赛中表现良好，获得了第三名。

Abstract: This paper describes our dialogue robot system, OSbot, developed for Dialogue
Robot Competition 2022. The dialogue flow is based on state transitions
described manually and the transition conditions use the results of keyword
extraction and sentiment analysis. The transitions can be easily viewed and
edited by managing them on a spreadsheet. The keyword extraction is based on
named entity extraction and our predefined keyword set. The sentiment analysis
is text-based and uses SVM, which was trained with the multimodal dialogue
corpus Hazumi. We quickly checked and edited a dialogue flow by using a logging
function. In the competition's preliminary round, our system ended up in third
place.

</details>


### [177] [What is it like to program with artificial intelligence?](https://arxiv.org/abs/2208.06213)
*Advait Sarkar,Andrew D. Gordon,Carina Negreanu,Christian Poelitz,Sruti Srinivasa Ragavan,Ben Zorn*

Main category: cs.HC

TL;DR: 本文探讨了大型语言模型辅助编程与现有编程辅助方式的异同，并强调了它作为一种新型编程方式的独特属性和挑战，尤其是在非专业用户场景下。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型在代码生成方面的兴起，本文旨在探讨LLM辅助编程与现有程序员辅助概念的异同，并讨论将LLM应用于终端用户编程时可能出现的问题和研究挑战。

Method: 作者借鉴了公开的LLM辅助编程经验报告、先前的可用性与设计研究，以及一项针对非专业终端用户程序员进行的用户研究观察。

Result: 研究发现，LLM辅助编程与编译、结对编程、通过搜索和重用进行编程等方式有一些共同点，但在技术可能性和实际体验上存在根本性差异。因此，LLM辅助编程应被视为一种具有独特属性和挑战的新型编程方式。用户研究揭示了将大型语言模型应用于终端用户编程，特别是针对编程经验较少的用户时，可能出现的问题和开放的研究挑战。

Conclusion: LLM辅助编程是一种独特的编程范式，具有其自身的特点和挑战，尤其是在非专业终端用户程序员使用时，这为未来的研究提供了新的方向。

Abstract: Large language models, such as OpenAI's codex and Deepmind's AlphaCode, can
generate code to solve a variety of problems expressed in natural language.
This technology has already been commercialised in at least one widely-used
programming editor extension: GitHub Copilot.
  In this paper, we explore how programming with large language models
(LLM-assisted programming) is similar to, and differs from, prior
conceptualisations of programmer assistance. We draw upon publicly available
experience reports of LLM-assisted programming, as well as prior usability and
design studies. We find that while LLM-assisted programming shares some
properties of compilation, pair programming, and programming via search and
reuse, there are fundamental differences both in the technical possibilities as
well as the practical experience. Thus, LLM-assisted programming ought to be
viewed as a new way of programming with its own distinct properties and
challenges.
  Finally, we draw upon observations from a user study in which non-expert end
user programmers use LLM-assisted tools for solving data tasks in spreadsheets.
We discuss the issues that might arise, and open research challenges, in
applying large language models to end-user programming, particularly with users
who have little or no programming expertise.

</details>


### [178] [MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization](https://arxiv.org/abs/2209.05739)
*Lu Ying,Xinhuan Shu,Dazhen Deng,Yuchen Yang,Tan Tang,Lingyun Yu,Yingcai Wu*

Main category: cs.HC

TL;DR: 本文提出了 MetaGlyph，一个自动生成基于隐喻的字形可视化（MGV）的系统，通过选择隐喻图像并利用蒙特卡洛树搜索来构建 MGV。


<details>
  <summary>Details</summary>
Motivation: 创建隐喻字形可视化（MGV）并非易事，需要深入理解数据和专业设计技能。

Method: 首先对现有 MGV 设计进行定性分析，以理解隐喻体现和字形设计。然后提出了一个新颖的 MGV 生成框架，通过自动选择隐喻图像并使用蒙特卡洛树搜索算法来探索 MGV 设计，该算法根据数据重要性、语义相关性和字形不重叠来关联视觉元素与数据维度。系统还提供编辑反馈功能。

Result: 通过一系列示例和一个使用场景展示了 MetaGlyph 的应用。

Conclusion: 通过专家访谈验证了其有效性。

Abstract: Glyph-based visualization achieves an impressive graphic design when
associated with comprehensive visual metaphors, which help audiences
effectively grasp the conveyed information through revealing data semantics.
However, creating such metaphoric glyph-based visualization (MGV) is not an
easy task, as it requires not only a deep understanding of data but also
professional design skills. This paper proposes MetaGlyph, an automatic system
for generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct
a qualitative analysis to understand the design of current MGVs from the
perspectives of metaphor embodiment and glyph design. Based on the results, we
introduce a novel framework for generating MGVs by metaphoric image selection
and an MGV construction. Specifically, MetaGlyph automatically selects
metaphors with corresponding images from online resources based on the input
data semantics. We then integrate a Monte Carlo tree search algorithm that
explores the design of an MGV by associating visual elements with data
dimensions given the data importance, semantic relevance, and glyph
non-overlap. The system also provides editing feedback that allows users to
customize the MGVs according to their design preferences. We demonstrate the
use of MetaGlyph through a set of examples, one usage scenario, and validate
its effectiveness through a series of expert interviews.

</details>


### [179] [PoVRPoint: Authoring Presentations in Mobile Virtual Reality](https://arxiv.org/abs/2201.06337)
*Verena Biener,Travis Gesslein,Daniel Schneider,Felix Kawala,Alexander Otte,Per Ola Kristensson,Michel Pahud,Eyal Ofek,Cuauhtli Campos,Matjaž Kljun,Klen Čopič Pucihar,Jens Grubert*

Main category: cs.HC

TL;DR: 该论文提出了PoVRPoint，一个基于VR的移动设备演示文稿制作工具，并展示了其在幻灯片识别和对象重排方面的优势。


<details>
  <summary>Details</summary>
Motivation: VR在知识工作（如文本输入和电子表格交互）中已有研究。本文受此启发，旨在探索VR在移动环境中制作演示文稿的设计空间，以补充传统输入设备，提供大型三维输出空间和空间输入。

Method: 提出PoVRPoint，一套结合移动设备（如平板电脑）上的笔触编辑与VR交互能力的工具。研究扩展显示空间在帮助用户识别目标幻灯片、支持幻灯片上对象的空间操作、创建动画以及促进多个（可能被遮挡的）形状的排列方面的效用。

Result: 1) 相比仅使用平板电脑的界面，VR的广阔视野显著加快了视觉突出目标幻灯片的识别时间。2) 相比两个基线界面，VR中的三维视图在存在遮挡的情况下，显著加快了对象重排的速度。用户研究进一步证实了交互技术可用且令人愉悦。

Conclusion: VR，特别是结合PoVRPoint，可以显著增强移动设备上的演示文稿制作，在幻灯片识别和对象操作等任务中带来益处，并受到用户好评。

Abstract: Virtual Reality (VR) has the potential to support mobile knowledge workers by
complementing traditional input devices with a large three-dimensional output
space and spatial input. Previous research on supporting VR knowledge work
explored domains such as text entry using physical keyboards and spreadsheet
interaction using combined pen and touch input. Inspired by such work, this
paper probes the VR design space for authoring presentations in mobile
settings. We propose PoVRPoint -- a set of tools coupling pen- and touch-based
editing of presentations on mobile devices, such as tablets, with the
interaction capabilities afforded by VR. We study the utility of extended
display space to, for example, assist users in identifying target slides,
supporting spatial manipulation of objects on a slide, creating animations, and
facilitating arrangements of multiple, possibly occluded, shapes. Among other
things, our results indicate that 1) the wide field of view afforded by VR
results in significantly faster target slide identification times compared to a
tablet-only interface for visually salient targets; and 2) the
three-dimensional view in VR enables significantly faster object reordering in
the presence of occlusion compared to two baseline interfaces. A user study
further confirmed that the interaction techniques were found to be usable and
enjoyable.

</details>


### [180] [Untidy Data: The Unreasonable Effectiveness of Tables](https://arxiv.org/abs/2106.15005)
*Lyn Bartram,Michael Correll,Melanie Tory*

Main category: cs.HC

TL;DR: 本研究探讨了数据工作者（非专业分析师）如何使用表格数据进行工作。研究发现，他们使用表格进行数据清理之外的更广泛目的，如在整个分析过程中与数据进行直接互动、重塑和扩充，以支持意义建构。论文认为交互式表格本身就是一种重要的可视化形式，其直接的数据交互为视觉分析提供了丰富的设计空间。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在理解数据工作者（非专业分析师或数据科学家）如何使用表格数据与数据互动并进行推理，并探讨电子表格在其信息生态系统中的关键作用，以及它们如何超越传统的数据准备阶段。

Method: 对数据工作者如何与数据互动及推理进行了定性研究。

Result: 研究发现，数据表不仅用于初始数据清理，用户在整个分析过程中都希望查看并“亲手操作”底层数据，通过重塑和扩充来支持意义建构。这种直接互动和人类可读的表格表示对于理解数据及其用途至关重要。

Conclusion: 交互式表格本身就是一种重要的可视化形式；它们提供的直接数据交互为视觉分析提供了丰富的设计空间；并且，通过比当前视觉分析工具更灵活的人机数据交互，可以丰富意义建构。

Abstract: Working with data in table form is usually considered a preparatory and
tedious step in the sensemaking pipeline; a way of getting the data ready for
more sophisticated visualization and analytical tools. But for many people,
spreadsheets -- the quintessential table tool -- remain a critical part of
their information ecosystem, allowing them to interact with their data in ways
that are hidden or abstracted in more complex tools. This is particularly true
for data workers: people who work with data as part of their job but do not
identify as professional analysts or data scientists. We report on a
qualitative study of how these workers interact with and reason about their
data. Our findings show that data tables serve a broader purpose beyond data
cleanup at the initial stage of a linear analytic flow: users want to see and
"get their hands on" the underlying data throughout the analytics process,
reshaping and augmenting it to support sensemaking. They reorganize, mark up,
layer on levels of detail, and spawn alternatives within the context of the
base data. These direct interactions and human-readable table representations
form a rich and cognitively important part of building understanding of what
the data mean and what they can do with it. We argue that interactive tables
are an important visualization idiom in their own right; that the direct data
interaction they afford offers a fertile design space for visual analytics; and
that sense making can be enriched by more flexible human-data interaction than
is currently supported in visual analytics tools.

</details>


### [181] [Defining and Adopting an End User Computing Policy: A Case Study](https://arxiv.org/abs/1909.00855)
*Roger Turner*

Main category: cs.HC

TL;DR: 本文是一项案例研究，探讨了卫斯理保险协会引入更新的终端用户计算（EUC）政策的过程，包括计划、遇到的挑战以及如何克服这些挑战，并介绍了一个风险评估应用。


<details>
  <summary>Details</summary>
Motivation: 终端用户计算（EUC）如果控制不当，会带来重大风险，因此需要更新政策并采取基于风险的方法。

Method: 本文采用案例研究的方法，描述了引入更新的终端用户计算政策的过程，包括计划、识别和克服各种挑战。此外，还开发了一个终端用户计算风险评估应用程序，该程序根据复杂性、重要性和控制（或缺乏控制）来计算风险等级。

Result: 成功引入了更新的终端用户计算政策，克服了实施中的挑战，并开发了一个基于风险方法的终端用户计算风险评估应用程序，该应用程序能根据复杂性、重要性和控制来评估风险。

Conclusion: 通过引入基于风险的终端用户计算政策并辅以风险评估工具，可以有效地评估和缓解最高风险，从而快速获得效益。

Abstract: End User Computing carries significant risks if not well controlled. This
paper is a case study of the introduction of an updated End User Computing
policy at the Wesleyan Assurance Society. The paper outlines the plan and
identifies various challenges. The paper explains how these challenges were
overcome. We wrote an End User Computing Risk Assessment Application which
calculates a risk rating band based on the Complexity, Materiality and Control
(or lack of it) pertaining to any given application and the basis of assessment
is given in this paper. The policy uses a risk based approach for assessing and
mitigating against the highest risks first and obtaining the quickest benefit.

</details>


### [182] [Calliope: Automatic Visual Data Story Generation from a Spreadsheet](https://arxiv.org/abs/2010.09975)
*Danqing Shi,Xinyue Xu,Fuling Sun,Yang Shi,Nan Cao*

Main category: cs.HC

TL;DR: Calliope是一个新颖的系统，它能通过自动过程从电子表格生成视觉数据故事，并提供在线编辑器进行修订，从而简化视觉数据故事的创建，提高效率。


<details>
  <summary>Details</summary>
Motivation: 视觉数据故事的生成面临技术障碍（如数据分析、可视化和脚本编写），现有工具效率低下且依赖用户技能和经验。

Method: Calliope系统采用逻辑导向的蒙特卡洛树搜索算法，探索数据空间以逐步生成数据事实并按逻辑顺序组织。数据事实的重要性通过信息论衡量，并自动可视化和添加描述。系统还包含一个在线故事编辑器。

Result: 通过三个示例故事、两次受控实验以及对10位领域专家的访谈评估，结果表明Calliope有助于高效生成视觉数据故事。

Conclusion: Calliope系统通过自动化和高效的方法，有效解决了视觉数据故事生成中的挑战，并提供了便捷的修订功能。

Abstract: Visual data stories shown in the form of narrative visualizations such as a
poster or a data video, are frequently used in data-oriented storytelling to
facilitate the understanding and memorization of the story content. Although
useful, technique barriers, such as data analysis, visualization, and
scripting, make the generation of a visual data story difficult. Existing
authoring tools rely on users' skills and experiences, which are usually
inefficient and still difficult. In this paper, we introduce a novel visual
data story generating system, Calliope, which creates visual data stories from
an input spreadsheet through an automatic process and facilities the easy
revision of the generated story based on an online story editor. Particularly,
Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm
that explores the data space given by the input spreadsheet to progressively
generate story pieces (i.e., data facts) and organize them in a logical order.
The importance of data facts is measured based on information theory, and each
data fact is visualized in a chart and captioned by an automatically generated
description. We evaluate the proposed technique through three example stories,
two controlled experiments, and a series of interviews with 10 domain experts.
Our evaluation shows that Calliope is beneficial to efficient visual data story
generation.

</details>


### [183] [Pen-based Interaction with Spreadsheets in Mobile Virtual Reality](https://arxiv.org/abs/2008.04543)
*Travis Gesslein,Verena Biener,Philipp Gagel,Daniel Schneider,Per Ola Kristensson,Eyal Ofek,Michel Pahud,Jens Grubert*

Main category: cs.HC

TL;DR: 虚拟现实（VR）可以增强移动设备上的电子表格交互，特别是针对移动知识工作者。本文介绍了一种工具集，结合VR头显和笔式输入与平板电脑，利用VR的巨大显示空间克服有限的物理交互空间，通过增强可视化和高效编辑来提高生产力。


<details>
  <summary>Details</summary>
Motivation: 电子表格广泛使用但在移动设备上交互困难。VR提供了沉浸式显示，但移动工作者交互空间有限。本文旨在弥合VR大显示与移动有限交互空间在电子表格使用上的鸿沟。

Method: 作者提出了一种工具集，用于在平板电脑上使用沉浸式VR头显和笔式输入来增强电子表格交互。这包括利用平板电脑周围和前方的空间进行增强可视化（例如，扩展表格显示，揭示隐藏依赖关系）和高效创建/编辑（例如，屏幕外分层菜单、表格依赖关系可视化、基于凝视和触摸的表格切换）。

Result: 该工具集的可行性通过视频在线调查和专家对指示性人类表现潜力的评估进行了研究。

Conclusion: 结合精确的屏幕笔输入和平板电脑周围的空间感应，为VR中增强电子表格生产力开辟了许多可能性，特别是对于移动知识工作，通过解决有限交互空间的挑战。

Abstract: Virtual Reality (VR) can enhance the display and interaction of mobile
knowledge work and in particular, spreadsheet applications. While spreadsheets
are widely used yet are challenging to interact with, especially on mobile
devices, using them in VR has not been explored in depth. A special uniqueness
of the domain is the contrast between the immersive and large display space
afforded by VR, contrasted by the very limited interaction space that may be
afforded for the information worker on the go, such as an airplane seat or a
small work-space. To close this gap, we present a tool-set for enhancing
spreadsheet interaction on tablets using immersive VR headsets and pen-based
input. This combination opens up many possibilities for enhancing the
productivity for spreadsheet interaction. We propose to use the space around
and in front of the tablet for enhanced visualization of spreadsheet data and
meta-data. For example, extending sheet display beyond the bounds of the
physical screen, or easier debugging by uncovering hidden dependencies between
sheet's cells. Combining the precise on-screen input of a pen with spatial
sensing around the tablet, we propose tools for the efficient creation and
editing of spreadsheets functions such as off-the-screen layered menus,
visualization of sheets dependencies, and gaze-and-touch-based switching
between spreadsheet tabs. We study the feasibility of the proposed tool-set
using a video-based online survey and an expert-based assessment of indicative
human performance potential.

</details>


### [184] [EQUS -- helping to see formulae](https://arxiv.org/abs/2007.00003)
*Chris Roast*

Main category: cs.HC

TL;DR: 本文介绍了一个名为 EQUS 的交互式电子表格公式可视化工具的设计、开发和评估。EQUS 通过迭代完善与青少年学习者合作开发，旨在帮助人们理解复杂的电子表格数据，现已成为 MS Excel 的完整插件。


<details>
  <summary>Details</summary>
Motivation: 可视化常被用来简化信息和帮助理解复杂数据。电子表格公式被广泛用于重要的数值处理和建模，但其公式很容易被误解。

Method: 本文描述了交互式电子表格公式可视化工具（EQUS）的设计、开发和评估。开发过程涉及迭代改进，初期目标受众为青少年学习者，并进行了重新设计和形成性评估。

Result: 所产生的可视化技术（EQUS）已被证明对初始目标受众之外的电子表格用户也具有广泛的适用性。EQUS 现已开发成为 MS Excel 的完整集成插件。

Conclusion: 本文成功设计并开发了 EQUS，一个帮助用户理解复杂电子表格公式的交互式可视化工具，并已集成到 MS Excel 中，证明了其广泛的适用性。

Abstract: Visualisation is often presented as a means of simplifying information and
helping people understand complex data. In this paper we describe the design,
development and evaluation of an interactive visualisation for spreadsheet
formulae (EQUS). The work is justified on the grounds that these are widely
used tools for significant numerical processing and modeling, yet the formula
developed can be easily misunderstood. The development process was one of
iterative refinement engaging an initial target audience of mid-teen learners,
involving re-design and formative evaluation. The resulting visualisation
techniques have been found to be broadly relevant to spreadsheet users beyond
the initial target audience. EQUS has since been developed as fully integrated
plug-in for MS Excel.

</details>


### [185] [Taggle: Combining Overview and Details in Tabular Data Visualizations](https://arxiv.org/abs/1712.05944)
*Katarina Furmanova,Samuel Gratzl,Holger Stitz,Thomas Zichner,Miroslava Jaresova,Alexander Lex,Marc Streit*

Main category: cs.HC

TL;DR: Taggle是一种表格可视化技术，用于探索和展示大型复杂表格，它以项目为中心，对数据子集进行数据驱动的聚合，并提供交互方法来回答特定的分析问题。


<details>
  <summary>Details</summary>
Motivation: 现有的表格数据可视化技术主要关注概览，但实际分析任务需要调查单个感兴趣的项目，同时需要将项目与大型表格的其余部分联系起来。

Method: Taggle采用以项目为中心的类似电子表格的方法，通过单元格的视觉编码单独可视化源数据中的每一行。同时，Taggle引入了数据子集的数据驱动聚合，并辅以量身定制的交互方法，例如基于多列排序以及丰富的数据选择和过滤功能。

Result: 通过一个案例研究展示了Taggle，该案例由领域专家对复杂的基因组数据分析进行，用于药物发现。

Conclusion: Taggle提供了一种有效的方式来探索和展示大型复杂表格，特别是在需要关注单个项目并将其与整体关联的分析任务中，如药物发现中的基因组数据分析。

Abstract: Most tabular data visualization techniques focus on overviews, yet many
practical analysis tasks are concerned with investigating individual items of
interest. At the same time, relating an item to the rest of a potentially large
table is important. In this work we present Taggle, a tabular visualization
technique for exploring and presenting large and complex tables. Taggle takes
an item-centric, spreadsheet-like approach, visualizing each row in the source
data individually using visual encodings for the cells. At the same time,
Taggle introduces data-driven aggregation of data subsets. The aggregation
strategy is complemented by interaction methods tailored to answer specific
analysis questions, such as sorting based on multiple columns and rich data
selection and filtering capabilities. We demonstrate Taggle using a case study
conducted by a domain expert on complex genomics data analysis for the purpose
of drug discovery.

</details>


### [186] [Are digital natives spreadsheet natives?](https://arxiv.org/abs/1909.00865)
*Maria Csernoch,Piroska Biró*

Main category: cs.HC

TL;DR: 本研究测试了信息学一年级学生在电子表格环境中的算法技能和知识迁移能力，发现尽管他们被认为是“数字原住民”并完成了相关培训，但他们在解决电子表格问题时仍存在严重困难。使用基于算法的多级数组公式的学生表现优于使用特定内置函数的学生。研究强调，学生需要正式的、高数学能力的、基于算法的培训。


<details>
  <summary>Details</summary>
Motivation: 评估信息学一年级学生在电子表格环境中的算法技能和知识迁移能力，并挑战“数字原住民”无需培训的假设，因为学生已完成电子表格培训且被认为是数字原住民。

Method: 测试信息学一年级学生在电子表格环境中解决问题的能力，以评估他们的算法技能和知识迁移能力。

Result: 学生在解决电子表格问题时遇到严重困难。使用基于算法、多级数组公式的学生表现优于使用特定问题、不相关内置函数的学生。

Conclusion: 无论学生的出生日期和所属数字世代如何，他们都非常需要由专家教师提供的正式的、高数学能力的、基于算法的培训。

Abstract: The present paper reports the results of testing first year students of
Informatics on their algorithmic skills and knowledge transfer abilities in
spreadsheet environments. The selection of students plays a crucial role in the
project. On the one hand, they have officially finished their spreadsheet
training - they know everything - while on the other hand, they do not need any
training, since they are digital natives, to whom digital skills are assigned
by birth. However, we found that the students had serious difficulties in
solving the spreadsheet problems presented: so low were their results that it
allowed us to form broad tendencies. Considering computational thinking,
algorithmic skills, and knowledge transfer abilities, it is clear that those
students performed better who used algorithm-based, multilevel array formulas
instead of problem specific, unconnected built-in functions. Furthermore, we
can conclude that students, regardless of their birth date and digital
generation assigned to them, are in great need of official, high-mathability,
algorithm-based training with expert teachers.

</details>


### [187] [Somewhere Around That Number: An Interview Study of How Spreadsheet Users Manage Uncertainty](https://arxiv.org/abs/1905.13072)
*Judith Borghouts,Andrew D. Gordon,Advait Sarkar,Kenton P. O'Hara,Neil Toronto*

Main category: cs.HC

TL;DR: 电子表格用户在处理数据不确定性时常遇到挑战，且现有工具支持有限。一项访谈研究发现，用户处理不确定性的方式受电子表格在工作中的角色和用户目标的影响，他们常需变通方法。


<details>
  <summary>Details</summary>
Motivation: 电子表格用户经常面临数据不确定性（如误差和估计），但他们常使用非正式启发式方法推理概率，导致错误结论或忽略不确定性。因此，需要理解用户当前如何遇到和处理电子表格中的不确定性。

Method: 我们对来自不同领域的11位电子表格用户进行了访谈研究，以了解他们目前如何遇到和处理不确定性。

Result: 用户处理不确定性的方式受电子表格在他们工作中的角色（如数据库、模板、计算工具、记事本和探索工具）以及用户目标（如计算和比较不同情景、理解不确定性本质、向他人简化复杂性）的影响。电子表格目前提供的工具对这些目标支持有限，用户因此采用了各种变通方法。

Conclusion: （隐性）电子表格用户在处理不确定性时面临挑战，现有工具支持不足，需依赖变通方法。了解他们的目标和电子表格的角色对于开发更好的工具至关重要。

Abstract: Spreadsheet users regularly deal with uncertainty in their data, for example
due to errors and estimates. While an insight into data uncertainty can help in
making better informed decisions, prior research suggests that people often use
informal heuristics to reason with probabilities, which leads to incorrect
conclusions. Moreover, people often ignore or simplify uncertainty. To
understand how people currently encounter and deal with uncertainty in
spreadsheets, we conducted an interview study with 11 spreadsheet users from a
range of domains. We found that how people deal with uncertainty is influenced
by the role the spreadsheet plays in people's work and the user's aims.
Spreadsheets are used as a database, template, calculation tool, notepad and
exploration tool. In doing so, participants' aims were to compute and compare
different scenarios, understand something about the nature of the uncertainty
in their situation, and translate the complexity of data uncertainty into
simplified presentations to other people, usually decision-makers. Spreadsheets
currently provide limited tools to support these aims, and participants had
various workarounds.

</details>


### [188] [Characterizing Scalability Issues in Spreadsheet Software using Online Forums](https://arxiv.org/abs/1801.03829)
*Kelly Mack,John Lee,Kevin Chang,Karrie Karahalios,Aditya Parameswaran*

Main category: cs.HC

TL;DR: 该论文通过抓取Reddit论坛数据，分析了用户在使用Excel电子表格（尤其是在处理大数据量时）遇到的挑战，以期为下一代电子表格软件的设计提供参考。


<details>
  <summary>Details</summary>
Motivation: 传统可用性研究成本高昂且耗时。本文旨在通过利用在线论坛的现有数据，以一种可扩展、低成本且广泛的方式来理解用户需求，特别是针对Excel电子表格用户遇到的挑战。

Method: 作者采用了利用在线论坛数据的方法，具体来说，是从Reddit网站抓取关于Excel的问题和抱怨帖子，收集了相关数据集。然后，他们探索并描述了用户在使用电子表格软件时面临的问题，特别是由于电子表格中数据量大而引起的问题。

Result: 收集了关于Excel的问题和抱怨数据集，并对用户在使用电子表格软件时遇到的问题进行了探索和描述，特别是处理大量数据时出现的问题。

Conclusion: 研究结果对下一代电子表格软件的设计具有指导意义。

Abstract: In traditional usability studies, researchers talk to users of tools to
understand their needs and challenges. Insights gained via such interviews
offer context, detail, and background. Due to costs in time and money, we are
beginning to see a new form of tool interrogation that prioritizes scale, cost,
and breadth by utilizing existing data from online forums. In this case study,
we set out to apply this method of using online forum data to a specific
issue---challenges that users face with Excel spreadsheets. Spreadsheets are a
versatile and powerful processing tool if used properly. However, with
versatility and power come errors, from both users and the software, which make
using spreadsheets less effective. By scraping posts from the website Reddit,
we collected a dataset of questions and complaints about Excel. Specifically,
we explored and characterized the issues users were facing with spreadsheet
software in general, and in particular, as resulting from a large amount of
data in their spreadsheets. We discuss the implications of our findings on the
design of next-generation spreadsheet software.

</details>


### [189] [The Reification of an Incorrect and Inappropriate Spreadsheet Model](https://arxiv.org/abs/1801.10249)
*Grenville J. Croll*

Main category: cs.HC

TL;DR: 电子表格中的信息即使不正确，也可能获得不应有的可信度。一项案例研究观察到，一个明显错误的电子表格模型在一家小型非营利组织中被具象化。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨电子表格中的信息如何获得不应有的属性，例如可信度、正确性，并导致信息被具象化。

Method: 本文采用案例研究的方法。

Result: 在一个小型非营利组织中，作者观察到一个明显不正确且不恰当的电子表格模型被具象化的过程。

Conclusion: 信息一旦加载到电子表格中，即使是错误的，也会获得不应有的属性，如可信度和权威性，从而被具象化。

Abstract: Once information is loaded into a spreadsheet, it acquires properties that it
may not deserve. These properties include believability, correctness,
appropriateness, concreteness, integrity, tangibility, objectivity and
authority. The information becomes reified. We describe a case study through
which we were able to observe at close hand the reification of a demonstrably
incorrect and inappropriate spreadsheet model within a small non profit
organisation.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [190] [Dynamic Kernel Matching for Non-conforming Data: A Case Study of T-cell Receptor Datasets](https://arxiv.org/abs/2103.10472)
*Jared Ostmeyer,Scott Christley,Lindsay Cowell*

Main category: q-bio.QM

TL;DR: 本文介绍了一种名为动态核匹配（DKM）的方法，用于修改现有统计分类器，使其能够处理非结构化数据，并在TCR序列数据集上成功应用，识别出与实验观察一致的模式。


<details>
  <summary>Details</summary>
Motivation: 大多数统计分类器难以处理非结构化数据，但许多类型的数据并不符合传统的行列表格结构。因此，需要一种方法来从这类非结构化数据中发现模式。

Method: 提出动态核匹配（DKM）方法，通过修改已有的统计分类器来处理非结构化数据。以T细胞受体（TCR）序列数据为例进行应用。

Result: 成功将增强了DKM的统计分类器应用于两种TCR序列数据集，并在保留数据上报告了其性能，包括使用标准指标和允许不确定诊断的指标。

Conclusion: 识别出分类器用于生成预测的模式，并证明这些模式与实验研究的观察结果一致。这表明DKM能够从非结构化数据中提取有意义的生物学模式。

Abstract: Most statistical classifiers are designed to find patterns in data where
numbers fit into rows and columns, like in a spreadsheet, but many kinds of
data do not conform to this structure. To uncover patterns in non-conforming
data, we describe an approach for modifying established statistical classifiers
to handle non-conforming data, which we call dynamic kernel matching (DKM). As
examples of non-conforming data, we consider (i) a dataset of T-cell receptor
(TCR) sequences labelled by disease antigen and (ii) a dataset of sequenced TCR
repertoires labelled by patient cytomegalovirus (CMV) serostatus, anticipating
that both datasets contain signatures for diagnosing disease. We successfully
fit statistical classifiers augmented with DKM to both datasets and report the
performance on holdout data using standard metrics and metrics allowing for
indeterminant diagnoses. Finally, we identify the patterns used by our
statistical classifiers to generate predictions and show that these patterns
agree with observations from experimental studies.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [191] [Radif Corpus: A Symbolic Dataset for Non-Metric Iranian Classical Music](https://arxiv.org/abs/2507.10456)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.SD

TL;DR: 该研究提出了一个伊朗古典音乐非度量Radif曲目的数字语料库，包含MIDI文件和详细数据，旨在促进计算研究。


<details>
  <summary>Details</summary>
Motivation: 伊朗古典音乐的核心是非度量音乐，而Radif是其基础。目前缺乏用于计算研究该音乐的数字语料库。

Method: 本研究构建了一个包含非度量Radif完整曲目（13个组成部分，228首作品，约281分钟MIDI）的数字语料库。该语料库提供了音符、音长、音程和分层结构的详细数据，忠实地再现了音调（包括四分音）和非度量特征。此外，还提供了基本的统计数据以及语料库的复杂性和相似性度量。

Result: 创建了一个全面的伊朗古典音乐（Radif）数字语料库，包含详细数据，准确地再现了音调和非度量特征。该语料库为计算研究提供了一个平台。

Conclusion: 该语料库将促进伊朗古典音乐的计算研究，例如旋律模式分析、即兴风格研究以及音乐信息检索、音乐理论和计算（民族）音乐学中的其他任务。

Abstract: Non-metric music forms the core of the repertoire in Iranian classical music.
Dastgahi music serves as the underlying theoretical system for both Iranian art
music and certain folk traditions. At the heart of Iranian classical music lies
the radif, a foundational repertoire that organizes melodic material central to
performance and pedagogy.
  In this study, we introduce a digital corpus representing the complete
non-metrical radif repertoire, covering all 13 existing components of this
repertoire. We provide MIDI files (about 281 minutes in total) and data
spreadsheets describing notes, note durations, intervals, and hierarchical
structures for 228 pieces of music. We faithfully represent the tonality
including quarter-tones, and the non-metric aspect. Furthermore, we provide
supporting basic statistics, and measures of complexity and similarity over the
corpus.
  Our corpus provides a platform for computational studies of Iranian classical
music. Researchers might employ it in studying melodic patterns, investigating
improvisational styles, or for other tasks in music information retrieval,
music theory, and computational (ethno)musicology.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [192] [Pivoting the paradigm: the role of spreadsheets in K-12 data science](https://arxiv.org/abs/2506.03232)
*Oren Tirschwell,Nicholas Jon Horton*

Main category: stat.OT

TL;DR: 该论文探讨了电子表格在K-12教育中培养数据和计算技能的价值。它回顾了现有框架，提出了学习成果，并讨论了电子表格如何发展数据敏锐度和计算流畅性，同时提供了活动、解决了挑战并提出了教学方法。


<details>
  <summary>Details</summary>
Motivation: 电子表格在K-12学生和教师中广泛使用，对于数据收集、组织和可视化至关重要。本文旨在探讨电子表格如何有效融入K-12教育，以培养学生的数据和计算技能。

Method: 1) 回顾了K-12数据工具的现有框架；2) 提出了通过将电子表格纳入课程可以实现的数据驱动学习成果；3) 讨论了电子表格如何帮助培养数据敏锐度和计算流畅性。此外，还提供了课堂活动示例，指出了采用的挑战和障碍，提出了教学方法，并讨论了专业发展的必要性。

Result: 该论文为K-12教育中电子表格的使用提供了全面的分析，提出了可实现的学习成果、实用的课堂活动以及克服实施障碍的策略，强调了电子表格在发展数据和计算技能方面的作用。

Conclusion: 电子表格是K-12数据教育的强大工具。通过适当的教学法和专业发展，有效整合电子表格可以显著提高学生的数据敏锐度和计算流畅性，为数据科学和STEM学科做好准备。

Abstract: Spreadsheet tools are widely accessible to and commonly used by K-12 students
and teachers. They have an important role in data collection and organization.
Beyond data organization, spreadsheets also make data visible and easy to
interact with, facilitating student engagement in data exploration and
analysis. Though not suitable for all circumstances, spreadsheets can and do
help foster data and computing skills for K-12 students. This paper 1) reviews
prior frameworks on K-12 data tools; 2) proposes data-driven learning outcomes
that can be accomplished by incorporating spreadsheets into the curriculum; and
3) discusses how spreadsheets can help develop data acumen and computational
fluency. We provide example class activities, identify challenges and barriers
to adoption, suggest pedagogical approaches to ease the learning curve for
instructors and students, and discuss the need for professional development to
facilitate deeper use of spreadsheets for data science and STEM disciplines.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [193] [Data Encryption Battlefield: A Deep Dive into the Dynamic Confrontations in Ransomware Attacks](https://arxiv.org/abs/2504.20681)
*Arash Mahboubi,Hamed Aboutorab,Seyit Camtepe,Hang Thanh Bui,Khanh Luong,Keyvan Ansari,Shenlu Wang,Bazara Barry*

Main category: cs.CR

TL;DR: 该研究探讨了勒索软件攻击者使用复杂加密方法（如Base64编码降低熵、部分或间歇性加密）来逃避传统检测的挑战。研究利用在线增量机器学习算法，通过大型数据集预测文件加密活动。结果表明，Hoeffding Tree算法在检测降低熵的加密方法方面表现出色，而Random Forest分类器在识别间歇性加密方面效果显著，强调了定制化机器学习解决方案的重要性。


<details>
  <summary>Details</summary>
Motivation: 勒索软件攻击者不断采用复杂的加密策略（例如通过Base64编码降低熵，以及部分或间歇性加密），以规避传统的检测方法，对网络安全构成重大挑战。因此，需要开发先进的对策来保护易受攻击的数据。

Method: 本研究调查了在线增量机器学习算法在预测文件加密活动方面的应用，旨在应对不断演变的混淆技术。分析使用了32.6 GB的广泛数据集，包含11,928个文件（包括多种格式，如Word、PowerPoint、Excel、图片、PDF、音频和视频文件），这些文件由75个不同的勒索软件家族加密。

Result: Hoeffding Tree算法在增量学习能力方面表现出卓越的性能，特别有效地检测了传统的和AES-Base64加密方法（用于降低熵）。相反，具有“暖启动”功能的Random Forest分类器在识别间歇性加密方法方面表现出色。

Conclusion: 研究结果表明，为了有效对抗复杂的勒索软件策略，需要采用定制化的机器学习解决方案，因为不同的算法在检测不同的加密策略时表现出不同的优势。

Abstract: In the rapidly evolving landscape of cybersecurity threats, ransomware
represents a significant challenge. Attackers increasingly employ sophisticated
encryption methods, such as entropy reduction through Base64 encoding, and
partial or intermittent encryption to evade traditional detection methods. This
study explores the dynamic battle between adversaries who continuously refine
encryption strategies and defenders developing advanced countermeasures to
protect vulnerable data. We investigate the application of online incremental
machine learning algorithms designed to predict file encryption activities
despite adversaries evolving obfuscation techniques. Our analysis utilizes an
extensive dataset of 32.6 GB, comprising 11,928 files across multiple formats,
including Microsoft Word documents (doc), PowerPoint presentations (ppt), Excel
spreadsheets (xlsx), image formats (jpg, jpeg, png, tif, gif), PDFs (pdf),
audio (mp3), and video (mp4) files. These files were encrypted by 75 distinct
ransomware families, facilitating a robust empirical evaluation of machine
learning classifiers effectiveness against diverse encryption tactics. Results
highlight the Hoeffding Tree algorithms superior incremental learning
capability, particularly effective in detecting traditional and AES-Base64
encryption methods employed to lower entropy. Conversely, the Random Forest
classifier with warm-start functionality excels at identifying intermittent
encryption methods, demonstrating the necessity of tailored machine learning
solutions to counter sophisticated ransomware strategies.

</details>


### [194] [JUBILEE: Secure Debt Relief and Forgiveness](https://arxiv.org/abs/2109.07267)
*David Cerezo Sánchez*

Main category: cs.CR

TL;DR: JUBILEE是一种安全的债务减免和豁免机制，无需第三方干预，通过激励双方真实披露信息，实现更和谐的债务和解。


<details>
  <summary>Details</summary>
Motivation: 旨在改善现有债务和解方法，通过激励各方真实披露私人信息，实现更和谐的债务和解，并解决私人信息下的债务减免和豁免机制的挑战。

Method: 引入了安全计算技术应用于债务减免，并通过“安全电子表格”和基于Raziel智能合约（使用Pravuil共识）的区块链实现了具体应用。

Result: 首次实现了“债务人的祝福”，即与不使用安全计算相比，债务和解具有更高的预期利润和更高的成功概率，并能达到个体理性、激励兼容、诚实/策略证明、事后高效且最优的机制。

Conclusion: JUBILEE通过引入安全计算，提供了一种创新且优越的债务减免机制，显著提高了债务和解的效率和成功率，为债务人带来了更高的预期利润和成功概率。

Abstract: JUBILEE is a securely computed mechanism for debt relief and forgiveness in a
frictionless manner without involving trusted third parties, leading to more
harmonious debt settlements by incentivising the parties to truthfully reveal
their private information. JUBILEE improves over all previous methods:
  - individually rational, incentive-compatible, truthful/strategy-proof,
ex-post efficient, optimal mechanism for debt relief and forgiveness with
private information
  - by the novel introduction of secure computation techniques to debt relief,
the "blessing of the debtor" is hereby granted for the first time: debt
settlements with higher expected profits and a higher probability of success
than without using secure computation
  A simple and practical implementation is included for "The Secure
Spreadsheet". Another implementation is realised using Raziel smart contracts
on a blockchain with Pravuil consensus.

</details>
